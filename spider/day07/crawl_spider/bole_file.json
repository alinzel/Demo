{"art_img": ["http://jbcdn2.b0.upaiyun.com/2017/11/3be8f2c50649ea9862264793b74bdc66.png"], "art_title": ["曲线点抽稀算法- Python 实现"], "art_create_time": ["2017/11/19"], "art_content": ["原文出处：j_hao104   何为抽稀在处理矢量化数据时，记录中往往会有很多重复数据，对进一步数据处理带来诸多不便。多余的数据一方面浪费了较多的存储空间，另一方面造成所要表达的图形不光滑或不符合标准。因此要通过某种规则，在保证矢量曲线形状不变的情况下，最大限度地减少数据点个数，这个过程称为抽稀。通俗的讲就是对曲线进行采样简化，即在曲线上取有限个点，将其变为折线，并且能够在一定程度保持原有形状。比较常用的两种抽稀算法是：道格拉斯-普克(Douglas-Peuker)算法和垂距限值法。道格拉斯-普克(Douglas-Peuker)算法Douglas-Peuker算法(DP算法)过程如下:1、连接曲线首尾两点A、B；2、依次计算曲线上所有点到A、B两点所在曲线的距离；3、计算最大距离D，如果D小于阈值threshold,则去掉曲线上出A、B外的所有点；如果D大于阈值threshold,则把曲线以最大距离分割成两段；4、对所有曲线分段重复1-3步骤，知道所有D均小于阈值。即完成抽稀。这种算法的抽稀精度与阈值有很大关系，阈值越大，简化程度越大，点减少的越多；反之简化程度越低，点保留的越多，形状也越趋于原曲线。下面是Python代码实现:Python#-*-coding:utf-8-*-\"\"\"-------------------------------------------------FileName：DouglasPeukerDescription:道格拉斯-普克抽稀算法Author:J_haodate：2017/8/16-------------------------------------------------ChangeActivity:2017/8/16:道格拉斯-普克抽稀算法-------------------------------------------------\"\"\"from__future__importdivisionfrommathimportsqrt,pow__author__='J_hao'THRESHOLD=0.0001#阈值defpoint2LineDistance(point_a,point_b,point_c):\"\"\"计算点a到点bc所在直线的距离:parampoint_a::parampoint_b::parampoint_c::return:\"\"\"#首先计算bc所在直线的斜率和截距ifpoint_b[0]==point_c[0]:return9999999slope=(point_b[1]-point_c[1])/(point_b[0]-point_c[0])intercept=point_b[1]-slope*point_b[0]#计算点a到bc所在直线的距离distance=abs(slope*point_a[0]-point_a[1]+intercept)/sqrt(1+pow(slope,2))returndistanceclassDouglasPeuker(object):def__init__(self):self.threshold=THRESHOLDself.qualify_list=list()self.disqualify_list=list()defdiluting(self,point_list):\"\"\"抽稀:parampoint_list:二维点列表:return:\"\"\"iflen(point_list)<3:self.qualify_list.extend(point_list[::-1])else:#找到与收尾两点连线距离最大的点max_distance_index,max_distance=0,0forindex,pointinenumerate(point_list):ifindexin[0,len(point_list)-1]:continuedistance=point2LineDistance(point,point_list[0],point_list[-1])ifdistance>max_distance:max_distance_index=indexmax_distance=distance#若最大距离小于阈值，则去掉所有中间点。反之，则将曲线按最大距离点分割ifmax_distance<self.threshold:self.qualify_list.append(point_list[-1])self.qualify_list.append(point_list[0])else:#将曲线按最大距离的点分割成两段sequence_a=point_list[:max_distance_index]sequence_b=point_list[max_distance_index:]forsequencein[sequence_a,sequence_b]:iflen(sequence)<3andsequence==sequence_b:self.qualify_list.extend(sequence[::-1])else:self.disqualify_list.append(sequence)defmain(self,point_list):self.diluting(point_list)whilelen(self.disqualify_list)>0:self.diluting(self.disqualify_list.pop())printself.qualify_listprintlen(self.qualify_list)if__name__=='__main__':d=DouglasPeuker()d.main([[104.066228,30.644527],[104.066279,30.643528],[104.066296,30.642528],[104.066314,30.641529],[104.066332,30.640529],[104.066383,30.639530],[104.066400,30.638530],[104.066451,30.637531],[104.066468,30.636532],[104.066518,30.635533],[104.066535,30.634533],[104.066586,30.633534],[104.066636,30.632536],[104.066686,30.631537],[104.066735,30.630538],[104.066785,30.629539],[104.066802,30.628539],[104.066820,30.627540],[104.066871,30.626541],[104.066888,30.625541],[104.066906,30.624541],[104.066924,30.623541],[104.066942,30.622542],[104.066960,30.621542],[104.067011,30.620543],[104.066122,30.620086],[104.065124,30.620021],[104.064124,30.620022],[104.063124,30.619990],[104.062125,30.619958],[104.061125,30.619926],[104.060126,30.619894],[104.059126,30.619895],[104.058127,30.619928],[104.057518,30.620722],[104.057625,30.621716],[104.057735,30.622710],[104.057878,30.623700],[104.057984,30.624694],[104.058094,30.625688],[104.058204,30.626682],[104.058315,30.627676],[104.058425,30.628670],[104.058502,30.629667],[104.058518,30.630667],[104.058503,30.631667],[104.058521,30.632666],[104.057664,30.633182],[104.056664,30.633174],[104.055664,30.633166],[104.054672,30.633289],[104.053758,30.633694],[104.052852,30.634118],[104.052623,30.635091],[104.053145,30.635945],[104.053675,30.636793],[104.054200,30.637643],[104.054756,30.638475],[104.055295,30.639317],[104.055843,30.640153],[104.056387,30.640993],[104.056933,30.641830],[104.057478,30.642669],[104.058023,30.643507],[104.058595,30.644327],[104.059152,30.645158],[104.059663,30.646018],[104.060171,30.646879],[104.061170,30.646855],[104.062168,30.646781],[104.063167,30.646823],[104.064167,30.646814],[104.065163,30.646725],[104.066157,30.646618],[104.066231,30.645620],[104.066247,30.644621],])123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109#-*-coding:utf-8-*-\"\"\"-------------------------------------------------  FileName：    DouglasPeuker  Description:  道格拉斯-普克抽稀算法  Author:        J_hao  date：          2017/8/16-------------------------------------------------  ChangeActivity:                  2017/8/16:道格拉斯-普克抽稀算法-------------------------------------------------\"\"\"from__future__importdivision frommathimportsqrt,pow __author__='J_hao' THRESHOLD=0.0001  #阈值  defpoint2LineDistance(point_a,point_b,point_c):    \"\"\"    计算点a到点bc所在直线的距离    :parampoint_a:    :parampoint_b:    :parampoint_c:    :return:    \"\"\"    #首先计算bc所在直线的斜率和截距    ifpoint_b[0]==point_c[0]:        return9999999    slope=(point_b[1]-point_c[1])/(point_b[0]-point_c[0])    intercept=point_b[1]-slope*point_b[0]     #计算点a到bc所在直线的距离    distance=abs(slope*point_a[0]-point_a[1]+intercept)/sqrt(1+pow(slope,2))    returndistance  classDouglasPeuker(object):    def__init__(self):        self.threshold=THRESHOLD        self.qualify_list=list()        self.disqualify_list=list()     defdiluting(self,point_list):        \"\"\"        抽稀        :parampoint_list:二维点列表        :return:        \"\"\"        iflen(point_list)<3:            self.qualify_list.extend(point_list[::-1])        else:            #找到与收尾两点连线距离最大的点            max_distance_index,max_distance=0,0            forindex,pointinenumerate(point_list):                ifindexin[0,len(point_list)-1]:                    continue                distance=point2LineDistance(point,point_list[0],point_list[-1])                ifdistance>max_distance:                    max_distance_index=index                    max_distance=distance             #若最大距离小于阈值，则去掉所有中间点。反之，则将曲线按最大距离点分割            ifmax_distance<self.threshold:                self.qualify_list.append(point_list[-1])                self.qualify_list.append(point_list[0])            else:                #将曲线按最大距离的点分割成两段                sequence_a=point_list[:max_distance_index]                sequence_b=point_list[max_distance_index:]                 forsequencein[sequence_a,sequence_b]:                    iflen(sequence)<3andsequence==sequence_b:                        self.qualify_list.extend(sequence[::-1])                    else:                        self.disqualify_list.append(sequence)     defmain(self,point_list):        self.diluting(point_list)        whilelen(self.disqualify_list)>0:            self.diluting(self.disqualify_list.pop())        printself.qualify_list        printlen(self.qualify_list)  if__name__=='__main__':    d=DouglasPeuker()    d.main([[104.066228,30.644527],[104.066279,30.643528],[104.066296,30.642528],[104.066314,30.641529],            [104.066332,30.640529],[104.066383,30.639530],[104.066400,30.638530],[104.066451,30.637531],            [104.066468,30.636532],[104.066518,30.635533],[104.066535,30.634533],[104.066586,30.633534],            [104.066636,30.632536],[104.066686,30.631537],[104.066735,30.630538],[104.066785,30.629539],            [104.066802,30.628539],[104.066820,30.627540],[104.066871,30.626541],[104.066888,30.625541],            [104.066906,30.624541],[104.066924,30.623541],[104.066942,30.622542],[104.066960,30.621542],            [104.067011,30.620543],[104.066122,30.620086],[104.065124,30.620021],[104.064124,30.620022],            [104.063124,30.619990],[104.062125,30.619958],[104.061125,30.619926],[104.060126,30.619894],            [104.059126,30.619895],[104.058127,30.619928],[104.057518,30.620722],[104.057625,30.621716],            [104.057735,30.622710],[104.057878,30.623700],[104.057984,30.624694],[104.058094,30.625688],            [104.058204,30.626682],[104.058315,30.627676],[104.058425,30.628670],[104.058502,30.629667],            [104.058518,30.630667],[104.058503,30.631667],[104.058521,30.632666],[104.057664,30.633182],            [104.056664,30.633174],[104.055664,30.633166],[104.054672,30.633289],[104.053758,30.633694],            [104.052852,30.634118],[104.052623,30.635091],[104.053145,30.635945],[104.053675,30.636793],            [104.054200,30.637643],[104.054756,30.638475],[104.055295,30.639317],[104.055843,30.640153],            [104.056387,30.640993],[104.056933,30.641830],[104.057478,30.642669],[104.058023,30.643507],            [104.058595,30.644327],[104.059152,30.645158],[104.059663,30.646018],[104.060171,30.646879],            [104.061170,30.646855],[104.062168,30.646781],[104.063167,30.646823],[104.064167,30.646814],            [104.065163,30.646725],[104.066157,30.646618],[104.066231,30.645620],[104.066247,30.644621],])垂距限值法垂距限值法其实和DP算法原理一样，但是垂距限值不是从整体角度考虑，而是依次扫描每一个点，检查是否符合要求。算法过程如下:1、以第二个点开始，计算第二个点到前一个点和后一个点所在直线的距离d；2、如果d大于阈值，则保留第二个点，计算第三个点到第二个点和第四个点所在直线的距离d;若d小于阈值则舍弃第二个点，计算第三个点到第一个点和第四个点所在直线的距离d;3、依次类推，直线曲线上倒数第二个点。下面是Python代码实现：Python#-*-coding:utf-8-*-\"\"\"-------------------------------------------------FileName：LimitVerticalDistanceDescription:垂距限值抽稀算法Author:J_haodate：2017/8/17-------------------------------------------------ChangeActivity:2017/8/17:-------------------------------------------------\"\"\"from__future__importdivisionfrommathimportsqrt,pow__author__='J_hao'THRESHOLD=0.0001#阈值defpoint2LineDistance(point_a,point_b,point_c):\"\"\"计算点a到点bc所在直线的距离:parampoint_a::parampoint_b::parampoint_c::return:\"\"\"#首先计算bc所在直线的斜率和截距ifpoint_b[0]==point_c[0]:return9999999slope=(point_b[1]-point_c[1])/(point_b[0]-point_c[0])intercept=point_b[1]-slope*point_b[0]#计算点a到bc所在直线的距离distance=abs(slope*point_a[0]-point_a[1]+intercept)/sqrt(1+pow(slope,2))returndistanceclassLimitVerticalDistance(object):def__init__(self):self.threshold=THRESHOLDself.qualify_list=list()defdiluting(self,point_list):\"\"\"抽稀:parampoint_list:二维点列表:return:\"\"\"self.qualify_list.append(point_list[0])check_index=1whilecheck_index<len(point_list)-1:distance=point2LineDistance(point_list[check_index],self.qualify_list[-1],point_list[check_index+1])ifdistance<self.threshold:check_index+=1else:self.qualify_list.append(point_list[check_index])check_index+=1returnself.qualify_listif__name__=='__main__':l=LimitVerticalDistance()diluting=l.diluting([[104.066228,30.644527],[104.066279,30.643528],[104.066296,30.642528],[104.066314,30.641529],[104.066332,30.640529],[104.066383,30.639530],[104.066400,30.638530],[104.066451,30.637531],[104.066468,30.636532],[104.066518,30.635533],[104.066535,30.634533],[104.066586,30.633534],[104.066636,30.632536],[104.066686,30.631537],[104.066735,30.630538],[104.066785,30.629539],[104.066802,30.628539],[104.066820,30.627540],[104.066871,30.626541],[104.066888,30.625541],[104.066906,30.624541],[104.066924,30.623541],[104.066942,30.622542],[104.066960,30.621542],[104.067011,30.620543],[104.066122,30.620086],[104.065124,30.620021],[104.064124,30.620022],[104.063124,30.619990],[104.062125,30.619958],[104.061125,30.619926],[104.060126,30.619894],[104.059126,30.619895],[104.058127,30.619928],[104.057518,30.620722],[104.057625,30.621716],[104.057735,30.622710],[104.057878,30.623700],[104.057984,30.624694],[104.058094,30.625688],[104.058204,30.626682],[104.058315,30.627676],[104.058425,30.628670],[104.058502,30.629667],[104.058518,30.630667],[104.058503,30.631667],[104.058521,30.632666],[104.057664,30.633182],[104.056664,30.633174],[104.055664,30.633166],[104.054672,30.633289],[104.053758,30.633694],[104.052852,30.634118],[104.052623,30.635091],[104.053145,30.635945],[104.053675,30.636793],[104.054200,30.637643],[104.054756,30.638475],[104.055295,30.639317],[104.055843,30.640153],[104.056387,30.640993],[104.056933,30.641830],[104.057478,30.642669],[104.058023,30.643507],[104.058595,30.644327],[104.059152,30.645158],[104.059663,30.646018],[104.060171,30.646879],[104.061170,30.646855],[104.062168,30.646781],[104.063167,30.646823],[104.064167,30.646814],[104.065163,30.646725],[104.066157,30.646618],[104.066231,30.645620],[104.066247,30.644621],])printlen(diluting)print(diluting)1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889#-*-coding:utf-8-*-\"\"\"-------------------------------------------------  FileName：    LimitVerticalDistance  Description:  垂距限值抽稀算法  Author:        J_hao  date：          2017/8/17-------------------------------------------------  ChangeActivity:                  2017/8/17:-------------------------------------------------\"\"\"from__future__importdivision frommathimportsqrt,pow __author__='J_hao' THRESHOLD=0.0001  #阈值  defpoint2LineDistance(point_a,point_b,point_c):    \"\"\"    计算点a到点bc所在直线的距离    :parampoint_a:    :parampoint_b:    :parampoint_c:    :return:    \"\"\"    #首先计算bc所在直线的斜率和截距    ifpoint_b[0]==point_c[0]:        return9999999    slope=(point_b[1]-point_c[1])/(point_b[0]-point_c[0])    intercept=point_b[1]-slope*point_b[0]     #计算点a到bc所在直线的距离    distance=abs(slope*point_a[0]-point_a[1]+intercept)/sqrt(1+pow(slope,2))    returndistance  classLimitVerticalDistance(object):    def__init__(self):        self.threshold=THRESHOLD        self.qualify_list=list()     defdiluting(self,point_list):        \"\"\"        抽稀        :parampoint_list:二维点列表        :return:        \"\"\"        self.qualify_list.append(point_list[0])        check_index=1        whilecheck_index<len(point_list)-1:            distance=point2LineDistance(point_list[check_index],                                          self.qualify_list[-1],                                          point_list[check_index+1])             ifdistance<self.threshold:                check_index+=1            else:                self.qualify_list.append(point_list[check_index])                check_index+=1        returnself.qualify_list  if__name__=='__main__':    l=LimitVerticalDistance()    diluting=l.diluting([[104.066228,30.644527],[104.066279,30.643528],[104.066296,30.642528],[104.066314,30.641529],            [104.066332,30.640529],[104.066383,30.639530],[104.066400,30.638530],[104.066451,30.637531],            [104.066468,30.636532],[104.066518,30.635533],[104.066535,30.634533],[104.066586,30.633534],            [104.066636,30.632536],[104.066686,30.631537],[104.066735,30.630538],[104.066785,30.629539],            [104.066802,30.628539],[104.066820,30.627540],[104.066871,30.626541],[104.066888,30.625541],            [104.066906,30.624541],[104.066924,30.623541],[104.066942,30.622542],[104.066960,30.621542],            [104.067011,30.620543],[104.066122,30.620086],[104.065124,30.620021],[104.064124,30.620022],            [104.063124,30.619990],[104.062125,30.619958],[104.061125,30.619926],[104.060126,30.619894],            [104.059126,30.619895],[104.058127,30.619928],[104.057518,30.620722],[104.057625,30.621716],            [104.057735,30.622710],[104.057878,30.623700],[104.057984,30.624694],[104.058094,30.625688],            [104.058204,30.626682],[104.058315,30.627676],[104.058425,30.628670],[104.058502,30.629667],            [104.058518,30.630667],[104.058503,30.631667],[104.058521,30.632666],[104.057664,30.633182],            [104.056664,30.633174],[104.055664,30.633166],[104.054672,30.633289],[104.053758,30.633694],            [104.052852,30.634118],[104.052623,30.635091],[104.053145,30.635945],[104.053675,30.636793],            [104.054200,30.637643],[104.054756,30.638475],[104.055295,30.639317],[104.055843,30.640153],            [104.056387,30.640993],[104.056933,30.641830],[104.057478,30.642669],[104.058023,30.643507],            [104.058595,30.644327],[104.059152,30.645158],[104.059663,30.646018],[104.060171,30.646879],            [104.061170,30.646855],[104.062168,30.646781],[104.063167,30.646823],[104.064167,30.646814],            [104.065163,30.646725],[104.066157,30.646618],[104.066231,30.645620],[104.066247,30.644621],])    printlen(diluting)    print(diluting)最后其实DP算法和垂距限值法原理一样，DP算法是从整体上考虑一条完整的曲线，实现时较垂距限值法复杂，但垂距限值法可能会在某些情况下导致局部最优。另外在实际使用中发现采用点到另外两点所在直线距离的方法来判断偏离，在曲线弧度比较大的情况下比较准确。如果在曲线弧度比较小，弯曲程度不明显时，这种方法抽稀效果不是很理想，建议使用三点所围成的三角形面积作为判断标准。下面是抽稀效果:1赞3收藏评论"], "art_url": ["http://python.jobbole.com/88892/"]}
{"art_img": ["http://jbcdn2.b0.upaiyun.com/2015/10/bfa0d07e7eb2fac2eb80cd5df9567931.jpg"], "art_title": ["Perl 与 Python 之间的一些异同"], "art_create_time": ["2017/11/10"], "art_content": ["原文出处：张颖1赞收藏评论"], "art_url": ["http://python.jobbole.com/88845/"]}
{"art_img": ["http://jbcdn2.b0.upaiyun.com/2014/12/6da94dec8f6f96417f14c8291e6345801.png"], "art_title": ["Numpy 小结"], "art_create_time": ["2017/11/15"], "art_content": ["原文出处：弄浪的鱼   Python真火来学习一下，先来看一个库NumPy。NumPy是Python语言的一个扩充程序库。支持高级大量的维度数组与矩阵运算，此外也针对数组运算提供大量的数学函数库。1.读取文件numpy.genfromtxt()用于读取txt文件，其中传入的参数依次为：需要读取的txt文件位置，此处文件与程序位于同一目录下分割的标记转换类型，如果文件中既有文本类型也有数字类型，就先转成文本类型help(numpy.genfromtxt)用于查看帮助文档：如果不想看API可以启动一个程序用help查看指令的详细用法Pythonimportnumpyworld_alcohol=numpy.genfromtxt(\"world_alcohol.txt\",delimiter=\",\",dtype=str)print(type(world_alcohol))print(world_alcohol)print(help(numpy.genfromtxt))123456importnumpy world_alcohol=numpy.genfromtxt(\"world_alcohol.txt\",delimiter=\",\",dtype=str)print(type(world_alcohol))print(world_alcohol)print(help(numpy.genfromtxt))2.构造ndarraynumpy.array()构造ndarraynumpy.array()中传入数组参数，可以是一维的也可以是二维三维的。numpy会将其转变成ndarray的结构。Pythonvector=numpy.array([1,2,3,4])matrix=numpy.array([[1,2,3],[4,5,6]])12vector=numpy.array([1,2,3,4])matrix=numpy.array([[1,2,3],[4,5,6]])传入的参数必须是同一结构,不是同一结构将发生转换。Pythonvector=numpy.array([1,2,3,4])array([1,2,3,4])123vector=numpy.array([1,2,3,4]) array([1,2,3,4])均为int类型Pythonvector=numpy.array([1,2,3,4.0])array([1.,2.,3.,4.])123vector=numpy.array([1,2,3,4.0]) array([1.,  2.,  3.,  4.])转为浮点数类型Pythonvector=numpy.array([1,2,'3',4])array(['1','2','3','4'],dtype='<U21')123vector=numpy.array([1,2,'3',4]) array(['1','2','3','4'],dtype='<U21')转为字符类型利用.shape查看结构能够了解array的结构，debug时通过查看结构能够更好地了解程序运行的过程。Pythonprint(vector.shape)print(matrix.shape)(4,)(2,3)1234print(vector.shape)print(matrix.shape)(4,)(2,3)利用dtype查看类型Pythonvector=numpy.array([1,2,3,4])vector.dtypedtype('int64')1234vector=numpy.array([1,2,3,4])vector.dtype dtype('int64')ndim查看维度一维Pythonvector=numpy.array([1,2,3,4])vector.ndim11234vector=numpy.array([1,2,3,4])vector.ndim 1二维Pythonmatrix=numpy.array([[1,2,3],[4,5,6],[7,8,9]])matrix.ndim2123456matrix=numpy.array([[1,2,3],                      [4,5,6],                    [7,8,9]])matrix.ndim 2size查看元素数量Pythonmatrix.size912matrix.size93.获取与计算numpy能使用切片获取数据Pythonmatrix=numpy.array([[1,2,3],[4,5,6],[7,8,9]])123matrix=numpy.array([[1,2,3],                      [4,5,6],                    [7,8,9]])根据条件获取numpy能够依次比较vector和元素之间是否相同Pythonvector=numpy.array([5,10,15,20])vector==10array([False,True,False,False],dtype=bool)1234vector=numpy.array([5,10,15,20])vector==10 array([False,  True,False,False],dtype=bool)根据返回值获取元素Pythonvector=numpy.array([5,10,15,20])equal_to_ten=(vector==10)print(equal_to_ten)print(vector[equal_to_ten])[FalseTrueFalseFalse][10]1234567vector=numpy.array([5,10,15,20])equal_to_ten=(vector==10)print(equal_to_ten)print(vector[equal_to_ten]) [False  TrueFalseFalse][10]进行运算之后获取Pythonvector=numpy.array([5,10,15,20])equal_to_ten_and_five=(vector==10)&(vector==5)12vector=numpy.array([5,10,15,20])equal_to_ten_and_five=(vector==10)&(vector==5)Pythonvector=numpy.array([5,10,15,20])equal_to_ten_or_five=(vector==10)|(vector==5)12vector=numpy.array([5,10,15,20])equal_to_ten_or_five=(vector==10)|(vector==5)类型转换将整体类型进行转换Pythonvector=numpy.array([5,10,15,20])print(vector.dtype)vector=vector.astype(str)print(vector.dtype)int64<U211234567vector=numpy.array([5,10,15,20])print(vector.dtype)vector=vector.astype(str)print(vector.dtype) int64<U21求和sum()能够对ndarray进行各种求和操作，比如分别按行按列进行求和Pythonmatrix=numpy.array([[1,2,3],[4,5,6],[7,8,9]])print(matrix.sum())print(matrix.sum(1))print(matrix.sum(0))45[61524][121518]12345678910matrix=numpy.array([[1,2,3],                      [4,5,6],                    [7,8,9]])print(matrix.sum())print(matrix.sum(1))print(matrix.sum(0)) 45[61524][121518]sum(1)是sum(axis=1))的缩写，1表示按照x轴方向求和，0表示按照y轴方向求和4.常用函数reshape生成从0-14的15个数字，使用reshape(3,5)将其构造成一个三行五列的array。Pythonimportnumpyasnparr=np.arange(15).reshape(3,5)arrarray([[0,1,2,3,4],[5,6,7,8,9],[10,11,12,13,14]])1234567importnumpyasnparr=np.arange(15).reshape(3,5)arr array([[0,  1,  2,  3,  4],      [5,  6,  7,  8,  9],      [10,11,12,13,14]])zeros生成指定结构的默认为0.的arrayPythonnp.zeros((3,4))array([[0.,0.,0.,0.],[0.,0.,0.,0.],[0.,0.,0.,0.]])12345np.zeros((3,4)) array([[0.,  0.,  0.,  0.],      [0.,  0.,  0.,  0.],      [0.,  0.,  0.,  0.]])ones生成一个三维的array,通过dtype指定类型Pythonnp.ones((2,3,4),dtype=np.int32)array([[[1,1,1,1],[1,1,1,1],[1,1,1,1]],[[1,1,1,1],[1,1,1,1],[1,1,1,1]]])123456789np.ones((2,3,4),dtype=np.int32) array([[[1,1,1,1],        [1,1,1,1],        [1,1,1,1]],       [[1,1,1,1],        [1,1,1,1],        [1,1,1,1]]])range指定范围和数值间的间隔生成array，注意范围包左不包右Pythonnp.arange(0,10,2)array([0,2,4,6,8])123np.arange(0,10,2) array([0,2,4,6,8])random随机数生成指定结构的随机数，可以用于生成随机权重Pythonnp.random.random((2,3))array([[0.86166627,0.37756207,0.94265883],[0.9768257,0.96915312,0.33495431]])1234np.random.random((2,3)) array([[0.86166627,  0.37756207,  0.94265883],      [0.9768257,  0.96915312,  0.33495431]])5.ndarray运算元素之间依次相减相减Pythona=np.array([10,20,30,40])b=np.array(4)a-barray([6,16,26,36])12345a=np.array([10,20,30,40])b=np.array(4) a-barray([6,16,26,36])乘方Pythona**2array([100,400,900,1600])12a**2array([100,  400,  900,1600])开根号Pythonnp.sqrt(B)array([[1.41421356,0.],[1.73205081,2.]])1234np.sqrt(B) array([[1.41421356,  0.        ],      [1.73205081,  2.        ]])e求方Pythonnp.exp(B)array([[7.3890561,1.],[20.08553692,54.59815003]])1234np.exp(B) array([[  7.3890561,  1.        ],      [20.08553692,  54.59815003]])向下取整Pythona=np.floor(10*np.random.random((2,2)))aarray([[0.,0.],[3.,6.]])12345a=np.floor(10*np.random.random((2,2)))a array([[0.,  0.],      [3.,  6.]])行列变换Pythona.Tarray([[0.,3.],[0.,6.]])1234a.T array([[0.,  3.],      [0.,  6.]])变换结构Pythona.resize(1,4)aarray([[0.,0.,3.,6.]])1234a.resize(1,4)a array([[0.,  0.,  3.,  6.]])6.矩阵运算矩阵之间的运算PythonA=np.array([[1,1],[0,1]])B=np.array([[2,0],[3,4]])1234A=np.array([[1,1],              [0,1]])B=np.array([[2,0],              [3,4]])对应位置一次相乘PythonA*Barray([[2,0],[0,4]])1234A*B array([[2,0],      [0,4]])矩阵乘法Pythonprint(A.dot(B))print(np.dot(A,B))[[54][34]]12345print(A.dot(B))print(np.dot(A,B)) [[54][34]]横向相加Pythona=np.floor(10*np.random.random((2,2)))b=np.floor(10*np.random.random((2,2)))print(a)print(b)print(np.hstack((a,b)))[[2.3.][9.3.]][[8.1.][0.0.]][[2.3.8.1.][9.3.0.0.]]12345678910111213a=np.floor(10*np.random.random((2,2)))b=np.floor(10*np.random.random((2,2))) print(a)print(b)print(np.hstack((a,b))) [[2.  3.][9.  3.]][[8.  1.][0.  0.]][[2.  3.  8.  1.][9.  3.  0.  0.]]纵向相加Pythonprint(np.vstack((a,b)))[[2.3.][9.3.][8.1.][0.0.]]123456print(np.vstack((a,b))) [[2.  3.][9.  3.][8.  1.][0.  0.]]矩阵分割Python#横向分割print(np.hsplit(a,3))#纵向风格print(np.vsplit(a,3))1234#横向分割print(np.hsplit(a,3))#纵向风格print(np.vsplit(a,3))7.复制的区别地址复制通过b=a复制a的值，b与a指向同一地址，改变b同时也改变a。Pythona=np.arange(12)b=aprint(aisb)print(a.shape)print(b.shape)b.shape=(3,4)print(a.shape)print(b.shape)True(12,)(12,)(3,4)(3,4)123456789101112131415a=np.arange(12)b=aprint(aisb) print(a.shape)print(b.shape)b.shape=(3,4)print(a.shape)print(b.shape) True(12,)(12,)(3,4)(3,4)复制值通过a.view()仅复制值，当对c值进行改变会改变a的对应的值，而改变c的shape不改变a的shapePythona=np.arange(12)c=a.view()print(cisa)c.shape=2,6c[0,0]=9999print(a)print(c)False[99991234567891011][[999912345][67891011]]1234567891011121314a=np.arange(12)c=a.view()print(cisa) c.shape=2,6c[0,0]=9999 print(a)print(c) False[9999    1    2    3    4    5    6    7    8    9  10  11][[9999    1    2    3    4    5][  6    7    8    9  10  11]]完整拷贝a.copy()进行的完整的拷贝，产生一份完全相同的独立的复制Pythona=np.arange(12)c=a.copy()print(cisa)c.shape=2,6c[0,0]=9999print(a)print(c)False[01234567891011][[999912345][67891011]]1234567891011121314a=np.arange(12)c=a.copy()print(cisa) c.shape=2,6c[0,0]=9999 print(a)print(c) False[0  1  2  3  4  5  6  7  8  91011][[9999    1    2    3    4    5][  6    7    8    9  10  11]]1赞4收藏评论"], "art_url": ["http://python.jobbole.com/88860/"]}
{"art_img": ["http://pytlab.org/assets/images/blog_img/2017-11-03-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E5%AE%9E%E8%B7%B5-%E6%A0%91%E5%9B%9E%E5%BD%92/feature.png"], "art_title": ["机器学习算法实践-树回归"], "art_create_time": ["2017/11/04"], "art_content": ["本文作者：伯乐在线-iPytLab。未经作者许可，禁止转载！欢迎加入伯乐在线专栏作者。前言最近由于开始要把精力集中在课题的应用上面了，这篇总结之后算法原理的学习先告一段落。本文主要介绍决策树用于回归问题的相关算法实现，其中包括回归树(regressiontree)和模型树(modeltree)的实现，并介绍了预剪枝(preprune)和后剪枝(postprune)的防止树过拟合的技术以及实现。最后对回归树和标准线性回归进行了对比。正文在之前的文章中我总结了通过使用构建决策树来进行类型预测。直观来看树结构最容易对分类问题进行处理，通过递归我们在数据中选取最佳分割特征对训练数据进行分割并进行树分裂最终到达触底条件获得训练出来决策树，可以通过可视化的方式直观的查看训练模型并对数据进行分类。通常决策树树分裂选择特征的方法有ID3,C4.5算法,C5.0算法和CART树。在《机器学习算法实践-决策树(DecisionTree)》中对ID3以及C4.5算法进行了介绍并使用ID3算法处理了分类问题。本文主要使用决策树解决回归问题，使用CART(ClassificationAndRegressionTrees)算法。CARTCART是一种二分递归分割的技术，分割方法采用基于最小距离的基尼指数估计函数，将当前的样本集分为两个子样本集，使得生成的的每个非叶子节点都有两个分支。因此，CART算法生成的决策树是结构简洁的二叉树。分类树是针对目标变量是离散型变量，通过二叉树将数据进行分割成离散类的方法。而回归树则是针对目标变量是连续性的变量，通过选取最优分割特征的某个值，然后数据根据大于或者小于这个值进行划分进行树分裂最终生成回归树。特征和最佳分割点的选取在使用决策树解决回归问题中我们需要不断的选取某一特征的一个值作为分割点来生成子树。选取的标准就是使得被分割的两部分数据能有最好的纯度。对于离散型数据我们可以通过计算分割两部分数据的基尼不纯度的变化来判定最有分割点；对于连续性变量我们通过计算最小平方残差，也就是选择使得分割后数据方差变得最小的特征和分割点。直观的理解就是使得分割的两部分数据能够有最相近的值。树分裂的终止条件有了选取分割特征和最佳分割点的方法，树便可以依此进行分裂，但是分裂的终止条件是什么呢?节点中所有目标变量的值相同,既然都已经是相同的值了自然没有必要在分裂了，直接返回这个值就好了.树的深度达到了预先指定的最大值不纯度的减小量小于预先定好的阈值,也就是之进一步的分割数据并不能更好的降低数据的不纯度的时候就可以停止树分裂了。节点的数据量小于预先定好的阈值回归树的Python实现本部分使用Python实现简单的回归树，并对给定的数据进行回归并可视化回归曲线和树结构。完整代码详见:https://github.com/PytLab/MLBox/tree/master/classification_and_regression_trees首先是加载数据的部分，这里的所有测试数据我均使用的《MachineLearninginAction》中的数据，格式比较规整加载方式也比较一致,这里由于做树回归，自变量和因变量都放在同一个二维数组中:Pythondefload_data(filename):'''加载文本文件中的数据.'''dataset=[]withopen(filename,'r')asf:forlineinf:line_data=[float(data)fordatainline.split()]dataset.append(line_data)returndataset123456789defload_data(filename):    '''加载文本文件中的数据.    '''    dataset=[]    withopen(filename,'r')asf:        forlineinf:            line_data=[float(data)fordatainline.split()]            dataset.append(line_data)    returndataset树回归中再找到分割特征和分割值之后需要将数据进行划分以便构建子树或者叶子节点:Pythondefsplit_dataset(dataset,feat_idx,value):'''根据给定的特征编号和特征值对数据集进行分割'''ldata,rdata=[],[]fordataindataset:ifdata[feat_idx]<value:ldata.append(data)else:rdata.append(data)returnldata,rdata12345678910defsplit_dataset(dataset,feat_idx,value):    '''根据给定的特征编号和特征值对数据集进行分割    '''    ldata,rdata=[],[]    fordataindataset:        ifdata[feat_idx]<value:            ldata.append(data)        else:            rdata.append(data)    returnldata,rdata然后就是重要的选取最佳分割特征和分割值了，这里我们通过找打使得分割后的方差最小的分割点最为最佳分割点:Pythondefchoose_best_feature(dataset,fleaf,ferr,opt):'''选取最佳分割特征和特征值dataset:待划分的数据集fleaf:创建叶子节点的函数ferr:计算数据误差的函数opt:回归树参数.err_tolerance:最小误差下降值;n_tolerance:数据切分最小样本数'''dataset=np.array(dataset)m,n=dataset.shapeerr_tolerance,n_tolerance=opt['err_tolerance'],opt['n_tolerance']err=ferr(dataset)best_feat_idx,best_feat_val,best_err=0,0,float('inf')#遍历所有特征forfeat_idxinrange(n-1):values=dataset[:,feat_idx]#遍历所有特征值forvalinvalues:#按照当前特征和特征值分割数据ldata,rdata=split_dataset(dataset.tolist(),feat_idx,val)iflen(ldata)<n_toleranceorlen(rdata)<n_tolerance:#如果切分的样本量太小continue#计算误差new_err=ferr(ldata)+ferr(rdata)ifnew_err<best_err:best_feat_idx=feat_idxbest_feat_val=valbest_err=new_err#如果误差变化并不大归为一类ifabs(err-best_err)<err_tolerance:returnNone,fleaf(dataset)#检查分割样本量是不是太小ldata,rdata=split_dataset(dataset.tolist(),best_feat_idx,best_feat_val)iflen(ldata)<n_toleranceorlen(rdata)<n_tolerance:returnNone,fleaf(dataset)returnbest_feat_idx,best_feat_val1234567891011121314151617181920212223242526272829303132333435363738defchoose_best_feature(dataset,fleaf,ferr,opt):    '''选取最佳分割特征和特征值    dataset:待划分的数据集    fleaf:创建叶子节点的函数    ferr:计算数据误差的函数    opt:回归树参数.        err_tolerance:最小误差下降值;        n_tolerance:数据切分最小样本数    '''    dataset=np.array(dataset)    m,n=dataset.shape    err_tolerance,n_tolerance=opt['err_tolerance'],opt['n_tolerance']    err=ferr(dataset)    best_feat_idx,best_feat_val,best_err=0,0,float('inf')    #遍历所有特征    forfeat_idxinrange(n-1):        values=dataset[:,feat_idx]        #遍历所有特征值        forvalinvalues:            #按照当前特征和特征值分割数据            ldata,rdata=split_dataset(dataset.tolist(),feat_idx,val)            iflen(ldata)<n_toleranceorlen(rdata)<n_tolerance:                #如果切分的样本量太小                continue            #计算误差            new_err=ferr(ldata)+ferr(rdata)            ifnew_err<best_err:                best_feat_idx=feat_idx                best_feat_val=val                best_err=new_err    #如果误差变化并不大归为一类    ifabs(err-best_err)<err_tolerance:        returnNone,fleaf(dataset)    #检查分割样本量是不是太小    ldata,rdata=split_dataset(dataset.tolist(),best_feat_idx,best_feat_val)    iflen(ldata)<n_toleranceorlen(rdata)<n_tolerance:        returnNone,fleaf(dataset)    returnbest_feat_idx,best_feat_val其中，停止选取的条件有两个:一个是当分割的子数据集的大小小于一定值；一个是当选取的最佳分割点分割的数据的方差减小量小于一定的值。fleaf是创建叶子节点的函数引用，不同的树结构此函数也是不同的，例如本部分的回归树，创建叶子节点就是根据分割后的数据集平均值，而对于模型树来说，此函数返回值是根据数据集得到的回归系数。ferr是计算数据集不纯度的函数，不同的树模型该函数也会不同，对于回归树，此函数计算数据集的方差来判定数据集的纯度，而对于模型树来说我们需要计算线性模型拟合程度也就是线性模型的残差平方和。然后就是最主要的回归树的生成函数了，树结构肯定需要通过递归创建的，选不出新的分割点的时候就触底：Pythondefcreate_tree(dataset,fleaf,ferr,opt=None):'''递归创建树结构dataset:待划分的数据集fleaf:创建叶子节点的函数ferr:计算数据误差的函数opt:回归树参数.err_tolerance:最小误差下降值;n_tolerance:数据切分最小样本数'''ifoptisNone:opt={'err_tolerance':1,'n_tolerance':4}#选择最优化分特征和特征值feat_idx,value=choose_best_feature(dataset,fleaf,ferr,opt)#触底条件iffeat_idxisNone:returnvalue#创建回归树tree={'feat_idx':feat_idx,'feat_val':value}#递归创建左子树和右子树ldata,rdata=split_dataset(dataset,feat_idx,value)ltree=create_tree(ldata,fleaf,ferr,opt)rtree=create_tree(rdata,fleaf,ferr,opt)tree['left']=ltreetree['right']=rtreereturntree1234567891011121314151617181920212223242526defcreate_tree(dataset,fleaf,ferr,opt=None):    '''递归创建树结构    dataset:待划分的数据集    fleaf:创建叶子节点的函数    ferr:计算数据误差的函数    opt:回归树参数.        err_tolerance:最小误差下降值;        n_tolerance:数据切分最小样本数    '''    ifoptisNone:        opt={'err_tolerance':1,'n_tolerance':4}    #选择最优化分特征和特征值    feat_idx,value=choose_best_feature(dataset,fleaf,ferr,opt)        #触底条件    iffeat_idxisNone:        returnvalue    #创建回归树    tree={'feat_idx':feat_idx,'feat_val':value}    #递归创建左子树和右子树    ldata,rdata=split_dataset(dataset,feat_idx,value)    ltree=create_tree(ldata,fleaf,ferr,opt)    rtree=create_tree(rdata,fleaf,ferr,opt)    tree['left']=ltree    tree['right']=rtree    returntree使用回归树对数据进行回归这里使用了现成的分段数据作为训练数据生成回归树，本文所有使用的数据详见: https://github.com/PytLab/MLBox/tree/master/classification_and_regression_trees可视化数据点Pythondataset=load_data('ex0.txt')dataset=np.array(dataset)#绘制散点plt.scatter(dataset[:,0],dataset[:,1])1234dataset=load_data('ex0.txt')dataset=np.array(dataset)#绘制散点plt.scatter(dataset[:,0],dataset[:,1])创建回归树并可视化看到这种分段的数据，回归树拟合它可是最合适不过了，我们创建回归树:Pythontree=create_tree(dataset,fleaf,ferr,opt={'n_tolerance':4,'err_tolerance':1})12tree=create_tree(dataset,fleaf,ferr,opt={'n_tolerance':4,                                              'err_tolerance':1})通过Python字典表示的回归树结构:Python{'feat_idx':0,'feat_val':0.40015800000000001,'left':{'feat_idx':0,'feat_val':0.20819699999999999,'left':-0.023838155555555553,'right':1.0289583666666666},'right':{'feat_idx':0,'feat_val':0.609483,'left':1.980035071428571,'right':{'feat_idx':0,'feat_val':0.81674199999999997,'left':2.9836209534883724,'right':3.9871631999999999}}}12345678910111213{'feat_idx':0,'feat_val':0.40015800000000001,'left':{'feat_idx':0,          'feat_val':0.20819699999999999,          'left':-0.023838155555555553,          'right':1.0289583666666666},'right':{'feat_idx':0,          'feat_val':0.609483,          'left':1.980035071428571,          'right':{'feat_idx':0,                    'feat_val':0.81674199999999997,                    'left':2.9836209534883724,                    'right':3.9871631999999999}}}这里我还是使用Graphviz来可视化回归树，类似之前决策树做分类的文章中的dotify函数，这里稍微修改下叶子节点的label，我们便可以递归得到决策树对应的dot文件, dotify函数的实现见:https://github.com/PytLab/MLBox/blob/master/classification_and_regression_trees/regression_tree.py#L159然后获取树结构图:Pythondatafile='ex0.txt'dotfile='{}.dot'.format(datafile.split('.')[0])withopen(dotfile,'w')asf:content=dotify(tree)f.write(content)12345datafile='ex0.txt'dotfile='{}.dot'.format(datafile.split('.')[0])withopen(dotfile,'w')asf:    content=dotify(tree)    f.write(content)生成回归树图片:Pythondot-Tpngex0.dot-oex0_tree.png1dot-Tpngex0.dot-oex0_tree.png其中节点上数字代表:特征编号:特征分割值绘制回归树回归曲线有了回归树，我们便可以绘制回归树回归曲线，看看它对于分段数据是否能有较好的回归效果：Python#绘制回归曲线x=np.linspace(0,1,50)y=[tree_predict([i],tree)foriinx]plt.plot(x,y,c='r')plt.show()12345#绘制回归曲线x=np.linspace(0,1,50)y=[tree_predict([i],tree)foriinx]plt.plot(x,y,c='r')plt.show()树剪枝在介绍树剪枝之前先使用上一部分的代码对两组类似的数据进行回归，可视化后的数据以及回归曲线如下(数据文件左&数据文件右):左右两边的数据的分布基本相同但是使用相同的参数得到的回归树却完全不同左边的回归树只有两个分支，而右边的分支则有很多，甚至有时候会为所有的数据点得到一个分支，这样回归树将会非常的庞大,如下是可视化得到的两个回归树:如果一棵树的节点过多则表明该模型可能对数据进行了“过拟合”。那么我们需要降低决策树的复杂度来避免过拟合，此过程就是剪枝。剪枝技术又分为预剪枝和后剪枝。预剪枝预剪枝是在生成决策树之前通过改变参数然后在树生成的过程中进行的。比如在上文中我们创建回归树的函数中有个opt参数，其中包含n_tolerance和err_tolerance，他们可以控制何时停止树的分裂，当增大叶子节点的最小数据量以及增大误差容忍度，树的分裂也会越提前的终止。当我们把误差变化容忍度增加到2000的时候得到的回归树以及回归曲线可视化如下:后剪枝预剪枝技术需要用于预先指定参数，但是后剪枝技术则是通过测试数据来自动进行剪枝不需要用户干预因此是一种更理想的剪枝技术，但是我们需要写剪枝函数来处理。后剪枝的大致思想就是我们针对一颗子树，尝试将其左右子树(节点)合并，通过测试数据计算合并前后的方差，如果合并后的方差比合并前的小，这说明可以合并此子树。对树进行塌陷处理:我们对一棵树进行塌陷处理，就是递归将这棵树进行合并返回这棵树的平均值。Pythondefcollapse(tree):'''对一棵树进行塌陷处理,得到给定树结构的平均值'''ifnot_tree(tree):returntreeltree,rtree=tree['left'],tree['right']return(collapse(ltree)+collapse(rtree))/21234567defcollapse(tree):    '''对一棵树进行塌陷处理,得到给定树结构的平均值    '''    ifnot_tree(tree):        returntree    ltree,rtree=tree['left'],tree['right']    return(collapse(ltree)+collapse(rtree))/2后剪枝的Python实现:Pythondefpostprune(tree,test_data):'''根据测试数据对树结构进行后剪枝'''ifnot_tree(tree):returntree#若没有测试数据则直接返回树平均值ifnottest_data:returncollapse(tree)ltree,rtree=tree['left'],tree['right']ifnot_tree(ltree)andnot_tree(rtree):#分割数据用于测试ldata,rdata=split_dataset(test_data,tree['feat_idx'],tree['feat_val'])#分别计算合并前和合并后的测试数据误差err_no_merge=(np.sum((np.array(ldata)-ltree)**2)+np.sum((np.array(rdata)-rtree)**2))err_merge=np.sum((np.array(test_data)-(ltree+rtree)/2)**2)iferr_merge<err_no_merge:print('merged')return(ltree+rtree)/2else:returntreetree['left']=postprune(tree['left'],test_data)tree['right']=postprune(tree['right'],test_data)returntree123456789101112131415161718192021222324defpostprune(tree,test_data):    '''根据测试数据对树结构进行后剪枝    '''    ifnot_tree(tree):        returntree    #若没有测试数据则直接返回树平均值    ifnottest_data:        returncollapse(tree)    ltree,rtree=tree['left'],tree['right']    ifnot_tree(ltree)andnot_tree(rtree):        #分割数据用于测试        ldata,rdata=split_dataset(test_data,tree['feat_idx'],tree['feat_val'])        #分别计算合并前和合并后的测试数据误差        err_no_merge=(np.sum((np.array(ldata)-ltree)**2)+                        np.sum((np.array(rdata)-rtree)**2))        err_merge=np.sum((np.array(test_data)-(ltree+rtree)/2)**2)        iferr_merge<err_no_merge:            print('merged')            return(ltree+rtree)/2        else:            returntree    tree['left']=postprune(tree['left'],test_data)    tree['right']=postprune(tree['right'],test_data)    returntree我们看一下不对刚才的树进行预剪枝而是使用测试数据进行后剪枝的效果:Pythondata_test=load_data('ex2test.txt')pruned_tree=postprune(tree,data_test)12data_test=load_data('ex2test.txt')pruned_tree=postprune(tree,data_test)Pythonmergedmergedmergedmergedmergedmergedmergedmerged12345678mergedmergedmergedmergedmergedmergedmergedmerged通过输出可以看到总共进行了8次剪枝操作，通过把剪枝前和剪枝后的树可视化对比下看看:树的规模的确是减小了。模型树上一部分叶子节点上放的是分割后数据的平均值并以他作为满足条件的样本的预测值，本部分我们将在叶子节点上放一个线性模型来做预测。也就是指我们的树是由多个线性模型组成的，显然会比强行用平均值来建模更有优势。模型树使用多个线性函数来做回归比用多个平均值组成一棵大树的模型更有可解释性而且线性模型的使用可以使树的规模减小，毕竟平均值的覆盖范围只是局部的，而线性模型可以覆盖所有具有线性关系的数据。模型树也具有更高的预测准确度创建模型树模型树和回归树的思想是完全一致的，只是在生成叶子节点的方法以及计算数据误差(不纯度)的方式不同。在模型树里针对一个叶子节点我们需要使用分割到的数据进行线性回归得到线性回归系数而不是简单的计算数据的平均值。不纯度的计算也不是简单的计算数据的方差，而是计算线性模型的残差平方和。为了能为叶子节点计算线性模型，我们还需要实现一个标准线性回归函数linear_regression,相应模型树的ferr和fleaf的Python实现Pythondeflinear_regression(dataset):'''获取标准线性回归系数'''dataset=np.matrix(dataset)#分割数据并添加常数列X_ori,y=dataset[:,:-1],dataset[:,-1]X_ori,y=np.matrix(X_ori),np.matrix(y)m,n=X_ori.shapeX=np.matrix(np.ones((m,n+1)))X[:,1:]=X_ori#回归系数w=(X.T*X).I*X.T*yreturnw,X,ydeffleaf(dataset):'''计算给定数据集的线性回归系数'''w,_,_=linear_regression(dataset)returnwdefferr(dataset):'''对给定数据集进行回归并计算误差'''w,X,y=linear_regression(dataset)y_prime=X*wreturnnp.var(y_prime-y)123456789101112131415161718192021222324deflinear_regression(dataset):    '''获取标准线性回归系数    '''    dataset=np.matrix(dataset)    #分割数据并添加常数列    X_ori,y=dataset[:,:-1],dataset[:,-1]    X_ori,y=np.matrix(X_ori),np.matrix(y)    m,n=X_ori.shape    X=np.matrix(np.ones((m,n+1)))    X[:,1:]=X_ori    #回归系数    w=(X.T*X).I*X.T*y    returnw,X,ydeffleaf(dataset):    '''计算给定数据集的线性回归系数    '''    w,_,_=linear_regression(dataset)    returnwdefferr(dataset):    '''对给定数据集进行回归并计算误差    '''    w,X,y=linear_regression(dataset)    y_prime=X*w    returnnp.var(y_prime-y)在分段线性数据上应用模型树本部分使用了事先准备好的分段线性数据来构建模型树，数据点可视化如下:现在我们使用这些数据构建一个模型树:Pythontree=create_tree(dataset,fleaf,ferr,opt={'err_tolerance':0.1,'n_tolerance':4})tree12tree=create_tree(dataset,fleaf,ferr,opt={'err_tolerance':0.1,'n_tolerance':4})tree得到的树结构：Python{'feat_idx':0,'feat_val':0.30440099999999998,'left':matrix([[3.46877936],[1.18521743]]),'right':matrix([[1.69855694e-03],[1.19647739e+01]])}123456{'feat_idx':0,'feat_val':0.30440099999999998,'left':matrix([[3.46877936],                [1.18521743]]),'right':matrix([[  1.69855694e-03],                  [  1.19647739e+01]])}可视化:绘制回归曲线:可以通过模型树看到对于此数据只需要两个分支，数的深度也只有2层。当x<0.304的时候，使用线性模型y=3.47+1.19x来回归当x>0.304的时候，使用线性模型y=0.0017+1.20x来回归回归树与线性回归的对比本部分我们使用标准线性回归和回归树分别对同一组数据进行回归，并使用同一组测试数据计算相关系数(CorrelationCoefficient)对两种模型的回归效果进行对比。数据我还是使用《MachinieLearninginAction》中的现成数据，数据可视化如下:现在我们分别使用标准线性回归和回归树对该数据进行回归，并计算模型预测值和测试样本的相关系数R2R2(完整代码见:https://github.com/PytLab/MLBox/blob/master/classification_and_regression_trees/compare.py)相关系数计算:Pythondefget_corrcoef(X,Y):#XY的协方差cov=np.mean(X*Y)-np.mean(X)*np.mean(Y)returncov/(np.var(X)*np.var(Y))**0.51234defget_corrcoef(X,Y):    #XY的协方差    cov=np.mean(X*Y)-np.mean(X)*np.mean(Y)    returncov/(np.var(X)*np.var(Y))**0.5获得的相关系数:Pythonlinearregressioncorrelationcoefficient:0.9434684235674773regressiontreecorrelationcoefficient:0.978030793270408912linearregressioncorrelationcoefficient:0.9434684235674773regressiontreecorrelationcoefficient:0.9780307932704089绘制线性回归和树回归的回归曲线(黄色会树回归曲线，红色会线性回归):可见树回归方法在预测复杂数据的时候会比简单的线性模型更有效。总结本文对决策树用于连续数值的回归预测进行了介绍，并实现了回归树,剪枝和模型树以及相应的树结构输出可视化等。对于模型树也给予了相应的Python实现并针对分段线性数据进行了回归测试。最后并对回归树模型和简单的标准线性回归模型进行了对比。参考《MachineLearninginAction》CART分类与回归树的原理与实现打赏支持我写出更多好文章，谢谢！打赏作者打赏支持我写出更多好文章，谢谢！1赞1收藏3评论关于作者：iPytLab喜欢写程序的计算化学狗，Python/C/C++/Fortran,个人博客http://pytlab.org个人主页·我的文章·22·"], "art_url": ["http://python.jobbole.com/88822/"]}
{"art_img": ["http://jbcdn2.b0.upaiyun.com/2017/10/74ba2ab488afb1a76de39b824fb36265.jpg"], "art_title": ["两句话掌握 Python 最难知识点——元类"], "art_create_time": ["2017/10/28"], "art_content": ["原文出处：刘羽冲   千万不要被所谓“元类是99%的python程序员不会用到的特性”这类的说辞吓住。因为每个中国人，都是天生的元类使用者学懂元类，你只需要知道两句话：道生一，一生二，二生三，三生万物我是谁？我从哪来里？我要到哪里去？在python世界，拥有一个永恒的道，那就是“type”，请记在脑海中，type就是道。如此广袤无垠的python生态圈，都是由type产生出来的。道生一，一生二，二生三，三生万物。道 即是type一 即是metaclass(元类，或者叫类生成器)二 即是class(类，或者叫实例生成器)三 即是instance(实例)万物 即是实例的各种属性与方法，我们平常使用python时，调用的就是它们。道和一，是我们今天讨论的命题，而二、三、和万物，则是我们常常使用的类、实例、属性和方法，用helloworld来举例：Python#创建一个Hello类，拥有属性say_hello----二的起源classHello():defsay_hello(self,name='world'):print('Hello,%s.'%name)#从Hello类创建一个实例hello----二生三hello=Hello()#使用hello调用方法say_hello----三生万物hello.say_hello()1234567891011#创建一个Hello类，拥有属性say_hello----二的起源classHello():    defsay_hello(self,name='world'):        print('Hello,%s.'%name)  #从Hello类创建一个实例hello----二生三hello=Hello() #使用hello调用方法say_hello----三生万物hello.say_hello()输出效果：PythonHello,world.1Hello,world.这就是一个标准的“二生三，三生万物”过程。 从类到我们可以调用的方法，用了这两步。那我们不由自主要问，类从何而来呢？回到代码的第一行。classHello其实是一个函数的“语义化简称”，只为了让代码更浅显易懂，它的另一个写法是：Pythondeffn(self,name='world'):#假如我们有一个函数叫fnprint('Hello,%s.'%name)Hello=type('Hello',(object,),dict(say_hello=fn))#通过type创建Helloclass----神秘的“道”，可以点化一切，这次我们直接从“道”生出了“二”1234deffn(self,name='world'):#假如我们有一个函数叫fn    print('Hello,%s.'%name)    Hello=type('Hello',(object,),dict(say_hello=fn))#通过type创建Helloclass----神秘的“道”，可以点化一切，这次我们直接从“道”生出了“二”这样的写法，就和之前的ClassHello写法作用完全相同，你可以试试创建实例并调用Python#从Hello类创建一个实例hello----二生三，完全一样hello=Hello()#使用hello调用方法say_hello----三生万物，完全一样hello.say_hello()12345#从Hello类创建一个实例hello----二生三，完全一样hello=Hello() #使用hello调用方法say_hello----三生万物，完全一样hello.say_hello()输出效果：PythonHello,world.----调用结果完全一样。1Hello,world.----调用结果完全一样。我们回头看一眼最精彩的地方，道直接生出了二：Hello=type(‘Hello’,(object,),dict(say_hello=fn))这就是“道”，python世界的起源，你可以为此而惊叹。注意它的三个参数！暗合人类的三大永恒命题：我是谁，我从哪里来，我要到哪里去。第一个参数：我是谁。在这里，我需要一个区分于其它一切的命名，以上的实例将我命名为“Hello”第二个参数：我从哪里来在这里，我需要知道从哪里来，也就是我的“父类”，以上实例中我的父类是“object”——python中一种非常初级的类。第三个参数：我要到哪里去在这里，我们将需要调用的方法和属性包含到一个字典里，再作为参数传入。以上实例中，我们有一个say_hello方法包装进了字典中。值得注意的是，三大永恒命题，是一切类，一切实例，甚至一切实例属性与方法都具有的。理所应当，它们的“创造者”，道和一，即type和元类，也具有这三个参数。但平常，类的三大永恒命题并不作为参数传入，而是以如下方式传入PythonclassHello(object){#class后声明“我是谁”#小括号内声明“我来自哪里”#中括号内声明“我要到哪里去”defsay_hello(){}}12345678classHello(object){#class后声明“我是谁”#小括号内声明“我来自哪里”#中括号内声明“我要到哪里去”    defsay_hello(){            }}造物主，可以直接创造单个的人，但这是一件苦役。造物主会先创造“人”这一物种，再批量创造具体的个人。并将三大永恒命题，一直传递下去。“道”可以直接生出“二”，但它会先生出“一”，再批量地制造“二”。type可以直接生成类（class），但也可以先生成元类（metaclass），再使用元类批量定制类（class）。元类——道生一，一生二一般来说，元类均被命名后缀为Metalass。想象一下，我们需要一个可以自动打招呼的元类，它里面的类方法呢，有时需要say_Hello，有时需要say_Hi，有时又需要say_Sayolala，有时需要say_Nihao。如果每个内置的say_xxx都需要在类里面声明一次，那将是多么可怕的苦役！不如使用元类来解决问题。以下是创建一个专门“打招呼”用的元类代码：PythonclassSayMetaClass(type):def__new__(cls,name,bases,attrs):attrs['say_'+name]=lambdaself,value,saying=name:print(saying+','+value+'!')returntype.__new__(cls,name,bases,attrs)12345classSayMetaClass(type):     def__new__(cls,name,bases,attrs):        attrs['say_'+name]=lambdaself,value,saying=name:print(saying+','+value+'!')        returntype.__new__(cls,name,bases,attrs)记住两点：1、元类是由“type”衍生而出，所以父类需要传入type。【道生一，所以一必须包含道】2、元类的操作都在__new__中完成，它的第一个参数是将创建的类，之后的参数即是三大永恒命题：我是谁，我从哪里来，我将到哪里去。它返回的对象也是三大永恒命题，接下来，这三个参数将一直陪伴我们。在__new__中，我只进行了一个操作，就是Pythonattrs['say_'+name]=lambdaself,value,saying=name:print(saying+','+value+'!')1attrs['say_'+name]=lambdaself,value,saying=name:print(saying+','+value+'!')它跟据类的名字，创建了一个类方法。比如我们由元类创建的类叫“Hello”，那创建时就自动有了一个叫“say_Hello”的类方法，然后又将类的名字“Hello”作为默认参数saying，传到了方法里面。然后把hello方法调用时的传参作为value传进去，最终打印出来。那么，一个元类是怎么从创建到调用的呢？来！一起根据道生一、一生二、二生三、三生万物的准则，走进元类的生命周期吧！Python#道生一：传入typeclassSayMetaClass(type):#传入三大永恒命题：类名称、父类、属性def__new__(cls,name,bases,attrs):#创造“天赋”attrs['say_'+name]=lambdaself,value,saying=name:print(saying+','+value+'!')#传承三大永恒命题：类名称、父类、属性returntype.__new__(cls,name,bases,attrs)#一生二：创建类classHello(object,metaclass=SayMetaClass):pass#二生三：创建实列hello=Hello()#三生万物：调用实例方法hello.say_Hello('world!')12345678910111213141516171819#道生一：传入typeclassSayMetaClass(type):     #传入三大永恒命题：类名称、父类、属性    def__new__(cls,name,bases,attrs):        #创造“天赋”        attrs['say_'+name]=lambdaself,value,saying=name:print(saying+','+value+'!')        #传承三大永恒命题：类名称、父类、属性        returntype.__new__(cls,name,bases,attrs) #一生二：创建类classHello(object,metaclass=SayMetaClass):    pass #二生三：创建实列hello=Hello() #三生万物：调用实例方法hello.say_Hello('world!')输出为PythonHello,world!1Hello,world!注意：通过元类创建的类，第一个参数是父类，第二个参数是metaclass普通人出生都不会说话，但有的人出生就会打招呼说“Hello”，“你好”,“sayolala”，这就是天赋的力量。它会给我们面向对象的编程省下无数的麻烦。现在，保持元类不变，我们还可以继续创建Sayolala，Nihao类，如下：Python#一生二：创建类classSayolala(object,metaclass=SayMetaClass):pass#二生三：创建实列s=Sayolala()#三生万物：调用实例方法s.say_Sayolala('japan!')123456789#一生二：创建类classSayolala(object,metaclass=SayMetaClass):    pass #二生三：创建实列s=Sayolala() #三生万物：调用实例方法s.say_Sayolala('japan!')输出PythonSayolala,japan!1Sayolala,japan!也可以说中文Python#一生二：创建类classNihao(object,metaclass=SayMetaClass):pass#二生三：创建实列n=Nihao()#三生万物：调用实例方法n.say_Nihao('中华!')123456789#一生二：创建类classNihao(object,metaclass=SayMetaClass):    pass #二生三：创建实列n=Nihao() #三生万物：调用实例方法n.say_Nihao('中华!')输出PythonNihao,中华!1Nihao,中华!再来一个小例子：Python#道生一classListMetaclass(type):def__new__(cls,name,bases,attrs):#天赋：通过add方法将值绑定attrs['add']=lambdaself,value:self.append(value)returntype.__new__(cls,name,bases,attrs)#一生二classMyList(list,metaclass=ListMetaclass):pass#二生三L=MyList()#三生万物L.add(1)12345678910111213141516#道生一classListMetaclass(type):    def__new__(cls,name,bases,attrs):        #天赋：通过add方法将值绑定        attrs['add']=lambdaself,value:self.append(value)        returntype.__new__(cls,name,bases,attrs)        #一生二classMyList(list,metaclass=ListMetaclass):    pass    #二生三L=MyList() #三生万物L.add(1)现在我们打印一下LPythonprint(L)>>>[1]123print(L) >>>[1]而普通的list没有add()方法PythonL2=list()L2.add(1)>>>AttributeError:'list'objecthasnoattribute'add'1234L2=list()L2.add(1) >>>AttributeError:'list'objecthasnoattribute'add'太棒了！学到这里，你是不是已经体验到了造物主的乐趣？python世界的一切，尽在掌握。年轻的造物主，请随我一起开创新世界。我们选择两个领域，一个是Django的核心思想，“ObjectRelationalMapping”，即对象-关系映射，简称ORM。这是Django的一大难点，但学完了元类，一切变得清晰。你对Django的理解将更上一层楼！另一个领域是爬虫领域（黑客领域），一个自动搜索网络上的可用代理，然后换着IP去突破别的人反爬虫限制。这两项技能非常有用，也非常好玩！挑战一：通过元类创建ORM准备工作，创建一个Field类PythonclassField(object):def__init__(self,name,column_type):self.name=nameself.column_type=column_typedef__str__(self):return'<%s:%s>'%(self.__class__.__name__,self.name)12345678classField(object):     def__init__(self,name,column_type):        self.name=name        self.column_type=column_type     def__str__(self):        return'<%s:%s>'%(self.__class__.__name__,self.name)它的作用是在Field类实例化时将得到两个参数，name和column_type，它们将被绑定为Field的私有属性，如果要将Field转化为字符串时，将返回“Field:XXX”，XXX是传入的name名称。准备工作：创建StringField和IntergerFieldPythonclassStringField(Field):def__init__(self,name):super(StringField,self).__init__(name,'varchar(100)')classIntegerField(Field):def__init__(self,name):super(IntegerField,self).__init__(name,'bigint')123456789classStringField(Field):     def__init__(self,name):        super(StringField,self).__init__(name,'varchar(100)') classIntegerField(Field):     def__init__(self,name):        super(IntegerField,self).__init__(name,'bigint')它的作用是在StringField,IntegerField实例初始化时，时自动调用父类的初始化方式。道生一PythonclassModelMetaclass(type):def__new__(cls,name,bases,attrs):ifname=='Model':returntype.__new__(cls,name,bases,attrs)print('Foundmodel:%s'%name)mappings=dict()fork,vinattrs.items():ifisinstance(v,Field):print('Foundmapping:%s==>%s'%(k,v))mappings[k]=vforkinmappings.keys():attrs.pop(k)attrs['__mappings__']=mappings#保存属性和列的映射关系attrs['__table__']=name#假设表名和类名一致returntype.__new__(cls,name,bases,attrs)12345678910111213141516classModelMetaclass(type):     def__new__(cls,name,bases,attrs):        ifname=='Model':            returntype.__new__(cls,name,bases,attrs)        print('Foundmodel:%s'%name)        mappings=dict()        fork,vinattrs.items():            ifisinstance(v,Field):                print('Foundmapping:%s==>%s'%(k,v))                mappings[k]=v        forkinmappings.keys():            attrs.pop(k)        attrs['__mappings__']=mappings#保存属性和列的映射关系        attrs['__table__']=name#假设表名和类名一致        returntype.__new__(cls,name,bases,attrs)它做了以下几件事创建一个新的字典mapping将每一个类的属性，通过.items()遍历其键值对。如果值是Field类，则打印键值，并将这一对键值绑定到mapping字典上。将刚刚传入值为Field类的属性删除。创建一个专门的__mappings__属性，保存字典mapping。创建一个专门的__table__属性，保存传入的类的名称。一生二PythonclassModel(dict,metaclass=ModelMetaclass):def__init__(self,**kwarg):super(Model,self).__init__(**kwarg)def__getattr__(self,key):try:returnself[key]exceptKeyError:raiseAttributeError(\"'Model'objecthasnoattribute'%s'\"%key)def__setattr__(self,key,value):self[key]=value#模拟建表操作defsave(self):fields=[]args=[]fork,vinself.__mappings__.items():fields.append(v.name)args.append(getattr(self,k,None))sql='insertinto%s(%s)values(%s)'%(self.__table__,','.join(fields),','.join([str(i)foriinargs]))print('SQL:%s'%sql)print('ARGS:%s'%str(args))123456789101112131415161718192021222324classModel(dict,metaclass=ModelMetaclass):     def__init__(self,**kwarg):        super(Model,self).__init__(**kwarg)     def__getattr__(self,key):        try:            returnself[key]        exceptKeyError:            raiseAttributeError(\"'Model'objecthasnoattribute'%s'\"%key)     def__setattr__(self,key,value):        self[key]=value     #模拟建表操作    defsave(self):        fields=[]        args=[]        fork,vinself.__mappings__.items():            fields.append(v.name)            args.append(getattr(self,k,None))        sql='insertinto%s(%s)values(%s)'%(self.__table__,','.join(fields),','.join([str(i)foriinargs]))        print('SQL:%s'%sql)        print('ARGS:%s'%str(args))如果从Model创建一个子类User：PythonclassUser(Model):#定义类的属性到列的映射：id=IntegerField('id')name=StringField('username')email=StringField('email')password=StringField('password')123456classUser(Model):    #定义类的属性到列的映射：    id=IntegerField('id')    name=StringField('username')    email=StringField('email')    password=StringField('password')这时id=IntegerField(‘id’)就会自动解析为：Model.__setattr__(self,‘id’,IntegerField(‘id’))因为IntergerField(‘id’)是Field的子类的实例，自动触发元类的__new__，所以将IntergerField(‘id’)存入__mappings__并删除这个键值对。二生三、三生万物当你初始化一个实例的时候并调用save()方法时候Pythonu=User(id=12345,name='Batman',email='batman@nasa.org',password='iamback')u.save()12u=User(id=12345,name='Batman',email='batman@nasa.org',password='iamback')u.save()这时先完成了二生三的过程：先调用Model.__setattr__，将键值载入私有对象然后调用元类的“天赋”，ModelMetaclass.__new__，将Model中的私有对象，只要是Field的实例，都自动存入u.__mappings__。接下来完成了三生万物的过程：通过u.save()模拟数据库存入操作。这里我们仅仅做了一下遍历__mappings__操作，虚拟了sql并打印，在现实情况下是通过输入sql语句与数据库来运行。输出结果为PythonFoundmodel:UserFoundmapping:name==><StringField:username>Foundmapping:password==><StringField:password>Foundmapping:id==><IntegerField:id>Foundmapping:email==><StringField:email>SQL:insertintoUser(username,password,id,email)values(Batman,iamback,12345,batman@nasa.org)ARGS:['Batman','iamback',12345,'batman@nasa.org']1234567Foundmodel:UserFoundmapping:name==><StringField:username>Foundmapping:password==><StringField:password>Foundmapping:id==><IntegerField:id>Foundmapping:email==><StringField:email>SQL:insertintoUser(username,password,id,email)values(Batman,iamback,12345,batman@nasa.org)ARGS:['Batman','iamback',12345,'batman@nasa.org']年轻的造物主，你已经和我一起体验了由“道”演化“万物”的伟大历程，这也是Django中的Model版块核心原理。接下来，请和我一起进行更好玩的爬虫实战（嗯，你现在已经是初级黑客了）：网络代理的爬取吧！挑战二：网络代理的爬取准备工作，先爬个页面玩玩请确保已安装requests和pyquery这两个包。Python#文件：get_page.pyimportrequestsbase_headers={'User-Agent':'Mozilla/5.0(WindowsNT10.0;Win64;x64)AppleWebKit/537.36(KHTML,likeGecko)Chrome/54.0.2840.71Safari/537.36','Accept-Encoding':'gzip,deflate,sdch','Accept-Language':'zh-CN,zh;q=0.8'}defget_page(url):headers=dict(base_headers)print('Getting',url)try:r=requests.get(url,headers=headers)print('Gettingresult',url,r.status_code)ifr.status_code==200:returnr.textexceptConnectionError:print('CrawlingFailed',url)returnNone123456789101112131415161718192021#文件：get_page.pyimportrequests base_headers={    'User-Agent':'Mozilla/5.0(WindowsNT10.0;Win64;x64)AppleWebKit/537.36(KHTML,likeGecko)Chrome/54.0.2840.71Safari/537.36',    'Accept-Encoding':'gzip,deflate,sdch',    'Accept-Language':'zh-CN,zh;q=0.8'}  defget_page(url):    headers=dict(base_headers)    print('Getting',url)    try:        r=requests.get(url,headers=headers)        print('Gettingresult',url,r.status_code)        ifr.status_code==200:            returnr.text    exceptConnectionError:        print('CrawlingFailed',url)        returnNone这里，我们利用request包，把百度的源码爬了出来。试一试抓百度把这一段粘在get_page.py后面，试完删除Pythonif(__name__=='__main__'):rs=get_page('https://www.baidu.com')print('result:\\r\\n',rs)123if(__name__=='__main__'):    rs=get_page('https://www.baidu.com')    print('result:\\r\\n',rs)试一试抓代理把这一段粘在get_page.py后面，试完删除Pythonif(__name__=='__main__'):frompyqueryimportPyQueryaspqstart_url='http://www.proxy360.cn/Region/China'print('Crawling',start_url)html=get_page(start_url)ifhtml:doc=pq(html)lines=doc('div[name=\"list_proxy_ip\"]').items()forlineinlines:ip=line.find('.tbBottomLine:nth-child(1)').text()port=line.find('.tbBottomLine:nth-child(2)').text()print(ip+':'+port)123456789101112if(__name__=='__main__'):    frompyqueryimportPyQueryaspq    start_url='http://www.proxy360.cn/Region/China'    print('Crawling',start_url)    html=get_page(start_url)    ifhtml:        doc=pq(html)        lines=doc('div[name=\"list_proxy_ip\"]').items()        forlineinlines:            ip=line.find('.tbBottomLine:nth-child(1)').text()            port=line.find('.tbBottomLine:nth-child(2)').text()            print(ip+':'+port)接下来进入正题：使用元类批量抓取代理批量处理抓取代理Pythonfromgetpageimportget_pagefrompyqueryimportPyQueryaspq#道生一：创建抽取代理的metaclassclassProxyMetaclass(type):\"\"\"元类，在FreeProxyGetter类中加入__CrawlFunc__和__CrawlFuncCount__两个参数，分别表示爬虫函数，和爬虫函数的数量。\"\"\"def__new__(cls,name,bases,attrs):count=0attrs['__CrawlFunc__']=[]attrs['__CrawlName__']=[]fork,vinattrs.items():if'crawl_'ink:attrs['__CrawlName__'].append(k)attrs['__CrawlFunc__'].append(v)count+=1forkinattrs['__CrawlName__']:attrs.pop(k)attrs['__CrawlFuncCount__']=countreturntype.__new__(cls,name,bases,attrs)#一生二：创建代理获取类classProxyGetter(object,metaclass=ProxyMetaclass):defget_raw_proxies(self,site):proxies=[]print('Site',site)forfuncinself.__CrawlFunc__:iffunc.__name__==site:this_page_proxies=func(self)forproxyinthis_page_proxies:print('Getting',proxy,'from',site)proxies.append(proxy)returnproxiesdefcrawl_daili66(self,page_count=4):start_url='http://www.66ip.cn/{}.html'urls=[start_url.format(page)forpageinrange(1,page_count+1)]forurlinurls:print('Crawling',url)html=get_page(url)ifhtml:doc=pq(html)trs=doc('.containerboxtabletr:gt(0)').items()fortrintrs:ip=tr.find('td:nth-child(1)').text()port=tr.find('td:nth-child(2)').text()yield':'.join([ip,port])defcrawl_proxy360(self):start_url='http://www.proxy360.cn/Region/China'print('Crawling',start_url)html=get_page(start_url)ifhtml:doc=pq(html)lines=doc('div[name=\"list_proxy_ip\"]').items()forlineinlines:ip=line.find('.tbBottomLine:nth-child(1)').text()port=line.find('.tbBottomLine:nth-child(2)').text()yield':'.join([ip,port])defcrawl_goubanjia(self):start_url='http://www.goubanjia.com/free/gngn/index.shtml'html=get_page(start_url)ifhtml:doc=pq(html)tds=doc('td.ip').items()fortdintds:td.find('p').remove()yieldtd.text().replace('','')if__name__=='__main__':#二生三：实例化ProxyGettercrawler=ProxyGetter()print(crawler.__CrawlName__)#三生万物forsite_labelinrange(crawler.__CrawlFuncCount__):site=crawler.__CrawlName__[site_label]myProxies=crawler.get_raw_proxies(site)1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586fromgetpageimportget_pagefrompyqueryimportPyQueryaspq  #道生一：创建抽取代理的metaclassclassProxyMetaclass(type):    \"\"\"        元类，在FreeProxyGetter类中加入        __CrawlFunc__和__CrawlFuncCount__        两个参数，分别表示爬虫函数，和爬虫函数的数量。    \"\"\"    def__new__(cls,name,bases,attrs):        count=0        attrs['__CrawlFunc__']=[]        attrs['__CrawlName__']=[]        fork,vinattrs.items():            if'crawl_'ink:                attrs['__CrawlName__'].append(k)                attrs['__CrawlFunc__'].append(v)                count+=1        forkinattrs['__CrawlName__']:            attrs.pop(k)        attrs['__CrawlFuncCount__']=count        returntype.__new__(cls,name,bases,attrs)  #一生二：创建代理获取类 classProxyGetter(object,metaclass=ProxyMetaclass):    defget_raw_proxies(self,site):        proxies=[]        print('Site',site)        forfuncinself.__CrawlFunc__:            iffunc.__name__==site:                this_page_proxies=func(self)                forproxyinthis_page_proxies:                    print('Getting',proxy,'from',site)                    proxies.append(proxy)        returnproxies      defcrawl_daili66(self,page_count=4):        start_url='http://www.66ip.cn/{}.html'        urls=[start_url.format(page)forpageinrange(1,page_count+1)]        forurlinurls:            print('Crawling',url)            html=get_page(url)            ifhtml:                doc=pq(html)                trs=doc('.containerboxtabletr:gt(0)').items()                fortrintrs:                    ip=tr.find('td:nth-child(1)').text()                    port=tr.find('td:nth-child(2)').text()                    yield':'.join([ip,port])     defcrawl_proxy360(self):        start_url='http://www.proxy360.cn/Region/China'        print('Crawling',start_url)        html=get_page(start_url)        ifhtml:            doc=pq(html)            lines=doc('div[name=\"list_proxy_ip\"]').items()            forlineinlines:                ip=line.find('.tbBottomLine:nth-child(1)').text()                port=line.find('.tbBottomLine:nth-child(2)').text()                yield':'.join([ip,port])     defcrawl_goubanjia(self):        start_url='http://www.goubanjia.com/free/gngn/index.shtml'        html=get_page(start_url)        ifhtml:            doc=pq(html)            tds=doc('td.ip').items()            fortdintds:                td.find('p').remove()                yieldtd.text().replace('','')  if__name__=='__main__':    #二生三：实例化ProxyGetter    crawler=ProxyGetter()    print(crawler.__CrawlName__)    #三生万物    forsite_labelinrange(crawler.__CrawlFuncCount__):        site=crawler.__CrawlName__[site_label]        myProxies=crawler.get_raw_proxies(site)道生一：元类的__new__中，做了四件事：将“crawl_”开头的类方法的名称推入ProxyGetter.__CrawlName__将“crawl_”开头的类方法的本身推入ProxyGetter.__CrawlFunc__计算符合“crawl_”开头的类方法个数删除所有符合“crawl_”开头的类方法怎么样？是不是和之前创建ORM的__mappings__过程极为相似？一生二：类里面定义了使用pyquery抓取页面元素的方法分别从三个免费代理网站抓取了页面上显示的全部代理。如果对yield用法不熟悉，可以查看：廖雪峰的python教程：生成器二生三：创建实例对象crawler略三生万物：遍历每一个__CrawlFunc__在ProxyGetter.__CrawlName__上面，获取可以抓取的的网址名。触发类方法ProxyGetter.get_raw_proxies(site)遍历ProxyGetter.__CrawlFunc__,如果方法名和网址名称相同的，则执行这一个方法把每个网址获取到的代理整合成数组输出。那么。。。怎么利用批量代理，冲击别人的网站，套取别人的密码，狂发广告水贴，定时骚扰客户？呃！想啥呢！这些自己悟！如果悟不到，请听下回分解！年轻的造物主，创造世界的工具已经在你手上，请你将它的威力发挥到极致！请记住挥动工具的口诀：道生一，一生二，二生三，三生万物我是谁，我来自哪里，我要到哪里去3赞15收藏7评论"], "art_url": ["http://python.jobbole.com/88795/"]}
{"art_img": ["http://jbcdn2.b0.upaiyun.com/2017/11/ac652d1b0feb1192f18b75d650e1c73b.png"], "art_title": ["探索 Php 和 Python 下对象的深拷贝和浅拷贝"], "art_create_time": ["2017/11/19"], "art_content": ["原文出处：南山younger   一、深拷贝与浅拷贝深拷贝：赋值时值完全复制，完全的copy，对其中一个作出改变，不会影响另一个浅拷贝：赋值时，引用赋值，相当于取了一个别名。对其中一个修改，会影响另一个对于PHP而言，=赋值时，普通对象是深拷贝，但对对象来说，是浅拷贝，即引用赋值。当对象作为参数传递时，无论参数前是否有&引用符号，都将被看做是赋值引用。对于python而言，情况可能会有点小复杂，因为python一切皆为对象，所以python的普通赋值、深拷贝和浅拷贝之间都是有细微区别的。二、php下的他们在php5中，对象的=赋值和传递都是引用。要想实现拷贝副本，php提供了clone函数实现。clone完全copy了一份副本。但是clone时，我们可能不希望copy源对象的所有内容，那我们可以利用__clone来操作。请看如下代码段：Python<?php//普通对象赋值，深拷贝，完全值复制$m=1;$n=$m;$n=2;echo$m;//值复制，对新对象的改变不会对m作出改变，输出1.深拷贝echoPHP_EOL;/*==================*///对象赋值，浅拷贝，引用赋值classTest{public$a=1;}$m=newTest();$n=$m;//引用赋值$m->a=2;//修改m，n也随之改变echo$n->a;//输出2，浅拷贝echoPHP_EOL;?>12345678910111213141516171819<?php//普通对象赋值，深拷贝，完全值复制$m=1;$n=$m;$n=2;echo$m;//值复制，对新对象的改变不会对m作出改变，输出1.深拷贝echoPHP_EOL;/*==================*///对象赋值，浅拷贝，引用赋值classTest{    public$a=1;}$m=newTest();$n=$m;//引用赋值$m->a=2;//修改m，n也随之改变echo$n->a;//输出2，浅拷贝echoPHP_EOL;?>由于对象的赋值时引用，要想实现值复制，php提供了clone函数来实现复制对象。但是clone函数存在这么一个问题，克隆对象时，原对象的普通属性能值复制，但是源对象的对象属性赋值时还是引用赋值，浅拷贝。Python<?phpclassTest{public$a=1;}classTestOne{public$b=1;public$obj;//包含了一个对象属性，clone时，它会是浅拷贝publicfunction__construct(){$this->obj=newTest();}}$m=newTestOne();$n=$m;//这是完全的浅拷贝，无论普通属性还是对象属性$p=clone$m;//普通属性实现了深拷贝，改变普通属性b，不会对源对象有影响$p->b=2;echo$m->b;//输出原来的1echoPHP_EOL;//对象属性是浅拷贝，改变对象属性中的a，源对象m中的对象属性中a也改变$p->obj->a=3;echo$m->obj->a;//输出3，随新对象改变?>12345678910111213141516171819202122232425262728<?phpclassTest{    public$a=1;}classTestOne{    public$b=1;    public$obj;    //包含了一个对象属性，clone时，它会是浅拷贝    publicfunction__construct(){        $this->obj=newTest();    }}$m=newTestOne();$n=$m;//这是完全的浅拷贝，无论普通属性还是对象属性$p=clone$m;//普通属性实现了深拷贝，改变普通属性b，不会对源对象有影响$p->b=2;echo$m->b;//输出原来的1echoPHP_EOL;//对象属性是浅拷贝，改变对象属性中的a，源对象m中的对象属性中a也改变$p->obj->a=3;echo$m->obj->a;//输出3，随新对象改变?>要想实现对象真正的深拷贝，有以下两种方法：1、利用序列化反序列化实现Python<?phpclassTest{public$a=1;}classTestOne{public$b=1;public$obj;//包含了一个对象属性，clone时，它会是浅拷贝publicfunction__construct(){$this->obj=newTest();}}$m=newTestOne();//方法二，序列化反序列化实现对象深拷贝$n=serialize($m);$n=unserialize($n);$n->b=2;echo$m->b;//输出原来的1echoPHP_EOL;//可以看到，普通属性实现了深拷贝，改变普通属性b，不会对源对象有影响$n->obj->a=3;echo$m->obj->a;//输出1，不随新对象改变，还是保持了原来的属性,可以看到，序列化和反序列化可以实现对象的深拷贝?>123456789101112131415161718192021222324252627282930<?phpclassTest{    public$a=1;}classTestOne{    public$b=1;    public$obj;    //包含了一个对象属性，clone时，它会是浅拷贝    publicfunction__construct(){        $this->obj=newTest();    }    }$m=newTestOne();//方法二，序列化反序列化实现对象深拷贝$n=serialize($m);$n=unserialize($n);$n->b=2;echo$m->b;//输出原来的1echoPHP_EOL;//可以看到，普通属性实现了深拷贝，改变普通属性b，不会对源对象有影响$n->obj->a=3;echo$m->obj->a;//输出1，不随新对象改变，还是保持了原来的属性,可以看到，序列化和反序列化可以实现对象的深拷贝?>2、写clone函数Python<?phpclassTest{public$a=1;}classTestOne{public$b=1;public$obj;//包含了一个对象属性，clone时，它会是浅拷贝publicfunction__construct(){$this->obj=newTest();}//方法一：重写clone函数publicfunction__clone(){$this->obj=clone$this->obj;}}$m=newTestOne();$n=clone$m;$n->b=2;echo$m->b;//输出原来的1echoPHP_EOL;//可以看到，普通属性实现了深拷贝，改变普通属性b，不会对源对象有影响//由于改写了clone函数，现在对象属性也实现了真正的深拷贝，对新对象的改变，不会影响源对象$n->obj->a=3;echo$m->obj->a;//输出1，不随新对象改变，还是保持了原来的属性?>1234567891011121314151617181920212223242526272829303132<?phpclassTest{    public$a=1;}classTestOne{    public$b=1;    public$obj;    //包含了一个对象属性，clone时，它会是浅拷贝    publicfunction__construct(){        $this->obj=newTest();    }        //方法一：重写clone函数    publicfunction__clone(){        $this->obj=clone$this->obj;    }}$m=newTestOne();$n=clone$m;$n->b=2;echo$m->b;//输出原来的1echoPHP_EOL;//可以看到，普通属性实现了深拷贝，改变普通属性b，不会对源对象有影响//由于改写了clone函数，现在对象属性也实现了真正的深拷贝，对新对象的改变，不会影响源对象$n->obj->a=3;echo$m->obj->a;//输出1，不随新对象改变，还是保持了原来的属性?>三、python下的他们“对一个对象进行浅拷贝其实是新创建了一个类型和原来对象一样，但是内容是原来对象元素的引用。换句话说，这个拷贝的对象本身是新的，但是它的内容不是”，摘自《Python核心编程》。这是我个人对python下浅拷贝和深拷贝的理解：赋值：简单地拷贝对象的引用，两个对象的id相同。浅拷贝：创建一个新的组合对象，这个新对象与原对象共享内存中的子对象。深拷贝：创建一个新的组合对象，同时递归地拷贝所有子对象，新的组合对象与原对象没有任何关联。虽然实际上会共享不可变的子对象，但不影响它们的相互独立性。浅拷贝和深拷贝的不同仅仅是对组合对象来说，所谓的组合对象就是包含了其它对象的可变对象，如列表，类实例。而对于数字、字符串以及其它“原子”类型，没有拷贝一说，产生的都是原对象的引用。下面的代码希望能对你有进一步的帮助；Python#!/usr/bin/python#-*-coding:UTF-8-*-importcopy#浅拷贝a=[1,\"a\",3,[4,5,6],[[7,8,9]]]b=ac=list(a)d=copy.deepcopy(a)print\"原地址&&&\"printid(a)print\"赋值地址&&&\"printid(b)print\"浅拷贝地址&&&\"printid(c)print\"深拷贝地址&&&\"printid(d)print\"赋值地址###\"fori,jinzip(a,b):printid(i),id(j)print\"浅拷贝地址###\"fori,jinzip(a,c):printid(i),id(j)print\"深拷贝地址###\"fori,jinzip(a,d):printid(i),id(j)print\"######\"a[0]=2a[3][0]=14print\"原值变化为%d,%d\"%(a[0],a[3][0])print\"*******\"print\"赋值变化\"printb[0],b[3][0]print\"浅拷贝变化\"printc[0],c[3][0]print\"深拷贝变化\"printd[0],d[3][0]print\"**##12345678910111213141516171819202122232425262728293031323334353637383940414243444546#!/usr/bin/python#-*-coding:UTF-8-*-?importcopy?#????a=[1,\"a\",3,[4,5,6],[[7,8,9]]]?b=a?c=list(a)?d=copy.deepcopy(a)print\"???&&&\"printid(a)print\"????&&&\"printid(b)print\"?????&&&\"printid(c)print\"?????&&&\"printid(d)?print\"????###\"fori,jinzip(a,b):????printid(i),id(j)print\"?????###\"fori,jinzip(a,c):????printid(i),id(j)print\"?????###\"fori,jinzip(a,d):????printid(i),id(j)print\"######\"?a[0]=2a[3][0]=14print\"?????%d,%d\"%(a[0],a[3][0])print\"*******\"print\"????\"printb[0],b[3][0]print\"?????\"printc[0],c[3][0]print\"?????\"printd[0],d[3][0]print\"**##\"/>\"printa</textarea></div><divclass=\"crayon-main\"style=\"\"><tableclass=\"crayon-table\"><trclass=\"crayon-row\"><tdclass=\"crayon-nums\"data-settings=\"show\"><divclass=\"crayon-nums-content\"style=\"font-size:13px!important;line-height:15px!important;\"><divclass=\"crayon-num\"data-line=\"crayon-5a9de2766148f471354834-1\">1</div><divclass=\"crayon-numcrayon-striped-num\"data-line=\"crayon-5a9de2766148f471354834-2\">2</div><divclass=\"crayon-num\"data-line=\"crayon-5a9de2766148f471354834-3\">3</div><divclass=\"crayon-numcrayon-striped-num\"data-line=\"crayon-5a9de2766148f471354834-4\">4</div><divclass=\"crayon-num\"data-line=\"crayon-5a9de2766148f471354834-5\">5</div><divclass=\"crayon-numcrayon-striped-num\"data-line=\"crayon-5a9de2766148f471354834-6\">6</div><divclass=\"crayon-num\"data-line=\"crayon-5a9de2766148f471354834-7\">7</div><divclass=\"crayon-numcrayon-striped-num\"data-line=\"crayon-5a9de2766148f471354834-8\">8</div><divclass=\"crayon-num\"data-line=\"crayon-5a9de2766148f471354834-9\">9</div><divclass=\"crayon-numcrayon-striped-num\"data-line=\"crayon-5a9de2766148f471354834-10\">10</div><divclass=\"crayon-num\"data-line=\"crayon-5a9de2766148f471354834-11\">11</div><divclass=\"crayon-numcrayon-striped-num\"data-line=\"crayon-5a9de2766148f471354834-12\">12</div><divclass=\"crayon-num\"data-line=\"crayon-5a9de2766148f471354834-13\">13</div><divclass=\"crayon-numcrayon-striped-num\"data-line=\"crayon-5a9de2766148f471354834-14\">14</div><divclass=\"crayon-num\"data-line=\"crayon-5a9de2766148f471354834-15\">15</div><divclass=\"crayon-numcrayon-striped-num\"data-line=\"crayon-5a9de2766148f471354834-16\">16</div><divclass=\"crayon-num\"data-line=\"crayon-5a9de2766148f471354834-17\">17</div><divclass=\"crayon-numcrayon-striped-num\"data-line=\"crayon-5a9de2766148f471354834-18\">18</div><divclass=\"crayon-num\"data-line=\"crayon-5a9de2766148f471354834-19\">19</div><divclass=\"crayon-numcrayon-striped-num\"data-line=\"crayon-5a9de2766148f471354834-20\">20</div><divclass=\"crayon-num\"data-line=\"crayon-5a9de2766148f471354834-21\">21</div><divclass=\"crayon-numcrayon-striped-num\"data-line=\"crayon-5a9de2766148f471354834-22\">22</div><divclass=\"crayon-num\"data-line=\"crayon-5a9de2766148f471354834-23\">23</div><divclass=\"crayon-numcrayon-striped-num\"data-line=\"crayon-5a9de2766148f471354834-24\">24</div><divclass=\"crayon-num\"data-line=\"crayon-5a9de2766148f471354834-25\">25</div><divclass=\"crayon-numcrayon-striped-num\"data-line=\"crayon-5a9de2766148f471354834-26\">26</div><divclass=\"crayon-num\"data-line=\"crayon-5a9de2766148f471354834-27\">27</div><divclass=\"crayon-numcrayon-striped-num\"data-line=\"crayon-5a9de2766148f471354834-28\">28</div><divclass=\"crayon-num\"data-line=\"crayon-5a9de2766148f471354834-29\">29</div><divclass=\"crayon-numcrayon-striped-num\"data-line=\"crayon-5a9de2766148f471354834-30\">30</div><divclass=\"crayon-num\"data-line=\"crayon-5a9de2766148f471354834-31\">31</div><divclass=\"crayon-numcrayon-striped-num\"data-line=\"crayon-5a9de2766148f471354834-32\">32</div><divclass=\"crayon-num\"data-line=\"crayon-5a9de2766148f471354834-33\">33</div><divclass=\"crayon-numcrayon-striped-num\"data-line=\"crayon-5a9de2766148f471354834-34\">34</div><divclass=\"crayon-num\"data-line=\"crayon-5a9de2766148f471354834-35\">35</div><divclass=\"crayon-numcrayon-striped-num\"data-line=\"crayon-5a9de2766148f471354834-36\">36</div><divclass=\"crayon-num\"data-line=\"crayon-5a9de2766148f471354834-37\">37</div><divclass=\"crayon-numcrayon-striped-num\"data-line=\"crayon-5a9de2766148f471354834-38\">38</div><divclass=\"crayon-num\"data-line=\"crayon-5a9de2766148f471354834-39\">39</div><divclass=\"crayon-numcrayon-striped-num\"data-line=\"crayon-5a9de2766148f471354834-40\">40</div><divclass=\"crayon-num\"data-line=\"crayon-5a9de2766148f471354834-41\">41</div><divclass=\"crayon-numcrayon-striped-num\"data-line=\"crayon-5a9de2766148f471354834-42\">42</div><divclass=\"crayon-num\"data-line=\"crayon-5a9de2766148f471354834-43\">43</div><divclass=\"crayon-numcrayon-striped-num\"data-line=\"crayon-5a9de2766148f471354834-44\">44</div><divclass=\"crayon-num\"data-line=\"crayon-5a9de2766148f471354834-45\">45</div><divclass=\"crayon-numcrayon-striped-num\"data-line=\"crayon-5a9de2766148f471354834-46\">46</div></div></td><tdclass=\"crayon-code\"><divclass=\"crayon-pre\"style=\"font-size:13px!important;line-height:15px!important;-moz-tab-size:4;-o-tab-size:4;-webkit-tab-size:4;tab-size:4;\"><divclass=\"crayon-line\"id=\"crayon-5a9de2766148f471354834-1\"><spanclass=\"crayon-c\">#!/usr/bin/python</span></div><divclass=\"crayon-linecrayon-striped-line\"id=\"crayon-5a9de2766148f471354834-2\"><spanclass=\"crayon-c\">#-*-coding:UTF-8-*-</span></div><divclass=\"crayon-line\"id=\"crayon-5a9de2766148f471354834-3\">?</div><divclass=\"crayon-linecrayon-striped-line\"id=\"crayon-5a9de2766148f471354834-4\"><spanclass=\"crayon-r\">import</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-k\">copy</span></div><divclass=\"crayon-line\"id=\"crayon-5a9de2766148f471354834-5\">?</div><divclass=\"crayon-linecrayon-striped-line\"id=\"crayon-5a9de2766148f471354834-6\"><spanclass=\"crayon-c\">#???</span></div><divclass=\"crayon-line\"id=\"crayon-5a9de2766148f471354834-7\">?</div><divclass=\"crayon-linecrayon-striped-line\"id=\"crayon-5a9de2766148f471354834-8\"><spanclass=\"crayon-v\">a</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-o\">=</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-sy\">[</span><spanclass=\"crayon-cn\">1</span><spanclass=\"crayon-sy\">,</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-s\">\"a\"</span><spanclass=\"crayon-sy\">,</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-cn\">3</span><spanclass=\"crayon-sy\">,</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-sy\">[</span><spanclass=\"crayon-cn\">4</span><spanclass=\"crayon-sy\">,</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-cn\">5</span><spanclass=\"crayon-sy\">,</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-cn\">6</span><spanclass=\"crayon-sy\">]</span><spanclass=\"crayon-sy\">,</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-sy\">[</span><spanclass=\"crayon-sy\">[</span><spanclass=\"crayon-cn\">7</span><spanclass=\"crayon-sy\">,</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-cn\">8</span><spanclass=\"crayon-sy\">,</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-cn\">9</span><spanclass=\"crayon-sy\">]</span><spanclass=\"crayon-sy\">]</span><spanclass=\"crayon-sy\">]</span></div><divclass=\"crayon-line\"id=\"crayon-5a9de2766148f471354834-9\">?</div><divclass=\"crayon-linecrayon-striped-line\"id=\"crayon-5a9de2766148f471354834-10\"><spanclass=\"crayon-v\">b</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-o\">=</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-i\">a</span></div><divclass=\"crayon-line\"id=\"crayon-5a9de2766148f471354834-11\">?</div><divclass=\"crayon-linecrayon-striped-line\"id=\"crayon-5a9de2766148f471354834-12\"><spanclass=\"crayon-v\">c</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-o\">=</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-k\">list</span><spanclass=\"crayon-sy\">(</span><spanclass=\"crayon-v\">a</span><spanclass=\"crayon-sy\">)</span></div><divclass=\"crayon-line\"id=\"crayon-5a9de2766148f471354834-13\">?</div><divclass=\"crayon-linecrayon-striped-line\"id=\"crayon-5a9de2766148f471354834-14\"><spanclass=\"crayon-v\">d</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-o\">=</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-k\">copy</span><spanclass=\"crayon-sy\">.</span><spanclass=\"crayon-e\">deepcopy</span><spanclass=\"crayon-sy\">(</span><spanclass=\"crayon-v\">a</span><spanclass=\"crayon-sy\">)</span></div><divclass=\"crayon-line\"id=\"crayon-5a9de2766148f471354834-15\"><spanclass=\"crayon-k\">print</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-s\">\"???&&&\"</span></div><divclass=\"crayon-linecrayon-striped-line\"id=\"crayon-5a9de2766148f471354834-16\"><spanclass=\"crayon-k\">print</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-k\">id</span><spanclass=\"crayon-sy\">(</span><spanclass=\"crayon-v\">a</span><spanclass=\"crayon-sy\">)</span></div><divclass=\"crayon-line\"id=\"crayon-5a9de2766148f471354834-17\"><spanclass=\"crayon-k\">print</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-s\">\"????&&&\"</span></div><divclass=\"crayon-linecrayon-striped-line\"id=\"crayon-5a9de2766148f471354834-18\"><spanclass=\"crayon-k\">print</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-k\">id</span><spanclass=\"crayon-sy\">(</span><spanclass=\"crayon-v\">b</span><spanclass=\"crayon-sy\">)</span></div><divclass=\"crayon-line\"id=\"crayon-5a9de2766148f471354834-19\"><spanclass=\"crayon-k\">print</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-s\">\"?????&&&\"</span></div><divclass=\"crayon-linecrayon-striped-line\"id=\"crayon-5a9de2766148f471354834-20\"><spanclass=\"crayon-k\">print</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-k\">id</span><spanclass=\"crayon-sy\">(</span><spanclass=\"crayon-v\">c</span><spanclass=\"crayon-sy\">)</span></div><divclass=\"crayon-line\"id=\"crayon-5a9de2766148f471354834-21\"><spanclass=\"crayon-k\">print</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-s\">\"?????&&&\"</span></div><divclass=\"crayon-linecrayon-striped-line\"id=\"crayon-5a9de2766148f471354834-22\"><spanclass=\"crayon-k\">print</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-k\">id</span><spanclass=\"crayon-sy\">(</span><spanclass=\"crayon-v\">d</span><spanclass=\"crayon-sy\">)</span></div><divclass=\"crayon-line\"id=\"crayon-5a9de2766148f471354834-23\">?</div><divclass=\"crayon-linecrayon-striped-line\"id=\"crayon-5a9de2766148f471354834-24\"><spanclass=\"crayon-k\">print</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-s\">\"????###\"</span></div><divclass=\"crayon-line\"id=\"crayon-5a9de2766148f471354834-25\"><spanclass=\"crayon-st\">for</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-v\">i</span><spanclass=\"crayon-sy\">,</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-i\">j</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-st\">in</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-k\">zip</span><spanclass=\"crayon-sy\">(</span><spanclass=\"crayon-v\">a</span><spanclass=\"crayon-sy\">,</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-v\">b</span><spanclass=\"crayon-sy\">)</span><spanclass=\"crayon-o\">:</span></div><divclass=\"crayon-linecrayon-striped-line\"id=\"crayon-5a9de2766148f471354834-26\"><spanclass=\"crayon-h\">????</span><spanclass=\"crayon-k\">print</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-k\">id</span><spanclass=\"crayon-sy\">(</span><spanclass=\"crayon-v\">i</span><spanclass=\"crayon-sy\">)</span><spanclass=\"crayon-sy\">,</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-k\">id</span><spanclass=\"crayon-sy\">(</span><spanclass=\"crayon-v\">j</span><spanclass=\"crayon-sy\">)</span></div><divclass=\"crayon-line\"id=\"crayon-5a9de2766148f471354834-27\"><spanclass=\"crayon-k\">print</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-s\">\"?????###\"</span></div><divclass=\"crayon-linecrayon-striped-line\"id=\"crayon-5a9de2766148f471354834-28\"><spanclass=\"crayon-st\">for</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-v\">i</span><spanclass=\"crayon-sy\">,</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-i\">j</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-st\">in</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-k\">zip</span><spanclass=\"crayon-sy\">(</span><spanclass=\"crayon-v\">a</span><spanclass=\"crayon-sy\">,</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-v\">c</span><spanclass=\"crayon-sy\">)</span><spanclass=\"crayon-o\">:</span></div><divclass=\"crayon-line\"id=\"crayon-5a9de2766148f471354834-29\"><spanclass=\"crayon-h\">????</span><spanclass=\"crayon-k\">print</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-k\">id</span><spanclass=\"crayon-sy\">(</span><spanclass=\"crayon-v\">i</span><spanclass=\"crayon-sy\">)</span><spanclass=\"crayon-sy\">,</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-k\">id</span><spanclass=\"crayon-sy\">(</span><spanclass=\"crayon-v\">j</span><spanclass=\"crayon-sy\">)</span></div><divclass=\"crayon-linecrayon-striped-line\"id=\"crayon-5a9de2766148f471354834-30\"><spanclass=\"crayon-k\">print</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-s\">\"?????###\"</span></div><divclass=\"crayon-line\"id=\"crayon-5a9de2766148f471354834-31\"><spanclass=\"crayon-st\">for</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-v\">i</span><spanclass=\"crayon-sy\">,</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-i\">j</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-st\">in</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-k\">zip</span><spanclass=\"crayon-sy\">(</span><spanclass=\"crayon-v\">a</span><spanclass=\"crayon-sy\">,</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-v\">d</span><spanclass=\"crayon-sy\">)</span><spanclass=\"crayon-o\">:</span></div><divclass=\"crayon-linecrayon-striped-line\"id=\"crayon-5a9de2766148f471354834-32\"><spanclass=\"crayon-h\">????</span><spanclass=\"crayon-k\">print</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-k\">id</span><spanclass=\"crayon-sy\">(</span><spanclass=\"crayon-v\">i</span><spanclass=\"crayon-sy\">)</span><spanclass=\"crayon-sy\">,</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-k\">id</span><spanclass=\"crayon-sy\">(</span><spanclass=\"crayon-v\">j</span><spanclass=\"crayon-sy\">)</span></div><divclass=\"crayon-line\"id=\"crayon-5a9de2766148f471354834-33\"><spanclass=\"crayon-k\">print</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-s\">\"######\"</span></div><divclass=\"crayon-linecrayon-striped-line\"id=\"crayon-5a9de2766148f471354834-34\">?</div><divclass=\"crayon-line\"id=\"crayon-5a9de2766148f471354834-35\"><spanclass=\"crayon-v\">a</span><spanclass=\"crayon-sy\">[</span><spanclass=\"crayon-cn\">0</span><spanclass=\"crayon-sy\">]</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-o\">=</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-cn\">2</span></div><divclass=\"crayon-linecrayon-striped-line\"id=\"crayon-5a9de2766148f471354834-36\"><spanclass=\"crayon-v\">a</span><spanclass=\"crayon-sy\">[</span><spanclass=\"crayon-cn\">3</span><spanclass=\"crayon-sy\">]</span><spanclass=\"crayon-sy\">[</span><spanclass=\"crayon-cn\">0</span><spanclass=\"crayon-sy\">]</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-o\">=</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-cn\">14</span></div><divclass=\"crayon-line\"id=\"crayon-5a9de2766148f471354834-37\"><spanclass=\"crayon-k\">print</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-s\">\"?????%d,%d\"</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-o\">%</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-sy\">(</span><spanclass=\"crayon-v\">a</span><spanclass=\"crayon-sy\">[</span><spanclass=\"crayon-cn\">0</span><spanclass=\"crayon-sy\">]</span><spanclass=\"crayon-sy\">,</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-v\">a</span><spanclass=\"crayon-sy\">[</span><spanclass=\"crayon-cn\">3</span><spanclass=\"crayon-sy\">]</span><spanclass=\"crayon-sy\">[</span><spanclass=\"crayon-cn\">0</span><spanclass=\"crayon-sy\">]</span><spanclass=\"crayon-sy\">)</span></div><divclass=\"crayon-linecrayon-striped-line\"id=\"crayon-5a9de2766148f471354834-38\"><spanclass=\"crayon-k\">print</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-s\">\"*******\"</span></div><divclass=\"crayon-line\"id=\"crayon-5a9de2766148f471354834-39\"><spanclass=\"crayon-k\">print</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-s\">\"????\"</span></div><divclass=\"crayon-linecrayon-striped-line\"id=\"crayon-5a9de2766148f471354834-40\"><spanclass=\"crayon-k\">print</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-v\">b</span><spanclass=\"crayon-sy\">[</span><spanclass=\"crayon-cn\">0</span><spanclass=\"crayon-sy\">]</span><spanclass=\"crayon-sy\">,</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-v\">b</span><spanclass=\"crayon-sy\">[</span><spanclass=\"crayon-cn\">3</span><spanclass=\"crayon-sy\">]</span><spanclass=\"crayon-sy\">[</span><spanclass=\"crayon-cn\">0</span><spanclass=\"crayon-sy\">]</span></div><divclass=\"crayon-line\"id=\"crayon-5a9de2766148f471354834-41\"><spanclass=\"crayon-k\">print</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-s\">\"?????\"</span></div><divclass=\"crayon-linecrayon-striped-line\"id=\"crayon-5a9de2766148f471354834-42\"><spanclass=\"crayon-k\">print</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-v\">c</span><spanclass=\"crayon-sy\">[</span><spanclass=\"crayon-cn\">0</span><spanclass=\"crayon-sy\">]</span><spanclass=\"crayon-sy\">,</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-v\">c</span><spanclass=\"crayon-sy\">[</span><spanclass=\"crayon-cn\">3</span><spanclass=\"crayon-sy\">]</span><spanclass=\"crayon-sy\">[</span><spanclass=\"crayon-cn\">0</span><spanclass=\"crayon-sy\">]</span></div><divclass=\"crayon-line\"id=\"crayon-5a9de2766148f471354834-43\"><spanclass=\"crayon-k\">print</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-s\">\"?????\"</span></div><divclass=\"crayon-linecrayon-striped-line\"id=\"crayon-5a9de2766148f471354834-44\"><spanclass=\"crayon-k\">print</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-v\">d</span><spanclass=\"crayon-sy\">[</span><spanclass=\"crayon-cn\">0</span><spanclass=\"crayon-sy\">]</span><spanclass=\"crayon-sy\">,</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-v\">d</span><spanclass=\"crayon-sy\">[</span><spanclass=\"crayon-cn\">3</span><spanclass=\"crayon-sy\">]</span><spanclass=\"crayon-sy\">[</span><spanclass=\"crayon-cn\">0</span><spanclass=\"crayon-sy\">]</span></div><divclass=\"crayon-line\"id=\"crayon-5a9de2766148f471354834-45\"><spanclass=\"crayon-k\">print</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-s\">\"**##\"printa输出如下： 参考博文http://www.cnblogs.com/taijun…http://blog.csdn.net/u0115085…http://www.cnblogs.com/zxlove…1赞2收藏1评论"], "art_url": ["http://python.jobbole.com/88889/"]}
{"art_img": ["http://jbcdn2.b0.upaiyun.com/2017/11/eefc6b90a73fee96a70b63f207f9a302.png"], "art_title": ["Python NLP入门教程"], "art_create_time": ["2017/11/19"], "art_content": ["原文出处：j_hao104   本文简要介绍Python自然语言处理(NLP)，使用Python的NLTK库。NLTK是Python的自然语言处理工具包，在NLP领域中，最常使用的一个Python库。什么是NLP？简单来说，自然语言处理(NLP)就是开发能够理解人类语言的应用程序或服务。这里讨论一些自然语言处理(NLP)的实际应用例子，如语音识别、语音翻译、理解完整的句子、理解匹配词的同义词，以及生成语法正确完整句子和段落。这并不是NLP能做的所有事情。NLP实现搜索引擎:比如谷歌，Yahoo等。谷歌搜索引擎知道你是一个技术人员，所以它显示与技术相关的结果；社交网站推送:比如FacebookNewsFeed。如果NewsFeed算法知道你的兴趣是自然语言处理，就会显示相关的广告和帖子。语音引擎:比如Apple的Siri。垃圾邮件过滤:如谷歌垃圾邮件过滤器。和普通垃圾邮件过滤不同，它通过了解邮件内容里面的的深层意义，来判断是不是垃圾邮件。NLP库下面是一些开源的自然语言处理库(NLP)：Naturallanguagetoolkit(NLTK);ApacheOpenNLP;StanfordNLPsuite;GateNLPlibrary其中自然语言工具包(NLTK)是最受欢迎的自然语言处理库(NLP)，它是用Python编写的，而且背后有非常强大的社区支持。NLTK也很容易上手，实际上，它是最简单的自然语言处理(NLP)库。在这个NLP教程中，我们将使用PythonNLTK库。安装NLTK如果您使用的是Windows/Linux/Mac，您可以使用pip安装NLTK:Pythonpipinstallnltk1pipinstallnltk打开python终端导入NLTK检查NLTK是否正确安装：Pythonimportnltk1importnltk如果一切顺利，这意味着您已经成功地安装了NLTK库。首次安装了NLTK，需要通过运行以下代码来安装NLTK扩展包:Pythonimportnltknltk.download()123importnltk nltk.download()这将弹出NLTK下载窗口来选择需要安装哪些包:您可以安装所有的包，因为它们的大小都很小，所以没有什么问题。使用PythonTokenize文本首先，我们将抓取一个web页面内容，然后分析文本了解页面的内容。我们将使用urllib模块来抓取web页面:Pythonimporturllib.requestresponse=urllib.request.urlopen('http://php.net/')html=response.read()print(html)12345importurllib.request response=urllib.request.urlopen('http://php.net/')html=response.read()print(html)从打印结果中可以看到，结果包含许多需要清理的HTML标签。然后BeautifulSoup模块来清洗这样的文字:Pythonfrombs4importBeautifulSoupimporturllib.requestresponse=urllib.request.urlopen('http://php.net/')html=response.read()soup=BeautifulSoup(html,\"html5lib\")#这需要安装html5lib模块text=soup.get_text(strip=True)print(text)123456789frombs4importBeautifulSoup importurllib.requestresponse=urllib.request.urlopen('http://php.net/')html=response.read()soup=BeautifulSoup(html,\"html5lib\")#这需要安装html5lib模块text=soup.get_text(strip=True)print(text)现在我们从抓取的网页中得到了一个干净的文本。下一步，将文本转换为tokens,像这样:Pythonfrombs4importBeautifulSoupimporturllib.requestresponse=urllib.request.urlopen('http://php.net/')html=response.read()soup=BeautifulSoup(html,\"html5lib\")text=soup.get_text(strip=True)tokens=text.split()print(tokens)123456789frombs4importBeautifulSoupimporturllib.request response=urllib.request.urlopen('http://php.net/')html=response.read()soup=BeautifulSoup(html,\"html5lib\")text=soup.get_text(strip=True)tokens=text.split()print(tokens)统计词频text已经处理完毕了，现在使用PythonNLTK统计token的频率分布。可以通过调用NLTK中的FreqDist()方法实现:Pythonfrombs4importBeautifulSoupimporturllib.requestimportnltkresponse=urllib.request.urlopen('http://php.net/')html=response.read()soup=BeautifulSoup(html,\"html5lib\")text=soup.get_text(strip=True)tokens=text.split()freq=nltk.FreqDist(tokens)forkey,valinfreq.items():print(str(key)+':'+str(val))123456789101112frombs4importBeautifulSoupimporturllib.requestimportnltk response=urllib.request.urlopen('http://php.net/')html=response.read()soup=BeautifulSoup(html,\"html5lib\")text=soup.get_text(strip=True)tokens=text.split()freq=nltk.FreqDist(tokens)forkey,valinfreq.items():    print(str(key)+':'+str(val))如果搜索输出结果，可以发现最常见的token是PHP。您可以调用plot函数做出频率分布图:Pythonfreq.plot(20,cumulative=False)#需要安装matplotlib库12freq.plot(20,cumulative=False)#需要安装matplotlib库这上面这些单词。比如of,a,an等等，这些词都属于停用词。一般来说，停用词应该删除，防止它们影响分析结果。处理停用词NLTK自带了许多种语言的停用词列表，如果你获取英文停用词:Pythonfromnltk.corpusimportstopwordsstopwords.words('english')123fromnltk.corpusimportstopwords stopwords.words('english')现在，修改下代码,在绘图之前清除一些无效的token:Pythonclean_tokens=list()sr=stopwords.words('english')fortokenintokens:iftokennotinsr:clean_tokens.append(token)12345clean_tokens=list()sr=stopwords.words('english')fortokenintokens:    iftokennotinsr:        clean_tokens.append(token)最终的代码应该是这样的:Pythonfrombs4importBeautifulSoupimporturllib.requestimportnltkfromnltk.corpusimportstopwordsresponse=urllib.request.urlopen('http://php.net/')html=response.read()soup=BeautifulSoup(html,\"html5lib\")text=soup.get_text(strip=True)tokens=text.split()clean_tokens=list()sr=stopwords.words('english')fortokenintokens:ifnottokeninsr:clean_tokens.append(token)freq=nltk.FreqDist(clean_tokens)forkey,valinfreq.items():print(str(key)+':'+str(val))123456789101112131415161718frombs4importBeautifulSoupimporturllib.requestimportnltkfromnltk.corpusimportstopwords response=urllib.request.urlopen('http://php.net/')html=response.read()soup=BeautifulSoup(html,\"html5lib\")text=soup.get_text(strip=True)tokens=text.split()clean_tokens=list()sr=stopwords.words('english')fortokenintokens:    ifnottokeninsr:        clean_tokens.append(token)freq=nltk.FreqDist(clean_tokens)forkey,valinfreq.items():    print(str(key)+':'+str(val))现在再做一次词频统计图，效果会比之前好些，因为剔除了停用词:Pythonfreq.plot(20,cumulative=False)1freq.plot(20,cumulative=False)使用NLTKTokenize文本在之前我们用split方法将文本分割成tokens，现在我们使用NLTK来Tokenize文本。文本没有Tokenize之前是无法处理的，所以对文本进行Tokenize非常重要的。token化过程意味着将大的部件分割为小部件。你可以将段落tokenize成句子，将句子tokenize成单个词，NLTK分别提供了句子tokenizer和单词tokenizer。假如有这样这段文本:PythonHelloAdam,howareyou?Ihopeeverythingisgoingwell.Todayisagoodday,seeyoudude.1HelloAdam,howareyou?Ihopeeverythingisgoingwell.Todayisagoodday,seeyoudude.使用句子tokenizer将文本tokenize成句子:Pythonfromnltk.tokenizeimportsent_tokenizemytext=\"HelloAdam,howareyou?Ihopeeverythingisgoingwell.Todayisagoodday,seeyoudude.\"print(sent_tokenize(mytext))1234fromnltk.tokenizeimportsent_tokenize mytext=\"HelloAdam,howareyou?Ihopeeverythingisgoingwell.Todayisagoodday,seeyoudude.\"print(sent_tokenize(mytext))输出如下:Python['HelloAdam,howareyou?','Ihopeeverythingisgoingwell.','Todayisagoodday,seeyoudude.']1['HelloAdam,howareyou?','Ihopeeverythingisgoingwell.','Todayisagoodday,seeyoudude.']这是你可能会想，这也太简单了，不需要使用NLTK的tokenizer都可以，直接使用正则表达式来拆分句子就行，因为每个句子都有标点和空格。那么再来看下面的文本:PythonHelloMr.Adam,howareyou?Ihopeeverythingisgoingwell.Todayisagoodday,seeyoudude.1HelloMr.Adam,howareyou?Ihopeeverythingisgoingwell.Todayisagoodday,seeyoudude.这样如果使用标点符号拆分,HelloMr将会被认为是一个句子，如果使用NLTK:Pythonfromnltk.tokenizeimportsent_tokenizemytext=\"HelloMr.Adam,howareyou?Ihopeeverythingisgoingwell.Todayisagoodday,seeyoudude.\"print(sent_tokenize(mytext))1234fromnltk.tokenizeimportsent_tokenize mytext=\"HelloMr.Adam,howareyou?Ihopeeverythingisgoingwell.Todayisagoodday,seeyoudude.\"print(sent_tokenize(mytext))输出如下:Python['HelloMr.Adam,howareyou?','Ihopeeverythingisgoingwell.','Todayisagoodday,seeyoudude.']1['HelloMr.Adam,howareyou?','Ihopeeverythingisgoingwell.','Todayisagoodday,seeyoudude.']这才是正确的拆分。接下来试试单词tokenizer:Pythonfromnltk.tokenizeimportword_tokenizemytext=\"HelloMr.Adam,howareyou?Ihopeeverythingisgoingwell.Todayisagoodday,seeyoudude.\"print(word_tokenize(mytext))1234fromnltk.tokenizeimportword_tokenize mytext=\"HelloMr.Adam,howareyou?Ihopeeverythingisgoingwell.Todayisagoodday,seeyoudude.\"print(word_tokenize(mytext))输出如下:Python['Hello','Mr.','Adam',',','how','are','you','?','I','hope','everything','is','going','well','.','Today','is','a','good','day',',','see','you','dude','.']1['Hello','Mr.','Adam',',','how','are','you','?','I','hope','everything','is','going','well','.','Today','is','a','good','day',',','see','you','dude','.']Mr.这个词也没有被分开。NLTK使用的是punkt模块的PunktSentenceTokenizer，它是NLTK.tokenize的一部分。而且这个tokenizer经过训练，可以适用于多种语言。非英文TokenizeTokenize时可以指定语言:Pythonfromnltk.tokenizeimportsent_tokenizemytext=\"BonjourM.Adam,commentallez-vous?J'espèrequetoutvabien.Aujourd'huiestunbonjour.\"print(sent_tokenize(mytext,\"french\"))1234fromnltk.tokenizeimportsent_tokenize mytext=\"BonjourM.Adam,commentallez-vous?J'espèrequetoutvabien.Aujourd'huiestunbonjour.\"print(sent_tokenize(mytext,\"french\"))输出结果如下:Python['BonjourM.Adam,commentallez-vous?',\"J'espèrequetoutvabien.\",\"Aujourd'huiestunbonjour.\"]1['BonjourM.Adam,commentallez-vous?',\"J'espèrequetoutvabien.\",\"Aujourd'huiestunbonjour.\"]同义词处理使用nltk.download()安装界面，其中一个包是WordNet。WordNet是一个为自然语言处理而建立的数据库。它包括一些同义词组和一些简短的定义。您可以这样获取某个给定单词的定义和示例:Pythonfromnltk.corpusimportwordnetsyn=wordnet.synsets(\"pain\")print(syn[0].definition())print(syn[0].examples())12345fromnltk.corpusimportwordnet syn=wordnet.synsets(\"pain\")print(syn[0].definition())print(syn[0].examples())输出结果是:Pythonasymptomofsomephysicalhurtordisorder['thepatientdevelopedseverepainanddistension']12asymptomofsomephysicalhurtordisorder['thepatientdevelopedseverepainanddistension']WordNet包含了很多定义：Pythonfromnltk.corpusimportwordnetsyn=wordnet.synsets(\"NLP\")print(syn[0].definition())syn=wordnet.synsets(\"Python\")print(syn[0].definition())123456fromnltk.corpusimportwordnet syn=wordnet.synsets(\"NLP\")print(syn[0].definition())syn=wordnet.synsets(\"Python\")print(syn[0].definition())结果如下:PythonthebranchofinformationsciencethatdealswithnaturallanguageinformationlargeOldWorldboas12thebranchofinformationsciencethatdealswithnaturallanguageinformationlargeOldWorldboas可以像这样使用WordNet来获取同义词:Pythonfromnltk.corpusimportwordnetsynonyms=[]forsyninwordnet.synsets('Computer'):forlemmainsyn.lemmas():synonyms.append(lemma.name())print(synonyms)1234567fromnltk.corpusimportwordnet synonyms=[]forsyninwordnet.synsets('Computer'):    forlemmainsyn.lemmas():        synonyms.append(lemma.name())print(synonyms)输出:Python['computer','computing_machine','computing_device','data_processor','electronic_computer','information_processing_system','calculator','reckoner','figurer','estimator','computer']1['computer','computing_machine','computing_device','data_processor','electronic_computer','information_processing_system','calculator','reckoner','figurer','estimator','computer']反义词处理也可以用同样的方法得到反义词：Pythonfromnltk.corpusimportwordnetantonyms=[]forsyninwordnet.synsets(\"small\"):forlinsyn.lemmas():ifl.antonyms():antonyms.append(l.antonyms()[0].name())print(antonyms)12345678fromnltk.corpusimportwordnet antonyms=[]forsyninwordnet.synsets(\"small\"):    forlinsyn.lemmas():        ifl.antonyms():            antonyms.append(l.antonyms()[0].name())print(antonyms)输出:Python['large','big','big']1['large','big','big']词干提取语言形态学和信息检索里，词干提取是去除词缀得到词根的过程，例如working的词干为work。搜索引擎在索引页面时就会使用这种技术，所以很多人为相同的单词写出不同的版本。有很多种算法可以避免这种情况，最常见的是波特词干算法。NLTK有一个名为PorterStemmer的类，就是这个算法的实现:Pythonfromnltk.stemimportPorterStemmerstemmer=PorterStemmer()print(stemmer.stem('working'))print(stemmer.stem('worked'))12345fromnltk.stemimportPorterStemmer stemmer=PorterStemmer()print(stemmer.stem('working'))print(stemmer.stem('worked'))输出结果是:Pythonworkwork12workwork还有其他的一些词干提取算法，比如Lancaster词干算法。非英文词干提取除了英文之外，SnowballStemmer还支持13种语言。支持的语言:Pythonfromnltk.stemimportSnowballStemmerprint(SnowballStemmer.languages)123fromnltk.stemimportSnowballStemmer print(SnowballStemmer.languages)Python'danish','dutch','english','finnish','french','german','hungarian','italian','norwegian','porter','portuguese','romanian','russian','spanish','swedish'1'danish','dutch','english','finnish','french','german','hungarian','italian','norwegian','porter','portuguese','romanian','russian','spanish','swedish'你可以使用SnowballStemmer类的stem函数来提取像这样的非英文单词：Pythonfromnltk.stemimportSnowballStemmerfrench_stemmer=SnowballStemmer('french')print(french_stemmer.stem(\"Frenchword\"))12345fromnltk.stemimportSnowballStemmer french_stemmer=SnowballStemmer('french') print(french_stemmer.stem(\"Frenchword\"))单词变体还原单词变体还原类似于词干，但不同的是，变体还原的结果是一个真实的单词。不同于词干，当你试图提取某些词时，它会产生类似的词:Pythonfromnltk.stemimportPorterStemmerstemmer=PorterStemmer()print(stemmer.stem('increases'))12345fromnltk.stemimportPorterStemmer stemmer=PorterStemmer() print(stemmer.stem('increases'))结果:Pythonincreas1increas现在，如果用NLTK的WordNet来对同一个单词进行变体还原，才是正确的结果:Pythonfromnltk.stemimportWordNetLemmatizerlemmatizer=WordNetLemmatizer()print(lemmatizer.lemmatize('increases'))12345fromnltk.stemimportWordNetLemmatizer lemmatizer=WordNetLemmatizer() print(lemmatizer.lemmatize('increases'))结果:Pythonincrease1increase结果可能会是一个同义词或同一个意思的不同单词。有时候将一个单词做变体还原时，总是得到相同的词。这是因为语言的默认部分是名词。要得到动词，可以这样指定：Pythonfromnltk.stemimportWordNetLemmatizerlemmatizer=WordNetLemmatizer()print(lemmatizer.lemmatize('playing',pos=\"v\"))12345fromnltk.stemimportWordNetLemmatizer lemmatizer=WordNetLemmatizer() print(lemmatizer.lemmatize('playing',pos=\"v\"))结果:Pythonplay1play实际上，这也是一种很好的文本压缩方式，最终得到文本只有原先的50%到60%。结果还可以是动词(v)、名词(n)、形容词(a)或副词(r)：Pythonfromnltk.stemimportWordNetLemmatizerlemmatizer=WordNetLemmatizer()print(lemmatizer.lemmatize('playing',pos=\"v\"))print(lemmatizer.lemmatize('playing',pos=\"n\"))print(lemmatizer.lemmatize('playing',pos=\"a\"))print(lemmatizer.lemmatize('playing',pos=\"r\"))1234567fromnltk.stemimportWordNetLemmatizer lemmatizer=WordNetLemmatizer()print(lemmatizer.lemmatize('playing',pos=\"v\"))print(lemmatizer.lemmatize('playing',pos=\"n\"))print(lemmatizer.lemmatize('playing',pos=\"a\"))print(lemmatizer.lemmatize('playing',pos=\"r\"))输出:Pythonplayplayingplayingplaying1234playplayingplayingplaying词干和变体的区别通过下面例子来观察:Pythonfromnltk.stemimportWordNetLemmatizerfromnltk.stemimportPorterStemmerstemmer=PorterStemmer()lemmatizer=WordNetLemmatizer()print(stemmer.stem('stones'))print(stemmer.stem('speaking'))print(stemmer.stem('bedroom'))print(stemmer.stem('jokes'))print(stemmer.stem('lisa'))print(stemmer.stem('purple'))print('----------------------')print(lemmatizer.lemmatize('stones'))print(lemmatizer.lemmatize('speaking'))print(lemmatizer.lemmatize('bedroom'))print(lemmatizer.lemmatize('jokes'))print(lemmatizer.lemmatize('lisa'))print(lemmatizer.lemmatize('purple'))123456789101112131415161718fromnltk.stemimportWordNetLemmatizerfromnltk.stemimportPorterStemmer stemmer=PorterStemmer()lemmatizer=WordNetLemmatizer()print(stemmer.stem('stones'))print(stemmer.stem('speaking'))print(stemmer.stem('bedroom'))print(stemmer.stem('jokes'))print(stemmer.stem('lisa'))print(stemmer.stem('purple'))print('----------------------')print(lemmatizer.lemmatize('stones'))print(lemmatizer.lemmatize('speaking'))print(lemmatizer.lemmatize('bedroom'))print(lemmatizer.lemmatize('jokes'))print(lemmatizer.lemmatize('lisa'))print(lemmatizer.lemmatize('purple'))输出:Pythonstonespeakbedroomjokelisapurpl---------------------stonespeakingbedroomjokelisapurple12345678910111213stonespeakbedroomjokelisapurpl---------------------stonespeakingbedroomjokelisapurple词干提取不会考虑语境，这也是为什么词干提取比变体还原快且准确度低的原因。个人认为，变体还原比词干提取更好。单词变体还原返回一个真实的单词，即使它不是同一个单词，也是同义词，但至少它是一个真实存在的单词。如果你只关心速度，不在意准确度，这时你可以选用词干提取。在此NLP教程中讨论的所有步骤都只是文本预处理。在以后的文章中，将会使用PythonNLTK来实现文本分析。我已经尽量使文章通俗易懂。希望能对你有所帮助。1赞6收藏6评论"], "art_url": ["http://python.jobbole.com/88874/"]}
{"art_img": ["http://pytlab.org/assets/images/blog_img/2017-10-27-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5-%E5%B2%AD%E5%9B%9E%E5%BD%92%E5%92%8CLASSO%E5%9B%9E%E5%BD%92/bias_var.png"], "art_title": ["机器学习算法实践-岭回归和LASSO"], "art_create_time": ["2017/10/29"], "art_content": ["本文作者：伯乐在线-iPytLab。未经作者许可，禁止转载！欢迎加入伯乐在线专栏作者。前言继续线性回归的总结,本文主要介绍两种线性回归的缩减(shrinkage)方法的基础知识:岭回归(RidgeRegression)和LASSO(LeastAbsoluteShrinkageandSelectionOperator)并对其进行了Python实现。同时也对一种更为简单的向前逐步回归计算回归系数的方法进行了相应的实现。正文通过上一篇《机器学习算法实践-标准与局部加权线性回归》中标准线性回归的公式w=(X^T*X)^(-1)X^T*y中可以看出在计算回归系数的时候我们需要计算矩阵X^TX的逆，但是如果该矩阵是个奇异矩阵，则无法对其进行求解。那么什么情况下该矩阵会有奇异性呢?X本身存在线性相关关系(多重共线性),即非满秩矩阵。如果数据的特征中存在两个相关的变量，即使并不是完全线性相关，但是也会造成矩阵求逆的时候造成求解不稳定。当数据特征比数据量还要多的时候,即m<n,这时候矩阵XX是一个矮胖型的矩阵，非满秩。对于上面的两种情况，我们需要对最初的标准线性回归做一定的变化使原先无法求逆的矩阵变得非奇异，使得问题可以稳定求解。我们可以通过缩减的方式来处理这些问题例如岭回归和LASSO.中心化和标准化这里先介绍下数据的中心化和标准化，在回归问题和一些机器学习算法中通常要对原始数据进行中心化和标准化处理，也就是需要将数据的均值调整到0，标准差调整为1,计算过程很简单就是将所有数据减去平均值后再除以标准差:这样调整后的均值:调整后的标准差:之所以需要进行中心化其实就是个平移过程，将所有数据的中心平移到原点。而标准化则是使得所有数据的不同特征都有相同的尺度Scale,这样在使用梯度下降法以及其他方法优化的时候不同特征参数的影响程度就会一致了。如下图所示，可以看出得到的标准化数据在每个维度上的尺度是一致的(图片来自网络，侵删)岭回归(RidgeRegression)标准最小二乘法优化问题:也可以通过矩阵表示:得到的回归系数为:这个问题解存在且唯一的条件就是XX列满秩:rank(X)=dim(X).即使X列满秩，但是当数据特征中存在共线性，即相关性比较大的时候，会使得标准最小二乘求解不稳定, XTX的行列式接近零，计算XTX的时候误差会很大。这个时候我们需要在costfunction上添加一个惩罚项λ，称为L2正则化。这个时候的costfunction的形式就为:通过加入此惩罚项进行优化后，限制了回归系数wiwi的绝对值，数学上可以证明上式的等价形式如下:其中t为某个阈值。将岭回归系数用矩阵的形式表示:可以看到，就是通过将XTX加上一个单位矩阵是的矩阵变成非奇异矩阵并可以进行求你运算。岭回归的几何意义以两个变量为例,残差平方和可以表示为w1,w2的一个二次函数，是一个在三维空间中的抛物面，可以用等值线来表示。而限制条件w21+w22<t，相当于在二维平面的一个圆。这个时候等值线与圆相切的点便是在约束条件下的最优点，如下图所示，岭回归的一些性质当岭参数λ=0时，得到的解是最小二乘解当岭参数λ趋向更大时，岭回归系数wi趋向于0，约束项t很小岭回归的Python实现通过矩阵的形式计算ŵ ,可以很简单的实现Pythondefridge_regression(X,y,lambd=0.2):'''获取岭回归系数'''XTX=X.T*Xm,_=XTX.shapeI=np.matrix(np.eye(m))w=(XTX+lambd*I).I*X.T*yreturnw12345678defridge_regression(X,y,lambd=0.2):    '''获取岭回归系数    '''    XTX=X.T*X    m,_=XTX.shape    I=np.matrix(np.eye(m))    w=(XTX+lambd*I).I*X.T*y    returnw岭迹图可以知道求得的岭系数wi是岭参数λ的函数，不同的λλ得到不同的岭参数wi,因此我们可以增大λλ的值来得到岭回归系数的变化，以及岭参数的变化轨迹图(岭迹图),不存在奇异性时，岭迹图应稳定的逐渐趋向于0。通过岭迹图我们可以:观察较佳的λ取值观察变量是否有多重共线性绘制岭迹图上面我们通过函数ridge_regression实现了计算岭回归系数的计算，我们使用《机器学习实战》中的鲍鱼年龄的数据来进行计算并绘制不同λλ的岭参数变化的轨迹图。数据以及完整代码详见 https://github.com/PytLab/MLBox/tree/master/linear_regression选取30组不同的λλ来获取岭系数矩阵包含30个不同的岭系数。Pythondefridge_traj(X,y,ntest=30):'''获取岭轨迹矩阵'''_,n=X.shapews=np.zeros((ntest,n))foriinrange(ntest):w=ridge_regression(X,y,lambd=exp(i-10))ws[i,:]=w.Treturnws123456789defridge_traj(X,y,ntest=30):    '''获取岭轨迹矩阵    '''    _,n=X.shape    ws=np.zeros((ntest,n))    foriinrange(ntest):        w=ridge_regression(X,y,lambd=exp(i-10))        ws[i,:]=w.T    returnws绘制岭轨迹图Pythonif'__main__'==__name__:ntest=30#加载数据X,y=load_data('abalone.txt')#中心化&标准化X,y=standarize(X),standarize(y)#绘制岭轨迹ws=ridge_traj(X,y,ntest)fig=plt.figure()ax=fig.add_subplot(111)lambdas=[i-10foriinrange(ntest)]ax.plot(lambdas,ws)plt.show()12345678910111213if'__main__'==__name__:    ntest=30    #加载数据    X,y=load_data('abalone.txt')    #中心化&标准化    X,y=standarize(X),standarize(y)    #绘制岭轨迹    ws=ridge_traj(X,y,ntest)    fig=plt.figure()    ax=fig.add_subplot(111)    lambdas=[i-10foriinrange(ntest)]    ax.plot(lambdas,ws)    plt.show()上图绘制了回归系数wi与log(λ)的关系，在最左边λλ系数最小时，可以得到所有系数的原始值(与标准线性回归相同);而在右边，系数全部缩减为0,从不稳定趋于稳定；为了定量的找到最佳参数值，还需要进行交叉验证。要判断哪些变量对结果的预测最具影响力，可以观察他们的系数大小即可。LASSO岭回归限定了所有回归系数的平方和不大于tt,在使用普通最小二乘法回归的时候当两个变量具有相关性的时候，可能会使得其中一个系数是个很大正数，另一个系数是很大的负数。通过岭回归正则项的限制，可以避免这个问题。LASSO(TheLeastAbsoluteShrinkageandSelectionOperator)是另一种缩减方法，将回归系数收缩在一定的区域内。LASSO的主要思想是构造一个一阶惩罚函数获得一个精炼的模型,通过最终确定一些变量的系数为0进行特征筛选。LASSO的惩罚项为:与岭回归的不同在于，此约束条件使用了绝对值的一阶惩罚函数代替了平方和的二阶函数。虽然只是形式稍有不同，但是得到的结果却又很大差别。在LASSO中，当λλ很小的时候，一些系数会随着变为0而岭回归却很难使得某个系数恰好缩减为0.我们可以通过几何解释看到LASSO与岭回归之间的不同。LASSO的几何解释同样以两个变量为例，标准线性回归的costfunction还是可以用二维平面的等值线表示，而约束条件则与岭回归的圆不同，LASSO的约束条件可以用方形表示，如下图:相比圆，方形的顶点更容易与抛物面相交，顶点就意味着对应的很多系数为0，而岭回归中的圆上的任意一点都很容易与抛物面相交很难得到正好等于0的系数。这也就意味着，lasso起到了很好的筛选变量的作用。LASSO回归系数的计算虽然惩罚函数只是做了细微的变化，但是相比岭回归可以直接通过矩阵运算得到回归系数相比，LASSO的计算变得相对复杂。由于惩罚项中含有绝对值，此函数的导数是连续不光滑的，所以无法进行求导并使用梯度下降优化。本部分使用坐标下降发对LASSO回归系数进行计算。坐标下降法是每次选择一个维度的参数进行一维优化，然后不断的迭代对多个维度进行更新直到函数收敛。SVM对偶问题的优化算法SMO也是类似的原理，这部分的详细介绍我在之前的一篇博客中进行了整理，参考《机器学习算法实践-SVM中的SMO算法》。下面我们分别对LASSO的costfunction的两部分求解：1）RSS部分求导:2）正则项关于惩罚项的求导我们需要使用subgradient，可以参考LASSO（leastabsoluteshrinkageandselectionoperator）回归中如何用梯度下降法求解？通过上面的公式我们便可以每次选取一维进行优化并不断跌打得到最优回归系数。LASSO的Python实现根据上面代码我们实现梯度下降法并使用其获取LASSO回归系数。Pythondeflasso_regression(X,y,lambd=0.2,threshold=0.1):'''通过坐标下降(coordinatedescent)法获取LASSO回归系数'''#计算残差平方和rss=lambdaX,y,w:(y-X*w).T*(y-X*w)#初始化回归系数w.m,n=X.shapew=np.matrix(np.zeros((n,1)))r=rss(X,y,w)#使用坐标下降法优化回归系数wniter=itertools.count(1)foritinniter:forkinrange(n):#计算常量值z_k和p_kz_k=(X[:,k].T*X[:,k])[0,0]p_k=0foriinrange(m):p_k+=X[i,k]*(y[i,0]-sum([X[i,j]*w[j,0]forjinrange(n)ifj!=k]))ifp_k<-lambd/2:w_k=(p_k+lambd/2)/z_kelifp_k>lambd/2:w_k=(p_k-lambd/2)/z_kelse:w_k=0w[k,0]=w_kr_prime=rss(X,y,w)delta=abs(r_prime-r)[0,0]r=r_primeprint('Iteration:{},delta={}'.format(it,delta))ifdelta<threshold:breakreturnw1234567891011121314151617181920212223242526272829303132deflasso_regression(X,y,lambd=0.2,threshold=0.1):    '''通过坐标下降(coordinatedescent)法获取LASSO回归系数    '''    #计算残差平方和    rss=lambdaX,y,w:(y-X*w).T*(y-X*w)    #初始化回归系数w.    m,n=X.shape    w=np.matrix(np.zeros((n,1)))    r=rss(X,y,w)    #使用坐标下降法优化回归系数w    niter=itertools.count(1)    foritinniter:        forkinrange(n):            #计算常量值z_k和p_k            z_k=(X[:,k].T*X[:,k])[0,0]            p_k=0            foriinrange(m):                p_k+=X[i,k]*(y[i,0]-sum([X[i,j]*w[j,0]forjinrange(n)ifj!=k]))            ifp_k<-lambd/2:                w_k=(p_k+lambd/2)/z_k            elifp_k>lambd/2:                w_k=(p_k-lambd/2)/z_k            else:                w_k=0            w[k,0]=w_k        r_prime=rss(X,y,w)        delta=abs(r_prime-r)[0,0]        r=r_prime        print('Iteration:{},delta={}'.format(it,delta))        ifdelta<threshold:            break    returnw我们选取λ=10,收敛阈值为0.1来获取回归系数Pythonif'__main__'==__name__:X,y=load_data('abalone.txt')X,y=standarize(X),standarize(y)w=lasso_regression(X,y,lambd=10)y_prime=X*w#计算相关系数corrcoef=get_corrcoef(np.array(y.reshape(1,-1)),np.array(y_prime.reshape(1,-1)))print('Correlationcoefficient:{}'.format(corrcoef))123456789if'__main__'==__name__:    X,y=load_data('abalone.txt')    X,y=standarize(X),standarize(y)    w=lasso_regression(X,y,lambd=10)    y_prime=X*w    #计算相关系数    corrcoef=get_corrcoef(np.array(y.reshape(1,-1)),                            np.array(y_prime.reshape(1,-1)))    print('Correlationcoefficient:{}'.format(corrcoef))迭代了150步收敛到0.1，计算相对比较耗时:PythonIteration:146,delta=0.1081124857935265Iteration:147,delta=0.10565615985365184Iteration:148,delta=0.10326058648411163Iteration:149,delta=0.10092418256476776Iteration:150,delta=0.09864540659987142Correlationcoefficient:0.7255254877587117123456Iteration:146,delta=0.1081124857935265Iteration:147,delta=0.10565615985365184Iteration:148,delta=0.10326058648411163Iteration:149,delta=0.10092418256476776Iteration:150,delta=0.09864540659987142Correlationcoefficient:0.7255254877587117LASSO回归系数轨迹类似岭轨迹，我们也可以改变λλ的值得到不同的回归系数，通过作图可以看到回归系数的轨迹Pythonntest=30#绘制轨迹ws=lasso_traj(X,y,ntest)fig=plt.figure()ax=fig.add_subplot(111)lambdas=[i-10foriinrange(ntest)]ax.plot(lambdas,ws)plt.show()12345678ntest=30#绘制轨迹ws=lasso_traj(X,y,ntest)fig=plt.figure()ax=fig.add_subplot(111)lambdas=[i-10foriinrange(ntest)]ax.plot(lambdas,ws)plt.show()得到的轨迹图如下:通过与岭轨迹图进行对比发现，随着λλ的增大，系数逐渐趋近于0，但是岭回归没有系数真正为0，而lasso中不断有系数变为0.逐步向前回归LASSO计算复杂度相对较高，本部分稍微介绍一下逐步向前回归，他属于一种贪心算法，给定初始系数向量，然后不断迭代遍历每个系数，增加或减小一个很小的数，看看代价函数是否变小，如果变小就保留，如果变大就舍弃，然后不断迭代直到回归系数达到稳定。下面给出实现Pythondefstagewise_regression(X,y,eps=0.01,niter=100):'''通过向前逐步回归获取回归系数'''m,n=X.shapew=np.matrix(np.zeros((n,1)))min_error=float('inf')all_ws=np.matrix(np.zeros((niter,n)))#计算残差平方和rss=lambdaX,y,w:(y-X*w).T*(y-X*w)foriinrange(niter):print('{}:w={}'.format(i,w.T[0,:]))forjinrange(n):forsignin[-1,1]:w_test=w.copy()w_test[j,0]+=eps*signtest_error=rss(X,y,w_test)iftest_error<min_error:min_error=test_errorw=w_testall_ws[i,:]=w.Treturnall_ws123456789101112131415161718192021defstagewise_regression(X,y,eps=0.01,niter=100):    '''通过向前逐步回归获取回归系数    '''    m,n=X.shape    w=np.matrix(np.zeros((n,1)))    min_error=float('inf')    all_ws=np.matrix(np.zeros((niter,n)))    #计算残差平方和    rss=lambdaX,y,w:(y-X*w).T*(y-X*w)    foriinrange(niter):        print('{}:w={}'.format(i,w.T[0,:]))        forjinrange(n):            forsignin[-1,1]:                w_test=w.copy()                w_test[j,0]+=eps*sign                test_error=rss(X,y,w_test)                iftest_error<min_error:                    min_error=test_error                    w=w_test        all_ws[i,:]=w.T    returnall_ws我们去变化量为0.005，迭代步数为1000次，得到回归系数随着迭代次数的变化曲线:逐步回归算法的主要有点在于他可以帮助人们理解现有的模型并作出改进。当构建了一个模型后，可以运行逐步回归算法找出重要的特征，即使停止那些不重要特征的收集。总结本文介绍了两种回归中的缩减方法，岭回归和LASSO。两种回归均是在标准线性回归的基础上加上正则项来减小模型的方差。这里其实便涉及到了权衡偏差(Bias)和方差(Variance)的问题。方差针对的是模型之间的差异，即不同的训练数据得到模型的区别越大说明模型的方差越大。而偏差指的是模型预测值与样本数据之间的差异。所以为了在过拟合和欠拟合之前进行权衡，我们需要确定适当的模型复杂度来使得总误差最小。参考《MachineLearninginAction》机器学习中的Bias(偏差)，Error(误差)，和Variance(方差)有什么区别和联系？Lasso回归的坐标下降法推导数据什么时候需要做中心化和标准化处理？打赏支持我写出更多好文章，谢谢！打赏作者打赏支持我写出更多好文章，谢谢！2赞2收藏2评论关于作者：iPytLab喜欢写程序的计算化学狗，Python/C/C++/Fortran,个人博客http://pytlab.org个人主页·我的文章·22·"], "art_url": ["http://python.jobbole.com/88799/"]}
{"art_img": ["http://jbcdn2.b0.upaiyun.com/2015/02/edecceebedd0d04aa17bccba430ddcaf.jpg"], "art_title": ["使用 Python 在 Linux 上实现一键回归测试"], "art_create_time": ["2017/11/10"], "art_content": ["原文出处：IBMdeveloperWorks   从代码库迁出代码—-pexpect的使用测试人员从代码库（例如CVS）迁出代码的过程中，需要手动输入访问密码，而Python提供了Pexpect模块则能够将手动输入密码这一过程自动化。当然Pexpect也可以用来和ssh、ftp、passwd、telnet等命令行进行自动化交互。这里我们以CVS为例展示如何利用Pexpect从代码库迁出代码。清单1.用pexpect迁出代码库代码try:chkout_cmd='cvscoproject_code'#从代码库迁出project_code的内容child=pexpect.spawn(chkout_cmd)child.expect('password:')child.sendline('your-password')#请替换\"your-password\"为真实密码child.interact()except:pass#忽略迁出代码中的错误12345678try:chkout_cmd='cvscoproject_code'#从代码库迁出project_code的内容child=pexpect.spawn(chkout_cmd)child.expect('password:')child.sendline('your-password')#请替换\"your-password\"为真实密码child.interact()except:        pass#忽略迁出代码中的错误在清单1中，我们用命令”cvscoproject_code”从代码库中迁出了project_code的内容，我们也可以用该命令来更新已经迁出的代码。只需要将命令”cvsupdate”传给类pexpect.spawn()即可，详细的实现请参考代码文件。这里interact()函数是必须的，用来在交互的方式下控制该子进程。有时代码库中会存在目录不一致行情况，迁出代码会因报错终止，所以需要异常处理(try…execpt)来忽略该错误。编译代码和运行测试脚本—-subprocess的使用测试人员获取最新的代码之后，就要对源码进行编译，并且运行测试用例。Python语言提供了多种方法如os.system()/os.popen()来执行一条命令，这里我们推荐用subprocess模块来创建子进程，完成代码编译和运行测试用例。因为subprocess支持主进程和子进程的交互，同时也支持主进程和子进程是同步执行还是异步执行。由于本文中的各个功能模块有都先后依赖关系，所以全部采用的是主进程和子进程同步模式执行。编译代码清单2.用subprocess编译代码build_cmd='build_command_for_your_code'#请在这里配置编译命令build_proc=subprocess.Popen(build_cmd,stdin=None,stdout=None,stderr=None,shell=True)build_proc.wait()#等待子进程结束assert(0==build_proc.returncode)1234build_cmd='build_command_for_your_code'#请在这里配置编译命令build_proc=subprocess.Popen(build_cmd,stdin=None,stdout=None,stderr=None,shell=True)build_proc.wait()#等待子进程结束    assert(0==build_proc.returncode)在一些系统中我们编译代码采用的是脚本文件（如shell脚本），那么我们仍然可以如下命令来完成代码编译工作。清单3.用subprocess的call函数执行脚本文件subprocess.call([\"code_compile.sh\"])1subprocess.call([\"code_compile.sh\"])运行测试脚本在编译完成代码之后，我们同样可以调用subprocess.Popen来创建子进程运行测试用例。如果测试人员的测试用例已经写成了测试例脚本，我们则可以用subprocess.call()来执行测试例脚本文件，代码实现就不再赘述。有些系统会直接把详细日志输出到屏幕上，那么我们可以用重定向命令”2>&1″把屏幕输出写文件。清单4.用重定向命令把输出写文件ut_cmd='Your_unit_test_command2>&1>%s'%self.debug_log#debug_log定义在__init__函数中，用来存储详细日志1ut_cmd='Your_unit_test_command  2>&1>%s'%self.debug_log#debug_log定义在__init__函数中，用来存储详细日志测试结果存储和发布—-XML解析我们的项目采用敏捷开发，为了更好的反应敏捷开发周期，我们希望存储日志的目录名不但能够指明的具体日期，同时也能反映敏捷（迭代）开发阶段，这样相关人员在查看相应目录中的日志时，能够清楚的明白日志实在在哪个迭代周期的哪一天产生的。本文使用文件summary作为运行测试用例后生成的汇总日志，用文件log.txt用来存储详细日志。如下图所示，在共享目录SharedFiles中存储了一些列迭代周期中的日志。清单5.共享目录结构SharedFiles├──Sprint10-20130823121500│├──log.txt│└──summary├──Sprint10-20130826152715│├──log.txt│└──summary├──Sprint10-2013082816523512345678SharedFiles├──Sprint10-20130823121500│  ├──log.txt│  └──summary├──Sprint10-20130826152715│  ├──log.txt│  └──summary├──Sprint10-20130828165235为了能够让目录名反映敏捷开发周期，我们需要自己定义一个配置文件（txt或xml均可）。由于Python已经很好的支持了XML解析，并且XML文件作为配置也是当前的流行趋势。本文就以XML解析为例进行说明。本文使用的XML文件名是Sprint.xml，清单6是该xml的概要内容清单6.Sprint.xml文件结构<sprint-schedule><min-sprint>10</min-sprint><max-sprint>20</max-sprint><sprint10>20130814</sprint10><sprint11>20130828</sprint11>……<sprint19>20131218</sprint19><sprint20>20140101</sprint20></sprint-schedule>123456789<sprint-schedule><min-sprint>10</min-sprint><max-sprint>20</max-sprint><sprint10>20130814</sprint10>    <sprint11>20130828</sprint11>……<sprint19>20131218</sprint19><sprint20>20140101</sprint20>    </sprint-schedule>关于xml解析Python提供了多种方法。本文采用minidom对xml文件进行解析，清单7是相关处理代码。清单7.xml解析代码cur_date=time.strftime('%Y%m%d%H%M%S',time.localtime(time.time()))#首先获取当前系统日期xmldoc=minidom.parse(xml_file)min_num_node=xmldoc.getElementsByTagName('min-sprint')[0]min_num=int(min_num_node.firstChild.data)#解析出迭代开发周期的起始周期max_num_node=xmldoc.getElementsByTagName('max-sprint')[0]max_num=int(max_num_node.firstChild.data)#解析出迭代开发周期的终止周期cur_num=min_num#遍历所有迭代周期，取出当前迭代周期的开始时间和当前的系统时间对比，从而确定当前位于哪一个迭代周期。whilecur_num<=max_num:node_name='sprint'+str(cur_num)cur_node=xmldoc.getElementsByTagName(node_name)[0]sprint_date=cur_node.firstChild.dataifsprint_date<cur_date[0:7]:cur_num=cur_num+1else:break12345678910111213141516171819cur_date=time.strftime('%Y%m%d%H%M%S',time.localtime(time.time()))#首先获取当前系统日期xmldoc=minidom.parse(xml_file)min_num_node=xmldoc.getElementsByTagName('min-sprint')[0]min_num=int(min_num_node.firstChild.data)#解析出迭代开发周期的起始周期max_num_node=xmldoc.getElementsByTagName('max-sprint')[0]max_num=int(max_num_node.firstChild.data)#解析出迭代开发周期的终止周期    cur_num=min_num#遍历所有迭代周期，取出当前迭代周期的开始时间和当前的系统时间对比，从而确定当前位于哪一个迭代周期。whilecur_num<=max_num:node_name='sprint'+str(cur_num)cur_node=xmldoc.getElementsByTagName(node_name)[0]sprint_date=cur_node.firstChild.dataifsprint_date<cur_date[0:7]:cur_num=cur_num+1else:            break这样cur_num就指向了当前的迭代开发周期。然后，我们就可以根据当前日期和开发阶段创建对应的日志目录名了，最后把运行结果存储到该目录下，参见清单8实现。清单8.日志存储代码log_dir=self.share_dir+'/Sprint'+str(cur_num)+'-'+cur_date#share_dir为共享目录，定义在初始化函数中os.mkdir(log_dir)os.system('mv%s%s'%(self.debug_fullname,log_dir))#debug_fullname，详细日志文件名（含目录），定义在初始化函数中os.system('mv%s%s'%(self.sum_fullname,log_dir))#sum_fullname，汇总日志的全路径文件名，定义在初始化函数中1234log_dir=self.share_dir+'/Sprint'+str(cur_num)+'-'+cur_date#share_dir为共享目录，定义在初始化函数中os.mkdir(log_dir)os.system('mv%s%s'%(self.debug_fullname,log_dir))#debug_fullname，详细日志文件名（含目录），定义在初始化函数中    os.system('mv%s%s'%(self.sum_fullname,log_dir))#sum_fullname，汇总日志的全路径文件名，定义在初始化函数中关于测试结果的发布，本文并没有把测试结果以自动化的形式发送邮件，而是手动在每个开发周期结束时，群发邮件给相关人员。或者在验证失败后，通知相关的开发人员，这是由于作者所在团队项目代码提交频率不是很高。在更大型的项目中，往往需要增加自动发送邮件的功能，相关实现本文不再赘述。也谈界面设计—-getopt的使用在日常的测试过程中，我们并不是每次都要迁出代码，编译代码，运行测试用例和收集测试结果。这样就需要我们能够有选择的运行部分程序功能，例如只运行测试用例和收集结果。这里我们提供了4个运行选泽：选项1：迁出代码–>编译版本–>运行测试用例–>收集测试结果选项2：更新代码–>编译版本–>运行测试用例–>收集测试结果选项3：编译版本–>运行测试用例–>收集测试结果选项4：运行测试用例–>收集测试结果当然我们还需要提供帮助信息，以方便不熟悉该脚本实现的人员使用。python也提供了getopt模块让我们轻松实现上述功能。实现代码参见清单9清单9.命令行写解析代码try:opts,args=getopt.getopt(sys.argv[1:],'bchu',['build','checkout','help','update'])exceptgetopt.error,msg:self.usage()sys.exit(2)build_flag=0#构建选项foro,ainopts:ifoin('-h','--help'):self.usage()sys.exit()elifoin('-c','--checkout'):print\"执行操作：迁出代码-->编译版本-->运行测试用例-->收集测试结果\"build_flag=1breakelifoin('-u','--update'):print\"执行操作：更新代码-->编译版本-->运行测试用例-->收集测试结果\"build_flag=2breakelifoin('-b','--build'):print\"执行操作：编译版本-->运行测试用例-->收集测试结果\"build_flag=3breakelse:self.usage()sys.exit()if(0==build_flag):if2<=len(sys.argv):self.usage()sys.exit()raw_input('\\n按Enter键继续。。。(Ctrl+C退出)\\t')if(1==build_flag):#迁出代码，并编译代码self.checkout_code()self.build_code()elif(2==build_flag):#更新代码，并编译代码self.update_code()self.build_code()elif(3==build_flag):#编译代码self.build_code()#运行测试用例并收集运行结果self.set_python()self.run_testsuite()self.store_logs()12345678910111213141516171819202122232425262728293031323334353637383940414243444546try:  opts,args=getopt.getopt(sys.argv[1:],'bchu',['build','checkout','help','update'])    exceptgetopt.error,msg:        self.usage()        sys.exit(2)    build_flag=0#构建选项    foro,ainopts:        ifoin('-h','--help'):            self.usage()            sys.exit()        elifoin('-c','--checkout'):            print\"执行操作：迁出代码-->编译版本-->运行测试用例-->收集测试结果\"            build_flag=1            break        elifoin('-u','--update'):            print\"执行操作：更新代码-->编译版本-->运行测试用例-->收集测试结果\"            build_flag=2            break        elifoin('-b','--build'):            print\"执行操作：编译版本-->运行测试用例-->收集测试结果\"            build_flag=3            break        else:            self.usage()            sys.exit()    if(0==build_flag):        if2<=len(sys.argv):            self.usage()            sys.exit()    raw_input('\\n按Enter键继续。。。(Ctrl+C退出)\\t')            if(1==build_flag):#迁出代码，并编译代码        self.checkout_code()        self.build_code()    elif(2==build_flag):#更新代码，并编译代码        self.update_code()        self.build_code()    elif(3==build_flag):#编译代码        self.build_code()    #运行测试用例并收集运行结果    self.set_python()    self.run_testsuite()    self.store_logs()如果我们在运行的过程中想中断（如利用Ctrl+C）一键回归测试进程的执行时，有时我们会发现虽然主进程已经被终止，但子进程仍在运行。我们能否在中断主进程的同时也中断子进程呢？答案当然是肯定的，我们可以用信号处理函数捕获信号（如捕获Ctrl+C产生的中断信号），然后在显式终止对应的子进程。这里就需要我们在创建子进程的时候，先保存子进程ID，当然把子进程ID保存到初始化函数中，是个不错的选择，清单10是相关实现。清单10.信号处理代码#终止子进程的运行defhandler(self,signum,frame):if(-1!=self.subproc_id):#subproc_id定义在初始化函数中，用来存储当前子进程的IDos.killpg(self.subproc_id,signal.SIGINT)sys.exit(-1)12345#终止子进程的运行defhandler(self,signum,frame):    if(-1!=self.subproc_id):#subproc_id定义在初始化函数中，用来存储当前子进程的ID        os.killpg(self.subproc_id,signal.SIGINT)    sys.exit(-1)这里我们需要在初始化函数中注册要捕获的信号，并且创建成员变量用来保存子进程的ID，详细实现请参见清单11。基于对象的设计—-class的使用最后终于轮到class登场了，提到class我们就不能不谈构造函数（初始化函数）和析构函数。之前我们多次提到初始化函数，初始化函数允许我们定义一些变量，这些变量在整个类对象的生存周期内均有效。由于本文没有向系统申请资源，就再不定义析构函数了。清单11.初始化处理代码def__init__(self):signal.signal(signal.SIGINT,self.handler)#注册需要捕获的信号量self.myafs_dir=os.getenv('myafs')self.subproc_id=-1#子进程ID，用来在终止主进程时也同时终止子进程self.debug_log='log.txt'#存储详细运行日志的文件名self.debug_fullname=os.getcwd()+os.sep+self.debug_log#全路径文件名（假设产生在该目录下）self.sum_log='summary'#存储汇总日志的文件名self.sum_fullname=os.getcwd()+os.sep+self.sum_log#全路径文件名（假设产生在当前目录下）self.share_dir=self.utafs_dir+'/SharedFiles'#共享目录文件名123456789def__init__(self):    signal.signal(signal.SIGINT,self.handler)#注册需要捕获的信号量    self.myafs_dir=os.getenv('myafs')    self.subproc_id=-1#子进程ID，用来在终止主进程时也同时终止子进程    self.debug_log='log.txt'#存储详细运行日志的文件名    self.debug_fullname=os.getcwd()+os.sep+self.debug_log#全路径文件名（假设产生在该目录下）    self.sum_log='summary'#存储汇总日志的文件名    self.sum_fullname=os.getcwd()+os.sep+self.sum_log#全路径文件名（假设产生在当前目录下）    self.share_dir=self.utafs_dir+'/SharedFiles'#共享目录文件名通常我们不需要太关注设计风格，只要Python脚本能完成我们的测试要求即可。对于较小的脚本，几条Python指令顺序执行即可。为了模块功能复用和可读性，我们通常会把功能模块封装成函数。本文将实现的所有函数都封装到一个类中，使得该脚本更加一体化。清单12.class框架结构代码classCOneClickRegTest:#设定一些经常使用的变量，如当前工作目录，日志名称、存储路径等def__init__(self):#设定python环境变量，实现参见代码文件defset_python(self):#更新代码，实现参见代码文件defupdate_code(self):#迁出代码，实现参见第2章代码defcheckout_code(self):#编译版本，实现参见清单1代码defbuild_code(self):#运行测试集，实现参见代码文件defrun_testsuite(self):#存储运行结果，实现参见清单7和清单8代码defstore_logs(self):#信号处理，实现参见清单10代码defhandler(self,signum,frame):#脚本使用说明，实现参见代码文件defusage(self):#命令行解析以及执行对应的功能，实现参见清单9代码defmain(self):123456789101112131415161718192021222324252627282930classCOneClickRegTest:    #设定一些经常使用的变量，如当前工作目录，日志名称、存储路径等    def__init__(self):    #设定python环境变量，实现参见代码文件    defset_python(self):    #更新代码，实现参见代码文件    defupdate_code(self):    #迁出代码，实现参见第2章代码    defcheckout_code(self):        #编译版本，实现参见清单1代码    defbuild_code(self):    #运行测试集，实现参见代码文件    defrun_testsuite(self):    #存储运行结果，实现参见清单7和清单8代码    defstore_logs(self):      #信号处理，实现参见清单10代码    defhandler(self,signum,frame):    #脚本使用说明，实现参见代码文件    defusage(self):    #命令行解析以及执行对应的功能，实现参见清单9代码    defmain(self):结束语Python语言是一个易学易用的脚本语言，笔者没有多久的Python开发经验，不过其他语言有的功能在Python中大都可以找到对应的实现，这也是笔者能够在很短的时间内完成该测试脚本的原因。因此，笔者把该语言和使用该语言完成一键回归测试介绍给大家，希望对大家有所帮助。正像笔者说的其他语言有的功能在Python中大都可以找到对应的实现，同样，如果大家对某一种特定的脚本语言或者开发语言特别熟悉，也完全可以采用所熟悉的语言来完成一键回归测试的工作。下载资源示例代码(sample.zip|3KB)相关主题Python网站是关于Python所有内容的起始点，其中包括正式Python文档简明Python教程为”AByteofPython”的简体中文译本，无论您刚接触电脑还是一个有经验的程序员，本教程都有助您学习使用Python语言。Python技术手册（第2版）是一本全面介绍有关Python语言和Python程序开发专业知识的参考手册。在PyUnit网站可以下载到最新的PyUnit软件包，以及详细的用户手册。在developerWorksLinux专区寻找为Linux开发人员（包括Linux新手入门）准备的更多参考资料，查阅我们最受欢迎的文章和教程。在developerWorks上查阅所有Linux技巧和Linux教程。1赞收藏评论"], "art_url": ["http://python.jobbole.com/88842/"]}
{"art_img": ["http://jbcdn2.b0.upaiyun.com/2017/11/c60f2447af38c6a464404d473257881e.jpg"], "art_title": ["走近 Python (类比 JS )"], "art_create_time": ["2017/11/11"], "art_content": ["原文出处：牧云云   Python是一门运用很广泛的语言，自动化脚本、爬虫，甚至在深度学习领域也都有Python的身影。作为一名前端开发者，也了解ES6中的很多特性借鉴自Python(比如默认参数、解构赋值、Decorator等)，同时本文会对Python的一些用法与JS进行类比。不管是提升自己的知识广度，还是更好地迎接AI时代，Python都是一门值得学习的语言。数据类型在Python中，最常用的能够直接处理的数据类型有以下几种：数字[整数(int)、浮点型(float)、长整型(long)、复数(complex)]字符串(str)布尔值(bool)空值(None)除此之外，Python还提供了列表（list）、字典（dict）等多种数据类型，这在下文中会介绍。类型转换与类型判断与JS十分类似，python也能实现不同数据类型间的强制与隐式转换，例子如下：强制类型转换:Pythonint('3')#3str(3.14)#'3.14'float('3.14')#3.14#区别于JS只有Number一种类型，Python中数字中的不同类型也能相互强制转换float(3)#3.0bool(3)#Truebool(0)#False1234567int('3')#3str(3.14)#'3.14'float('3.14')#3.14#区别于JS只有Number一种类型，Python中数字中的不同类型也能相互强制转换float(3)#3.0bool(3)#Truebool(0)#False隐式类型转换:Python1+1.0#2.01+False#11.0+True#2.0#区别于JS的String+Number=String,py中str+int会报错1+'1'#TypeError:cannotconcatenate'str'and'int'objects123451+1.0#2.01+False#11.0+True#2.0#区别于JS的String+Number=String,py中str+int会报错1+'1'#TypeError:cannotconcatenate'str'and'int'objects此外写代码的时候经常会需要判断值的类型，可以使用python提供的type()函数获取变量的类型，或者使用isinstance(x,type)来判断x是否属于相应的type类型。Pythontype(1.3)==float#Trueisinstance('a',str)#Trueisinstance(1.3,int)#Falseisinstance(True,bool)#Trueisinstance([],list)#Trueisinstance({},dict)#True123456type(1.3)==float#Trueisinstance('a',str)#Trueisinstance(1.3,int)#Falseisinstance(True,bool)#Trueisinstance([],list)#Trueisinstance({},dict)#True有序集合类型集合是指包含一组元素的数据结构，有序集合即集合里面的元素是是按照顺序排列的，Python中的有序集合大概有以下几类：list,tuple,str,unicode。list类型Python中List类型类似于JS中的Array,PythonL=[1,2,3]printL[-1]#'3'L.append(4)#末尾添加元素printL#[1,2,3,4]L.insert(0,'hi')#指定索引位置添加元素printL#['hi',1,2,3,4]L.pop()#末尾移除元素L.pop(2)??????2???printL#['hi',1,2,3]1234567891011L=[1,2,3]printL[-1]#'3' L.append(4)#末尾添加元素printL#[1,2,3,4] L.insert(0,'hi')#指定索引位置添加元素printL#['hi',1,2,3,4] L.pop()#末尾移除元素L.pop(2)??????2???printL#['hi',1,2,3]tuple类型tuple类型是另一种有序的列表，中文翻译为“元组”。tuple和list非常类似，但是，tuple一旦创建完毕，就不能修改了。Pythont=(1,2,3)printt[0]#1t[0]=11#TypeError:'tuple'objectdoesnotsupportitemassignmentt=(1)printt#1t的结果是整数1t=(1,)#为了避免出现如上有歧义的单元素tuple，所以Python规定，单元素tuple要多加一个逗号“,”printt#(1,)123456789t=(1,2,3)printt[0]#1t[0]=11#TypeError:'tuple'objectdoesnotsupportitemassignment t=(1)printt#1  t的结果是整数1 t=(1,)#为了避免出现如上有歧义的单元素tuple，所以Python规定，单元素tuple要多加一个逗号“,”printt#(1,)无序集合类型dict类型Python中的dict类型类似于JS中的{}(最大的不同是它是没有顺序的),它有如下特点:查找速度快(无论dict有10个元素还是10万个元素，查找速度都一样)占用内存大(与list类型相反)dict中的key不能重复dict中存储的key-value序对是没有顺序的Pythond={'a':1,'b':2,'c':3}printd#{'a':1,'c':3,'b':2}可以看出打印出的序对没有按正常的顺序打出#遍历dictforkey,valueind.items():print('%s:%s'%(key,value))#a:1#c:3#b:21234567891011121314d={    'a':1,    'b':2,    'c':3} printd#{'a':1,'c':3,'b':2}  可以看出打印出的序对没有按正常的顺序打出 #遍历dictforkey,valueind.items():    print('%s:%s'%(key,value))#a:1#c:3#b:2set类型有的时候，我们只想要dict的key，不关心key对应的value，而且要保证这个集合的元素不会重复，这时，set类型就派上用场了。set类型有如下特点：set存储的元素和dict的key类似，必须是不变对象set存储的元素也是没有顺序的Pythons=set(['A','B','C','C'])prints#set(['A','C','B'])s.add('D')prints#set(['A','C','B','D'])s.remove('D')prints#set(['A','C','B'])12345678s=set(['A','B','C','C'])prints#set(['A','C','B']) s.add('D')prints#set(['A','C','B','D']) s.remove('D')prints#set(['A','C','B'])Python中的迭代在介绍完Python中的有序集合和无序集合类型后，必然存在遍历集合的for循环。但是和其它语言的标准for循环不同，Python中的所有迭代是通过for…in来完成的。以下给出一些常用的迭代demos:索引迭代：PythonL=['apple','banana','orange']forindex,nameinenumerate(L):#enumerate()函数把['apple','banana','orange']变成了类似[(0,'apple),(1,'banana'),(2,'orange')]的形式printindex,'-',name#0-apple#1-banana#2-orange1234567L=['apple','banana','orange']forindex,nameinenumerate(L):  #enumerate()函数把['apple','banana','orange']变成了类似[(0,'apple),(1,'banana'),(2,'orange')]的形式    printindex,'-',name #0-apple#1-banana#2-orange迭代dict的value:Pythond={'apple':6,'banana':8,'orange':5}printd.values()#[6,8,5]forvind.values()printv#6#8#51234567d={'apple':6,'banana':8,'orange':5}printd.values()#[6,8,5]forvind.values()    printv#6#8#5迭代dict的key和value:Pythond={'apple':6,'banana':8,'orange':5}forkey,valueind.items()printkey,':',value#apple:6#banana:8#orange:5123456d={'apple':6,'banana':8,'orange':5}forkey,valueind.items()    printkey,':',value#apple:6#banana:8#orange:5切片操作符Python提供的切片操作符类似于JS提供的原生函数slice()。有了切片操作符，大大简化了一些原来得用循环的操作。PythonL=['apple','banana','orange','pear']L[0:2]#['apple','banana']取前2个元素L[:2]#['apple','banana']如果第一个索引是0，可以省略L[:]#['apple','banana','orange','pear']只用一个:，表示从头到尾L[::2]#['apple','orange']第三个参数表示每N个取一个，这里表示从头开始，每2个元素取出一个来12345L=['apple','banana','orange','pear']L[0:2]#['apple','banana']取前2个元素L[:2]#['apple','banana']如果第一个索引是0，可以省略L[:]#['apple','banana','orange','pear']只用一个:，表示从头到尾L[::2]#['apple','orange']第三个参数表示每N个取一个，这里表示从头开始，每2个元素取出一个来列表生成器如果要生成[1×1,2×2,3×3,…,10×10]怎么做？方法一是循环：PythonL=[]forxinrange(1,11):L.append(x*x)123L=[]forxinrange(1,11):    L.append(x*x)但是循环太繁琐，而列表生成式则可以用一行语句代替循环生成上面的list：Python#把要生成的元素x*x放到前面，后面跟for循环，就可以把list创建出来[x*xforxinrange(1,11)]#[1,4,9,16,25,36,49,64,81,100]123#把要生成的元素x*x放到前面，后面跟for循环，就可以把list创建出来[x*xforxinrange(1,11)]#[1,4,9,16,25,36,49,64,81,100]列表生成式的for循环后面还可以加上if判断(类似于JS中的filter()函数)，示例如下：Python[x*xforxinrange(1,11)ifx%2==0]#[4,16,36,64,100]12[x*xforxinrange(1,11)ifx%2==0]#[4,16,36,64,100]for循环可以嵌套，因此，在列表生成式中，也可以用多层for循环来生成列表。Python[m+nformin'ABC'fornin'123']#['A1','A2','A3','B1','B2','B3','C1','C2','C3']12[m+nformin'ABC'fornin'123']#['A1','A2','A3','B1','B2','B3','C1','C2','C3']Python函数默认参数JS中ES6的默认参数正是借鉴于Python，用法如下：Pythondefgreet(name='World'):print'Hello,'+name+'.'greet()#Hello,World.greet('Python')#Hello,Python.12345defgreet(name='World'):    print'Hello,'+name+'.' greet()#Hello,World.greet('Python')#Hello,Python.可变参数类似于JS函数中自动识别传入参数的个数，Python也提供了定义可变参数，即在可变参数的名字前面带上个*号。Pythondeffn(*args):printargsfn()#()fn('a')#('a',)fn('a','b')#('a','b')123456deffn(*args):    printargs fn()  #()fn('a')#('a',)fn('a','b')#('a','b')Python解释器会把传入的一组参数组装成一个tuple传递给可变参数，因此，在函数内部，直接把变量args看成一个tuple就好了。常用高阶函数Python中常用的函数(map、reduce、filter)的作用和JS中一致，只是用法稍微不同。map函数:接收一个函数f和一个list，并通过把函数f依次作用在list的每个元素上，得到一个新的list并返回。Pythondeff(x):returnx*xprintmap(f,[1,2,3,4,5,6,7,8,9])#[1,4,9,16,25,36,49,64,81]123deff(x):    returnx*xprintmap(f,[1,2,3,4,5,6,7,8,9])#[1,4,9,16,25,36,49,64,81]reduce函数:接收一个函数f和一个list(可以接受第三个值作为初始值)，reduce()对list的每个元素反复调用函数f，并返回最终结果值。Pythondeff(x,y):returnx*yreduce(f,[1,3,5])#151234deff(x,y):    returnx*y reduce(f,[1,3,5])#15filter函数:接收一个函数f和一个list，这个函数f的作用是对每个元素进行判断，返回True或False，filter()根据判断结果自动过滤掉不符合条件的元素，返回由符合条件元素组成的新list。Pythondefis_odd(x):returnx%2==1filter(is_odd,[1,4,6,7,9,12,17])#[1,7,9,17]1234defis_odd(x):    returnx%2==1 filter(is_odd,[1,4,6,7,9,12,17])#[1,7,9,17]匿名函数和JS的匿名函数不同的地方是，Python的匿名函数中只能有一个表达式，且不能写return。拿map()函数为例：Pythonmap(lambdax:x*x,[1,2,3,4,5,6,7,8,9])#[1,4,9,16,25,36,49,64,81]1map(lambdax:x*x,[1,2,3,4,5,6,7,8,9])#[1,4,9,16,25,36,49,64,81]关键词lambda表示匿名函数，冒号前面的x表示函数参数，可以看出匿名函数lambdax:x*x实际上就是:Pythondeff(x):returnx*x12deff(x):    returnx*x闭包之前写过一些关于JS闭包的文章，比如深入浅出JavaScript之闭包（Closure）、以及读书笔记-你不知道的JavaScript(上)，Python中闭包的定义和JS中的是一致的即：内层函数引用了外层函数的变量，然后返回内层函数。下面来看下Py中闭包之for循环经典问题：Python#希望一次返回3个函数，分别计算1x1,2x2,3x3:defcount():fs=[]foriinrange(1,4):deff():returni*ifs.append(f)returnfsf1,f2,f3=count()#这种写法相当于ES6中的解构赋值printf1(),f2(),f3()#9991234567891011#希望一次返回3个函数，分别计算1x1,2x2,3x3:defcount():    fs=[]    foriinrange(1,4):        deff():            returni*i        fs.append(f)    returnfs f1,f2,f3=count()#这种写法相当于ES6中的解构赋值printf1(),f2(),f3()#999老问题了，f1(),f2(),f3()结果不应该是1,4,9吗，实际结果为什么都是9呢？原因就是当count()函数返回了3个函数时，这3个函数所引用的变量i的值已经变成了3。由于f1、f2、f3并没有被调用，所以，此时他们并未计算i*i，当f1被调用时，i已经变为3了。要正确使用闭包，就要确保引用的局部变量在函数返回后不能变。代码修改如下:方法一:可以理解为创建了一个封闭的作用域，i的值传给j之后，就和i没任何关系了。每次循环形成的闭包都存进了内存中。Pythondefcount():fs=[]foriinrange(1,4):deff(j):defg():#方法一returnj*jreturngr=f(i)fs.append(r)returnfsf1,f2,f3=count()printf1(),f2(),f3()#14912345678910111213defcount():    fs=[]    foriinrange(1,4):        deff(j):            defg():#方法一                returnj*j            returng        r=f(i)        fs.append(r)    returnfs f1,f2,f3=count()printf1(),f2(),f3()#149方法二：思路比较巧妙，用到了默认参数j在函数定义时可以获取到i的值，虽然没有用到闭包，但是和方法一有异曲同工之处。Pythondefcount():fs=[]foriinrange(1,4):deff(j=i):#方法二returnj*jfs.append(f)returnfsf1,f2,f3=count()printf1(),f2(),f3()#14912345678910defcount():    fs=[]    foriinrange(1,4):        deff(j=i):#方法二            returnj*j        fs.append(f)    returnfs f1,f2,f3=count()printf1(),f2(),f3()#149decorator装饰器ES6的语法中的decorator正是借鉴了Python的decorator。decorator本质上就是一个高阶函数，它接收一个函数作为参数，然后返回一个新函数。那装饰器的作用在哪呢？先上一段日常项目中用ts写的网关代码：Python@Post('/rider/detail')//URL路由@log()//打印日志@ResponseBodypublicasyncgetRiderBasicInfo(@RequestBody('riderId')riderId:number,@RequestBody('cityId')cityId:number,){constresult=awaitthis.riderManager.findDetail(cityId,riderId)returnresult}12345678910@Post('/rider/detail')  //URL路由@log()                  //打印日志  @ResponseBody  publicasyncgetRiderBasicInfo(    @RequestBody('riderId')riderId:number,    @RequestBody('cityId')cityId:number,  ){    constresult=awaitthis.riderManager.findDetail(cityId,riderId)    returnresult  }可以看出使用装饰器可以极大地简化代码，避免每个函数(比如日志、路由、性能检测)编写重复性代码。回到Python上，Python提供的@语法来使用decorator，@等价于f=decorate(f)。下面来看看@log()在Python中的实现:Python#我们想把调用的函数名字给打印出来@log()deffactorial(n):returnreduce(lambdax,y:x*y,range(1,n+1))printfactorial(10)#来看看@log()的定义deflog():deflog_decorator(f):deffn(x):print'调用了函数'+f.__name__+'()'returnf(x)returnfnreturnlog_decorator#结果#调用了函数factorial()#3628800123456789101112131415161718#我们想把调用的函数名字给打印出来@log()deffactorial(n):    returnreduce(lambdax,y:x*y,range(1,n+1))printfactorial(10) #来看看@log()的定义deflog():    deflog_decorator(f):        deffn(x):            print'调用了函数'+f.__name__+'()'            returnf(x)        returnfn    returnlog_decorator #结果#调用了函数factorial()#3628800class面向对象编程面向对象编程是一种程序设计范式，基本思想是：用类定义抽象类型，然后根据类的定义创建出实例。在掌握其它语言的基础上，还是比较容易理解这块知识点的，比如从下面两种写法可以看出不同语言的语言特性间竟然有如此多的共性。es6:(附：本文的主题是python，所以只是初略展示下js中类的定义以及实例的创建，为了说明写法的相似性)PythonclassPerson{constructor(name,age){this.name=namethis.age=age}}constchild1=newPerson('XiaoMing',10)12345678classPerson{    constructor(name,age){        this.name=name        this.age=age    }} constchild1=newPerson('XiaoMing',10)Python:(核心要点写在注释中)Python#定义一个Person类：根据Person类就可以造成很多child实例classPerson(object):address='Earth'#类属性(实例公有)def__init__(self,name,age):#创建实例时，__init__()方法被自动调用self.name=nameself.age=agedefget_age(self):#定义实例方法，它的第一个参数永远是self，指向调用该方法的实例本身，其他参数和普通函数是一样的returnself.agechild1=Person('XiaoMing',10)child2=Person('XiaoHong',9)printchild1.name#'XiaoMing'printchild2.get_age()#9printchild1.address#'Earth'printchild2.address#'Earth'12345678910111213141516#定义一个Person类：根据Person类就可以造成很多child实例classPerson(object):    address='Earth'#类属性(实例公有)    def__init__(self,name,age):#创建实例时，__init__()方法被自动调用        self.name=name        self.age=age    defget_age(self):#定义实例方法，它的第一个参数永远是self，指向调用该方法的实例本身，其他参数和普通函数是一样的        returnself.age child1=Person('XiaoMing',10)child2=Person('XiaoHong',9) printchild1.name#'XiaoMing'printchild2.get_age()#9printchild1.address#'Earth'printchild2.address#'Earth'继承child属于Student类，Student类属于People类，这就引出了继承:即获得了父类的方法属性后又能添加自己的方法属性。PythonclassPerson(object):def__init__(self,name,age):self.name=nameself.age=ageclassStudent(Person):def__init__(self,name,age,grade):super(Student,self).__init__(name,age)#这里也能写出Person.__init__(self,name,age)self.grade=grades=Student('XiaoMing',10,90)prints.name#'XiaoMing'prints.grade#9012345678910111213classPerson(object):    def__init__(self,name,age):        self.name=name        self.age=age classStudent(Person):    def__init__(self,name,age,grade):        super(Student,self).__init__(name,age)#这里也能写出Person.__init__(self,name,age)        self.grade=grade s=Student('XiaoMing',10,90)prints.name#'XiaoMing'prints.grade#90可以看到子类在父类的基础上又增加了grade属性。我们可以再来看看s的类型。Pythonisinstance(s,Person)isinstance(s,Student)12isinstance(s,Person)isinstance(s,Student)可以看出，Python中在一条继承链上，一个实例可以看成它本身的类型，也可以看成它父类的类型。1赞1收藏评论"], "art_url": ["http://python.jobbole.com/88850/"]}
{"art_img": ["http://jbcdn2.b0.upaiyun.com/2017/11/cf07224a24a4f2e382a74152a82bbce6.png"], "art_title": ["Jupyter 常见可视化框架选择"], "art_create_time": ["2017/11/15"], "art_content": ["原文出处：三次方根   对于以Python作为技术栈的数据科学工作者，Jupyter是不得不提的数据报告工具。可能对于R社区而言，鼎鼎大名的ggplot2是常见的可视化框架，而大家对于Python，以及Jupyter为核心的交互式报告的可个视化方案就并没有那么熟悉。本文试图比较几个常用的解决方案，方便大家选择。选择标准称述式还是命令式数据工作者使用的图的类别，常见的就三类：GIS可视化、网络可视化和统计图。因此，大多数场景下，我们并不想接触非常底层的基于点、线、面的命令，所以，选择一个好的封装的框架相当重要。当然，公认较好的封装是基于《TheGrammarofGraphics(StatisticsandComputing)》一书，R中的ggplot2基本上就是一个很好的实现。我们基本上可以像用「自然语言」（NaturalLanguage）一样使用这些绘图命令。我们姑且采用计算机科学领域的「陈述式」来表达这种绘图方式。相反，有时候，以下情形时，我们可能对于这种绘图命令可能并不在意：出图相当简单，要求绘制速度，一般大的框架较重（当然只是相对而言）；想要对细节做非常详尽的微调，一般大框架在微调方面会相对复杂或者退缩成一句句命令；是统计作图可视化的创新者，想要尝试做出新的可视化实践。这些情况下，显然，简单操作式并提供底层绘制命令的框架更让人愉快，与上面类似，我们借用「命令式」描述这类框架。是否交互与传统的交付静态图标不同，基于Web端的Jupter的一大好处就是可以绘制交互的图标（最近的RNotebook也有实现），因此，是否选择交互式，也是一个需要权衡的地方。交互图的优势：可以提供更多的数据维度和信息；用户端可以做更多诸如放大、选取、转存的操作；可以交付BI工程师相应的JavaScript代码用以工程化；效果上比较炫酷，考虑到报告接受者的特征可以选择。非交互图的优势：报告文件直接导出成静态文件时相对问题，不会因为转换而损失信息；图片可以与报告分离，必要时作为其他工作的成果；不需要在运行Notebook时花很多世界载入各类前端框架。是非内核交互Jupyter上大多数命令通过以下方式获取数据，而大多数绘图方式事实上只是通过Notebook内的代码在Notebook与内核交互后展示出输出结果。但ipywidgets框架则可以实现CodeCell中的代码与Notebook中的前端控件（比如按钮等）绑定来进行操作内核，提供不同的绘图结果，甚至某些绘图框架的每个元素都可以直接和内核进行交互。 用这些框架，可以搭建更复杂的Notebook的可视化应用，但缺点是因为基于内核，所以在呈递、展示报告时如果使用离线文件时，这些交互就会无效。框架罗列matplotlib最家喻户晓的绘图框架是matplotlib，它提供了几乎所有python内静态绘图框架的底层命令。如果按照上面对可视化框架的分法，matplotlib属于非交互式的的「命令式」作图框架。Python##matplotlib代码示例frompylabimport*X=np.linspace(-np.pi,np.pi,256,endpoint=True)C,S=np.cos(X),np.sin(X)plot(X,C)plot(X,S)show()12345678910##matplotlib代码示例frompylabimport* X=np.linspace(-np.pi,np.pi,256,endpoint=True)C,S=np.cos(X),np.sin(X) plot(X,C)plot(X,S) show()优点是相对较快，底层操作较多。缺点是语言繁琐，内置默认风格不够美观。matplotlib在jupyter中需要一些配置，可以展现更好的效果，详情参见这篇文章.ggplot和plotnine值得一说，对于R迁移过来的人来说，ggplot和plotnine简直是福音，基本克隆了ggplot2所有语法。横向比较的话，plotnine的效果更好。这两个绘图包的底层依旧是matplotlib，因此，在引用时别忘了使用%matplotlibinline语句。值得一说的是plotnine也移植了ggplot2中良好的配置语法和逻辑。Python##plotnine示例(ggplot(mtcars,aes('wt','mpg',color='factor(gear)'))+geom_point()+stat_smooth(method='lm')+facet_wrap('~gear'))12345##plotnine示例(ggplot(mtcars,aes('wt','mpg',color='factor(gear)'))+geom_point()+stat_smooth(method='lm')+facet_wrap('~gear'))Seabornseaborn准确上说属于matplotlib的扩展包，在其上做了许多非常有用的封装，基本上可以满足大部分统计作图的需求，以matplotlib+seaborn基本可以满足大部分业务场景，语法也更加「陈述式」。缺点是封装较高，基本上API不提供的图就完全不可绘制，对于各类图的拼合也不适合；此外配置语句语法又回归「命令式」，相对复杂且不一致。Python##seaborn示例importseabornassns;sns.set(color_codes=True)iris=sns.load_dataset(\"iris\")species=iris.pop(\"species\")g=sns.clustermap(iris)12345##seaborn示例importseabornassns;sns.set(color_codes=True)iris=sns.load_dataset(\"iris\")species=iris.pop(\"species\")g=sns.clustermap(iris)plotlyplotly是跨平台JavaScript交互式绘图包，由于开发者的核心是javascript，所以整个语法类似于写json配置，语法特质也介于「陈述式」和「命令式」之间，无服务版本是免费的。有点是学习成本不高，可以很快将语句移植到javascript版本；缺点是语言相对繁琐。Python##plotly示例importplotly.plotlyaspyimportplotly.graph_objsasgo#Adddatamonth=['January','February','March','April','May','June','July','August','September','October','November','December']high_2000=[32.5,37.6,49.9,53.0,69.1,75.4,76.5,76.6,70.7,60.6,45.1,29.3]low_2000=[13.8,22.3,32.5,37.2,49.9,56.1,57.7,58.3,51.2,42.8,31.6,15.9]high_2007=[36.5,26.6,43.6,52.3,71.5,81.4,80.5,82.2,76.0,67.3,46.1,35.0]low_2007=[23.6,14.0,27.0,36.8,47.6,57.7,58.9,61.2,53.3,48.5,31.0,23.6]high_2014=[28.8,28.5,37.0,56.8,69.7,79.7,78.5,77.8,74.1,62.6,45.3,39.9]low_2014=[12.7,14.3,18.6,35.5,49.9,58.0,60.0,58.6,51.7,45.2,32.2,29.1]#Createandstyletracestrace0=go.Scatter(x=month,y=high_2014,name='High2014',line=dict(color=('rgb(205,12,24)'),width=4))trace1=go.Scatter(x=month,y=low_2014,name='Low2014',line=dict(color=('rgb(22,96,167)'),width=4,))trace2=go.Scatter(x=month,y=high_2007,name='High2007',line=dict(color=('rgb(205,12,24)'),width=4,dash='dash')#dashoptionsinclude'dash','dot',and'dashdot')trace3=go.Scatter(x=month,y=low_2007,name='Low2007',line=dict(color=('rgb(22,96,167)'),width=4,dash='dash'))trace4=go.Scatter(x=month,y=high_2000,name='High2000',line=dict(color=('rgb(205,12,24)'),width=4,dash='dot'))trace5=go.Scatter(x=month,y=low_2000,name='Low2000',line=dict(color=('rgb(22,96,167)'),width=4,dash='dot'))data=[trace0,trace1,trace2,trace3,trace4,trace5]#Editthelayoutlayout=dict(title='AverageHighandLowTemperaturesinNewYork',xaxis=dict(title='Month'),yaxis=dict(title='Temperature(degreesF)'),)fig=dict(data=data,layout=layout)py.iplot(fig,filename='styled-line')1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677##plotly示例importplotly.plotlyaspyimportplotly.graph_objsasgo #Adddatamonth=['January','February','March','April','May','June','July',        'August','September','October','November','December']high_2000=[32.5,37.6,49.9,53.0,69.1,75.4,76.5,76.6,70.7,60.6,45.1,29.3]low_2000=[13.8,22.3,32.5,37.2,49.9,56.1,57.7,58.3,51.2,42.8,31.6,15.9]high_2007=[36.5,26.6,43.6,52.3,71.5,81.4,80.5,82.2,76.0,67.3,46.1,35.0]low_2007=[23.6,14.0,27.0,36.8,47.6,57.7,58.9,61.2,53.3,48.5,31.0,23.6]high_2014=[28.8,28.5,37.0,56.8,69.7,79.7,78.5,77.8,74.1,62.6,45.3,39.9]low_2014=[12.7,14.3,18.6,35.5,49.9,58.0,60.0,58.6,51.7,45.2,32.2,29.1] #Createandstyletracestrace0=go.Scatter(    x=month,    y=high_2014,    name='High2014',    line=dict(        color=('rgb(205,12,24)'),        width=4))trace1=go.Scatter(    x=month,    y=low_2014,    name='Low2014',    line=dict(        color=('rgb(22,96,167)'),        width=4,))trace2=go.Scatter(    x=month,    y=high_2007,    name='High2007',    line=dict(        color=('rgb(205,12,24)'),        width=4,        dash='dash')#dashoptionsinclude'dash','dot',and'dashdot')trace3=go.Scatter(    x=month,    y=low_2007,    name='Low2007',    line=dict(        color=('rgb(22,96,167)'),        width=4,        dash='dash'))trace4=go.Scatter(    x=month,    y=high_2000,    name='High2000',    line=dict(        color=('rgb(205,12,24)'),        width=4,        dash='dot'))trace5=go.Scatter(    x=month,    y=low_2000,    name='Low2000',    line=dict(        color=('rgb(22,96,167)'),        width=4,        dash='dot'))data=[trace0,trace1,trace2,trace3,trace4,trace5] #Editthelayoutlayout=dict(title='AverageHighandLowTemperaturesinNewYork',              xaxis=dict(title='Month'),              yaxis=dict(title='Temperature(degreesF)'),              ) fig=dict(data=data,layout=layout)py.iplot(fig,filename='styled-line')注意：此框架在jupyter中使用需要使用init_notebook_mode()加载JavaScript框架。bokehbokeh是pydata维护的比较具有潜力的开源交互可视化框架。值得一说的是，该框架同时提供底层语句和「陈述式」绘图命令。相对来说语法也比较清楚，但其配置语句依旧有很多可视化框架的问题，就是与「陈述式」命令不符，没有合理的结构。此外，一些常见的交互效果都是以底层命令的方式使用的，因此如果要快速实现Dashboard或者作图时就显得较为不便了。Python##Bokeh示例importnumpyasnpimportscipy.specialfrombokeh.layoutsimportgridplotfrombokeh.plottingimportfigure,show,output_filep1=figure(title=\"NormalDistribution(μ=0,σ=0.5)\",tools=\"save\",background_fill_color=\"#E8DDCB\")mu,sigma=0,0.5measured=np.random.normal(mu,sigma,1000)hist,edges=np.histogram(measured,density=True,bins=50)x=np.linspace(-2,2,1000)pdf=1/(sigma*np.sqrt(2*np.pi))*np.exp(-(x-mu)**2/(2*sigma**2))cdf=(1+scipy.special.erf((x-mu)/np.sqrt(2*sigma**2)))/2p1.quad(top=hist,bottom=0,left=edges[:-1],right=edges[1:],fill_color=\"#036564\",line_color=\"#033649\")p1.line(x,pdf,line_color=\"#D95B43\",line_width=8,alpha=0.7,legend=\"PDF\")p1.line(x,cdf,line_color=\"white\",line_width=2,alpha=0.7,legend=\"CDF\")p1.legend.location=\"center_right\"p1.legend.background_fill_color=\"darkgrey\"p1.xaxis.axis_label='x'p1.yaxis.axis_label='Pr(x)'p2=figure(title=\"LogNormalDistribution(μ=0,σ=0.5)\",tools=\"save\",background_fill_color=\"#E8DDCB\")mu,sigma=0,0.5measured=np.random.lognormal(mu,sigma,1000)hist,edges=np.histogram(measured,density=True,bins=50)x=np.linspace(0.0001,8.0,1000)pdf=1/(x*sigma*np.sqrt(2*np.pi))*np.exp(-(np.log(x)-mu)**2/(2*sigma**2))cdf=(1+scipy.special.erf((np.log(x)-mu)/(np.sqrt(2)*sigma)))/2p2.quad(top=hist,bottom=0,left=edges[:-1],right=edges[1:],fill_color=\"#036564\",line_color=\"#033649\")p2.line(x,pdf,line_color=\"#D95B43\",line_width=8,alpha=0.7,legend=\"PDF\")p2.line(x,cdf,line_color=\"white\",line_width=2,alpha=0.7,legend=\"CDF\")p2.legend.location=\"center_right\"p2.legend.background_fill_color=\"darkgrey\"p2.xaxis.axis_label='x'p2.yaxis.axis_label='Pr(x)'p3=figure(title=\"GammaDistribution(k=1,θ=2)\",tools=\"save\",background_fill_color=\"#E8DDCB\")k,theta=1.0,2.0measured=np.random.gamma(k,theta,1000)hist,edges=np.histogram(measured,density=True,bins=50)x=np.linspace(0.0001,20.0,1000)pdf=x**(k-1)*np.exp(-x/theta)/(theta**k*scipy.special.gamma(k))cdf=scipy.special.gammainc(k,x/theta)/scipy.special.gamma(k)p3.quad(top=hist,bottom=0,left=edges[:-1],right=edges[1:],fill_color=\"#036564\",line_color=\"#033649\")p3.line(x,pdf,line_color=\"#D95B43\",line_width=8,alpha=0.7,legend=\"PDF\")p3.line(x,cdf,line_color=\"white\",line_width=2,alpha=0.7,legend=\"CDF\")p3.legend.location=\"center_right\"p3.legend.background_fill_color=\"darkgrey\"p3.xaxis.axis_label='x'p3.yaxis.axis_label='Pr(x)'p4=figure(title=\"WeibullDistribution(λ=1,k=1.25)\",tools=\"save\",background_fill_color=\"#E8DDCB\")lam,k=1,1.25measured=lam*(-np.log(np.random.uniform(0,1,1000)))**(1/k)hist,edges=np.histogram(measured,density=True,bins=50)x=np.linspace(0.0001,8,1000)pdf=(k/lam)*(x/lam)**(k-1)*np.exp(-(x/lam)**k)cdf=1-np.exp(-(x/lam)**k)p4.quad(top=hist,bottom=0,left=edges[:-1],right=edges[1:],fill_color=\"#036564\",line_color=\"#033649\")p4.line(x,pdf,line_color=\"#D95B43\",line_width=8,alpha=0.7,legend=\"PDF\")p4.line(x,cdf,line_color=\"white\",line_width=2,alpha=0.7,legend=\"CDF\")p4.legend.location=\"center_right\"p4.legend.background_fill_color=\"darkgrey\"p4.xaxis.axis_label='x'p4.yaxis.axis_label='Pr(x)'output_file('histogram.html',title=\"histogram.pyexample\")show(gridplot(p1,p2,p3,p4,ncols=2,plot_width=400,plot_height=400,toolbar_location=None))123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106##Bokeh示例importnumpyasnpimportscipy.special frombokeh.layoutsimportgridplotfrombokeh.plottingimportfigure,show,output_file p1=figure(title=\"NormalDistribution(μ=0,σ=0.5)\",tools=\"save\",            background_fill_color=\"#E8DDCB\") mu,sigma=0,0.5 measured=np.random.normal(mu,sigma,1000)hist,edges=np.histogram(measured,density=True,bins=50) x=np.linspace(-2,2,1000)pdf=1/(sigma*np.sqrt(2*np.pi))*np.exp(-(x-mu)**2/(2*sigma**2))cdf=(1+scipy.special.erf((x-mu)/np.sqrt(2*sigma**2)))/2 p1.quad(top=hist,bottom=0,left=edges[:-1],right=edges[1:],        fill_color=\"#036564\",line_color=\"#033649\")p1.line(x,pdf,line_color=\"#D95B43\",line_width=8,alpha=0.7,legend=\"PDF\")p1.line(x,cdf,line_color=\"white\",line_width=2,alpha=0.7,legend=\"CDF\") p1.legend.location=\"center_right\"p1.legend.background_fill_color=\"darkgrey\"p1.xaxis.axis_label='x'p1.yaxis.axis_label='Pr(x)'   p2=figure(title=\"LogNormalDistribution(μ=0,σ=0.5)\",tools=\"save\",            background_fill_color=\"#E8DDCB\") mu,sigma=0,0.5 measured=np.random.lognormal(mu,sigma,1000)hist,edges=np.histogram(measured,density=True,bins=50) x=np.linspace(0.0001,8.0,1000)pdf=1/(x*sigma*np.sqrt(2*np.pi))*np.exp(-(np.log(x)-mu)**2/(2*sigma**2))cdf=(1+scipy.special.erf((np.log(x)-mu)/(np.sqrt(2)*sigma)))/2 p2.quad(top=hist,bottom=0,left=edges[:-1],right=edges[1:],        fill_color=\"#036564\",line_color=\"#033649\")p2.line(x,pdf,line_color=\"#D95B43\",line_width=8,alpha=0.7,legend=\"PDF\")p2.line(x,cdf,line_color=\"white\",line_width=2,alpha=0.7,legend=\"CDF\") p2.legend.location=\"center_right\"p2.legend.background_fill_color=\"darkgrey\"p2.xaxis.axis_label='x'p2.yaxis.axis_label='Pr(x)'   p3=figure(title=\"GammaDistribution(k=1,θ=2)\",tools=\"save\",            background_fill_color=\"#E8DDCB\") k,theta=1.0,2.0 measured=np.random.gamma(k,theta,1000)hist,edges=np.histogram(measured,density=True,bins=50) x=np.linspace(0.0001,20.0,1000)pdf=x**(k-1)*np.exp(-x/theta)/(theta**k*scipy.special.gamma(k))cdf=scipy.special.gammainc(k,x/theta)/scipy.special.gamma(k) p3.quad(top=hist,bottom=0,left=edges[:-1],right=edges[1:],        fill_color=\"#036564\",line_color=\"#033649\")p3.line(x,pdf,line_color=\"#D95B43\",line_width=8,alpha=0.7,legend=\"PDF\")p3.line(x,cdf,line_color=\"white\",line_width=2,alpha=0.7,legend=\"CDF\") p3.legend.location=\"center_right\"p3.legend.background_fill_color=\"darkgrey\"p3.xaxis.axis_label='x'p3.yaxis.axis_label='Pr(x)'   p4=figure(title=\"WeibullDistribution(λ=1,k=1.25)\",tools=\"save\",            background_fill_color=\"#E8DDCB\") lam,k=1,1.25 measured=lam*(-np.log(np.random.uniform(0,1,1000)))**(1/k)hist,edges=np.histogram(measured,density=True,bins=50) x=np.linspace(0.0001,8,1000)pdf=(k/lam)*(x/lam)**(k-1)*np.exp(-(x/lam)**k)cdf=1-np.exp(-(x/lam)**k) p4.quad(top=hist,bottom=0,left=edges[:-1],right=edges[1:],      fill_color=\"#036564\",line_color=\"#033649\")p4.line(x,pdf,line_color=\"#D95B43\",line_width=8,alpha=0.7,legend=\"PDF\")p4.line(x,cdf,line_color=\"white\",line_width=2,alpha=0.7,legend=\"CDF\") p4.legend.location=\"center_right\"p4.legend.background_fill_color=\"darkgrey\"p4.xaxis.axis_label='x'p4.yaxis.axis_label='Pr(x)'   output_file('histogram.html',title=\"histogram.pyexample\") show(gridplot(p1,p2,p3,p4,ncols=2,plot_width=400,plot_height=400,toolbar_location=None))bqplotbqplot是基于ipywidgets和d3.js组合发展的内核交互式的可视化框架。语法上采用了和matplotlib大致一致的语法已经相对封装较高的「陈述式语法」。优点是直接和内核交互，可以使用大量控件来实现更多的图像处理，缺点也是直接的，离线文档则不会显示任何图案、控件也都失效。Python##bqplot示例importnumpyasnpfromIPython.displayimportdisplayfrombqplotimport(OrdinalScale,LinearScale,Bars,Lines,Axis,Figure)size=20np.random.seed(0)x_data=np.arange(size)x_ord=OrdinalScale()y_sc=LinearScale()bar=Bars(x=x_data,y=np.random.randn(2,size),scales={'x':x_ord,'y':y_sc},type='stacked')line=Lines(x=x_data,y=np.random.randn(size),scales={'x':x_ord,'y':y_sc},stroke_width=3,colors=['red'],display_legend=True,labels=['Linechart'])ax_x=Axis(scale=x_ord,grid_lines='solid',label='X')ax_y=Axis(scale=y_sc,orientation='vertical',tick_format='0.2f',grid_lines='solid',label='Y')Figure(marks=[bar,line],axes=[ax_x,ax_y],title='APIExample',legend_location='bottom-right')1234567891011121314151617181920212223242526##bqplot示例importnumpyasnpfromIPython.displayimportdisplayfrombqplotimport(    OrdinalScale,LinearScale,Bars,Lines,Axis,Figure) size=20np.random.seed(0) x_data=np.arange(size) x_ord=OrdinalScale()y_sc=LinearScale() bar=Bars(x=x_data,y=np.random.randn(2,size),scales={'x':x_ord,'y':y_sc},type='stacked')line=Lines(x=x_data,y=np.random.randn(size),scales={'x':x_ord,'y':y_sc},            stroke_width=3,colors=['red'],display_legend=True,labels=['Linechart']) ax_x=Axis(scale=x_ord,grid_lines='solid',label='X')ax_y=Axis(scale=y_sc,orientation='vertical',tick_format='0.2f',            grid_lines='solid',label='Y') Figure(marks=[bar,line],axes=[ax_x,ax_y],title='APIExample',      legend_location='bottom-right')其他特殊需求的作图除了统计作图，网络可视化和GIS可视化也是很常用的，在此只做一个简单的罗列：GIS类：gmap：交互，使用googlemaps接口ipyleaflet：交互，使用leaflet接口网络类：networkx：底层为matplotlibplotly总结底层实现交互方式语法语言结构备注推荐程度matplotlib–无命令式底层语言可以实现复杂底层操作★★★gglotmatplotlib无陈述式类ggplot2建议选择plotnine★★plotninematplotlib无陈述式类ggplot2完全移植ggplot2★★★★★seabornmatplotlib无陈述式高级语言有很多有用的统计图类的封装；但不适合做图拼装★★★★★plotlyplotly.js前端交互介于命令式和陈述式之间类似JavaScript语法类似于json配置★★★★bokeh–前端交互命令、陈述式同时有底层语言和高级语言社区具有潜力★★★bqplotd3.js内核交互命令、陈述式有类似matplotlib底层语言，已经封装好的高级语言内核交互★★★★1赞5收藏2评论"], "art_url": ["http://python.jobbole.com/88862/"]}
{"art_img": ["http://jbcdn2.b0.upaiyun.com/2015/02/edecceebedd0d04aa17bccba430ddcaf.jpg"], "art_title": ["Python数据分析 - Numpy"], "art_create_time": ["2017/10/21"], "art_content": ["原文出处：by777   前言NUMPY（以下简称NP）是Python数据分析必不可少的第三方库，np的出现一定程度上解决了Python运算性能不佳的问题，同时提供了更加精确的数据类型。如今，np被Python其它科学计算包作为基础包，已成为Python数据分析的基础，可以说，NP是SciPy、Pandas等数据处理或科学计算库最基本的函数功能库。因此，理解np的数据类型对python数据分析十分有帮助。下面，本文将介绍Np的常用操作和基本数据类型。NP提供了以下重点功能。一个强大的N维数组对象ndarray广功能函数整合C/C++/Fortran代码的工具提供了线性代数、傅里叶变换、随机数生成的相关功能为了更加直观的了解Np的强大与作用，我们先看作用再看方法：使用NUMPY操作数据集在操作数据之前，我们先来理解什么是维度：什么是维度维度是一组数据的组织形式，不同数据维度可能表示不同的含义。一维数据由对等关系的有序或无序数据构成，采用线性方式组织，可以用数组表示。通俗来讲，1，2，3，4这么一行数据就可以称之为一维数据，但如果我们再对其折叠：1，2，3，4那么他就成为了二维数据，又可以称之为矩阵。什么是数据集数据集，顾名思义就是数据的集合，是用以训练程序的数据集合，一般是二维或者多维数表。如果我们想自己手工新建一个数据集，可以直接新建一个文本文件，只要有恰当的数据，都可以称之为数据集：Python城市,环比,同比,定基北京,100.1,100.2,100.3上海,111.1,111.2,111.3南京,133.0,133.3,133.41234城市,环比,同比,定基北京,100.1,100.2,100.3上海,111.1,111.2,111.3南京,133.0,133.3,133.4比如这样，我们就可以称上面的文件称之为数据集。我们还注意到，上面数据是使用逗号作为分隔符分隔数据的，它简单描述了数据的内容和含义，并使用半角逗号作为分隔符。像这样，用逗号分隔的数据集就称之为CSV（Comma-SeparatedValue,逗号分隔值）数据集，它是一种常见的文件格式，用来存储批量的数据。它就像一张excel表，用来存储简单结构的数据。怎么样，数据集的概念是否特别简单呢？生成数据集数据集是一个简单的概念，但每次使用手工的方式去写毕竟不方便，所以，我们可以使用np的内置函数来生成数据集：Pythonnp.savetxt(frame,array,fmt='%.18e\",delimiter=None)1np.savetxt(frame,array,fmt='%.18e\",delimiter=None)frame：文件、字符串、或产生器的名字，可以是.gz，.bz2的压缩文件arrray：存入文件的NP的数组fmt(format):写入文件的格式，如%d,%.2f,%.18e(默认，科学计数法保留18位)delemiter:分割字符串，默认是任何空格。我们可以这样写下代码：Pythona=np.arange(20).reshape(4,5)np.savetxt('demo.csv',a,fmt='%d',delimiter=',')12a=np.arange(20).reshape(4,5)np.savetxt('demo.csv',a,fmt='%d',delimiter=',')这样，我们就会在当前的工作目录下发现一个新的demo.csv，用记事本打开，里面是一个4*5的矩阵，元素0~19。读取数据集既然生成，那就可以读取，同样使用np：Pythonnp.loadtxt(frame,dtype=np.float,delimiter=None,inpack=False)1np.loadtxt(frame,dtype=np.float,delimiter=None,inpack=False)frame:指定读入的文件来源dtype:数据类型，默认为np.float。delimiter:分割字符串unpack：默认为False读入文件写入一个数组，如果为True，读入属性将分别写入不同变量同样的我们只需要写下代码：Pythonnp.loadtxt(\"demo.csv\",delimiter=\",\")1np.loadtxt(\"demo.csv\",delimiter=\",\")就可以查看到我们先前写入的数组a。CSV文件的局限可以发现，CSV文件只能有效存储和读取一维和二维数组，因为更高的维度无法更直观的文本下显现出来，这时，更加灵活的存取方式就呼之欲出了，但讲之前先卖个关子，再介绍一个不太常用的方法：tofile：对于NP中的ndarray数组，我们可以使用NP中的tofile方法。Pythona.tofile(frame,sep='',format='%d')1a.tofile(frame,sep='',format='%d')frame:文件，字符串数据分割字符串，如果不写，将使用二进制文件存储format：写入数据的格式同样，我们只需要命令：Pythonimportnumpyasnpa=np.arange(100).reshape(5,10,2)a.tofile(\"a.dat\",sep=',',format='%d')123importnumpyasnpa=np.arange(100).reshape(5,10,2)a.tofile(\"a.dat\",sep=',',format='%d')就可以生成新的CSV数据集。此时，我们如果打开a.dat文件，我们可以看到数组1,2,3……99。但是与CSV不同，这个文件并没有包含数字的维度信息，他只是将数组所有元素逐一的列出。而且如果我们不指定sep，将保存为二进制文件，虽然对人不可读，但将占用更小的空间。既然tofile可以保存文本文件，那么也很容易猜到对应的fromfile可以还原这些信息。Pythonnp.fromfile(frame,dtype=float,count=-1,sep='')1np.fromfile(frame,dtype=float,count=-1,sep='')frame：文件dtype：读取元素使用的数据类型，默认为floatcount：读文件的个数，默认-1，读取全部sep:数据分割字符串，如果是空串，写入文件为二进制。如果我们想要重新恢复数据的维度信息，我们需要重新使用reshape来恢复维度信息：Pythonc=np.fromfile(\"b.dat\",sep=',',dtype=np.int).reshape(5,10,2)1c=np.fromfile(\"b.dat\",sep=',',dtype=np.int).reshape(5,10,2)不得不说，当我看到这个方法时感觉这两个真是蠢爆了，使用savetxt/loadtxt至少还能保存个二维信息，而使用了tofile/fromfile方法居然把数被伸展为一维的，然后自己记住维度信息(╯‵□′)╯︵┻━┻。因此，为了保存更复杂的数据类型，二维以上的数据信息，save/load函数成功解决了这个问题：（为了方便，两个函数就放到一起了）保存/读取高维度数据Pythonnp.save(frame,array)或np.savez(fname,array)(压缩)+frame：文件名，以.npy为扩展名，压缩扩展名为.npz+array：数组变量np.load(fname)1234np.save(frame,array)或np.savez(fname,array)(压缩)+frame：文件名，以.npy为扩展名，压缩扩展名为.npz+array：数组变量np.load(fname)Demo:Pythona=np.arange(100).reshape(5,10,2)np.save(\"a.npy\",a)b=np.load(\"a.npy\")123a=np.arange(100).reshape(5,10,2)np.save(\"a.npy\",a)b=np.load(\"a.npy\")附录附录中提供NP的常用方法及注释，做查询用。np数组定义Python>>>lst=[[1,3,5],[2,4,6]]>>>np_lst=np.array(lst,dtype=np.float)>>>print(np_lst.shape)#返回数组的行列>>>print(np_lst.ndim)#返回数组的维数>>>print(np_lst.dtype)#返回数据类型，float默认为64>>>print(np_lst.itemsize)#np.array每个元素的大小，float64占8个字节>>>print(np_lst.size)#大小，6个元素(2,3)2float6486123456789101112>>>lst=[[1,3,5],[2,4,6]]>>>np_lst=np.array(lst,dtype=np.float)>>>print(np_lst.shape)#返回数组的行列>>>print(np_lst.ndim)#返回数组的维数>>>print(np_lst.dtype)#返回数据类型，float默认为64>>>print(np_lst.itemsize)#np.array每个元素的大小，float64占8个字节>>>print(np_lst.size)#大小，6个元素(2,3)2float6486初始化数组Python>>>print(np.zeros([2,4])#初始化一个2行4列的数组>>>print(np.ones([2,4])[[0.0.0.0.][0.0.0.0.]][[1.1.1.1.][1.1.1.1.]]123456>>>print(np.zeros([2,4])#初始化一个2行4列的数组>>>print(np.ones([2,4])[[0.  0.  0.  0.][0.  0.  0.  0.]][[1.  1.  1.  1.][1.  1.  1.  1.]]随机序列Python>>>print(np.random.rand(2,4))#将生成一个处于0~1之间2行4列的随机数序列（不加参数将只返回一个）[[0.395312860.48450.14631680.82327991][0.890422550.650499310.438901630.89577744]]123>>>print(np.random.rand(2,4))#将生成一个处于0~1之间2行4列的随机数序列（不加参数将只返回一个）[[0.39531286  0.4845      0.1463168  0.82327991][0.89042255  0.65049931  0.43890163  0.89577744]]如果想要多个随机整数：Pythonprint(np.random.randint(22,55,3))#必须有（前两个参数）指定范围，第三个参数用于指定生成的个数[274029]print(np.random.randn(2，4))#生成标准正态随机数[[-1.155615480.36899530.38253231-1.16346441][-1.32625322-0.41707673-0.11822205-0.95807535]]print(np.random.choice([10,20,40,33]))#从指定可迭代的数组中生成随机数20print(np.random.beta(1,10,4))#生成4个beta分布[0.022585480.258488960.006968990.0609543]123456789print(np.random.randint(22,55,3))#必须有（前两个参数）指定范围，第三个参数用于指定生成的个数[274029]print(np.random.randn(2，4))#生成标准正态随机数[[-1.15561548  0.3689953  0.38253231-1.16346441][-1.32625322-0.41707673-0.11822205-0.95807535]]print(np.random.choice([10,20,40,33]))#从指定可迭代的数组中生成随机数20print(np.random.beta(1,10,4))#生成4个beta分布[0.02258548  0.25848896  0.00696899  0.0609543]多维数组运算Pythonprint(np.arange(1,11,2))#得到step为2的range序列[13579]12print(np.arange(1,11,2))#得到step为2的range序列[13579]还可以使用reshape函数，对数组结构重定义：Pythonprint(np.arange(1,11).reshape(2,5))#（5可以缺省为-1）[[12345][678910]]123print(np.arange(1,11).reshape(2,5))#（5可以缺省为-1）[[1  2  3  4  5][6  7  8  910]]下面介绍一些常用的运算操作：Pythonlst=np.arange(1,11).reshape(2,5)print(np.exp(lst))#自然指数操作[[2.71828183e+007.38905610e+002.00855369e+015.45981500e+011.48413159e+02][4.03428793e+021.09663316e+032.98095799e+038.10308393e+032.20264658e+04]]1234lst=np.arange(1,11).reshape(2,5)print(np.exp(lst))#自然指数操作[[  2.71828183e+00  7.38905610e+00  2.00855369e+01  5.45981500e+01    1.48413159e+02][  4.03428793e+02  1.09663316e+03  2.98095799e+03  8.10308393e+03    2.20264658e+04]]此外，还可以sqrt、log、sin、sum、max等操作：我们下定义一个三维数组：Pythonlst=np.array([[[1,2,3,4],[4,5,6,7]],[[7,8,9,10],[10,11,12,13]],[[14,15,16,17],[18,19,20,21]]])print(lst.sum())2521234567lst=np.array([                [[1,2,3,4],[4,5,6,7]],                [[7,8,9,10],[10,11,12,13]],                [[14,15,16,17],[18,19,20,21]]            ])print(lst.sum())252我们可以看到sum方法对lst的所有元素都进行了求和，此外我们还可以通过对sum方法增加参数axis的方式来设置求和的深入维度：Pythonprint(lst.sum(axis=0))[[22252831]#22=1+7+14；25=2+8+15[32353841]]print(lst.sum(axis=1))[[57911]#5=1+4；7=2+5[17192123][32343638]]print(lst.sum(axis=2))[[1022]#10=1+2+3+4；22=4+5+6+7[3446][6278]]1234567891011print(lst.sum(axis=0))[[22252831]#22=1+7+14；25=2+8+15[32353841]]print(lst.sum(axis=1))[[5  7  911]#5=1+4；7=2+5[17192123][32343638]]print(lst.sum(axis=2))[[1022]#10=1+2+3+4；22=4+5+6+7[3446][6278]]这里的axis取值为数组维数-1，axis可以理解为进行运算操作时的深入程度，axis越大，深入程度越大。同理，不仅sum函数，max等函数也可以一样理解。相加运算numpy.array是np最简单的数据结构。np.array相比与Python原生列表其强大之处在于可以实现对数组数据的运算。我们知道，list只能对元素的追加。而numpy是真正意义上的数据运算。例如PythonIn[1]:importnumpyasnpIn[2]:list1=np.array([10,20,30,40])In[3]:list2=np.array([4,3,2,1])In[4]:print(list1)[10203040]In[5]:print(list1+list2)[14233241]1234567    In[1]:importnumpyasnp    In[2]:list1=np.array([10,20,30,40])    In[3]:list2=np.array([4,3,2,1])    In[4]:print(list1)    [10203040]    In[5]:print(list1+list2)    [14233241]但np最强大的地方不在于简单的一维运算，Np对矩阵也能进行基本的运算操作：Pythonlst1=np.array([10,20,30,40])lst2=np.array([4,3,2,1])print(np.dot(lst1.reshape([2,2]),lst2.reshape([2,2])))[[1022][3446][6278]][[8050][200130]]12345678lst1=np.array([10,20,30,40])lst2=np.array([4,3,2,1])print(np.dot(lst1.reshape([2,2]),lst2.reshape([2,2])))[[1022][3446][6278]][[80  50][200130]]此外，由于原生list没有确定的数据类型，所以维护起来成本较高，而使用C编写的numpy，则可以声明各种常见的数据类型：Pythonlst=[[1,3,5],[2,4,6]]np_lst=np.array(lst,dtype=np.float)12lst=[[1,3,5],[2,4,6]]np_lst=np.array(lst,dtype=np.float)np所支持的数据类型都有bool、int8/16/32/64/128/、uint8/16/32/64/128、float16/32/43、complex64/128、string。总结Python作为一门弱类型语言，有其不可避免的缺点。但NP的出现，弥补了这些缺点，使其具备了构造复杂数据类型的能力，为Python数据分析提供了基础。2赞9收藏2评论"], "art_url": ["http://python.jobbole.com/88726/"]}
{"art_img": ["/wp-content/uploads/vb/1141-thumb_python1.png"], "art_title": ["每个程序员都应该学习使用Python或Ruby"], "art_create_time": ["2011/07/25"], "art_content": ["如果你是个学生，你应该会C，C++和Java。还会一些VB，或C#/.NET。多少你还可能开发过一些Web网页，你知道一些HTML，CSS和JavaScript知识。总体上说，我们很难发现会有学生显露出掌握超出这几种语言范围外的语言的才能。这真让人遗憾，因为还有很多种编程语言，它们能让你成为一个更好的程序员。在这篇文章里，我将会告诉你，为什么你一定要学习Python或Ruby语言。跟C/C++/Java相比—Python/Ruby能让你用少的多的多的代码写出相同的程序。有人计算过，Python或Ruby写出的程序的代码行数只相当于相对应的Java代码的行数的五分之一。如果没有绝对的必要，为什么要花这么多时间写出这么多的代码呢？而且有人说，一个优秀的程序员能维护的代码量最多是2万行。这不区分用的语言究竟是汇编，C还是Python/Ruby/PHP/Lisp。所以，如果你用Python/Ruby写，你一个人干的，不管是干什么，如果换用Java/C/C++，那都需要一个5人的小团队来干。跟VB/PHP比较—跟PHP/VB相比，Python/Ruby的是一种从设计上讲比它们好的不知多少倍的语言。PHP和VB分别是在开发网站和桌面应用程序上非常流行的语言。它们流行的原因是非常的易学。不懂计算机的人也很容易的上手。如果你用这些语言开发过大型的项目，你就会发现这些语言的设计是如此的糟糕。是朋友，他就不会劝你使用PHP/VB。跟Lisp/Scala/Haskell/Closure/Erlang相比—Python/Ruby跟它们比起来显得相当的“主流”。确实，这些语言每种都有其很酷的特征，对于高级编程人员，了解这些语言能给他们对编程的思考带来实际的提升。但这些应该在你以后的职业生涯中才去决定学哪一两种。对于现在，Python/Ruby是在语言功能和实际运用之间平衡后的更好的选择。跟Perl相比—Python和Ruby都受恩于Perl，在这两种语言异军突起前，Perl是最好、最大的一种动态语言。但现在，Perl已是昨日黄花，越来越多的人转向Ruby/Python。我感觉Perl的面向对象机制有点做作，很不好用。通常认为，Perl一种比较难学的语言，因为它提供你了太多不同的方法去完成同一个任务，它的语法有点像密码，非常不直观—除非你对它掌握的非常好。总之，我感觉Perl是一种对于学生来说不是很合适的语言—除非你有特殊的理由去学它(例如，你有很多正则表达式要处理，这是Perl的闪光点)。跟sh/sed/awk/bash相比—如果你使用Linux/Unix，你可能需要做一些shell编程，甚至会编写一些不小的程序。但是，对于这些语言，一旦程序达到一定的行数，事情就会开始变得让你痛苦不堪，你最好是用Python去做这些事情。当然，做这种事情，Perl是最好的选择，Python排第二。(Ruby对于系统shell脚本不是很合适)。你可以在Google上搜一下“为什么X比Y好”—其中把X换成Python或Ruby，把Y换成另外一种语言—你就会发现，有无数的文章来说明它们为什么这么好。如果你有选择你的毕业设计使用的编程语言的自由，你应该选择Python或Ruby，它们能让你在开发项目的过程中节省一半的时间(除非你要开发的是移动应用，这样你必须要使用Java或Objective-C)。下面是xkcd上的一幅漫画，告诉你掌握Python后你会变得多么的强大：如何去学它们呢？很多很多的网站上都提供了学习Python和Ruby的教材和课程。下面的是我从中选出的一些：谷歌的Python课程，学习Python的好资源。RubyLearning，学习Ruby的一个好网站。有疑问吗？请在评论了写出来，我会尽量回答你们。尾注：1：我的这篇文章可能会让很多Perl爱好者很郁闷，现在回味一下，我认识到对这种语言的要求过于苛刻了。因此，我把关于Perl的一节改写了一下。Python和Ruby都受恩于Perl，在这两种语言出现之前，Perl是最大、最好的动态语言。但Perl现在太老了。它的面向对象性不完整。它很久没有升级更新了，它的市场份额正在丢失。对于一些新的、很火的事物(例如Web编程框架，WebAPI)，它不如Python&Ruby那样能跟上时代的步伐。基本上，Python/Ruby在兴起，Perl在衰退。2：本文中的所有语言的比较都是用来给印度计算机科学专业的学生选编程语言时做参考的。像“X比Y好”这样的句子准确的讲是毫无意义的，因为所有的语言都是经过时间的考验而存活下来的，有些语言会在某些领域比另外一种要强，这也是它们存活下来的原因。换句话说，总有一些情况下，PHP/Java/C/C++/Perl看起来会比Ruby/Python等其它语言显的更适合。译文：外刊IT评论　　原文：reliscore1赞收藏3评论"], "art_url": ["http://python.jobbole.com/1141/"]}
{"art_img": ["http://jbcdn2.b0.upaiyun.com/2014/12/6da94dec8f6f96417f14c8291e6345801.png"], "art_title": ["Python Re模块"], "art_create_time": ["2017/10/23"], "art_content": ["原文出处：chichao   re模块下的函数compile(pattern)：创建模式对象Pythonimportrepat=re.compile('A')m=pat.search('CBA')#等价于re.search('A','CBA')printm<_sre.SRE_Matchobjectat0x9d690c8>#匹配到了，返回MatchObject（True）m=pat.search('CBD')printmNone#没有匹配到，返回None（False）123456789importrepat=re.compile('A')m=pat.search('CBA')                    #等价于re.search('A','CBA')printm<_sre.SRE_Matchobjectat0x9d690c8>  #匹配到了，返回MatchObject（True） m=pat.search('CBD')printmNone                                  #没有匹配到，返回None（False）search(pattern,string)：在字符串中寻找模式Pythonm=re.search('asd','ASDasd')printm<_sre.SRE_Matchobjectat0xb72cd6e8>#匹配到了，返回MatchObject（True）m=re.search('asd','ASDASD')printmNone#没有匹配到，返回None（False）123456m=re.search('asd','ASDasd')printm<_sre.SRE_Matchobjectat0xb72cd6e8>  #匹配到了，返回MatchObject（True）m=re.search('asd','ASDASD')printmNone                                  #没有匹配到，返回None（False）match(pattern,string)：在字符串开始处匹配模式Pythonm=re.search('asd','ASDasd')printm<_sre.SRE_Matchobjectat0xb72cd6e8>#匹配到了，返回MatchObject（True）m=re.search('asd','ASDASD')printmNone#没有匹配到，返回None（False）123456m=re.search('asd','ASDasd')printm<_sre.SRE_Matchobjectat0xb72cd6e8>  #匹配到了，返回MatchObject（True）m=re.search('asd','ASDASD')printmNone                                  #没有匹配到，返回None（False）等价于Pythonpat=re.compile('a')printpat.match('Aasd')Noneprintpat.match('aASD')<_sre.SRE_Matchobjectat0xb72cd6e8>12345pat=re.compile('a')printpat.match('Aasd')Noneprintpat.match('aASD')<_sre.SRE_Matchobjectat0xb72cd6e8>上面的函数返回都可以在if条件语句中进行判断：Pythonifpat.search('asd'):...print'OK'...OK#找到返回ifre.search('a','ASD'):...print\"OK\"...#没有找到1234567ifpat.search('asd'):...    print'OK'...OK        #找到返回ifre.search('a','ASD'):...    print\"OK\"...      #没有找到split(pattern,string)：根据模式分割字符串,返回列表Pythonre.split(',','a,s,d,asd')['a','s','d','asd']#返回列表pat=re.compile(',')pat.split('a,s,d,asd')['a','s','d','asd']#返回列表re.split('[,]+','a,s,d,,,,,asd')#正则匹配：[,]+，后面说明['a','s','d','asd']re.split('[,]+','a,s,d,,,,,asd',maxsplit=2)#maxsplit最多分割次数['a','s','d,,,,,asd']pat=re.compile('[,]+')#正则匹配：[,]+，后面说明pat.split('a,s,d,,,,,asd',maxsplit=2)#maxsplit最多分割次数['a','s','d,,,,,asd']12345678910111213141516re.split(',','a,s,d,asd')['a','s','d','asd']          #返回列表 pat=re.compile(',')pat.split('a,s,d,asd')['a','s','d','asd']          #返回列表 re.split('[,]+','a,  s  ,d    ,,,,,asd')  #正则匹配：[,]+，后面说明['a','s','d','asd'] re.split('[,]+','a,  s  ,d    ,,,,,asd',maxsplit=2)#maxsplit最多分割次数['a','s','d    ,,,,,asd'] pat=re.compile('[,]+')                    #正则匹配：[,]+，后面说明pat.split('a,  s  ,d    ,,,,,asd',maxsplit=2)        #maxsplit最多分割次数['a','s','d    ,,,,,asd']findall(pattern,string)：列表形式返回匹配项Pythonre.findall('a','ASDaDFGAa')['a','a']#列表形式返回匹配到的字符串pat=re.compile('a')pat.findall('ASDaDFGAa')['a','a']#列表形式返回匹配到的字符串pat=re.compile('[A-Z]+')#正则匹配：'[A-Z]+'后面有说明pat.findall('ASDcDFGAa')['ASD','DFGA']#找到匹配到的字符串pat=re.compile('[A-Z]')pat.findall('ASDcDFGAa')#正则匹配：'[A-Z]+'后面有说明['A','S','D','D','F','G','A']#找到匹配到的字符串pat=re.compile('[A-Za-z]')#正则匹配：'[A-Za-z]+'匹配所有单词，后面有说明pat.findall('ASDcDFGAa')['A','S','D','c','D','F','G','A','a']123456789101112131415161718re.findall('a','ASDaDFGAa')['a','a']                          #列表形式返回匹配到的字符串 pat=re.compile('a')pat.findall('ASDaDFGAa')['a','a']                          #列表形式返回匹配到的字符串 pat=re.compile('[A-Z]+')      #正则匹配：'[A-Z]+'后面有说明pat.findall('ASDcDFGAa')['ASD','DFGA']                      #找到匹配到的字符串 pat=re.compile('[A-Z]')pat.findall('ASDcDFGAa')        #正则匹配：'[A-Z]+'后面有说明['A','S','D','D','F','G','A']  #找到匹配到的字符串 pat=re.compile('[A-Za-z]')    #正则匹配：'[A-Za-z]+'匹配所有单词，后面有说明pat.findall('ASDcDFGAa')['A','S','D','c','D','F','G','A','a']sub(pat,repl,string)：用repl替换pat匹配项(留的是中间的，因为中间在中心)Pythonre.sub('a','A','abcasd')#找到a用A替换，后面见和group的配合使用'AbcAsd'pat=re.compile('a')pat.sub('A','abcasd')'AbcAsd'pat=re.compile(r'www\\.(.*)\\..{3}')#正则表达式#在Python的string前面加上‘r’，是为了告诉编译器这个string是个rawstring，不要转译反斜杠'\\'。#例如，\\n在rawstring中，是两个字符，\\和n，而不会转译为换行符。#由于正则表达式和\\会有冲突，因此，当一个字符串使用了正则表达式后，最好在前面加上'r'。#与大多数编程语言相同，正则表达式里使用\"\\\"作为转义字符，这就可能造成反斜杠困扰。#假如你需要匹配文本中的字符\"\\\"，那么使用编程语言表示的正则表达式里将需要4个反斜杠\"\\\\\\\\\"：#前两个和后两个分别用于在编程语言里转义成反斜杠，转换成两个反斜杠后再在正则表达式里转义成一个反斜杠。#Python里的原生字符串很好地解决了这个问题，这个例子中的正则表达式可以使用r\"\\\\\"表示。#同样，匹配一个数字的\"\\\\d\"可以写成r\"\\d\"。#有了原生字符串，你再也不用担心是不是漏写了反斜杠，写出来的表达式也更直观。#不是说加了r\\就没有转译功能，好乱，就直接记住1句话：#当一个字符串使用了正则表达式后，最好在前面加上'r'，这样你再也不用担心是不是漏写了反斜杠，写出来的表达式也更直观pat.match('www.dxy.com').group(1)'dxy're.sub(r'www\\.(.*)\\..{3}',r'\\1','hello,www.dxy.com')pat.sub(r'\\1','hello,www.dxy.com')'hello,dxy'#r'1'是第一组的意思#通过正则匹配找到符合规则的\"www.dxy.com\"，取得组1字符串去替换整个匹配。pat=re.compile(r'(\\w+)(\\w+)')#正则表达式s='helloworld!hellohz!'pat.findall('helloworld!hellohz!')[('hello','world'),('hello','hz')]pat.sub(r'\\2\\1',s)#通过正则得到组1(hello)，组2(world)，再通过sub去替换。即组1替换组2，组2替换组1，调换位置。'worldhello!hzhello!'1234567891011121314151617181920212223242526272829303132333435363738394041re.sub('a','A','abcasd')  #找到a用A替换，后面见和group的配合使用'AbcAsd' pat=re.compile('a')pat.sub('A','abcasd')'AbcAsd' pat=re.compile(r'www\\.(.*)\\..{3}')#正则表达式  #在Python的string前面加上‘r’，是为了告诉编译器这个string是个rawstring，不要转译反斜杠'\\'。  #例如，\\n在rawstring中，是两个字符，\\和n，而不会转译为换行符。  #由于正则表达式和\\会有冲突，因此，当一个字符串使用了正则表达式后，最好在前面加上'r'。      #与大多数编程语言相同，正则表达式里使用\"\\\"作为转义字符，这就可能造成反斜杠困扰。  #假如你需要匹配文本中的字符\"\\\"，那么使用编程语言表示的正则表达式里将需要4个反斜杠\"\\\\\\\\\"：  #前两个和后两个分别用于在编程语言里转义成反斜杠，转换成两个反斜杠后再在正则表达式里转义成一个反斜杠。  #Python里的原生字符串很好地解决了这个问题，这个例子中的正则表达式可以使用r\"\\\\\"表示。  #同样，匹配一个数字的\"\\\\d\"可以写成r\"\\d\"。  #有了原生字符串，你再也不用担心是不是漏写了反斜杠，写出来的表达式也更直观。    #不是说加了r\\就没有转译功能，好乱，就直接记住1句话：  #当一个字符串使用了正则表达式后，最好在前面加上'r'，这样你再也不用担心是不是漏写了反斜杠，写出来的表达式也更直观pat.match('www.dxy.com').group(1)'dxy' re.sub(r'www\\.(.*)\\..{3}',r'\\1','hello,www.dxy.com') pat.sub(r'\\1','hello,www.dxy.com')'hello,dxy'#r'1'是第一组的意思#通过正则匹配找到符合规则的\"www.dxy.com\"，取得组1字符串去替换整个匹配。  pat=re.compile(r'(\\w+)(\\w+)')    #正则表达式s='helloworld!hellohz!' pat.findall('helloworld!hellohz!')[('hello','world'),('hello','hz')]pat.sub(r'\\2\\1',s)                #通过正则得到组1(hello)，组2(world)，再通过sub去替换。即组1替换组2，组2替换组1，调换位置。  'worldhello!hzhello!'escape(string)：对字符串里面的特殊字符串进行转义Pythonre.escape('www.dxy.cn')'www\\\\.dxy\\\\.cn'#转义12re.escape('www.dxy.cn')'www\\\\.dxy\\\\.cn'                  #转义上面的函数中，只有match、search有group方法，其他的函数没有。函数的方法group：获取子模式(组)的匹配项Pythonpat=re.compile(r'www\\.(.*)\\.(.*)')#用()表示1个组，2个组m=pat.match('www.dxy.com')m.group()#默认为0，表示匹配整个字符串'www.dxy.com'm.group(1)#返回给定组1匹配的子字符串'dxy'm.group(2)'com'12345678910pat=re.compile(r'www\\.(.*)\\.(.*)')      #用()表示1个组，2个组m=pat.match('www.dxy.com')m.group()                                  #默认为0，表示匹配整个字符串  'www.dxy.com' m.group(1)                                #返回给定组1匹配的子字符串'dxy' m.group(2)'com'start：给定组匹配项的开始位置Pythonm.start(2)#组2开始的索引812m.start(2)                                #组2开始的索引8end：给定组匹配项的结束位置Pythonm.end(2)#组2结束的索引1112m.end(2)                                  #组2结束的索引11span：给定组匹配项的开始结束位置Pythonm.span(2)#组2开始、结束的索引(8,11)12m.span(2)                                  #组2开始、结束的索引(8,11)正则表达式元字符“.”：通配符,除换行符外的任意的1个字符Pythonpat=re.compile('.')pat.match('abc')<_sre.SRE_Matchobjectat0xb72b6170>pat.match('abc').group()'a'#匹配到了首个字符pat.search('abc').group()'a'pat.match('\\n').group()#换行符匹配出错Traceback(mostrecentcalllast):File\"<stdin>\",line1,in<module>AttributeError:'NoneType'objecthasnoattribute'group'1234567891011pat=re.compile('.')pat.match('abc')<_sre.SRE_Matchobjectat0xb72b6170>pat.match('abc').group()'a'                                #匹配到了首个字符pat.search('abc').group()'a'pat.match('\\n').group()        #换行符匹配出错Traceback(mostrecentcalllast):  File\"<stdin>\",line1,in<module>AttributeError:'NoneType'objecthasnoattribute'group'“\\”:转义符Pythonpat=re.compile('\\.')pat.search('abc.efg').group()#匹配到.'.'pat.findall('abc.efg')#不用group,返回列表['.']12345pat=re.compile('\\.')pat.search('abc.efg').group()  #匹配到.'.'pat.findall('abc.efg')        #不用group,返回列表['.']“[…]”:字符集合，匹配里面的任意一个元素Python>>>pat=re.compile('[abc]')>>>pat.match('axbycz').group()'a'>>>pat.search('axbycz').group()'a'>>>pat.findall('axbycz')['a','b','c']1234567>>>pat=re.compile('[abc]')>>>pat.match('axbycz').group()'a'>>>pat.search('axbycz').group()'a'>>>pat.findall('axbycz')['a','b','c']“\\d”:数字Python>>>pat=re.compile('\\d')>>>pat.search('ax1by2cz3').group()#匹配到第一个数字:1，返回'1'>>>pat.match('ax1by2cz3').group()#匹配不到（首个不是）返回None，报错，match匹配字符串头Traceback(mostrecentcalllast):File\"<stdin>\",line1,in<module>AttributeError:'NoneType'objecthasnoattribute'group'>>>pat.findall('ax1by2cz3')#匹配所有的数字，列表返回['1','2','3']1234567891011>>>pat=re.compile('\\d')          >>>pat.search('ax1by2cz3').group()  #匹配到第一个数字:1，返回'1' >>>pat.match('ax1by2cz3').group()    #匹配不到（首个不是）返回None，报错，match匹配字符串头Traceback(mostrecentcalllast):  File\"<stdin>\",line1,in<module>AttributeError:'NoneType'objecthasnoattribute'group' >>>pat.findall('ax1by2cz3')          #匹配所有的数字，列表返回['1','2','3']“\\D”:非数字Python>>>pat=re.compile('\\D')>>>pat.match('ax1by2cz3').group()'a'>>>pat.search('ax1by2cz3').group()'a'>>>pat.findall('ax1by2cz3')['a','x','b','y','c','z']1234567>>>pat=re.compile('\\D')>>>pat.match('ax1by2cz3').group()'a'>>>pat.search('ax1by2cz3').group()'a'>>>pat.findall('ax1by2cz3')['a','x','b','y','c','z']“\\s”：空白字符、\\t、\\r、\\n、空格Python>>>pat=re.compile('\\s')>>>pat.findall('\\rax1\\nby2\\tcz3')['\\r','','\\n','','\\t']>>>pat.search('\\rax1\\nby2\\tcz3').group()'\\r'>>>pat.match('\\rax1\\nby2\\tcz3').group()'\\r'1234567>>>pat=re.compile('\\s')>>>pat.findall('\\rax1\\nby2\\tcz3')['\\r','','\\n','','\\t']>>>pat.search('\\rax1\\nby2\\tcz3').group()'\\r'>>>pat.match('\\rax1\\nby2\\tcz3').group()'\\r'“S”:非空白字符Python>>>pat=re.compile('\\S')>>>pat.search('\\rax1\\nby2\\tcz3').group()'a'>>>pat.findall('\\rax1\\nby2\\tcz3')['a','x','1','b','y','2','c','z','3']12345>>>pat=re.compile('\\S')>>>pat.search('\\rax1\\nby2\\tcz3').group()'a'>>>pat.findall('\\rax1\\nby2\\tcz3')['a','x','1','b','y','2','c','z','3']“\\w”：单个的数字和字母，[A-Za-z0-9]Python>>>pat=re.compile('\\w')>>>pat.search('1a2b3c').group()'1'>>>pat.findall('1a2b3c')['1','a','2','b','3','c']>>>pat.match('1a2b3c').group()'1'1234567>>>pat=re.compile('\\w')>>>pat.search('1a2b3c').group()'1'>>>pat.findall('1a2b3c')['1','a','2','b','3','c']>>>pat.match('1a2b3c').group()'1'“\\W”:非单词字符,除数字和字母外Python>>>pat=re.compile('\\W')>>>pat.findall('1a2我b3c')#python是用三字节表示一个汉字['\\xe6','\\x88','\\x91']>>>pat.search('1a2我b3c').group()'\\xe6'12345>>>pat=re.compile('\\W')>>>pat.findall('1a2我b3c')#python是用三字节表示一个汉字['\\xe6','\\x88','\\x91']>>>pat.search('1a2我b3c').group()'\\xe6'数量词“*”：0次或多次（ 乘0会变成0）Python>>>pat=re.compile('[abc]*')>>>pat.match('abcabcdefabc').group()'abcabc'#2次>>>pat.search('abcabcdefabc').group()'abcabc'#2次>>>pat.findall('abcabcdefabc')['abcabc','','','','abc','']#2次和1次,因为有0次，所以匹配了''1234567>>>pat=re.compile('[abc]*')>>>pat.match('abcabcdefabc').group()'abcabc'                              #2次>>>pat.search('abcabcdefabc').group()'abcabc'                              #2次>>>pat.findall('abcabcdefabc')['abcabc','','','','abc','']    #2次和1次,因为有0次，所以匹配了''“+”：1次或多次（ 加0不会变成0）Python>>>pat=re.compile('[abc]+')>>>pat.match('abcdefabcabc').group()'abc'>>>pat.search('abcdefabcabc').group()'abc'>>>pat.findall('abcdefabcabc')['abc','abcabc']1234567>>>pat=re.compile('[abc]+')>>>pat.match('abcdefabcabc').group()'abc'>>>pat.search('abcdefabcabc').group()'abc'>>>pat.findall('abcdefabcabc')['abc','abcabc']“?”：0次或1次，match,search不会出现none，会出现’‘（因为0次也是符合的）0次或1次不是指[xxx]这个集合，而是其中的任何的一个字符Python>>>pat=re.compile('[abc]?')>>>pat.match('defabc').group()#0次''>>>pat.match('abcdefabc').group()'a'>>>pat.search('defabc').group()#0次''>>>pat.findall('defabc')#0次和1次['','','','a','b','c','']#后面总再加个''123456789>>>pat=re.compile('[abc]?')>>>pat.match('defabc').group()    #0次''>>>pat.match('abcdefabc').group()'a'>>>pat.search('defabc').group()    #0次''>>>pat.findall('defabc')          #0次和1次['','','','a','b','c','']    #后面总再加个''“数量词?”：非贪婪模式：只匹配最少的（尽可能少）；默认贪婪模式：匹配最多的（尽可能多）Python>>>pat=re.compile('[abc]+')#贪婪模式>>>pat.match('abcdefabcabc').group()#匹配尽可能多的：abc'abc'>>>pat.match('bbabcdefabcabc').group()'bbabc'>>>pat.search('dbbabcdefabcabc').group()'bbabc'>>>pat.findall('abcdefabcabc')['abc','abcabc']>>>pat=re.compile('[abc]+?')#非贪婪模式：+?>>>pat.match('abcdefabcabc').group()#匹配尽可能少的：a、b、c'a'>>>pat.search('dbbabcdefabcabc').group()'b'>>>pat.findall('abcdefabcabc')['a','b','c','a','b','c','a','b','c']1234567891011121314151617>>>pat=re.compile('[abc]+')        #贪婪模式>>>pat.match('abcdefabcabc').group()  #匹配尽可能多的：abc'abc'>>>pat.match('bbabcdefabcabc').group()'bbabc'>>>pat.search('dbbabcdefabcabc').group()'bbabc'>>>pat.findall('abcdefabcabc')['abc','abcabc'] >>>pat=re.compile('[abc]+?')        #非贪婪模式：+?>>>pat.match('abcdefabcabc').group()  #匹配尽可能少的：a、b、c'a'>>>pat.search('dbbabcdefabcabc').group()'b'>>>pat.findall('abcdefabcabc')['a','b','c','a','b','c','a','b','c']“{m}”：匹配字符串出现m次Python>>>pat=re.compile('[op]{2}')#o或p出现2次>>>pat.search('abcooapp').group()#匹配第一次出现的字符串,o比p先出现'oo'>>>pat.findall('abcooapp')#匹配出现的所有字符串，列表形式返回['oo','pp']12345>>>pat=re.compile('[op]{2}')    #o或p出现2次>>>pat.search('abcooapp').group()  #匹配第一次出现的字符串,o比p先出现'oo'>>>pat.findall('abcooapp')        #匹配出现的所有字符串，列表形式返回['oo','pp']“{m,n}”：匹配字符串出现m到n次Python>>>pat=re.compile('[op]{2,4}')#o或则p出现2到4次>>>pat.match('pppabcooapp').group()#匹配开头'ppp'>>>pat.search('pppabcooapp').group()#匹配第一次出现'ppp'>>>pat.findall('pppabcooapp')#匹配所有['ppp','oo','pp']1234567>>>pat=re.compile('[op]{2,4}')    #o或则p出现2到4次>>>pat.match('pppabcooapp').group()  #匹配开头'ppp'>>>pat.search('pppabcooapp').group()#匹配第一次出现'ppp'>>>pat.findall('pppabcooapp')        #匹配所有['ppp','oo','pp'].group()#匹配第一次出现边界“^”：匹配字符串开头或行头Python>>>pat=re.compile('^[abc]')#开头是a、b、c中的任意一个>>>pat.search('defabc').group()>>>pat.match('defabc').group()#均找不到>>>pat.findall('defabc')[]>>>pat.search('adefabc').group()'a'>>>pat.match('adefabc').group()#开头是a、b、c中的任意一个'a'>>>pat.findall('adefabc')['a']>>>pat=re.compile('^[abc]+')#开头是a、b、c中的任意一个的一次或则多次，贪婪：匹配多个>>>pat.findall('cbadefab')['cba']>>>pat=re.compile(r'^[abc]+?')#开头是a、b、c中的任意一个的一次或则多次，非贪婪：匹配一个>>>pat.findall('cbadefab')['c']12345678910111213141516171819>>>pat=re.compile('^[abc]')    #开头是a、b、c中的任意一个>>>pat.search('defabc').group()    >>>pat.match('defabc').group()    #均找不到>>>pat.findall('defabc')[] >>>pat.search('adefabc').group()'a'>>>pat.match('adefabc').group()  #开头是a、b、c中的任意一个'a'>>>pat.findall('adefabc')['a'] >>>pat=re.compile('^[abc]+')    #开头是a、b、c中的任意一个的一次或则多次，贪婪：匹配多个>>>pat.findall('cbadefab')['cba']>>>pat=re.compile(r'^[abc]+?')  #开头是a、b、c中的任意一个的一次或则多次，非贪婪：匹配一个>>>pat.findall('cbadefab')['c']“$”：匹配字符串结尾或则行尾Python>>>pat=re.compile('[abc]$')>>>pat.match('adefAbc').group()#match匹配的是字符串开头，所以查找$的时，总是返回None>>>pat.search('adefAbc').group()#结尾是a、b、c中的任意一个'c'>>>pat.findall('adefAbc')['c']>>>pat=re.compile('[abc]+$')>>>pat.search('adefAbc').group()#结尾是a、b、c中的任意一个的一次或则多次，贪婪：匹配多个'bc'>>>pat.findall('adefAbc')['bc']1234567891011>>>pat=re.compile('[abc]$')>>>pat.match('adefAbc').group()  #match匹配的是字符串开头，所以查找$的时，总是返回None>>>pat.search('adefAbc').group()  #结尾是a、b、c中的任意一个'c'>>>pat.findall('adefAbc')        ['c']>>>pat=re.compile('[abc]+$')>>>pat.search('adefAbc').group()  #结尾是a、b、c中的任意一个的一次或则多次，贪婪：匹配多个'bc'>>>pat.findall('adefAbc')['bc']“\\A”：匹配字符串开头Python>>>pat=re.compile('\\A[abc]+')>>>pat.findall('cbadefab')['cba']>>>pat.search('cbadefab').group()'cba'12345>>>pat=re.compile('\\A[abc]+')>>>pat.findall('cbadefab')['cba']>>>pat.search('cbadefab').group()'cba'“\\Z”：匹配字符串结尾Python>>>pat=re.compile('[abc]+\\Z')>>>pat.search('cbadefab').group()'ab'>>>pat.findall('cbadefab')['ab']12345>>>pat=re.compile('[abc]+\\Z')>>>pat.search('cbadefab').group()'ab'>>>pat.findall('cbadefab')['ab']分组(…)：分组匹配,从左到右,每遇到一个(编号+1，分组后面可加数量词Python>>>pat=re.compile(r'(a)\\w(c)')#\\w:单个的数字或字母[A-Za-z0-9]>>>pat.match('abcdef').group()'abc'>>>pat=re.compile('(a)b(c)')#分2组，匿名分组>>>pat.match('abcdef').group()#默认返回匹配的字符串'abc'>>>pat.match('abcdef').group(1)#取分组1，适用于search'a'>>>pat.match('abcdef').group(2)#取分组2，适用于search'c'>>>pat.match('abcdef').groups()#取所有分组，元组形式返回('a','c')12345678910111213>>>pat=re.compile(r'(a)\\w(c)')  #\\w:单个的数字或字母[A-Za-z0-9]>>>pat.match('abcdef').group()'abc'>>>pat=re.compile('(a)b(c)')    #分2组，匿名分组                                >>>pat.match('abcdef').group()  #默认返回匹配的字符串'abc'>>>pat.match('abcdef').group(1)#取分组1，适用于search'a'>>>pat.match('abcdef').group(2)#取分组2，适用于search'c'>>>pat.match('abcdef').groups()#取所有分组，元组形式返回('a','c')<number>：引用编号为<number>的分组匹配到的字符串Python>>>pat=re.compile(r'www\\.(.*)\\..{3}')>>>pat.match('www.dxy.com').group(1)'dxy'123>>>pat=re.compile(r'www\\.(.*)\\..{3}')>>>pat.match('www.dxy.com').group(1)'dxy'“(?P<name>…)” ：在模式里面用()来表示分组（命名分组）,适用于提取目标字符串中的某一些部位。Python>>>pat=re.compile(r'(?P<K>a)\\w(c)')#分2组：命名分组+匿名分组>>>pat.search('abcdef').groups()#取所有分组，元组形式返回('a','c')>>>pat.search('abcdef').group(1)#取分组1，适用于match'a'>>>pat.search('abcdef').group(2)#取分组2，适用于match'c'>>>pat.search('abcdef').group()#默认返回匹配的字符串'abc'>>>pat.search('abcdef').groupdict()#命名分组可以返回一个字典【专有】，匿名分组也没有{'K':'a'}1234567891011>>>pat=re.compile(r'(?P<K>a)\\w(c)')    #分2组：命名分组+匿名分组>>>pat.search('abcdef').groups()      #取所有分组，元组形式返回('a','c')>>>pat.search('abcdef').group(1)      #取分组1，适用于match'a'>>>pat.search('abcdef').group(2)      #取分组2，适用于match'c'>>>pat.search('abcdef').group()        #默认返回匹配的字符串'abc'>>>pat.search('abcdef').groupdict()    #命名分组可以返回一个字典【专有】，匿名分组也没有{'K':'a'}“(?P=name)”：引用别名为<name>的分组匹配到的串Python>>>pat=re.compile(r'(?P<K>a)\\w(c)(?P=K)')#(?P=K)引用分组1的值，就是a>>>pat.search('abcdef').group()#匹配不到，因为完整'a\\wca',模式的第4位是aTraceback(mostrecentcalllast):File\"<stdin>\",line1,in<module>AttributeError:'NoneType'objecthasnoattribute'group'>>>pat.search('abcadef').group()#匹配到，模式的第4位和组1一样,值是c'abca'>>>pat.search('abcadef').groups()('a','c')>>>pat.search('abcadef').group(1)'a'>>>pat.search('abcadef').group(2)'c1234567891011121314>>>pat=re.compile(r'(?P<K>a)\\w(c)(?P=K)')    #(?P=K)引用分组1的值，就是a>>>pat.search('abcdef').group()              #匹配不到，因为完整'a\\wca',模式的第4位是aTraceback(mostrecentcalllast):  File\"<stdin>\",line1,in<module>AttributeError:'NoneType'objecthasnoattribute'group' >>>pat.search('abcadef').group()            #匹配到，模式的第4位和组1一样,值是c'abca'>>>pat.search('abcadef').groups()('a','c')>>>pat.search('abcadef').group(1)'a'>>>pat.search('abcadef').group(2)'c“<number>”：引用分组编号匹配：Python>>>pat=re.compile(r'(?P<K>a)\\w(c)(?P=K)\\2')#\\2引用分组2的值，就是c>>>pat.findall('Aabcadef')#匹配不到，因为完整'a\\wcac',模式的第5位是c[]>>>pat.findall('Aabcacdef')#匹配到，模式的第5位和组2一样,值是c[('a','c')]>>>pat.search('Aabcacdef').groups()('a','c')>>>pat.search('Aabcacdef').group()'abcac'>>>pat.search('Aabcacdef').group(1)'a'>>>pat.search('Aabcacdef').group(2)'c'12345678910111213>>>pat=re.compile(r'(?P<K>a)\\w(c)(?P=K)\\2')  #\\2引用分组2的值，就是c>>>pat.findall('Aabcadef')                  #匹配不到，因为完整'a\\wcac',模式的第5位是c[]>>>pat.findall('Aabcacdef')                  #匹配到，模式的第5位和组2一样,值是c[('a','c')]>>>pat.search('Aabcacdef').groups()('a','c')>>>pat.search('Aabcacdef').group()'abcac'>>>pat.search('Aabcacdef').group(1)'a'>>>pat.search('Aabcacdef').group(2)'c'特殊构造(?:…)(…)不分组版本,用于使用|或者后接数量词(?iLmsux)iLmsux的每个字符代表一个匹配模式,只能用在正则表达式的开头,可选多个(?#…)#号后的内容将作为注释(?=…)之后的字符串内容需要匹配表达式才能成功匹配(?!…)之后的字符串不匹配表达式才能成功(?(?(?(id/name)yes|no)如果编号为id/名字为name的组匹配到字符串,则需要匹配yes,否则匹配no,no可以省略“(?:…)”：()里面有?:表示该()不是分组Python>>>pat=re.compile(r'a(?:bc)')>>>pat.findall('abc')['abc']>>>pat.match('abc').groups()#显示不出分组1234>>>pat=re.compile(r'a(?:bc)')>>>pat.findall('abc')['abc']>>>pat.match('abc').groups()      #显示不出分组“(?=…)”：匹配…表达式，返回。对后进行匹配，总是对后面进行匹配Python>>>pat=re.compile(r'\\w(?=\\d)')#匹配表达式\\d，返回数字的前一位，\\w：单词字符[A-Za-z0-9]>>>pat.findall('abc1def1xyz1')['c','f','z']>>>pat.findall('zhoujy20130628hangzhou')#匹配数字的前一位，列表返回['y','2','0','1','3','0','6','2']>>>pat=re.compile(r'\\w+(?=\\d)')>>>pat.findall('abc1,def1,xyz1')#匹配最末数字的前字符串，列表返回['abc','def','xyz']>>>pat.findall('abc21,def31,xyz41')['abc2','def3','xyz4']>>>pat.findall('zhoujy20130628hangzhou')['zhoujy2013062']>>>pat=re.compile(r'[A-Za-z]+(?=\\d)')#[A-Za-z],匹配字母,可以用其他的正则方法>>>pat.findall('zhoujy20130628hangzhou123')#匹配后面带有数字的字符串，列表返回['zhoujy','hangzhou']>>>pat.findall('abc21,def31,xyz41')['abc','def','xyz']1234567891011121314151617>>>pat=re.compile(r'\\w(?=\\d)')    #匹配表达式\\d，返回数字的前一位，\\w：单词字符[A-Za-z0-9]>>>pat.findall('abc1def1xyz1')['c','f','z']>>>pat.findall('zhoujy20130628hangzhou')  #匹配数字的前一位，列表返回['y','2','0','1','3','0','6','2']>>>pat=re.compile(r'\\w+(?=\\d)')>>>pat.findall('abc1,def1,xyz1')          #匹配最末数字的前字符串，列表返回['abc','def','xyz']>>>pat.findall('abc21,def31,xyz41')['abc2','def3','xyz4']>>>pat.findall('zhoujy20130628hangzhou')['zhoujy2013062']>>>pat=re.compile(r'[A-Za-z]+(?=\\d)')      #[A-Za-z],匹配字母,可以用其他的正则方法>>>pat.findall('zhoujy20130628hangzhou123')#匹配后面带有数字的字符串，列表返回['zhoujy','hangzhou']>>>pat.findall('abc21,def31,xyz41')['abc','def','xyz']“(?!…)”不匹配…表达式，返回。对后进行匹配Python>>>pat=re.compile(r'[A-Za-z]+(?!\\d)')#[A-Za-z],匹配字母,可以用其他的正则方法>>>pat.findall('zhoujy20130628hangzhou123,12,binjiang310')#匹配后面不是数字的字符串，列表返回['zhouj','hangzho','binjian']>>>pat.findall('abc21,def31,xyz41')['ab','de','xy']12345>>>pat=re.compile(r'[A-Za-z]+(?!\\d)')      #[A-Za-z],匹配字母,可以用其他的正则方法>>>pat.findall('zhoujy20130628hangzhou123,12,binjiang310')  #匹配后面不是数字的字符串，列表返回['zhouj','hangzho','binjian']>>>pat.findall('abc21,def31,xyz41')['ab','de','xy']“(?<=…)”：匹配…表达式，返回。对前进行匹配,总是对前面进行匹配Python>>>pat=re.compile(r'(?<=\\d)[A-Za-z]+')#匹配前面是数字的字母>>>pat.findall('abc21,def31,xyz41')[]>>>pat.findall('1abc21,2def31,3xyz41')['abc','def','xyz']>>>pat.findall('zhoujy20130628hangzhou123,12,binjiang310')['hangzhou']1234567>>>pat=re.compile(r'(?<=\\d)[A-Za-z]+')      #匹配前面是数字的字母>>>pat.findall('abc21,def31,xyz41')[]>>>pat.findall('1abc21,2def31,3xyz41')['abc','def','xyz']>>>pat.findall('zhoujy20130628hangzhou123,12,binjiang310')['hangzhou']“(?<!…)”：不匹配…表达式，返回。对前进行匹配,总是对前面进行匹配Python>>>pat=re.compile(r'(?<!\\d)[A-Za-z]+')#匹配前面不是数字的字母>>>pat.findall('abc21,def31,xyz41')['abc','def','xyz']>>>pat.findall('zhoujy20130628hangzhou123,12,binjiang310')['zhoujy','angzhou','binjiang']12345>>>pat=re.compile(r'(?<!\\d)[A-Za-z]+')      #匹配前面不是数字的字母>>>pat.findall('abc21,def31,xyz41')['abc','def','xyz']>>>pat.findall('zhoujy20130628hangzhou123,12,binjiang310')['zhoujy','angzhou','binjiang']“(?(id/name)yes|no)”:组是否匹配，匹配返回Python>>>pat=re.compile(r'a(\\d)?bc(?(1)\\d)')#no省略了，完整的是a\\dbc\\d==>a2bc3,总共5位，第2位是可有可无的数字，第5为是数字>>>pat.findall('abc9')#返回组1，但第2位（组1）没有，即返回了''['']>>>pat.findall('a8bc9')#完整的模式，返回组1['8']>>>pat.match('a8bc9').group()'a8bc9'>>>pat.match('a8bc9').group(1)'8'>>>pat.findall('a8bc')#第5位不存在，则没有匹配到[]1234567891011>>>pat=re.compile(r'a(\\d)?bc(?(1)\\d)')  #no省略了，完整的是a\\dbc\\d==>a2bc3,总共5位，第2位是可有可无的数字，第5为是数字>>>pat.findall('abc9')                  #返回组1，但第2位（组1）没有，即返回了''['']>>>pat.findall('a8bc9')                  #完整的模式，返回组1['8']>>>pat.match('a8bc9').group()'a8bc9'>>>pat.match('a8bc9').group(1)'8'>>>pat.findall('a8bc')                  #第5位不存在，则没有匹配到[]“(?iLmsux)”:这里就介绍下i参数：大小写区分匹配Python>>>pat=re.compile(r'abc')>>>pat.findall('abc')['abc']>>>pat.findall('ABC')[]>>>pat=re.compile(r'(?i)abc')#(?i)不区分大小写>>>pat.findall('ABC')['ABC']>>>pat.findall('abc')['abc']>>>pat.findall('aBc')['aBc']>>>pat.findall('aBC')['aBC']>>>pat=re.compile(r'abc',re.I)#re.I作为参数使用，推荐>>>pat.findall('aBC')['aBC']>>>pat.findall('abc')['abc']>>>pat.findall('ABC')['ABC']123456789101112131415161718192021>>>pat=re.compile(r'abc')>>>pat.findall('abc')['abc']>>>pat.findall('ABC')[]>>>pat=re.compile(r'(?i)abc')            #(?i)不区分大小写>>>pat.findall('ABC')['ABC']>>>pat.findall('abc')['abc']>>>pat.findall('aBc')['aBc']>>>pat.findall('aBC')['aBC']>>>pat=re.compile(r'abc',re.I)          #re.I作为参数使用，推荐>>>pat.findall('aBC')['aBC']>>>pat.findall('abc')['abc']>>>pat.findall('ABC')['ABC']http://www.cnblogs.com/huxi/a…2赞14收藏5评论"], "art_url": ["http://python.jobbole.com/88729/"]}
{"art_img": ["http://jbcdn2.b0.upaiyun.com/2011/11/How-I-Became-a-Programmer.jpeg"], "art_title": ["我是如何在12周内成为一名程序员的"], "art_create_time": ["2011/11/25"], "art_content": ["英文出处：MattDeboard，译文出处：36KR我的故事在海军陆战队服役超过10年后，我于去年7月份退役了。随后在8月份找到了一份州彩票（statelottery）做公关的工作，到今年2月中旬的时候又被辞退了。到5月中旬的时候我在DE协会找到了一份临时的“初级用户体验工程师”工作，而到了8月底我则成了正式的“用户体验工程师”。当我丢掉州彩票的那份工作时，我就在想公关这行可能真的不适合我。我想做一名程序员。于是我开始节衣缩食学习编程。家人对我的情况非常担心。从2月份到5月份的那段时间，我几乎只要是没睡着就是在学习编程，学习Linux以及计算机科学。我自学Python，自学Django。我学了一些函数式编程和命令式编程。对Linux命令行有了一个不错的了解。我没有做的有人问我：“你如何在11周内学会了Django？”事实上，我并没有针对Django本身去学。而是在为了使得用Python编写应用更加容易的情况下学到的。也就是说完全出于偶然。因此我不想被称作一名“Django开发者”。换句话说，如果我过去是在花大量的时间去专门学Django本身，而不是去学如何使用Django工作，我可能远没有现在的编程能力。以下是好奇的朋友们给我的一些问题以及我的回答你是以网络资源开始的还是以书本资源开始的？我都有用。这包括Djangoproject，StackOverflow和MIT关于计算机科学的开放课。你的每天或者是每周时间是怎么安排的？有时每天8个小时，有时12个小时，有时16个小时，总之每天都要花很多时间。因为没工作，又是单身父亲，所以会特别有意志特别有激情。这样的一个境况使得我能十倍努力的工作，不玩游戏，不看电视，甚至整天不睡觉，完全沉浸在代码，编程里。你有指导老师吗？是的我有。他是一个非常聪明而且成功的人，几乎在我成为程序员路上的每一个方面都给了我指导。这其中包括很多非常具体的编程知识（比如Python&Django），还有职业建议等。之前有过什么特别的经历对于你现在自学成为程序员有什么帮助的吗？没有很多特殊的经历。我很早之前对电脑有过狂热的爱好，学过一点QBasic&VisualBasic，后来又断断续续的弄过一点Python，但大部分时间都没怎么弄。除此之外没有其他的了。你怎样选择学习Django的？这个很简单。因为我想要模仿的一个人就是通过Django取得了很大的成功。可以分享一下你的学习过程吗？我想重申一下我并不是一个自学天才也没有什么很特殊的天分。我只是很努力的学习罢了，因为我穷困潦倒而且没有其他选择。我几乎消耗了所以可以帮助我达到现在这个地步的资源——一个既可以让我赚钱又喜欢的工作。这就是我的学习过程。想了解一下你具体学习Django的过程，或者给我一些建议或推荐一些学习资源（HTML/CSS，JavaScript）？事实上，我唯一的建议就是动手去做。我真的花了很多时间学习，而且我也享受学习的过程。正如我在上面已经说过，我没有刻意去学Django，RubyonRails或者Noir。我想帮助我成功的一个重要因素是学习语言以及其背后的工作思想，然后再通过一个网络框架去更好的学习那门语言。因此对网络框架的学习都是我在学习编程语言中偶然所得的。我建议想学的朋友去看看irc.freenode.net，去读读Django文档并不懂就问。我就是这样做的，而且效果也很不错。不过我并不是完全坐着读文档，大部分的时间我都会自己做一些东西以更好的理解背后的工作思想。我个人是一个动手学习者，有些人可能不是，但是动手帮助我获得了成功。而你可以选择更适合你自己的方式。你是如何向公司展示你自己的技能的？是给他们看你的项目了吗？Github，Github还是Github。我觉得强调的再多有不过分，做一些东西，放到Github上去，让人们知道你很富有激情也很聪明好学。另外还有网络。参加一些行业活动，发微博，写博客，和你周围的圈子进行互动。在我看到我现在这份工作的前一周，我就曾在一个论坛上发布过一个简短的演讲，这也使得有些人注意到了我。结论如果要我概括我的整个学习过程的话，我想以下几点值得一说：1.问问题，有好奇心，富于热情2.学习一门语言，而不是一个网络开发框架3.努力学习4.构建一个网络，参加行业活动，写博客等，告诉人们你是一个值得共事的人5.（选择性的）将你自己放到一个没有退路的地方，破釜沉舟最后我想说的就是我觉得自己非常幸运。我现在还算不上一个很棒的开发者，而且我的职业也才刚刚开始。但是我很高兴通过自己的努力改变了我的生活轨迹。我也希望我的经历可以帮到你们。后记：有人可能会对文中提到的神秘老师感兴趣，作者Matt并未在文中说明他是如何遇到这位老师的，不过在读者评论下面他给出了回复。Matt是通过经常逛这位前辈的网络论坛认识他的。后来随着逐渐的熟悉便开始寻求他的帮助指导，最后也才有了Matt今天的程序员之路。1赞1收藏1评论"], "art_url": ["http://python.jobbole.com/8464/"]}
{"art_img": ["http://pytlab.org/assets/images/blog_img/2017-10-15-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E5%AE%9E%E8%B7%B5-Platt-SMO%E5%92%8C%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95%E4%BC%98%E5%8C%96SVM/platt_smo.png"], "art_title": ["机器学习算法实践-Platt SMO和遗传算法优化SVM"], "art_create_time": ["2017/10/16"], "art_content": ["本文作者：伯乐在线-iPytLab。未经作者许可，禁止转载！欢迎加入伯乐在线专栏作者。前言之前实现了简单的SMO算法来优化SVM的对偶问题，其中在选取α的时候使用的是两重循环通过完全随机的方式选取，具体的实现参考《机器学习算法实践-SVM中的SMO算法》。本文在之前简化版SMO算法的基础上实现了使用启发式选取α对的方式的PlattSMO算法来优化SVM。另外由于最近自己也实现了一个遗传算法框架GAFT，便也尝试使用遗传算法对于SVM的原始形式进行了优化。对于本文算法的相应实现，参考:https://github.com/PytLab/MLBox/tree/master/svm遗传算法框架GAFT项目地址: https://github.com/PytLab/gaft正文SMO中启发式选择变量在SMO算法中，我们每次需要选取一对α来进行优化，通过启发式的选取我们可以更高效的选取待优化的变量使得目标函数下降的最快。针对第一个α1和第二个α2 PlattSMO采取不同的启发式手段。第一个变量的选择第一个变量的选择为外循环，与之前便利整个αα列表不同，在这里我们在整个样本集和非边界样本集间进行交替:首先我们对整个训练集进行遍历,检查是否违反KKT条件，如果改点的αiαi和xi,yixi,yi违反了KKT条件则说明改点需要进行优化。Karush-Kuhn-Tucker(KKT)条件是正定二次规划问题最优点的充分必要条件。针对SVM对偶问题，KKT条件非常简单:在遍历了整个训练集并优化了相应的α后第二轮迭代我们仅仅需要遍历其中的非边界α.所谓的非边界α就是指那些不等于边界0或者C的α值。同样这些点仍然需要检查是否违反KKT条件并进行优化.之后就是不断地在两个数据集中来回交替，最终所有的α都满足KKT条件的时候，算法中止。为了能够快速选取有最大步长的α，我们需要对所有数据对应的误差进行缓存，因此特地写了个SVMUtil类来保存svm中重要的变量以及一些辅助方法:PythonclassSVMUtil(object):'''StructtosaveallimportantvaluesinSVM.'''def__init__(self,dataset,labels,C,tolerance=0.001):self.dataset,self.labels,self.C=dataset,labels,Cself.m,self.n=np.array(dataset).shapeself.alphas=np.zeros(self.m)self.b=0self.tolerance=tolerance#Cachederrors,f(x_i)-y_iself.errors=[self.get_error(i)foriinrange(self.m)]#其他方法......1234567891011121314classSVMUtil(object):    '''    StructtosaveallimportantvaluesinSVM.    '''    def__init__(self,dataset,labels,C,tolerance=0.001):        self.dataset,self.labels,self.C=dataset,labels,C        self.m,self.n=np.array(dataset).shape        self.alphas=np.zeros(self.m)        self.b=0        self.tolerance=tolerance        #Cachederrors,f(x_i)-y_i        self.errors=[self.get_error(i)foriinrange(self.m)]    #其他方法......下面为第一个变量选择交替遍历的大致代码，相应完整的Python实现(完整实现见https://github.com/PytLab/MLBox/blob/master/svm/svm_platt_smo.py):Pythonwhile(it<max_iter):pair_changed=0ifentire:foriinrange(svm_util.m):pair_changed+=examine_example(i,svm_util)print('Fullset-iter:{},pairchanged:{}'.format(i,pair_changed))else:alphas=svm_util.alphasnon_bound_indices=[iforiinrange(svm_util.m)ifalphas[i]>0andalphas[i]<C]foriinnon_bound_indices:pair_changed+=examine_example(i,svm_util)......1234567891011121314while(it<max_iter):    pair_changed=0    ifentire:        foriinrange(svm_util.m):            pair_changed+=examine_example(i,svm_util)            print('Fullset-iter:{},pairchanged:{}'.format(i,pair_changed))    else:        alphas=svm_util.alphas        non_bound_indices=[iforiinrange(svm_util.m)                            ifalphas[i]>0andalphas[i]<C]        foriinnon_bound_indices:            pair_changed+=examine_example(i,svm_util)    ......第二个变量的选择SMO中的第二个变量的选择过程为内循环，当我们已经选取第一个α1之后，我们希望我们选取的第二个变量α2优化后能有较大的变化。根据我们之前推导的式子可以知道，新的α2的变化依赖于|E1−E2|,当E1为正时，那么选择最小的Ei作为E2，通常将每个样本的Ei缓存到一个列表中，通过在列表中选择具有|E1−E2|的α2来近似最大化步长。有时候按照上述的启发式方式仍不能够是的函数值有足够的下降，这是按下述步骤进行选择:在非边界数据集上选择能够使函数值足够下降的样本作为第二个变量如果非边界数据集上没有，则在整个数据仅上进行第二个变量的选择如果仍然没有则重新选择第一个α1第二个变量选取的Python实现:Pythondefselect_j(i,svm_util):'''通过最大化步长的方式来获取第二个alpha值的索引.'''errors=svm_util.errorsvalid_indices=[ifori,ainenumerate(svm_util.alphas)if0<a<svm_util.C]iflen(valid_indices)>1:j=-1max_delta=0forkinvalid_indices:ifk==i:continuedelta=abs(errors[i]-errors[j])ifdelta>max_delta:j=kmax_delta=deltaelse:j=select_j_rand(i,svm_util.m)returnj123456789101112131415161718defselect_j(i,svm_util):    '''通过最大化步长的方式来获取第二个alpha值的索引.    '''    errors=svm_util.errors    valid_indices=[ifori,ainenumerate(svm_util.alphas)if0<a<svm_util.C]    iflen(valid_indices)>1:        j=-1        max_delta=0        forkinvalid_indices:            ifk==i:                continue            delta=abs(errors[i]-errors[j])            ifdelta>max_delta:                j=k                max_delta=delta    else:        j=select_j_rand(i,svm_util.m)    returnjKKT条件允许一定的误差在Platt论文中的KKT条件的判断中有一个tolerance允许一定的误差，相应的Python实现：Pythonr=E_i*y_i#是否违反KKT条件if(r<-toleranceandalpha<C)or(r>toleranceandalpha>0):...1234r=E_i*y_i#是否违反KKT条件if(r<-toleranceandalpha<C)or(r>toleranceandalpha>0):    ...关于PlattSMO的完整实现详见:https://github.com/PytLab/MLBox/blob/master/svm/svm_platt_smo.py针对之前的数据集我们使用PlattSMO进行优化可以得到：Pythonw=[0.8289668843516077,-0.26578914269411114]b=-3.929258304055944812w=[0.8289668843516077,-0.26578914269411114]b=-3.9292583040559448将分割线和支持向量可视化：可见通过PlattSMO优化出来的支持向量与简化版的SMO算法有些许不同。使用遗传算法优化SVM由于最近自己写了个遗传算法框架，遗传算法作为一个启发式无导型的搜索算法非常易用，于是我就尝试使用遗传算法来优化SVM。使用遗传算法优化，我们就可以直接优化SVM的最初形式了也就是最直观的形式:顺便再安利下自己的遗传算法框架，在此框架的帮助下，优化SVM算法我们只需要写几十行的Python代码即可。其中最主要的就是编写适应度函数，根据上面的公式我们需要计算数据集中每个点到分割线的距离并返回最小的距离即可，然后放到遗传算法中进行进化迭代。遗传算法框架GAFT项目地址: https://github.com/PytLab/gaft ,使用方法详见README。Ok，我们开始构建种群用于进化迭代。创建个体与种群对于二维数据点，我们需要优化的参数只有三个也就是[w1,w2]和b,个体的定义如下:Pythonindv_template=GAIndividual(ranges=[(-2,2),(-2,2),(-5,5)],encoding='binary',eps=[0.001,0.001,0.005])123indv_template=GAIndividual(ranges=[(-2,2),(-2,2),(-5,5)],                            encoding='binary',                            eps=[0.001,0.001,0.005])种群大小这里取600，创建种群Pythonpopulation=GAPopulation(indv_template=indv_template,size=600).init()1population=GAPopulation(indv_template=indv_template,size=600).init()创建遗传算子和GA引擎这里没有什么特别的，直接使用框架中内置的算子就好了。Pythonselection=RouletteWheelSelection()crossover=UniformCrossover(pc=0.8,pe=0.5)mutation=FlipBitBigMutation(pm=0.1,pbm=0.55,alpha=0.6)engine=GAEngine(population=population,selection=selection,crossover=crossover,mutation=mutation,analysis=[ConsoleOutput,FitnessStore])123456selection=RouletteWheelSelection()crossover=UniformCrossover(pc=0.8,pe=0.5)mutation=FlipBitBigMutation(pm=0.1,pbm=0.55,alpha=0.6)engine=GAEngine(population=population,selection=selection,                  crossover=crossover,mutation=mutation,                  analysis=[ConsoleOutput,FitnessStore])适应度函数这一部分只要把上面svm初始形式描述出来就好了，只需要三行代码:Python@engine.fitness_registerdeffitness(indv):w,b=indv.variants[:-1],indv.variants[-1]min_dis=min([y*(np.dot(w,x)+b)forx,yinzip(dataset,labels)])returnfloat(min_dis)12345@engine.fitness_registerdeffitness(indv):    w,b=indv.variants[:-1],indv.variants[-1]    min_dis=min([y*(np.dot(w,x)+b)forx,yinzip(dataset,labels)])    returnfloat(min_dis)开始迭代这里迭代300代种群Pythonif'__main__'==__name__:engine.run(300)12if'__main__'==__name__:    engine.run(300)绘制遗传算法优化的分割线Pythonvariants=engine.population.best_indv(engine.fitness).variantsw=variants[:-1]b=variants[-1]#分类数据点classified_pts={'+1':[],'-1':[]}forpoint,labelinzip(dataset,labels):iflabel==1.0:classified_pts['+1'].append(point)else:classified_pts['-1'].append(point)fig=plt.figure()ax=fig.add_subplot(111)#绘制数据点forlabel,ptsinclassified_pts.items():pts=np.array(pts)ax.scatter(pts[:,0],pts[:,1],label=label)#绘制分割线x1,_=max(dataset,key=lambdax:x[0])x2,_=min(dataset,key=lambdax:x[0])a1,a2=wy1,y2=(-b-a1*x1)/a2,(-b-a1*x2)/a2ax.plot([x1,x2],[y1,y2])plt.show()1234567891011121314151617181920212223variants=engine.population.best_indv(engine.fitness).variantsw=variants[:-1]b=variants[-1]#分类数据点classified_pts={'+1':[],'-1':[]}forpoint,labelinzip(dataset,labels):    iflabel==1.0:        classified_pts['+1'].append(point)    else:        classified_pts['-1'].append(point)fig=plt.figure()ax=fig.add_subplot(111)#绘制数据点forlabel,ptsinclassified_pts.items():    pts=np.array(pts)    ax.scatter(pts[:,0],pts[:,1],label=label)#绘制分割线x1,_=max(dataset,key=lambdax:x[0])x2,_=min(dataset,key=lambdax:x[0])a1,a2=wy1,y2=(-b-a1*x1)/a2,(-b-a1*x2)/a2ax.plot([x1,x2],[y1,y2])plt.show()得到的分割曲线如下图：完整的代码详见: https://github.com/PytLab/MLBox/blob/master/svm/svm_ga.py总结本文对SVM的优化进行了介绍，主要实现了PlattSMO算法优化SVM模型，并尝试使用遗传算法框架GAFT对初始SVM进行了优化。参考SequentialMinimalOptimization:AFastAlgorithmforTrainingSupportVectorMachines打赏支持我写出更多好文章，谢谢！打赏作者打赏支持我写出更多好文章，谢谢！1赞2收藏评论关于作者：iPytLab喜欢写程序的计算化学狗，Python/C/C++/Fortran,个人博客http://pytlab.org个人主页·我的文章·22·"], "art_url": ["http://python.jobbole.com/88706/"]}
{"art_img": ["http://wx4.sinaimg.cn/mw690/63918611gy1fkmh74kp2lj21400u0417.jpg"], "art_title": ["这 6 段代码，成就了如今的深度学习"], "art_create_time": ["2017/10/18"], "art_content": ["本文由伯乐在线-MentosZ翻译，黄利民校稿。未经许可，禁止转载！英文出处：EmilWallner。欢迎加入翻译组。从代码中追溯深度学习的历史深度学习发展到如今的地位，离不开下面这6段代码。本文介绍了这些代码的创作者及其完成这些突破性成就的故事背景。每个故事都有简单的代码示例，读者们可以在FloydHub和GitHub找到相关代码。Source:Googlepressimage图片来源：Google新闻图片 要运行FloydHub上的代码示例，请确保您的电脑已经安装了Floyd命令行工具，并将我上传的代码示例克隆到本地计算机。如果您是第一次使用FloydHub，可以先阅读我之前发布的文章中关于如何使用FloydHub的部分。在本地计算机示例项目中初始化命令行界面之后，您就可以运行以下命令在FloydHub上启动项目：floydrun--dataemilwallner/datasets/mnist/1:mnist--tensorboard--modejupyter1floydrun--dataemilwallner/datasets/mnist/1:mnist--tensorboard--modejupyter最小二乘法所有的深度学习算法都始于下面这个数学公式（我已将其转成Python代码）Python#y=mx+b#misslope,bisy-interceptdefcompute_error_for_line_given_points(b,m,coordinates):totalError=0foriinrange(0,len(coordinates)):x=coordinates[i][0]y=coordinates[i][1]totalError+=(y-(m*x+b))**2returntotalError/float(len(coordinates))#examplecompute_error_for_line_given_points(1,2,[[3,6],[6,9],[12,18]])123456789101112#y=mx+b#misslope,bisy-intercept defcompute_error_for_line_given_points(b,m,coordinates):    totalError=0    foriinrange(0,len(coordinates)):        x=coordinates[i][0]        y=coordinates[i][1]        totalError+=(y-(m*x+b))**2    returntotalError/float(len(coordinates))#examplecompute_error_for_line_given_points(1,2,[[3,6],[6,9],[12,18]])最小二乘法在1805年由Adrien-MarieLegendre首次提出（1805,Legendre），这位巴黎数学家也以测量仪器闻名。他极其痴迷于预测彗星的方位，坚持不懈地寻找一种可以基于彗星方位历史数据计算其轨迹的算法。他尝试了许多种算法，一遍遍试错，终于找到了一个算法与结果相符。Legendre的算法是首先预测彗星未来的方位，然后计算误差的平方，最终目的是通过修改预测值以减少误差平方和。而这也正是线性回归的基本思想。读者可以在Jupyternotebook中运行上述代码来加深对这个算法的理解。m是系数，b是预测的常数项，coordinates是彗星的位置。目标是找到合适的m和b使其误差尽可能小。这是深度学习的核心思想：给定输入值和期望的输出值，然后寻找两者之间的相关性。梯度下降Legendre这种通过手动尝试来降低错误率的方法非常耗时。荷兰的诺贝尔奖得主PeterDebye在一个世纪后（1909年）正式提出了一种简化这个过程的方法。假设Legendre的算法需要考虑一个参数——我们称之为X。Y轴表示每个X的误差值。Legendre的算法是找到使得误差最小的X。在下图中，我们可以看到当X=1.1时，误差Y取到最小值。PeterDebye注意到最低点左边的斜率是负的，而另一边则是正的。因此，如果知道了任意给定X的斜率值，就可以找到Y的最小值点。这便是梯度下降算法的基本思想。几乎所有的深度学习模型都会用到梯度下降算法。要实现这个算法，我们假设误差函数是Error=x^5-2x^3-2。要得到任意给定X的斜率，我们需要对其求导，即5x^4–6x^2：如果您需要复习导数的相关知识，请观看KhanAcademy的视频。下面我们用Python实现Debye的算法：Pythoncurrent_x=0.5#thealgorithmstartsatx=0.5learning_rate=0.01#stepsizemultipliernum_iterations=60#thenumberoftimestotrainthefunction#thederivativeoftheerrorfunction(x**4=thepowerof4orx^4)defslope_at_given_x_value(x):return5*x**4-6*x**2#MoveXtotherightorleftdependingontheslopeoftheerrorfunctionforiinrange(num_iterations):previous_x=current_xcurrent_x+=-learning_rate*slope_at_given_x_value(previous_x)print(previous_x)print(\"Thelocalminimumoccursat%f\"%current_x)123456789101112131415current_x=0.5#thealgorithmstartsatx=0.5learning_rate=0.01#stepsizemultipliernum_iterations=60#thenumberoftimestotrainthefunction #thederivativeoftheerrorfunction(x**4=thepowerof4orx^4)defslope_at_given_x_value(x):  return5*x**4-6*x**2 #MoveXtotherightorleftdependingontheslopeoftheerrorfunctionforiinrange(num_iterations):  previous_x=current_x  current_x+=-learning_rate*slope_at_given_x_value(previous_x)  print(previous_x) print(\"Thelocalminimumoccursat%f\"%current_x)这里的窍门在于learning_rate。我们通过沿斜率的相反方向行进来逼近最低点。此外，越接近最低点，斜率越小。因此当斜率接近零时，每一步下降的幅度会越来越小。num_iterations是你预计到达最小值之前所需的迭代次数。可以通过调试该参数训练自己关于梯度下降算法的直觉。线性回归最小二乘法配合梯度下降算法，就是一个完整的线性回归过程。在20世纪50年代和60年代，一批实验经济学家在早期的计算机上实现了这些想法。这个过程是通过实体打卡——真正的手工软件程序实现的。准备这些打孔卡就需要几天的时间，而通过计算机进行一次回归分析最多需要24小时。下面是用Python实现线性回归的一个示例（我们不需要在打卡机上完成这个操作）：Python#Priceofwheat/kgandtheaveragepriceofbreadwheat_and_bread=[[0.5,5],[0.6,5.5],[0.8,6],[1.1,6.8],[1.4,7]]defstep_gradient(b_current,m_current,points,learningRate):b_gradient=0m_gradient=0N=float(len(points))foriinrange(0,len(points)):x=points[i][0]y=points[i][1]b_gradient+=-(2/N)*(y-((m_current*x)+b_current))m_gradient+=-(2/N)*x*(y-((m_current*x)+b_current))new_b=b_current-(learningRate*b_gradient)new_m=m_current-(learningRate*m_gradient)return[new_b,new_m]defgradient_descent_runner(points,starting_b,starting_m,learning_rate,num_iterations):b=starting_bm=starting_mforiinrange(num_iterations):b,m=step_gradient(b,m,points,learning_rate)return[b,m]gradient_descent_runner(wheat_and_bread,1,1,0.01,100)123456789101112131415161718192021222324#Priceofwheat/kgandtheaveragepriceofbreadwheat_and_bread=[[0.5,5],[0.6,5.5],[0.8,6],[1.1,6.8],[1.4,7]] defstep_gradient(b_current,m_current,points,learningRate):    b_gradient=0    m_gradient=0    N=float(len(points))    foriinrange(0,len(points)):        x=points[i][0]        y=points[i][1]        b_gradient+=-(2/N)*(y-((m_current*x)+b_current))        m_gradient+=-(2/N)*x*(y-((m_current*x)+b_current))    new_b=b_current-(learningRate*b_gradient)    new_m=m_current-(learningRate*m_gradient)    return[new_b,new_m] defgradient_descent_runner(points,starting_b,starting_m,learning_rate,num_iterations):    b=starting_b    m=starting_m    foriinrange(num_iterations):        b,m=step_gradient(b,m,points,learning_rate)    return[b,m] gradient_descent_runner(wheat_and_bread,1,1,0.01,100)线性回归本身并没有引入什么新的内容。但是，如何将梯度下降算法运用到误差函数上就需要动动脑子了。运行代码并使用这个线性回归模拟器来加深你的理解吧。感知机接下来让我们来认识一下FrankRosenblatt。这是一个白天解剖老鼠大脑，晚上寻找外星生命迹象的家伙。1958年，他发明了一个模仿神经元的机器（1958,Rosenblatt），并因此登上《纽约时报》的头条：“NewNavyDeviceLearnsByDoing”。如果向Rosenblatt的机器展示50组分别在左右两侧有标记的图像，它可以在没有预先编程的情况下分辨出两张图像（标记的位置）。大众被这个可能真正拥有学习能力的机器震惊了。如上图所示，每个训练周期都是从左侧输入数据开始。给所有输入数据添加一个初始的随机权重。然后将它们相加。如果总和为负，将其输出为0，否则输出为1。如果预测结果是正确的，就不改变循环中的权重。如果预测结果是错误的，可以用误差乘以学习率来相应地调整权重。我们用经典的“或”逻辑来运行感知机。输入输出00=001=110=111=1下面是用Python实现的感知机模型：Pythonfromrandomimportchoicefromnumpyimportarray,dot,random1_or_0=lambdax:0ifx<0else1training_data=[(array([0,0,1]),0),(array([0,1,1]),1),(array([1,0,1]),1),(array([1,1,1]),1),]weights=random.rand(3)errors=[]learning_rate=0.2num_iterations=100foriinrange(num_iterations):input,truth=choice(training_data)result=dot(weights,input)error=truth-1_or_0(result)errors.append(error)weights+=learning_rate*error*inputforx,_intraining_data:result=dot(x,w)print(\"{}:{}->{}\".format(input[:2],result,1_or_0(result)))12345678910111213141516171819202122fromrandomimportchoicefromnumpyimportarray,dot,random1_or_0=lambdax:0ifx<0else1training_data=[(array([0,0,1]),0),                    (array([0,1,1]),1),                    (array([1,0,1]),1),                    (array([1,1,1]),1),]weights=random.rand(3)errors=[]learning_rate=0.2num_iterations=100 foriinrange(num_iterations):    input,truth=choice(training_data)    result=dot(weights,input)    error=truth-1_or_0(result)    errors.append(error)    weights+=learning_rate*error*input    forx,_intraining_data:    result=dot(x,w)    print(\"{}:{}->{}\".format(input[:2],result,1_or_0(result)))经过最初的炒作一年之后，MarvinMinsky和SeymourPapert击碎了这个想法（1969,Minsky&Papert）。当时，Minsky和Papert都在麻省理工学院的AI实验室工作。他们写了一本书，证明感知机只能解决线性问题。他们还批判了关于多层感知机的想法。可悲的是，FrankRosenblatt两年后因船难去世。在Minsky和Papert的书籍出版一年之后，一位芬兰硕士研究生提出了用多层感知机解决非线性问题的理论（Linnainmaa,1970）。由于业内主流对感知机普遍不看好，十多年来AI的研究资金也非常短缺。这是AI首次遇冷。Minsky和Papert对感知机的批判主要针对“异或”问题。这个逻辑与“或”逻辑相同，但有一个例外——对两个true语句取和（1＆1）时，结果返回False（0）。如上图所示，在“或”逻辑中，我们可以将true和false分开。但是可以看出，我们无法使用一个线性函数将“异或”逻辑的结果进行区分。人工神经网络到1986年，几项实验证明，神经网络可以解决复杂的非线性问题（Rumelhartetal.,1986）。当时计算机的运算速度比该理论提出的时候快了一万倍。Rumelhart等人是这样介绍他们赫赫有名的论文的：我们描述了一种新的类神经元网络学习过程——反向传播。该过程通过反复调整网络中的连接权重，最小化网络的实际输出向量与期望输出向量之间的差异。调整权重的结果就是，不属于输入或输出的内部“隐藏”单元成为了描述任务域的重要特征，并且这些单元的交互项还捕获了任务中的正则条件。相较于早期更简单的方法，如“感知机收敛过程”Nature323,533–536(09October1986)，反向传播可以创造出有用的新特征。为了理解这篇文章的核心内容，我会在下面重现DeepMind团队AndrewTrask的代码。这不是一段普通的代码。它曾被用于斯坦福大学AndrewKarpathy的深度学习课程，以及SirajRaval的Udacity课程。最重要的是，它解决了“异或”问题，也结束了AI遇冷的时代。学习这段代码之前，我们首先通过这个模拟器交互学习一到两个小时来掌握神经网络的核心逻辑。然后阅读Trask的博客，然后再阅读四次。需要注意到，X_XOR数据中添加的参数[1]是偏置神经元，它们等价于线性函数中的常数项。PythonimportnumpyasnpX_XOR=np.array([[0,0,1],[0,1,1],[1,0,1],[1,1,1]])y_truth=np.array([[0],[1],[1],[0]])np.random.seed(1)syn_0=2*np.random.random((3,4))-1syn_1=2*np.random.random((4,1))-1defsigmoid(x):output=1/(1+np.exp(-x))returnoutputdefsigmoid_output_to_derivative(output):returnoutput*(1-output)forjinrange(60000):layer_1=sigmoid(np.dot(X_XOR,syn_0))layer_2=sigmoid(np.dot(layer_1,syn_1))error=layer_2-y_truthlayer_2_delta=error*sigmoid_output_to_derivative(layer_2)layer_1_error=layer_2_delta.dot(syn_1.T)layer_1_delta=layer_1_error*sigmoid_output_to_derivative(layer_1)syn_1-=layer_1.T.dot(layer_2_delta)syn_0-=X_XOR.T.dot(layer_1_delta)print(\"OutputAfterTraining:n\",layer_2)1234567891011121314151617181920212223242526importnumpyasnp X_XOR=np.array([[0,0,1],[0,1,1],[1,0,1],[1,1,1]])y_truth=np.array([[0],[1],[1],[0]]) np.random.seed(1)syn_0=2*np.random.random((3,4))-1syn_1=2*np.random.random((4,1))-1 defsigmoid(x):    output=1/(1+np.exp(-x))    returnoutputdefsigmoid_output_to_derivative(output):    returnoutput*(1-output) forjinrange(60000):    layer_1=sigmoid(np.dot(X_XOR,syn_0))    layer_2=sigmoid(np.dot(layer_1,syn_1))    error=layer_2-y_truth    layer_2_delta=error*sigmoid_output_to_derivative(layer_2)    layer_1_error=layer_2_delta.dot(syn_1.T)    layer_1_delta=layer_1_error*sigmoid_output_to_derivative(layer_1)    syn_1-=layer_1.T.dot(layer_2_delta)    syn_0-=X_XOR.T.dot(layer_1_delta)    print(\"OutputAfterTraining:n\",layer_2)反向传播，矩阵乘法和梯度下降放在一起会让人很难理解。这个过程的可视化通常是对其背后原理的简化。专注于理解其背后的逻辑，但不要过多地考虑直觉上的理解。另外，读者们也可以看看AndrewKarpathy关于反向传播的课程，在这个可视化网站交互学习，以及阅读MichaelNielsen关于反向传播的章节。深度神经网络深度神经网络就是在输入层和输出层之间具有多个中间层的神经网络。这个概念最早是由RinaDechter(Dechter,1986)引入的，但在2012年，也就是在IBM的人工智能程序Watson赢得美国电视智力竞赛节目Jeopardy和Google推出猫咪识别器之后才受到广泛关注。深度神经网络与之前神经网络的核心结构相同，但是应用于一些不同的问题。在正则化方面也有很大改进。最初，这只是一组用来简化冗杂的地球数据的数学函数（Tikhonov，A.N.，1963）。而现在被用于神经网络中，以加强其泛化能力。这种技术创新很大程度上依赖于计算机的运算能力。而运算能力的提升大大缩短了研究者的创新周期——如今的GPU技术只需半秒钟就可以完成一个八十年代中期的超级计算机一年的运算量。计算成本的降低和各种深度学习库的发展将深度学习带入了大众视野。我们来看一个常见的深度学习堆栈示例，从底层开始：GPU>NvidiaTeslaK80。该硬件常用于图形处理。它们深度学习的速度平均要比CPU快50-200倍。CUDA>GPU的底层编程语言CuDNN>Nvidia的库，用来优化CUDATensorflow>由Google开发，基于CuDNN的深度学习框架TFlearn>Tensorflow的前端框架下面我们来看看MNIST数字分类图像，它被称作深度学习的“HelloWorld”。我们用TFlearn来实现：Pythonfrom__future__importdivision,print_function,absolute_importimporttflearnfromtflearn.layers.coreimportdropout,fully_connectedfromtensorflow.examples.tutorials.mnistimportinput_datafromtflearn.layers.convimportconv_2d,max_pool_2dfromtflearn.layers.normalizationimportlocal_response_normalizationfromtflearn.layers.estimatorimportregression#Dataloadingandpreprocessingmnist=input_data.read_data_sets(\"/data/\",one_hot=True)X,Y,testX,testY=mnist.train.images,mnist.train.labels,mnist.test.images,mnist.test.labelsX=X.reshape([-1,28,28,1])testX=testX.reshape([-1,28,28,1])#Buildingconvolutionalnetworknetwork=tflearn.input_data(shape=[None,28,28,1],name='input')network=conv_2d(network,32,3,activation='relu',regularizer=\"L2\")network=max_pool_2d(network,2)network=local_response_normalization(network)network=conv_2d(network,64,3,activation='relu',regularizer=\"L2\")network=max_pool_2d(network,2)network=local_response_normalization(network)network=fully_connected(network,128,activation='tanh')network=dropout(network,0.8)network=fully_connected(network,256,activation='tanh')network=dropout(network,0.8)network=fully_connected(network,10,activation='softmax')network=regression(network,optimizer='adam',learning_rate=0.01,loss='categorical_crossentropy',name='target')#Trainingmodel=tflearn.DNN(network,tensorboard_verbose=0)model.fit({'input':X},{'target':Y},n_epoch=20,validation_set=({'input':testX},{'target':testY}),snapshot_step=100,show_metric=True,run_id='convnet_mnist')1234567891011121314151617181920212223242526272829303132333435from__future__importdivision,print_function,absolute_importimporttflearnfromtflearn.layers.coreimportdropout,fully_connectedfromtensorflow.examples.tutorials.mnistimportinput_datafromtflearn.layers.convimportconv_2d,max_pool_2dfromtflearn.layers.normalizationimportlocal_response_normalizationfromtflearn.layers.estimatorimportregression #Dataloadingandpreprocessingmnist=input_data.read_data_sets(\"/data/\",one_hot=True)X,Y,testX,testY=mnist.train.images,mnist.train.labels,mnist.test.images,mnist.test.labelsX=X.reshape([-1,28,28,1])testX=testX.reshape([-1,28,28,1]) #Buildingconvolutionalnetworknetwork=tflearn.input_data(shape=[None,28,28,1],name='input')network=conv_2d(network,32,3,activation='relu',regularizer=\"L2\")network=max_pool_2d(network,2)network=local_response_normalization(network)network=conv_2d(network,64,3,activation='relu',regularizer=\"L2\")network=max_pool_2d(network,2)network=local_response_normalization(network)network=fully_connected(network,128,activation='tanh')network=dropout(network,0.8)network=fully_connected(network,256,activation='tanh')network=dropout(network,0.8)network=fully_connected(network,10,activation='softmax')network=regression(network,optimizer='adam',learning_rate=0.01,                        loss='categorical_crossentropy',name='target') #Trainingmodel=tflearn.DNN(network,tensorboard_verbose=0)model.fit({'input':X},{'target':Y},n_epoch=20,            validation_set=({'input':testX},{'target':testY}),            snapshot_step=100,show_metric=True,run_id='convnet_mnist')关于MNIST问题，有很多不错的文章：https://www.tensorflow.org/get_started/mnist/beginnershttps://www.youtube.com/watch?v=NMd7WjZiCzchttps://www.oreilly.com/learning/not-another-mnist-tutorial-with-tensorflow如果读者想更深入地了解TFlearn，可以浏览我之前的文章。小结我们从TFlearn的示例中可以看出，深度学习的基础逻辑仍与Rosenblatt的感知机类似。如今的神经网络大多使用Relu激活函数，而不是二元Heaviside阶跃函数。在卷积神经网络的最后一层，损失函数使用的是categorical_crossentropy，即分类交叉熵。这是对Legendre最小二乘法的改进，可用于多分类逻辑回归问题。这里的优化算法adam来源于Debye的梯度下降。Tikhonov的正则化概念以dropout层和正则化函数L1/L2的形式得到广泛应用。如果您想要对神经网络的概念以及如何实现有一个更直观的理解，请阅读我在FloydHub博客上发表的文章：我的第一个深度学习周末。感谢IgnacioTonoli，BrianYoung，PaalRingstad，TomasMoska和CharlieHarrington帮我审阅本文的草稿。代码来源储存在Jupyternotebook中。关于EmilWallner这是Emil博客中深度学习系列的第二部分。Emil花了十年时间探索人工学习。他曾在牛津大学商学院工作，投资教育创业公司，并开创了了技术教学业务。去年，他加入Ecole42，并将自己在人工学习领域的知识应用于机器学习。2赞23收藏4评论关于作者：MentosZ统计学小硕，R/Python，数据挖掘，机器学习个人主页·我的文章·11·"], "art_url": ["http://python.jobbole.com/88713/"]}
{"art_img": ["http://jbcdn2.b0.upaiyun.com/2012/02/python-logo.png"], "art_title": ["为什么Python对程序员重要？"], "art_create_time": ["2012/02/14"], "art_content": ["FredrikHaard最近发表了一篇“为什么Python对你如此重要”的文章，引起了开发者的热烈讨论。iteye 对其简要编译。我相信Python对软件开发人员很重要。现今已经诞生了不少的编程语言，它们都有各自不同的特性：强大者如Lisp，快速如C，运用广泛如Java，论古怪则如Haskell。与这些语言不同，Python是一门比较中庸的编程语言，它将语言的很多特性进行了融合，迄今我还未有其他语言如Python这般协调。 Python知道开发人员阅读代码比编写它花的时间要多得多，因此将精力集中于引导开发者编写易读的代码。当然，Python也能写出令人晦涩（obfuscated）的代码，但是写代码最舒服的方式还是（假如你了解Python）保持适度精炼，即：代码总能明确反映你的意图。这一点很重要。使用Python开发可谓轻而易举。甚至包括许多库，也能用Python完美编写，保证其易读性（你可以比较一下其他语言的框架实现，比如用Java编写的Spring）。 同样Python也意识到对开发人员而言开发速度的重要性。易读而精炼的代码只是一部分，另一部分取决于强大的构造函数，可避免许多繁琐重复的代码。此外，可维护性也是很重要的——代码行数（LineofCode，LoC）在很多度量结果都会中出现，或许没什么用，不过它至少说明了你需要审查多少代码，需要理解多少代码并从中发现问题。此外，FredrikHaard还提到了Python的另一个优势——Toolmaking。快速的软件开发速度、简练的技巧（其他语言开发人员也能轻松掌握Python基本技巧）、庞大的标准库维系了这一优势。任何项目都会遭遇任务自动化任务情况，在我的经验中，用Python写的自动化任务比其他主流语言要快一个数量级——事实上，这也是我学习Python的原因。…… 能够轻松开发customtool其实还包含了另一层意思，即开发和维护customsoftware也会很容易。这也是为什么，在庞大的Django成为最著名的PythonWeb框架之后，还是有大量成功的小巧甚至微型框架存在的原因。当使用一门强大的编程语言，拥有大量标准及第三方库的时候，你并不经常需要考虑妥协（trade-off），而这在使用许多现成的（off-the-shelf）大型框架时是必然会遭遇的。根据Fredrik的观点，编写能够很好契合客户模型而不是一个框架的软件这点很重要。而许多开发人员将时间都耗在了框架配置以及掩盖它们的缺点上，而不是真正的开发。你是如何看的呢？1赞收藏评论"], "art_url": ["http://python.jobbole.com/13153/"]}
{"art_img": ["http://ww1.sinaimg.cn/mw690/005N3SJDgw1eknq3byyroj30er0crgrb.jpg"], "art_title": ["谁说不能用Python写出让人迷惑的代码？"], "art_create_time": ["2011/09/13"], "art_content": ["本文由伯乐在线-伯乐在线读者翻译。未经许可，禁止转载！英文出处：preshing。欢迎加入翻译组。这里是显示彭罗斯点阵的Python的脚本。是的，这是可以运行的有效Phython代码。译注：彭罗斯点阵，物理学术语。上世纪70年代英国数学家彭罗斯第一次提出了这个概念，称为彭罗斯点阵(Pen-rosetiles)。Python_=\\\"\"\"if!1:\"e,V=1000,(0j-1)**-.2;v,S=.5/V.real,[(0,0,4*e,4*e*V)];w=1-v\"def!E(T,A,B,C):P,Q,R=B*w+A*v,B*w+C*v,A*w+B*v;return[(1,Q,C,A),(1,P,Q,B),(0,Q,P,A)]*T+[(0,C,R,B),(1,R,C,A)]*(1-T)\"for!i!in!_[:11]:S=sum([E(*x)for!x!in!S],[])\"import!cairo!as!O;s=O.ImageSurface(1,e,e);c=O.Context(s);M,L,G=c.move_to,c.line_to,c.set_source_rgba\"def!z(f,a):f(-a.imag,a.real-e-e)\"for!T,A,B,C!in[i!for!i!in!S!if!i[\"\"\";exec(reduce(lambdax,i:x.replace(chr(i),\"\\n\"[34-i:]),range(35),_+\"\"\"0]]:z(M,A);z(L,B);z(L,C);c.close_path()\"G(.4,.3,1);c.paint();G(.7,.7,1);c.fill()\"for!i!in!range(9):\"!g=1-i/8;d=i/4*g;G(d,d,d,1-g*.8)\"!def!y(f,a):z(f,a+(1+2j)*(1j**(i/2.))*g)\"!for!T,A,B,C!in!S:y(M,C);y(L,A);y(M,A);y(L,B)\"!c.stroke()\"s.write_to_png('penrose.png')\"\"\"))123456789101112131415161718192021222324252627282930_                                =\\                                \"\"\"if!                              1:\"e,V=100                            0,(0j-1)**-.2;                          v,S=.5/  V.real,                        [(0,0,4      *e,4*e*                      V)];w=1          -v\"def!                      E(T,A,              B,C):P                  ,Q,R=B*w+                A*v,B*w+C            *v,A*w+B*v;retur              n[(1,Q,C,A),(1,P    ,Q,B),(0,Q,P,A)]*T+[(0,C            ,R,B),(1,R,C,A)]*(1-T)\"for!i!in!_[:11]:S      =sum([E          (*x)for      !x!in!S],[])\"imp  ort!cair              o!as!O;      s=O.Ima              geSurfac  e(1,e,e)              ;c=O.Con  text(s);              M,L,G=c.    move_to                ,c.line_to,c.s                et_sour      ce_rgb                a\"def!z(f,a)                :f(-a.        imag,a.      real-e-e)\"for!T,A,B,C!in[i      !for!i!          in!S!if!i[\"\"\";exec(reduce(lambdax,i:x.replace(chr          (i),\"\\n\"[34-i:]),  range(  35),_+\"\"\"0]]:z(M,A            );z(L,B);z        (L,C);        c.close_pa            th()\"G            (.4,.3            ,1);c.            paint(            );G(.7            ,.7,1)            ;c.fil            l()\"fo            r!i!in            !range            (9):\"!            g=1-i/            8;d=i/          4*g;G(d,d,d,          1-g*.8            )\"!def    !y(f,a):z(f,a+(1+2j)*(    1j**(i            /2.))*g)\"!for!T,A,B,C!in!S:y(M,C);y(L,A);y(M            ,A);y(L,B)\"!c.st            roke()\"s.write_t            o_png('pen                        rose.png')            \"\"\"                                      ))当这个程序运行时，它输出了一个1000×1000的图像文件，包含大约2212个由3D立体效应渲染的彭罗斯点阵。这里是该图像的一部分（点击放大）。运行该脚本需要Pycairo。它只在Python它是标准的Python脚本，但我努力想把它变得更简洁，于是我又从中删减了一些。编注：Pycairo是一组Python版本的Cario图形库。彭罗斯点阵很酷，因为它们非周期性地覆盖了整个平面——图片的转换副本与原型从来不会一致。它们是由RogerPenrose先生通过将五边形的平面平铺在一起的一系列尝试而发明的。与C或Perl相比，Python并不是让人迷惑的编程语言。这种比较似乎也从未发生，而且在网上也没有多少让人费解的Python的例子：你可以在官方的Python常见问题中或各种网页如这里和这里找到一些例子。在2011年的PyCon对此还有专题讨论。我相信输出一个高分辨率的图像是第一个让人费解的Python程序。如果你知道其它的例子，可以在评论中告诉我。 翻译：伯乐在线 –张秀君1赞收藏评论关于作者：伯乐在线读者①本账号用于发布那些在伯乐在线无账号的读者的投稿，包括译文和原创文章。②欢迎加入伯乐在线专栏作者：http://blog.jobbole.com/99322/个人主页·我的文章·33"], "art_url": ["http://python.jobbole.com/1414/"]}
{"art_img": ["http://jbcdn2.b0.upaiyun.com/2015/02/2c224095b3bd5c6fee7c908a481292f7.jpg"], "art_title": ["SocketServer ——网络通信服务器"], "art_create_time": ["2017/09/20"], "art_content": ["原文出处：LearnPython   SocketServer是Python标准库中的一个模块，其作用是创建网络服务器。SocketServer模块定义了一些类来处理诸如TCP、UDP、UNIX流和UNIX数据报之上的同步网络请求。SocketServer模块处理网络请求的功能，可以通过两个主要的类来实现：一个是服务器类，一个是请求处理类。服务器类处理通信问题，如监听一个套接字并接收连接等；请求处理类处理“协议”问题，如解释到来的数据、处理数据并把数据发回给客户端等。这种实现将服务器的实现过程和请求处理的实现过程解耦，这意味着我们可以将不同的服务器实现和请求处理实现结合起来来处理一些定制的协议，例如一个TCP服务器类和一个流请求处理类结合，处理基于TCP的网络请求。同时，也可以基于SocketServer模块中的服务器类和请求处理类，实现网络层之上应用层的服务器和请求处理实现，例如基于TCP服务器类实现HTTP服务器，基于流处理请求类实现HTTP请求处理类等。服务器类SocketServer模块中定义了五种服务器类。BaseServer(服务器的基类，定义了API)TCPServer(使用TCP/IP套接字)UDPServer(使用数据报套接字)UnixStreamServer(使用UNIX域套接字，只适用UNIX平台)UnixDatagramServer(使用UNIX域套接字，只适用UNIX平台)1.构造服务器对象要构建一个服务器对象，需要向它传递一个地址server_address（服务器将在这个地址上监听请求），以及一个请求处理类RequestHandlerClass（不是请求处理实例）。服务器类基类的构造函数如下：PythonclassBaseServer:def__init__(self,server_address,RequestHandlerClass):\"\"\"Constructor.Maybeextended,donotoverride.\"\"\"self.server_address=server_addressself.RequestHandlerClass=RequestHandlerClassself.__is_shut_down=threading.Event()self.__shutdown_request=False1234567classBaseServer:    def__init__(self,server_address,RequestHandlerClass):        \"\"\"Constructor.  Maybeextended,donotoverride.\"\"\"        self.server_address=server_address        self.RequestHandlerClass=RequestHandlerClass        self.__is_shut_down=threading.Event()        self.__shutdown_request=False之后，可以构造TCPServer、UDPServer、UnixStreamServer、UnixDatagramServer。其中，TCPServer继承自BaseServer，UDPServer和UnixStreamServer继承自TCPServer，UnixDatagramServer继承自UDPServer。各个服务器类型可以根据自己的特点对基类进行扩展，例如创建监听套接字、绑定监听地址和端口、进行监听等。一旦实例化服务器对象，便可以使用服务器的方法来监听和处理请求。2.实现服务器由于SocketServer模块中定义的五种服务器类中，除了基类BaseServer和TCPServer外，其余的三个类都是直接或间接地继承自TCPServer。因此，以下以TCPServer的实现过程为例进行说明。构造TCPServer。构造TCPServer时，构造函数创建了一个套接字（这个套接字可以通过更改地址簇和类型用于其他服务器）用于监听请求。并且调用server_bind()绑定监听的地址和端口，调用server_activate()开始监听。启动服务器。服务器实例化后，可以使用serve_forever()或者handle_request()来监听和处理请求，实现服务器功能。这两个方法的具体实现依赖于_handle_request_noblock()方法。这个方法是BaseServer类中定义的。具体实现如下：Pythondef_handle_request_noblock(self):\"\"\"Handleonerequest,withoutblocking.Iassumethatselect.selecthasreturnedthatthesocketisreadablebeforethisfunctionwascalled,sothereshouldbenoriskofblockinginget_request().\"\"\"try:request,client_address=self.get_request()exceptsocket.error:returnifself.verify_request(request,client_address):try:self.process_request(request,client_address)except:self.handle_error(request,client_address)self.shutdown_request(request)else:self.shutdown_request(request)123456789101112131415161718def_handle_request_noblock(self):    \"\"\"Handleonerequest,withoutblocking.    Iassumethatselect.selecthasreturnedthatthesocketis    readablebeforethisfunctionwascalled,sothereshouldbe    noriskofblockinginget_request().    \"\"\"    try:        request,client_address=self.get_request()    exceptsocket.error:        return    ifself.verify_request(request,client_address):        try:            self.process_request(request,client_address)        except:            self.handle_error(request,client_address)            self.shutdown_request(request)    else:        self.shutdown_request(request)处理请求。根据上一步骤启动服务器后，服务器便开始监听请求。如果接收到请求信息，便开始处理请求。由_handle_request_noblock()可以看出有几个函数比较重要。get_request()——这个函数可以在子类中重写。在TCPServer中，该函数调用监听套接字的accept()方法，返回请求request和客户端地址client_address。verify_request(request,client_address)——这个函数可以在子类中重写。该函数返回True表示处理请求，返回False表示忽略请求。process_request(request,client_address)——这个函数可以在子类中重写。该函数将调用finish_request()具体完成请求的处理过程，并且在处理完请求后关闭请求。finish_request(request,client_address)——该函数将构造一个请求处理类的实例。请求处理类被实例化后将调用其handle()方法处理请求。3.进程/线程支持SocketServer模块中还提供了一些”mix-in”类：ForkingMixIn和ThreadingMixIn。这些类可以和服务器类混合使用，很容易改变服务器，为每个请求使用一个单独的进程或线程。具体的服务器类有：classForkingUDPServer(ForkingMixIn,UDPServer)classForkingTCPServer(ForkingMixIn,TCPServer)classThreadingUDPServer(ThreadingMixIn,UDPServer)classThreadingTCPServer(ThreadingMixIn,TCPServer)classThreadingUnixStreamServer(ThreadingMixIn,UnixStreamServer)classThreadingUnixDatagramServer(ThreadingMixIn,UnixDatagramServer)请求处理类要接收到来的请求以及确定采取什么行动，其中大部分的工作都是由请求处理类完成的。请求处理类负责在套接字层之上实现协议。具体过程为：读取请求、处理请求、写回响应。请求处理类基类中定义了3个方法，子类中需要重写。setup()——为请求准备请求处理器handle()——对请求完成具体的工作。诸如解析到来的请求，处理数据，并发回响应等。finish()——清理setup()期间创建的所有数据1赞5收藏评论"], "art_url": ["http://python.jobbole.com/88626/"]}
{"art_img": ["http://jbcdn2.b0.upaiyun.com/2017/11/92f71271d02d70372302b5e0886d112d.png"], "art_title": ["使用 GC、Objgraph 干掉 Python 内存泄露与循环引用！"], "art_create_time": ["2017/11/06"], "art_content": ["原文出处：xybaby   Python使用引用计数和垃圾回收来做内存管理，前面也写过一遍文章《Python内存优化》，介绍了在python中，如何profile内存使用情况，并做出相应的优化。本文介绍两个更致命的问题：内存泄露与循环引用。内存泄露是让所有程序员都闻风丧胆的问题，轻则导致程序运行速度减慢，重则导致程序崩溃；而循环引用是使用了引用计数的数据结构、编程语言都需要解决的问题。本文揭晓这两个问题在python语言中是如何存在的，然后试图利用gc模块和objgraph来解决这两个问题。注意：本文的目标是Cpython，测试代码都是运行在Python2.7。另外，本文不考虑C扩展造成的内存泄露，这是另一个复杂且头疼的问题。一分钟版本python使用引用计数和垃圾回收来释放（free）Python对象引用计数的优点是原理简单、将消耗均摊到运行时；缺点是无法处理循环引用Python垃圾回收用于处理循环引用，但是无法处理循环引用中的对象定义了__del__的情况，而且每次回收会造成一定的卡顿gcmodule是python垃圾回收机制的接口模块，可以通过该module启停垃圾回收、调整回收触发的阈值、设置调试选项如果没有禁用垃圾回收，那么Python中的内存泄露有两种情况：要么是对象被生命周期更长的对象所引用，比如global作用域对象；要么是循环引用中存在__del__使用gcmodule、objgraph可以定位内存泄露，定位之后，解决很简单垃圾回收比较耗时，因此在对性能和内存比较敏感的场景也是无法接受的，如果能解除循环引用，就可以禁用垃圾回收。使用gcmodule的DEBUG选项可以很方便的定位循环引用，解除循环引用的办法要么是手动解除，要么是使用weakrefpython内存管理Python中，一切都是对象，又分为mutable和immutable对象。二者区分的标准在于是否可以原地修改，“原地“”可以理解为相同的地址。可以通过id()查看一个对象的“地址”，如果通过变量修改对象的值，但id没发生变化，那么就是mutable，否则就是immutable。比如：Python>>>a=5;id(a)35170056>>>a=6;id(a)35170044>>>lst=[1,2,3];id(lst)39117168>>>lst.append(4);id(lst)39117168123456789>>>a=5;id(a) 35170056>>>a=6;id(a)35170044>>>lst=[1,2,3];id(lst)39117168>>>lst.append(4);id(lst)39117168a指向的对象（int类型）就是immutable，赋值语句只是让变量a指向了一个新的对象，因为id发生了变化。而lst指向的对象（list类型）为可变对象，通过方法（append）可以修改对象的值，同时保证id一致。判断两个变量是否相等（值相同）使用==，而判断两个变量是否指向同一个对象使用is。比如下面a1a2这两个变量指向的都是空的列表，值相同，但是不是同一个对象。Python>>>a1,a2=[],[]>>>a1==a2True>>>a1isa2False12345>>>a1,a2=[],[]>>>a1==a2True>>>a1isa2False为了避免频繁的申请、释放内存，避免大量使用的小对象的构造析构，python有一套自己的内存管理机制。在巨著《Python源码剖析》中有详细介绍，在python源码obmalloc.h中也有详细的描述。如下所示：可以看到，python会有自己的内存缓冲池（layer2）以及对象缓冲池（layer3）。在Linux上运行过Python服务器的程序都知道，python不会立即将释放的内存归还给操作系统，这就是内存缓冲池的原因。而对于可能被经常使用、而且是immutable的对象，比如较小的整数、长度较短的字符串，python会缓存在layer3，避免频繁创建和销毁。例如：Python>>>a,b=1,1>>>aisbTrue>>>a,b=(),()>>>aisbTrue>>>a,b={},{}>>>aisbFalse123456789>>>a,b=1,1>>>aisbTrue>>>a,b=(),()>>>aisbTrue>>>a,b={},{}>>>aisbFalse本文并不关心python是如何管理内存块、如何管理小对象，感兴趣的读者可以参考伯乐在线和csdn上的这两篇文章。本文关心的是，一个普通的对象的生命周期，更明确的说，对象是什么时候被释放的。当一个对象理论上（或者逻辑上）不再被使用了，但事实上没有被释放，那么就存在内存泄露；当一个对象事实上已经不可达（unreachable），即不能通过任何变量找到这个对象，但这个对象没有立即被释放，那么则可能存在循环引用。引用计数引用计数（Referencescount），指的是每个Python对象都有一个计数器，记录着当前有多少个变量指向这个对象。将一个对象直接或者间接赋值给一个变量时，对象的计数器会加1；当变量被del删除，或者离开变量所在作用域时，对象的引用计数器会减1。当计数器归零的时候，代表这个对象再也没有地方可能使用了，因此可以将对象安全的销毁。Python源码中，通过Py_INCREF和Py_DECREF两个宏来管理对象的引用计数，代码在object.h#definePy_INCREF(op)(\\_Py_INC_REFTOTAL_Py_REF_DEBUG_COMMA\\((PyObject*)(op))->ob_refcnt++)#definePy_DECREF(op)\\do{\\if(_Py_DEC_REFTOTAL_Py_REF_DEBUG_COMMA\\--((PyObject*)(op))->ob_refcnt!=0)\\_Py_CHECK_REFCNT(op)\\else\\_Py_Dealloc((PyObject*)(op));\\}while(0)123456789101112#definePy_INCREF(op)(                        \\    _Py_INC_REFTOTAL  _Py_REF_DEBUG_COMMA      \\    ((PyObject*)(op))->ob_refcnt++) #definePy_DECREF(op)                                  \\    do{                                                \\        if(_Py_DEC_REFTOTAL  _Py_REF_DEBUG_COMMA      \\        --((PyObject*)(op))->ob_refcnt!=0)            \\            _Py_CHECK_REFCNT(op)                        \\        else                                            \\        _Py_Dealloc((PyObject*)(op));                  \\    }while(0)通过sys.getrefcount(obj)对象可以获得一个对象的引用数目，返回值是真实引用数目加1（加1的原因是obj被当做参数传入了getrefcount函数），例如：Python>>>importsys>>>s='asdf'>>>sys.getrefcount(s)2>>>a=1>>>sys.getrefcount(a)6051234567>>>importsys>>>s='asdf'>>>sys.getrefcount(s)2>>>a=1>>>sys.getrefcount(a)605从对象1的引用计数信息也可以看到，python的对象缓冲池会缓存十分常用的immutable对象，比如这里的整数1。引用计数的优点在于原理通俗易懂；且将对象的回收分布在代码运行时：一旦对象不再被引用，就会被释放掉（befreed），不会造成卡顿。但也有缺点：额外的字段（ob_refcnt）；频繁的加减ob_refcnt，而且可能造成连锁反应。但这些缺点跟循环引用比起来都不算事儿。什么是循环引用，就是一个对象直接或者间接引用自己本身，引用链形成一个环。且看下面的例子：#-*-coding:utf-8-*-importobjgraph,sysclassOBJ(object):passdefshow_direct_cycle_reference():a=OBJ()a.attr=aobjgraph.show_backrefs(a,max_depth=5,filename=\"direct.dot\")defshow_indirect_cycle_reference():a,b=OBJ(),OBJ()a.attr_b=bb.attr_a=aobjgraph.show_backrefs(a,max_depth=5,filename=\"indirect.dot\")if__name__=='__main__':iflen(sys.argv)>1:show_direct_cycle_reference()else:show_indirect_cycle_reference()123456789101112131415161718192021#-*-coding:utf-8-*-importobjgraph,sysclassOBJ(object):    pass defshow_direct_cycle_reference():    a=OBJ()    a.attr=a    objgraph.show_backrefs(a,max_depth=5,filename=\"direct.dot\") defshow_indirect_cycle_reference():    a,b=OBJ(),OBJ()    a.attr_b=b    b.attr_a=a    objgraph.show_backrefs(a,max_depth=5,filename=\"indirect.dot\") if__name__=='__main__':    iflen(sys.argv)>1:        show_direct_cycle_reference()    else:        show_indirect_cycle_reference()运行上面的代码，使用graphviz工具集（本文使用的是dotty）打开生成的两个文件，direct.dot和indirect.dot，得到下面两个图通过属性名(attr,attr_a,attr_b）可以很清晰的看出循环引用是怎么产生的前面已经提到，对于一个对象，当没有任何变量指向自己时，引用计数降到0，就会被释放掉。我们以上面左边那个图为例，可以看到，红框里面的OBJ对象想在有两个引用（两个入度），分别来自帧对象frame（代码中，函数局部空间持有对OBJ实例的引用）、attr变量。我们再改一下代码，在函数运行技术之后看看是否还有OBJ类的实例存在，引用关系是怎么样的：#-*-coding:utf-8-*-importobjgraph,sysclassOBJ(object):passdefdirect_cycle_reference():a=OBJ()a.attr=aif__name__=='__main__':direct_cycle_reference()objgraph.show_backrefs(objgraph.by_type('OBJ')[0],max_depth=5,filename=\"direct.dot\"123456789101112#-*-coding:utf-8-*-importobjgraph,sysclassOBJ(object):    pass defdirect_cycle_reference():    a=OBJ()    a.attr=a    if__name__=='__main__':    direct_cycle_reference()    objgraph.show_backrefs(objgraph.by_type('OBJ')[0],max_depth=5,filename=\"direct.dot\"修改后的代码，OBJ实例(a)存在于函数的local作用域。因此，当函数调用结束之后，来自帧对象frame的引用被解除。从图中可以看到，当前对象的计数器（入度）为1，按照引用计数的原理，是不应该被释放的，但这个对象在函数调用结束之后就是事实上的垃圾，这个时候就需要另外的机制来处理这种情况了。python的世界，很容易就会出现循环引用，比如标准库Collections中OrderedDict的实现（已去掉无关注释）：classOrderedDict(dict):def__init__(self,*args,**kwds):iflen(args)>1:raiseTypeError('expectedatmost1arguments,got%d'%len(args))try:self.__rootexceptAttributeError:self.__root=root=[]#sentinelnoderoot[:]=[root,root,None]self.__map={}self.__update(*args,**kwds)1234567891011classOrderedDict(dict):    def__init__(self,*args,**kwds):        iflen(args)>1:            raiseTypeError('expectedatmost1arguments,got%d'%len(args))        try:            self.__root        exceptAttributeError:            self.__root=root=[]                    #sentinelnode            root[:]=[root,root,None]            self.__map={}        self.__update(*args,**kwds)注意第8、9行，root是一个列表，列表里面的元素之自己本身！垃圾回收这里强调一下，本文中的的垃圾回收是狭义的垃圾回收，是指当出现循环引用，引用计数无计可施的时候采取的垃圾清理算法。在python中，使用标记-清除算法（mark-sweep）和分代（generational）算法来垃圾回收。在《GarbageCollectionforPython》一文中有对标记回收算法，然后在《Python内存管理机制及优化简析》一文中，有对前文的翻译，并且有分代回收的介绍。在这里，引用后面一篇文章：在Python中,所有能够引用其他对象的对象都被称为容器(container).因此只有容器之间才可能形成循环引用.Python的垃圾回收机制利用了这个特点来寻找需要被释放的对象.为了记录下所有的容器对象,Python将每一个容器都链到了一个双向链表中,之所以使用双向链表是为了方便快速的在容器集合中插入和删除对象.有了这个维护了所有容器对象的双向链表以后,Python在垃圾回收时使用如下步骤来寻找需要释放的对象:对于每一个容器对象,设置一个gc_refs值,并将其初始化为该对象的引用计数值.对于每一个容器对象,找到所有其引用的对象,将被引用对象的gc_refs值减1.执行完步骤2以后所有gc_refs值还大于0的对象都被非容器对象引用着,至少存在一个非循环引用.因此不能释放这些对象,将他们放入另一个集合.在步骤3中不能被释放的对象,如果他们引用着某个对象,被引用的对象也是不能被释放的,因此将这些对象也放入另一个集合中.此时还剩下的对象都是无法到达的对象.现在可以释放这些对象了.关于分代回收：除此之外,Python还将所有对象根据’生存时间’分为3代,从0到2.所有新创建的对象都分配为第0代.当这些对象经过一次垃圾回收仍然存在则会被放入第1代中.如果第1代中的对象在一次垃圾回收之后仍然存货则被放入第2代.对于不同代的对象Python的回收的频率也不一样.可以通过gc.set_threshold(threshold0[,threshold1[,threshold2]]) 来定义.当Python的垃圾回收器中新增的对象数量减去删除的对象数量大于threshold0时,Python会对第0代对象执行一次垃圾回收.每当第0代被检查的次数超过了threshold1时,第1代对象就会被执行一次垃圾回收.同理每当第1代被检查的次数超过了threshold2时,第2代对象也会被执行一次垃圾回收.注意，threshold0，threshold1，threshold2的意义并不相同！为什么要分代呢，这个算法的根源来自于weak generational hypothesis。这个假说由两个观点构成：首先是年亲的对象通常死得也快，比如大量的对象都存在于local作用域；而老对象则很有可能存活更长的时间，比如全局对象，module，class。垃圾回收的原理就如上面提示，详细的可以看Python源码，只不过事实上垃圾回收器还要考虑__del__，弱引用等情况，会略微复杂一些。什么时候会触发垃圾回收呢，有三种情况：达到了垃圾回收的阈值，Python虚拟机自动执行手动调用gc.collect()Python虚拟机退出的时候对于垃圾回收，有两个非常重要的术语，那就是reachable与collectable（当然还有与之对应的unreachable与uncollectable），后文也会大量提及。reachable是针对python对象而言，如果从根集（root）能到找到对象，那么这个对象就是reachable，与之相反就是unreachable，事实上就是只存在于循环引用中的对象，Python的垃圾回收就是针对unreachable对象。而collectable是针对unreachable对象而言，如果这种对象能被回收，那么是collectable；如果不能被回收，即循环引用中的对象定义了__del__，那么就是uncollectable。Python垃圾回收对uncollectable对象无能为力，会造成事实上的内存泄露。gcmodule这里的gc（garbagecollector）是Python标准库，该module提供了与上一节“垃圾回收”内容相对应的接口。通过这个module，可以开关gc、调整垃圾回收的频率、输出调试信息。gc模块是很多其他模块（比如objgraph）封装的基础，在这里先介绍gc的核心API。gc.enable();gc.disable();gc.isenabled()开启gc（默认情况下是开启的）；关闭gc；判断gc是否开启gc.collection()　执行一次垃圾回收，不管gc是否处于开启状态都能使用gc.set_threshold(t0,t1,t2);gc.get_threshold()设置垃圾回收阈值；获得当前的垃圾回收阈值注意：gc.set_threshold(0)也有禁用gc的效果gc.get_objects()返回所有被垃圾回收器（collector）管理的对象。这个函数非常基础！只要python解释器运行起来，就有大量的对象被collector管理，因此，该函数的调用比较耗时！比如，命令行启动pythonPython>>>importgc>>>len(gc.get_objects())3749123>>>importgc>>>len(gc.get_objects())3749gc.get_referents(*obj)返回obj对象直接指向的对象gc.get_referrers(*obj)返回所有直接指向obj的对象下面的实例展示了get_referents与get_referrers两个函数Python>>>classOBJ(object):...pass...>>>a,b=OBJ(),OBJ()>>>hex(id(a)),hex(id(b))('0x250e730','0x250e7f0')>>>gc.get_referents(a)[<class'__main__.OBJ'>]>>>a.attr=b>>>gc.get_referents(a)[{'attr':<__main__.OBJobjectat0x0250E7F0>},<class'__main__.OBJ'>]>>>gc.get_referrers(b)[{'attr':<__main__.OBJobjectat0x0250E7F0>},{'a':<__main__.OBJobjectat0x0250E730>,'b':<__main__.OBJobjectat0x0250E7F0>,'OBJ':<class'__main__.OBJ'>,'__builtins__':<module'__builtin__'(built-in)>,'__package__':None,'gc':<module'gc'(built-in)>,'__name__':'__main__','__doc__':None}]>>>123456789101112131415161718>>>classOBJ(object): ...pass...>>>a,b=OBJ(),OBJ()>>>hex(id(a)),hex(id(b))('0x250e730','0x250e7f0')  >>>gc.get_referents(a)[<class'__main__.OBJ'>]>>>a.attr=b>>>gc.get_referents(a)[{'attr':<__main__.OBJobjectat0x0250E7F0>},<class'__main__.OBJ'>]>>>gc.get_referrers(b)[{'attr':<__main__.OBJobjectat0x0250E7F0>},{'a':<__main__.OBJobjectat0x0250E730>,'b':<__main__.OBJobjectat0x0250E7F0>,'OBJ':<class'__main__.OBJ'>,'__builtins__':<module'__builtin__'(built-in)>,'__package__':None,'gc':<module'gc'(built-in)>,'__name__':'__main__','__doc__':None}]>>>a,b都是类OBJ的实例，执行”a.attr=b”之后，a就通过‘’attr“这个属性指向了b。gc.set_debug(flags)设置调试选项，非常有用，常用的flag组合包含以下gc.DEBUG_COLLETABLE：打印可以被垃圾回收器回收的对象gc.DEBUG_UNCOLLETABLE：打印无法被垃圾回收器回收的对象，即定义了__del__的对象gc.DEBUG_SAVEALL：当设置了这个选项，可以被拉起回收的对象不会被真正销毁（free），而是放到gc.garbage这个列表里面，利于在线上查找问题内存泄露既然Python中通过引用计数和垃圾回收来管理内存，那么什么情况下还会产生内存泄露呢？有两种情况：第一是对象被另一个生命周期特别长的对象所引用，比如网络服务器，可能存在一个全局的单例ConnectionManager，管理所有的连接Connection，如果当Connection理论上不再被使用的时候，没有从ConnectionManager中删除，那么就造成了内存泄露。第二是循环引用中的对象定义了__del__函数，这个在《程序员必知的Python陷阱与缺陷列表》一文中有详细介绍，简而言之，如果定义了__del__函数，那么在循环引用中Python解释器无法判断析构对象的顺序，因此就不错处理。在任何环境，不管是服务器，客户端，内存泄露都是非常严重的事情。如果是线上服务器，那么一定得有监控，如果发现内存使用率超过设置的阈值则立即报警，尽早发现些许还有救。当然，谁也不希望在线上修复内存泄露，这无疑是给行驶的汽车换轮子，因此尽量在开发环境或者压力测试环境发现并解决潜在的内存泄露。在这里，发现问题最为关键，只要发现了问题，解决问题就非常容易了，因为按照前面的说法，出现内存泄露只有两种情况，在第一种情况下，只要在适当的时机解除引用就可以了；在第二种情况下，要么不再使用__del__函数，换一种实现方式，要么解决循环引用。那么怎么查找哪里存在内存泄露呢？武器就是两个库：gc、objgraph在上面已经介绍了gc这个模块，理论上，通过gc模块能够拿到所有的被garbagecollector管理的对象，也能知道对象之间的引用和被引用关系，就可以画出对象之间完整的引用关系图。但事实上还是比较复杂的，因为在这个过程中一不小心又会引入新的引用关系，所以，有好的轮子就直接用吧，那就是objgraph。objgraphobjgraph的实现调用了gc的这几个函数：gc.get_objects(),gc.get_referents(),gc.get_referers()，然后构造出对象之间的引用关系。objgraph的代码和文档都写得比较好，建议一读。下面先介绍几个十分实用的APIdefcount(typename)返回该类型对象的数目，其实就是通过gc.get_objects()拿到所用的对象，然后统计指定类型的数目。defby_type(typename)返回该类型的对象列表。线上项目，可以用这个函数很方便找到一个单例对象defshow_most_common_types(limits=10)打印实例最多的前N（limits）个对象，这个函数非常有用。在《Python内存优化》一文中也提到，该函数能发现可以用slots进行内存优化的对象defshow_growth()统计自上次调用以来增加得最多的对象，这个函数非常有利于发现潜在的内存泄露。函数内部调用了gc.collect()，因此即使有循环引用也不会对判断造成影响。值得一提，该函数的实现非常有意思，简化后的代码如下：defshow_growth(limit=10,peak_stats={},shortnames=True,file=None):gc.collect()stats=typestats(shortnames=shortnames)deltas={}forname,countiniteritems(stats):old_count=peak_stats.get(name,0)ifcount>old_count:deltas[name]=count-old_countpeak_stats[name]=countdeltas=sorted(deltas.items(),key=operator.itemgetter(1),reverse=True)1234567891011defshow_growth(limit=10,peak_stats={},shortnames=True,file=None):    gc.collect()    stats=typestats(shortnames=shortnames)    deltas={}    forname,countiniteritems(stats):        old_count=peak_stats.get(name,0)        ifcount>old_count:            deltas[name]=count-old_count            peak_stats[name]=count    deltas=sorted(deltas.items(),key=operator.itemgetter(1),                    reverse=True)注意形参peak_stats使用了可变参数作为默认形参，这样很方便记录上一次的运行结果。在《程序员必知的Python陷阱与缺陷列表》中提到，使用可变对象做默认形参是最为常见的python陷阱，但在这里，却成为了方便的利器！def show_backrefs()生产一张有关objs的引用图，看出看出对象为什么不释放，后面会利用这个API来查内存泄露。该API有很多有用的参数，比如层数限制(max_depth)、宽度限制(too_many)、输出格式控制(filenameoutput)、节点过滤(filter,extra_ignore)，建议使用之间看一些document。def find_backref_chain(obj, predicate, max_depth=20, extra_ignore=()):找到一条指向obj对象的最短路径，且路径的头部节点需要满足predicate函数（返回值为True）可以快捷、清晰指出对象的被引用的情况，后面会展示这个函数的威力defshow_chain():将find_backref_chain找到的路径画出来,该函数事实上调用show_backrefs，只是排除了所有不在路径中的节点。查找内存泄露在这一节，介绍如何利用objgraph来查找内存是怎么泄露的如果我们怀疑一段代码、一个模块可能会导致内存泄露，那么首先调用一次obj.show_growth()，然后调用相应的函数，最后再次调用obj.show_growth()，看看是否有增加的对象。比如下面这个简单的例子：#-*-coding:utf-8-*-importobjgraph_cache=[]classOBJ(object):passdeffunc_to_leak():o=OBJ()_cache.append(o)#dosomethingwitho,thenremoveitfrom_cacheifTrue:#thisseemugly,butitalwaysexistsreturn_cache.remove(o)if__name__=='__main__':objgraph.show_growth()try:func_to_leak()except:passprint'aftercallfunc_to_leak'objgraph.show_growth()12345678910111213141516171819202122232425#-*-coding:utf-8-*-importobjgraph _cache=[] classOBJ(object):    pass deffunc_to_leak():    o  =OBJ()    _cache.append(o)    #dosomethingwitho,thenremoveitfrom_cache     ifTrue:#thisseemugly,butitalwaysexists        return    _cache.remove(o) if__name__=='__main__':    objgraph.show_growth()    try:        func_to_leak()    except:        pass    print'aftercallfunc_to_leak'    objgraph.show_growth()运行结果（我们只关心后一次show_growth的结果）如下Pythonwrapper_descriptor1073+13member_descriptor204+5getset_descriptor168+5weakref338+3dict458+3OBJ1+1123456wrapper_descriptor1073+13member_descriptor204+5getset_descriptor168+5weakref338+3dict458+3OBJ1+1代码很简单，函数开始的时候讲对象加入了global作用域的_cache列表，然后期望是在函数退出之前从_cache删除，但是由于提前返回或者异常，并没有执行到最后的remove语句。从运行结果可以发现，调用函数之后，增加了一个类OBJ的实例，然而理论上函数调用结束之后，所有在函数作用域（local）中声明的对象都改被销毁，因此这里就存在内存泄露。当然，在实际的项目中，我们也不清楚泄露是在哪段代码、哪个模块中发生的，而且往往是发生了内存泄露之后再去排查，这个时候使用obj.show_most_common_types就比较合适了，如果一个自定义的类的实例数目特别多，那么就可能存在内存泄露。如果在压力测试环境，停止压测，调用gc.collet，然后再用obj.show_most_common_types查看，如果对象的数目没有相应的减少，那么肯定就是存在泄露。当我们定位了哪个对象发生了内存泄露，那么接下来就是分析怎么泄露的，引用链是怎么样的，这个时候就该show_backrefs出马了，还是以之前的代码为例，稍加修改：importobjgraph_cache=[]classOBJ(object):passdeffunc_to_leak():o=OBJ()_cache.append(o)#dosomethingwitho,thenremoveitfrom_cacheifTrue:#thisseemugly,butitalwaysexistsreturn_cache.remove(o)if__name__=='__main__':try:func_to_leak()except:passobjgraph.show_backrefs(objgraph.by_type('OBJ')[0],max_depth=10,filename='obj.dot')12345678910111213141516171819202122importobjgraph _cache=[] classOBJ(object):    pass deffunc_to_leak():    o  =OBJ()    _cache.append(o)    #dosomethingwitho,thenremoveitfrom_cache     ifTrue:#thisseemugly,butitalwaysexists        return    _cache.remove(o) if__name__=='__main__':    try:        func_to_leak()    except:        pass    objgraph.show_backrefs(objgraph.by_type('OBJ')[0],max_depth=10,filename='obj.dot')show_backrefs查看内存泄露注意，上面的代码中，max_depth参数非常关键，如果这个参数太小，那么看不到完整的引用链，如果这个参数太大，运行的时候又非常耗时间。然后打开dot文件，结果如下可以看到泄露的对象（红框表示），是被一个叫_cache的list所引用，而_cache又是被__main__这个module所引用。对于示例代码，dot文件的结果已经非常清晰，但是对于真实项目，引用链中的节点可能成百上千，看起来非常头大，下面用tornado起一个最最简单的web服务器（代码不知道来自哪里，且没有内存泄露，这里只是为了显示引用关系），然后绘制socket的引用关关系图，代码和引用关系图如下：importobjgraphimporterrnoimportfunctoolsimporttornado.ioloopimportsocketdefconnection_ready(sock,fd,events):whileTrue:try:connection,address=sock.accept()print'connection_ready',addressexceptsocket.errorase:ife.args[0]notin(errno.EWOULDBLOCK,errno.EAGAIN):raisereturnconnection.setblocking(0)#dosthwithconnectionif__name__=='__main__':sock=socket.socket(socket.AF_INET,socket.SOCK_STREAM,0)sock.setsockopt(socket.SOL_SOCKET,socket.SO_REUSEADDR,1)sock.setblocking(0)sock.bind((\"\",8888))sock.listen(128)io_loop=tornado.ioloop.IOLoop.current()callback=functools.partial(connection_ready,sock)io_loop.add_handler(sock.fileno(),callback,io_loop.READ)#objgraph.show_backrefs(sock,max_depth=10,filename='tornado.dot')#objgraph.show_chain(#objgraph.find_backref_chain(#sock,#objgraph.is_proper_module#),#filename='obj_chain.dot'#)io_loop.start()tornado_server实例12345678910111213141516171819202122232425262728293031323334353637383940importobjgraphimporterrnoimportfunctoolsimporttornado.ioloopimportsocket defconnection_ready(sock,fd,events):    whileTrue:        try:            connection,address=sock.accept()            print'connection_ready',address        exceptsocket.errorase:            ife.args[0]notin(errno.EWOULDBLOCK,errno.EAGAIN):                raise            return        connection.setblocking(0)        #dosthwithconnection  if__name__=='__main__':    sock=socket.socket(socket.AF_INET,socket.SOCK_STREAM,0)    sock.setsockopt(socket.SOL_SOCKET,socket.SO_REUSEADDR,1)    sock.setblocking(0)    sock.bind((\"\",8888))    sock.listen(128)     io_loop=tornado.ioloop.IOLoop.current()    callback=functools.partial(connection_ready,sock)    io_loop.add_handler(sock.fileno(),callback,io_loop.READ)    #objgraph.show_backrefs(sock,max_depth=10,filename='tornado.dot')    #objgraph.show_chain(    #    objgraph.find_backref_chain(    #        sock,    #        objgraph.is_proper_module    #    ),    #    filename='obj_chain.dot'    #)    io_loop.start() tornado_server实例可见，代码越复杂，相互之间的引用关系越多，show_backrefs越难以看懂。这个时候就使用show_chain和find_backref_chain吧，这种方法，在官方文档也是推荐的，我们稍微改改代码，结果如下：importobjgraph_cache=[]classOBJ(object):passdeffunc_to_leak():o=OBJ()_cache.append(o)#dosomethingwitho,thenremoveitfrom_cacheifTrue:#thisseemugly,butitalwaysexistsreturn_cache.remove(o)if__name__=='__main__':try:func_to_leak()except:pass#objgraph.show_backrefs(objgraph.by_type('OBJ')[0],max_depth=10,filename='obj.dot')objgraph.show_chain(objgraph.find_backref_chain(objgraph.by_type('OBJ')[0],objgraph.is_proper_module),filename='obj_chain.dot')1234567891011121314151617181920212223242526272829importobjgraph _cache=[] classOBJ(object):    pass deffunc_to_leak():    o  =OBJ()    _cache.append(o)    #dosomethingwitho,thenremoveitfrom_cache     ifTrue:#thisseemugly,butitalwaysexists        return    _cache.remove(o) if__name__=='__main__':    try:        func_to_leak()    except:        pass    #objgraph.show_backrefs(objgraph.by_type('OBJ')[0],max_depth=10,filename='obj.dot')    objgraph.show_chain(        objgraph.find_backref_chain(            objgraph.by_type('OBJ')[0],            objgraph.is_proper_module        ),        filename='obj_chain.dot'    )上面介绍了内存泄露的第一种情况，对象被“非期望”地引用着。下面看看第二种情况，循环引用中的__del__，看下面的代码：#-*-coding:utf-8-*-importobjgraph,gcclassOBJ(object):def__del__(self):print('Dangerous!')defshow_leak_by_del():a,b=OBJ(),OBJ()a.attr_b=bb.attr_a=adela,bprintgc.collect()objgraph.show_backrefs(objgraph.by_type('OBJ')[0],max_depth=10,filename='del_obj.dot')123456789101112131415#-*-coding:utf-8-*-importobjgraph,gcclassOBJ(object):    def__del__(self):        print('Dangerous!') defshow_leak_by_del():    a,b=OBJ(),OBJ()    a.attr_b=b    b.attr_a=a     dela,b    printgc.collect()     objgraph.show_backrefs(objgraph.by_type('OBJ')[0],max_depth=10,filename='del_obj.dot')上面的代码存在循环引用，而且OBJ类定义了__del__函数。如果没有定义__del__函数，那么上述的代码会报错，因为gc.collect会将循环引用删除，objgraph.by_type(‘OBJ’)返回空列表。而因为定义了__del__函数，gc.collect也无能为力，结果如下：从图中可以看到，对于这种情况，还是比较好辨识的，因为objgraph将__del__函数用特殊颜色标志出来，一眼就看见了。另外，可以看见gc.garbage（类型是list）也引用了这两个对象，原因在document中有描述，当执行垃圾回收的时候，会将定义了__del__函数的类实例（被称为uncollectableobject）放到gc.garbage列表，因此，也可以直接通过查看gc.garbage来找出定义了__del__的循环引用。在这里，通过增加extra_ignore来排除gc.garbage的影响：将上述代码的最后一行改成：Python　　objgraph.show_backrefs(objgraph.by_type('OBJ')[0],extra_ignore=(id(gc.garbage),),max_depth=10,filename='del_obj.dot')1　　objgraph.show_backrefs(objgraph.by_type('OBJ')[0],extra_ignore=(id(gc.garbage),),  max_depth=10,filename='del_obj.dot')另外，也可以设置DEBUG_UNCOLLECTABLE选项，直接将uncollectable对象输出到标准输出，而不是放到gc.garbage循环引用除非定义了__del__方法，那么循环引用也不是什么万恶不赦的东西，因为垃圾回收器可以处理循环引用，而且不准是python标准库还是大量使用的第三方库，都可能存在循环引用。如果存在循环引用，那么Python的gc就必须开启（gc.isenabled()返回True），否则就会内存泄露。但是在某些情况下，我们还是不希望有gc，比如对内存和性能比较敏感的应用场景，在这篇文章中，提到instagram通过禁用gc，性能提升了10%；另外，在一些应用场景，垃圾回收带来的卡顿也是不能接受的，比如RPG游戏。从前面对垃圾回收的描述可以看到，执行一次垃圾回收是很耗费时间的，因为需要遍历所有被collector管理的对象（即使很多对象不属于垃圾）。因此，要想禁用GC，就得先彻底干掉循环引用。同内存泄露一样，解除循环引用的前提是定位哪里出现了循环引用。而且，如果需要在线上应用关闭gc，那么需要自动、持久化的进行检测。下面介绍如何定位循环引用，以及如何解决循环引用。定位循环引用这里还是是用GC模块和objgraph来定位循环引用。需要注意的事，一定要先禁用gc（调用gc.disable()），防止误差。这里利用之前介绍循环引用时使用过的例子：a，b两个OBJ对象形成循环引用#-*-coding:utf-8-*-importobjgraph,gcclassOBJ(object):passdefshow_cycle_reference():a,b=OBJ(),OBJ()a.attr_b=bb.attr_a=aif__name__=='__main__':gc.disable()for_inxrange(50):show_cycle_reference()objgraph.show_most_common_types(20)123456789101112131415#-*-coding:utf-8-*-importobjgraph,gcclassOBJ(object):    pass defshow_cycle_reference():    a,b=OBJ(),OBJ()    a.attr_b=b    b.attr_a=a if__name__=='__main__':    gc.disable()    for_inxrange(50):        show_cycle_reference()    objgraph.show_most_common_types(20)运行结果（部分）：Pythonwrapper_descriptor1060dict555OBJ100123wrapper_descriptor1060dict555OBJ100上面的代码中使用的是show_most_common_types，而没有使用show_growth（因为growth会手动调用gc.collect()），通过结果可以看到，内存中现在有100个OBJ对象，符合预期。当然这些OBJ对象没有在函数调用后被销毁，不一定是循环引用的问题，也可能是内存泄露，比如前面OBJ对象被global作用域中的_cache引用的情况。怎么排除是否是被global作用域的变量引用的情况呢，方法还是objgraph.find_backref_chain(obj)，在__doc__中指出，如果找不到符合条件的应用链（chain），那么返回[obj]，稍微修改上面的代码：#-*-coding:utf-8-*-importobjgraph,gcclassOBJ(object):passdefshow_cycle_reference():a,b=OBJ(),OBJ()a.attr_b=bb.attr_a=aif__name__=='__main__':gc.disable()for_inxrange(50):show_cycle_reference()ret=objgraph.find_backref_chain(objgraph.by_type('OBJ')[0],objgraph.is_proper_module)printret12345678910111213141516#-*-coding:utf-8-*-importobjgraph,gcclassOBJ(object):    pass defshow_cycle_reference():    a,b=OBJ(),OBJ()    a.attr_b=b    b.attr_a=a if__name__=='__main__':    gc.disable()    for_inxrange(50):        show_cycle_reference()    ret=objgraph.find_backref_chain(objgraph.by_type('OBJ')[0],objgraph.is_proper_module)    printret上面的代码输出：Python[<__main__.OBJobjectat0x0244F810>]1[<__main__.OBJobjectat0x0244F810>]验证了我们的想法，OBJ对象不是被global作用域的变量所引用。在实际项目中，不大可能到处用objgraph.show_most_common_types或者objgraph.by_type来排查循环引用，效率太低。有没有更好的办法呢，有的，那就是使用gc模块的debug选项。在前面介绍gc模块的时候，就介绍了gc.DEBUG_COLLECTABLE选项，我们来试试：#-*-coding:utf-8-*-importgc,timeclassOBJ(object):passdefshow_cycle_reference():a,b=OBJ(),OBJ()a.attr_b=bb.attr_a=aif__name__=='__main__':gc.disable()#这里是否disable事实上无所谓gc.set_debug(gc.DEBUG_COLLECTABLE|gc.DEBUG_OBJECTS)for_inxrange(1):show_cycle_reference()gc.collect()time.sleep(5)1234567891011121314151617#-*-coding:utf-8-*-importgc,timeclassOBJ(object):    pass defshow_cycle_reference():    a,b=OBJ(),OBJ()    a.attr_b=b    b.attr_a=a if__name__=='__main__':    gc.disable()#这里是否disable事实上无所谓    gc.set_debug(gc.DEBUG_COLLECTABLE|gc.DEBUG_OBJECTS)    for_inxrange(1):        show_cycle_reference()    gc.collect()    time.sleep(5)上面代码第13行设置了debugflag，可以打印出collectable对象。另外，只用调用一次show_cycle_reference函数就足够了（这也比objgraph.show_most_common_types方便一点）。在第16行手动调用gc.collect()，输出如下：Pythongc:collectable<OBJ023B46F0>gc:collectable<OBJ023B4710>gc:collectable<dict023B7AE0>gc:collectable<dict023B7930>1234gc:collectable<OBJ023B46F0>gc:collectable<OBJ023B4710>gc:collectable<dict023B7AE0>gc:collectable<dict023B7930>注意：只有当对象是unreachable且collectable的时候，在collect的时候才会被输出，也就是说，如果是reachable，比如被global作用域的变量引用，那么也是不会输出的。通过上面的输出，我们已经知道OBJ类的实例存在循环引用，但是这个时候，obj实例已经被回收了。那么如果我想通过show_backrefs找出这个引用关系，需要重新调用show_cycle_reference函数，然后不调用gc.collect，通过show_backrefs和by_type绘制。有没有更好的办法呢，可以让我在一次运行中发现循环引用，并找出引用链？答案就是使用DEBUG_SAVEALL，下面为了展示方便，直接在命令行中操作（当然，使用ipython更好）Python>>>importgc,objgraph>>>classOBJ(object):...pass...>>>defshow_cycle_reference():...a,b=OBJ(),OBJ()...a.attr_b=b...b.attr_a=a...>>>gc.set_debug(gc.DEBUG_SAVEALL|gc.DEBUG_OBJECTS)>>>show_cycle_reference()>>>print'beforecollect',gc.garbagebeforecollect[]>>>printgc.collect()4>>>>>>foroingc.garbage:...printo...<__main__.OBJobjectat0x024BB7D0><__main__.OBJobjectat0x02586850>{'attr_b':<__main__.OBJobjectat0x02586850>}{'attr_a':<__main__.OBJobjectat0x024BB7D0>}>>>>>>objgraph.show_backrefs(objgraph.at(0x024BB7D0),5,filename='obj.dot')Graphwrittentoobj.dot(13nodes)>>>123456789101112131415161718192021222324252627>>>importgc,objgraph>>>classOBJ(object):...pass...>>>defshow_cycle_reference():...a,b=OBJ(),OBJ()...a.attr_b=b...b.attr_a=a...>>>gc.set_debug(gc.DEBUG_SAVEALL|gc.DEBUG_OBJECTS)>>>show_cycle_reference()>>>print'beforecollect',gc.garbagebeforecollect[]>>>printgc.collect()4>>>>>>foroingc.garbage:...printo...<__main__.OBJobjectat0x024BB7D0><__main__.OBJobjectat0x02586850>{'attr_b':<__main__.OBJobjectat0x02586850>}{'attr_a':<__main__.OBJobjectat0x024BB7D0>}>>>>>>objgraph.show_backrefs(objgraph.at(0x024BB7D0),5,filename='obj.dot')Graphwrittentoobj.dot(13nodes)>>>上面在调用gc.collect之前，gc.garbage里面是空的，由于设置了DEBUG_SAVEALL，那么调用gc.collect时，会将collectable对象放到gc.garbage。此时，对象没有被释放，我们就可以直接绘制出引用关系，这里使用了objgraph.at，当然也可以使用objgraph.by_type，或者直接从gc.garbage取对象，结果如下：出了循环引用，可以看见还有两个引用，gc.garbage与局部变量o，相信大家也能理解。消灭循环引用找到循环引用关系之后，解除循环引用就不是太难的事情，总的来说，有两种办法：手动解除与使用weakref。手动解除很好理解，就是在合适的时机，解除引用关系。比如，前面提到的collections.OrderedDict：Python>>>root=[]>>>root[:]=[root,root,None]>>>>>>root[[...],[...],None]>>>>>>delroot[:]>>>root[]123456789>>>root=[]>>>root[:]=[root,root,None]>>>>>>root[[...],[...],None]>>>>>>delroot[:]>>>root[]更常见的情况，是我们自定义的对象之间存在循环引用：要么是单个对象内的循环引用，要么是多个对象间的循环引用，我们看一个单个对象内循环引用的例子：classConnection(object):MSG_TYPE_CHAT=0X01MSG_TYPE_CONTROL=0X02def__init__(self):self.msg_handlers={self.MSG_TYPE_CHAT:self.handle_chat_msg,self.MSG_TYPE_CONTROL:self.handle_control_msg}defon_msg(self,msg_type,*args):self.msg_handlers[msg_type](*args)defhandle_chat_msg(self,msg):passdefhandle_control_msg(self,msg):pass1234567891011121314151617classConnection(object):    MSG_TYPE_CHAT=0X01    MSG_TYPE_CONTROL=0X02    def__init__(self):        self.msg_handlers={            self.MSG_TYPE_CHAT:self.handle_chat_msg,            self.MSG_TYPE_CONTROL:self.handle_control_msg        }     defon_msg(self,msg_type,*args):        self.msg_handlers[msg_type](*args)     defhandle_chat_msg(self,msg):        pass     defhandle_control_msg(self,msg):        pass上面的代码非常常见，代码也很简单，初始化函数中为每种消息类型定义响应的处理函数，当消息到达(on_msg)时根据消息类型取出处理函数。但这样的代码是存在循环引用的，感兴趣的读者可以用objgraph看看引用图。如何手动解决呢，为Connection增加一个destroy（或者叫clear）函数，该函数将self.msg_handlers清空（self.msg_handlers.clear()）。当Connection理论上不在被使用的时候调用destroy函数即可。对于多个对象间的循环引用，处理方法也是一样的，就是在“适当的时机”调用destroy函数，难点在于什么是适当的时机。另外一种更方便的方法，就是使用弱引用weakref，weakref是Python提供的标准库，旨在解决循环引用。weakref模块提供了以下一些有用的API：（1）weakref.ref(object,callback=None)创建一个对object的弱引用，返回值为weakref对象，callback:当object被删除的时候，会调用callback函数，在标准库logging（__init__.py）中有使用范例。使用的时候要用()解引用，如果referant已经被删除，那么返回None。比如下面的例子#-*-coding:utf-8-*-importweakrefclassOBJ(object):deff(self):print'HELLO'if__name__=='__main__':o=OBJ()w=weakref.ref(o)w().f()delow().f()123456789101112#-*-coding:utf-8-*-importweakrefclassOBJ(object):    deff(self):        print'HELLO' if__name__=='__main__':    o=OBJ()    w=weakref.ref(o)    w().f()    delo    w().f()运行上面的代码，第12行会抛出异常：AttributeError:‘NoneType’objecthasnoattribute‘f’。因为这个时候被引用的对象已经被删除了（2）weakref.proxy(object,callback=None)创建一个代理，返回值是一个weakproxy对象，callback的作用同上。使用的时候直接用和object一样，如果object已经被删除那么跑出异常  ReferenceError:weakly-referencedobjectnolongerexists。#-*-coding:utf-8-*-importweakrefclassOBJ(object):deff(self):print'HELLO'if__name__=='__main__':o=OBJ()w=weakref.proxy(o)w.f()delow.f()123456789101112#-*-coding:utf-8-*-importweakrefclassOBJ(object):    deff(self):        print'HELLO' if__name__=='__main__':    o=OBJ()    w=weakref.proxy(o)    w.f()    delo    w.f()注意第10行12行与weakref.ref示例代码的区别（3）weakref.WeakSet这个是一个弱引用集合，当WeakSet中的元素被回收的时候，会自动从WeakSet中删除。WeakSet的实现使用了weakref.ref，当对象加入WeakSet的时候，使用weakref.ref封装，指定的callback函数就是从WeakSet中删除。感兴趣的话可以直接看源码（_weakrefset.py），下面给出一个参考例子：#-*-coding:utf-8-*-importweakrefclassOBJ(object):deff(self):print'HELLO'if__name__=='__main__':o=OBJ()ws=weakref.WeakSet()ws.add(o)printlen(ws)#1deloprintlen(ws)#012345678910111213#-*-coding:utf-8-*-importweakrefclassOBJ(object):    deff(self):        print'HELLO' if__name__=='__main__':    o=OBJ()    ws=weakref.WeakSet()    ws.add(o)    printlen(ws)#  1    delo    printlen(ws)#0（4）weakref.WeakValueDictionary，weakref.WeakKeyDictionary实现原理和使用方法基本同WeakSet总结本文的篇幅略长，首选是简单介绍了python的内存管理，重点介绍了引用计数与垃圾回收，然后阐述Python中内存泄露与循环引用产生的原因与危害，最后是利用gc、objgraph、weakref等工具来分析并解决内存泄露、循环引用问题。referencesGarbageCollectorInterfaceobjgraphGarbageCollectionforPython禁用Python的GC机制后，Instagram性能提升10%Python内存管理机制及优化简析libraryweakref2赞3收藏评论"], "art_url": ["http://python.jobbole.com/88827/"]}
{"art_img": ["http://jbcdn2.b0.upaiyun.com/2014/12/6da94dec8f6f96417f14c8291e6345801.png"], "art_title": ["Flask 应用中的 URL 处理"], "art_create_time": ["2017/09/20"], "art_content": ["原文出处：LearnPython   在文章：一个Flask应用运行过程剖析中，在一个上下文环境中可以处理请求。如果不考虑在处理请求前后做的一些操作，Flask源码中真正处理请求的是dispatch_request()方法。其源码如下：Pythondefdispatch_request(self):\"\"\"Doestherequestdispatching.MatchestheURLandreturnsthereturnvalueofthevieworerrorhandler.Thisdoesnothavetobearesponseobject.Inordertoconvertthereturnvaluetoaproperresponseobject,call:func:`make_response`.\"\"\"try:endpoint,values=self.match_request()returnself.view_functions[endpoint](**values)exceptHTTPException,e:handler=self.error_handlers.get(e.code)ifhandlerisNone:returnereturnhandler(e)exceptException,e:handler=self.error_handlers.get(500)ifself.debugorhandlerisNone:raisereturnhandler(e)12345678910111213141516171819defdispatch_request(self):    \"\"\"Doestherequestdispatching.  MatchestheURLandreturnsthe    returnvalueofthevieworerrorhandler.  Thisdoesnothaveto    bearesponseobject.  Inordertoconvertthereturnvaluetoa    properresponseobject,call:func:`make_response`.    \"\"\"    try:        endpoint,values=self.match_request()        returnself.view_functions[endpoint](**values)    exceptHTTPException,e:        handler=self.error_handlers.get(e.code)        ifhandlerisNone:            returne        returnhandler(e)    exceptException,e:        handler=self.error_handlers.get(500)        ifself.debugorhandlerisNone:            raise        returnhandler(e)从上面的源码中可以看到，dispatch_request()方法做了如下的工作：对请求的URL进行匹配；如果URL可以匹配，则返回相对应视图函数的结果；如果不可以匹配，则进行错误处理。对于错误的处理，本文暂不做介绍。本文主要对Flask应用的URL模式以及请求处理过程中的URL匹配进行剖析。Flask应用的url_mapFlask应用实例化的时候，会为应用增添一个url_map属性。这个属性是一个Map类，这个类在werkzeug.routing模块中定义，其主要的功能是为了给应用增加一些URL规则，这些URL规则形成一个Map实例的过程中会生成对应的正则表达式，可以进行URL匹配。相关的概念和内容可以参考：Werkzeug库——routing模块。在Flask源码中，它通过两个方法可以很方便地定制应用的URL。这两个方法是：route装饰器和add_url_rule方法。1.add_url_rulePythondefadd_url_rule(self,rule,endpoint,**options):options['endpoint']=endpointoptions.setdefault('methods',('GET',))self.url_map.add(Rule(rule,**options))1234defadd_url_rule(self,rule,endpoint,**options):    options['endpoint']=endpoint    options.setdefault('methods',('GET',))    self.url_map.add(Rule(rule,**options))add_url_rule方法很简单，只要向其传递一条URL规则rule和一个endpoint即可。endpoint一般为和这条URL相关的视图函数的名字，这样处理就可以将URL和视图函数关联起来。除此之外，还可以传递一些关键字参数。调用该方法后，会调用Map实例的add方法，它会将URL规则添加进Map实例中。2.route装饰器为了更加方便、优雅地写应用的URL，Flask实现了一个route装饰器。Pythondefroute(self,rule,**options):defdecorator(f):self.add_url_rule(rule,f.__name__,**options)self.view_functions[f.__name__]=freturnfreturndecorator123456defroute(self,rule,**options):    defdecorator(f):        self.add_url_rule(rule,f.__name__,**options)        self.view_functions[f.__name__]=f        returnf    returndecoratorroute装饰器会装饰一个视图函数。经route装饰的视图函数首先会调用add_url_rule方法，将装饰器中的URL规则添加进Map实例中，视图函数的名字会作为endpoint进行传递。然后在该应用的view_functions中增加endpoint和视图函数的对应关系。这种对应关系可以在请求成功时方便地调用对应的视图函数。3.一个简单的例子我们用一个简单的例子来说明以上过程的实现：Python>>>fromflaskimportFlask>>>app=Flask(__name__)>>>@app.route('/')defindex():return\"Hello,World!\">>>@app.route('/<username>')defuser(username):return\"Hello,%s\"%username>>>@app.route('/page/<int:id>')defpage(id):return\"Thisispage%d\"%id1234567891011>>>fromflaskimportFlask>>>app=Flask(__name__)>>>@app.route('/')    defindex():        return\"Hello,World!\">>>@app.route('/<username>')    defuser(username):        return\"Hello,%s\"%username>>>@app.route('/page/<int:id>')    defpage(id):        return\"Thisispage%d\"%id以上代码，我们创建了一个Flask应用app，并且通过route装饰器的形式为app增加了3条URL规则。首先：我们看一下Flask应用的url_map长啥样：Python>>>url_map=app.url_map>>>url_mapMap([<Rule'/'(HEAD,GET)->index>,<Rule'/static/<filename>'->static>,<Rule'/page/<id>'(HEAD,GET)->page>,<Rule'/<username>'(HEAD,GET)->user>])1234567>>>url_map=app.url_map>>>url_mapMap([<Rule'/'(HEAD,GET)->index>,    <Rule'/static/<filename>'->static>,    <Rule'/page/<id>'(HEAD,GET)->page>,    <Rule'/<username>'(HEAD,GET)->user>    ])可以看到，url_map是一个Map实例，这个实例中包含4个Rule实例，分别对应4条URL规则，其中/static/<filename>在Flask应用实例化时会自动添加，其余3条是用户创建的。整个Map类便构成了Flask应用app的URL“地图”，可以用作URL匹配的依据。接下来：我们看一下url_map中的一个属性：_rules_by_endpoint：Python>>>rules_by_endpoint=url_map._rules_by_endpoint>>>rules_by_endpoint{'index':[<Rule'/'(HEAD,GET)->index>],'page':[<Rule'/page/<id>'(HEAD,GET)->page>],'static':[<Rule'/static/<filename>'->static>],'user':[<Rule'/<username>'(HEAD,GET)->user>]}1234567>>>rules_by_endpoint=url_map._rules_by_endpoint>>>rules_by_endpoint{'index':[<Rule'/'(HEAD,GET)->index>],'page':[<Rule'/page/<id>'(HEAD,GET)->page>],'static':[<Rule'/static/<filename>'->static>],'user':[<Rule'/<username>'(HEAD,GET)->user>]}可以看出，_rules_by_endpoint属性是一个字典，反映了endpoint和URL规则的对应关系。由于用route装饰器创建URL规则时，会将视图函数的名字作为endpoint进行传递，所以以上字典的内容也反映了视图函数和URL规则的对应关系。再接下来：我们看一下Flask应用的view_functions：Python>>>view_functions=app.view_functions>>>view_functions{'index':<function__main__.index>,'page':<function__main__.page>,'user':<function__main__.user>}123456>>>view_functions=app.view_functions>>>view_functions{'index':<function__main__.index>,'page':<function__main__.page>,'user':<function__main__.user>}在用route装饰器创建URL规则时，它还会做一件事情：self.view_functions[f.__name__]=f。这样做是将函数名和视图函数的对应关系放在Flask应用的view_functions。由于Map实例中存储了函数名和URL规则的对应关系，这样只要在匹配URL规则时，如果匹配成功，只要返回一个函数名，那么便可以在view_functions中运行对应的视图函数。最后：我们看一下URL如何和Map实例中的URL规则进行匹配。我们以/page/<int:id>这条规则为例：Python>>>rule=url_map._rules[2]>>>rule<Rule'/page/<id>'(HEAD,GET)->page>>>>rule._regexre.compile(ur'^\\|\\/page\\/(?P<id>\\d+)$',re.UNICODE)>>>rule._regex.patternu'^\\\\|\\\\/page\\\\/(?P<id>\\\\d+)$'1234567>>>rule=url_map._rules[2]>>>rule<Rule'/page/<id>'(HEAD,GET)->page>>>>rule._regexre.compile(ur'^\\|\\/page\\/(?P<id>\\d+)$',re.UNICODE)>>>rule._regex.patternu'^\\\\|\\\\/page\\\\/(?P<id>\\\\d+)$'可以看到，在将一条URL规则的实例Rule添加进Map实例的时候，会为这个Rule生成一个正则表达式的属性_regex。这样当这个Flask应用处理请求时，实际上会将请求中的url和Flask应用中每一条URL规则的正则表达式进行匹配。如果匹配成功，则会返回endpoint和一些参数，返回的endpoint可以用来在view_functions找到对应的视图函数，返回的参数可以传递给视图函数。具体的过程就是：Pythontry:#match_request()可以进行URL匹配endpoint,values=self.match_request()returnself.view_functions[endpoint](**values)...12345try:    #match_request()可以进行URL匹配    endpoint,values=self.match_request()    returnself.view_functions[endpoint](**values)    ...1赞2收藏评论"], "art_url": ["http://python.jobbole.com/88618/"]}
{"art_img": ["http://jbcdn2.b0.upaiyun.com/2015/10/bfa0d07e7eb2fac2eb80cd5df9567931.jpg"], "art_title": ["BaseHTTPServer ——实现 Web 服务器"], "art_create_time": ["2017/09/20"], "art_content": ["原文出处：LearnPython   在SocketServer——网络通信服务器中我们介绍了Python标准库中的SocketServer模块，了解了要实现网络通信服务，就要构建一个服务器类和请求处理类。同时，该模块还为我们创建了不同的服务器类和请求处理类。1.服务器类BaseServerTCPServer(BaseServer)UDPServer(TCPServer)UnixStreamServerUnixDatagramServer2.请求处理类BaseRequestHandlerStreamRequestHandler(BaseRequestHandler)DatagramRequestHandler(BaseRequestHandler)通过服务器类和请求处理类的搭配，我们可以创建不同类型的服务器，实现不同的协议类型。本文介绍的BaseHTTPServer模块便是继承TCPServer和StreamRequestHandler，实现了Web服务器的通信。HTTP服务器HTTP服务器继承自SocketServer模块中的TCPServer类。它的定义非常简单，只是重写了其中的一个方法。PythonclassHTTPServer(SocketServer.TCPServer):allow_reuse_address=1#Seemstomakesenseintestingenvironmentdefserver_bind(self):\"\"\"Overrideserver_bindtostoretheservername.\"\"\"SocketServer.TCPServer.server_bind(self)host,port=self.socket.getsockname()[:2]self.server_name=socket.getfqdn(host)self.server_port=port12345678classHTTPServer(SocketServer.TCPServer):    allow_reuse_address=1    #Seemstomakesenseintestingenvironment    defserver_bind(self):        \"\"\"Overrideserver_bindtostoretheservername.\"\"\"        SocketServer.TCPServer.server_bind(self)        host,port=self.socket.getsockname()[:2]        self.server_name=socket.getfqdn(host)        self.server_port=port重写的server_bind()方法主要是为了获取服务器名和端口。其余方法以及服务器的实现过程详见SocketServer——网络通信服务器。此外，还可以从SocketServer模块中引入’mix-in’类，基于HTTPServer创建支持进程或线程的服务器。HTTP请求处理基类为了处理HTTP请求，BaseHTTPServer模块构造了HTTP请求处理基类BaseHTTPRequestHandler，它继承自SocketServer模块中的StreamRequestHandler类。HTTP请求处理基类中有一些重要的方法：1.handle()——这个方法是请求处理类真正处理请求具体工作的方法，例如解析到来的请求，处理数据，并发回响应等。在BaseHTTPRequestHandler中它是一个入口文件，将调用其他的方法完成请求处理。2.handle_one_request()——由handle()调用，用于处理请求。其主要工作包括：调用parse_request()方法，解析请求，获取请求报文中的信息，包括请求的方法、请求URL、请求的HTTP版本号、请求首部等。如果解析失败，则调用send_error()方法发回一个错误响应。调用do_SPAM()方法。这个方法中的SPAM指代GET、POST、HEAD等请求方法，需要在请求处理类中构建具体的请求处理方法，例如do_GET处理GET请求，do_POST处理POST请求。do_SPAM()方法可以调用send_response()、send_header()、end_headers()等方法创建响应首行和响应首部等内容。3.parse_request()——解析请求。4.send_error()——发回错误响应。5.send_response()——创建响应首行和响应首部等内容。6.send_header()——设置响应首部内容。7.end_headers()——调用此方法可以在首部后增加一个空行，表示首部内容结束（不适用于HTTP/0.9）8.还包括其他的一些辅助函数。需要注意的是：BaseHTTPRequestHandler是HTTP请求处理的基类，并不包含诸如do_GET、do_POST等方法，其他继承该类的请求处理类需要自己实现这些方法，已完成对具体请求的处理。对此，可以参考SimpleHTTPServer模块，也可查看文章SimpleHTTPServer——一个简单的HTTP服务器。1赞5收藏评论"], "art_url": ["http://python.jobbole.com/88629/"]}
{"art_img": ["http://jbcdn2.b0.upaiyun.com/2017/10/eba5f145122207e6dd3575ad30702bed.png"], "art_title": ["基于概率论的分类方法：朴素贝叶斯"], "art_create_time": ["2017/10/20"], "art_content": ["原文出处：imze5z   基于概率论的分类方法：朴素贝叶斯1.概述贝叶斯分类是一类分类算法的总称，这类算法均以贝叶斯定理为基础，故统称为贝叶斯分类。本章首先介绍贝叶斯分类算法的基础——贝叶斯定理。最后，我们通过实例来讨论贝叶斯分类的中最简单的一种:朴素贝叶斯分类。2.贝叶斯理论&条件概率2.1贝叶斯理论我们现在有一个数据集，它由两类数据组成，数据分布如下图所示：我们现在用p1(x,y)表示数据点(x,y)属于类别1（图中用圆点表示的类别）的概率，用p2(x,y)表示数据点(x,y)属于类别2（图中三角形表示的类别）的概率，那么对于一个新数据点(x,y)，可以用下面的规则来判断它的类别：Python如果p1(x,y)>p2(x,y)，那么类别为1如果p2(x,y)>p1(x,y)，那么类别为212如果p1(x,y)>p2(x,y)，那么类别为1如果p2(x,y)>p1(x,y)，那么类别为2也就是说，我们会选择高概率对应的类别。这就是贝叶斯决策理论的核心思想，即选择具有最高概率的决策。2.1.2条件概率如果你对p(x,y|c1)符号很熟悉，那么可以跳过本小节。有一个装了7块石头的罐子，其中3块是白色的，4块是黑色的。如果从罐子中随机取出一块石头，那么是白色石头的可能性是多少？由于取石头有7种可能，其中3种为白色，所以取出白色石头的概率为3/7。那么取到黑色石头的概率又是多少呢？很显然，是4/7。我们使用P(white)来表示取到白色石头的概率，其概率值可以通过白色石头数目除以总的石头数目来得到。如果这7块石头如下图所示，放在两个桶中，那么上述概率应该如何计算？计算P(white)或者P(black)，如果事先我们知道石头所在桶的信息是会改变结果的。这就是所谓的条件概率（conditionalprobablity）。假定计算的是从B桶取到白色石头的概率，这个概率可以记作P(white|bucketB)，我们称之为“在已知石头出自B桶的条件下，取出白色石头的概率”。很容易得到，P(white|bucketA)值为2/4，P(white|bucketB)的值为1/3。条件概率的计算公式如下：P(white|bucketB)=P(whiteandbucketB)/P(bucketB)首先，我们用B桶中白色石头的个数除以两个桶中总的石头数，得到P(whiteandbucketB)=1/7.其次，由于B桶中有3块石头，而总石头数为7，于是P(bucketB)就等于3/7。于是又P(white|bucketB)=P(whiteandbucketB)/P(bucketB)=(1/7)/(3/7)=1/3。另外一种有效计算条件概率的方法称为贝叶斯准则。贝叶斯准则告诉我们如何交换条件概率中的条件与结果，即如果已知P(x|c)，要求P(c|x)，那么可以使用下面的计算方法：使用条件概率来分类上面我们提到贝叶斯决策理论要求计算两个概率p1(x,y)和p2(x,y):Python如果p1(x,y)>p2(x,y),那么属于类别1;如果p2(x,y)>p1(X,y),那么属于类别2.12如果p1(x,y)>p2(x,y),那么属于类别1;如果p2(x,y)>p1(X,y),那么属于类别2.这并不是贝叶斯决策理论的所有内容。使用p1()和p2()只是为了尽可能简化描述，而真正需要计算和比较的是p(c1|x,y)和p(c2|x,y).这些符号所代表的具体意义是:给定某个由x、y表示的数据点，那么该数据点来自类别c1的概率是多少？数据点来自类别c2的概率又是多少？注意这些概率与概率p(x,y|c1)并不一样，不过可以使用贝叶斯准则来交换概率中条件与结果。具体地，应用贝叶斯准则得到:使用上面这些定义，可以定义贝叶斯分类准则为:Python如果P(c1|x,y)>P(c2|x,y),那么属于类别c1;如果P(c2|x,y)>P(c1|x,y),那么属于类别c2.12如果P(c1|x,y)>P(c2|x,y),那么属于类别c1;如果P(c2|x,y)>P(c1|x,y),那么属于类别c2.在文档分类中，整个文档（如一封电子邮件）是实例，而电子邮件中的某些元素则构成特征。我们可以观察文档中出现的词，并把每个词作为一个特征，而每个词的出现或者不出现作为该特征的值，这样得到的特征数目就会跟词汇表中的词的数目一样多。我们假设特征之间相互独立。所谓独立(independence)指的是统计意义上的独立，即一个特征或者单词出现的可能性与它和其他单词相邻没有关系，比如说，“我们”中的“我”和“们”出现的概率与这两个字相邻没有任何关系。这个假设正是朴素贝叶斯分类器中朴素(naive)一词的含义。朴素贝叶斯分类器中的另一个假设是，每个特征同等重要。Note:朴素贝叶斯分类器通常有两种实现方式:一种基于伯努利模型实现，一种基于多项式模型实现。这里采用前一种实现方式。该实现方式中并不考虑词在文档中出现的次数，只考虑出不出现，因此在这个意义上相当于假设词是等权重的。2.2朴素贝叶斯场景机器学习的一个重要应用就是文档的自动分类。在文档分类中，整个文档（如一封电子邮件）是实例，而电子邮件中的某些元素则构成特征。我们可以观察文档中出现的词，并把每个词作为一个特征，而每个词的出现或者不出现作为该特征的值，这样得到的特征数目就会跟词汇表中的词的数目一样多。朴素贝叶斯是上面介绍的贝叶斯分类器的一个扩展，是用于文档分类的常用算法。下面我们会进行一些朴素贝叶斯分类的实践项目。2.3朴素贝叶斯原理朴素贝叶斯工作原理提取所有文档中的词条并进行去重获取文档的所有类别计算每个类别中的文档数目对每篇训练文档:Python对每个类别:如果词条出现在文档中-->增加该词条的计数值（for循环或者矩阵相加）增加所有词条的计数值（此类别下词条总数）123对每个类别:    如果词条出现在文档中-->增加该词条的计数值（for循环或者矩阵相加）    增加所有词条的计数值（此类别下词条总数）对每个类别:Python对每个词条:将该词条的数目除以总词条数目得到的条件概率（P(词条|类别)）12对每个词条:    将该词条的数目除以总词条数目得到的条件概率（P(词条|类别)）返回该文档属于每个类别的条件概率（P(类别|文档的所有词条)）2.4朴素贝叶斯开发流程收集数据:可以使用任何方法。准备数据:需要数值型或者布尔型数据。分析数据:有大量特征时，绘制特征作用不大，此时使用直方图效果更好。训练算法:计算不同的独立特征的条件概率。测试算法:计算错误率。使用算法:一个常见的朴素贝叶斯应用是文档分类。可以在任意的分类场景中使用朴素贝叶斯分类器，不一定非要是文本。2.5朴素贝叶斯算法特点优点:在数据较少的情况下仍然有效，可以处理多类别问题。缺点:对于输入数据的准备方式较为敏感。适用数据类型:标称型数据。2.6朴素贝叶斯项目案例2.6.1项目案例1屏蔽社区留言板的侮辱性言论2.6.1.1项目概述构建一个快速过滤器来屏蔽在线社区留言板上的侮辱性言论。如果某条留言使用了负面或者侮辱性的语言，那么就将该留言标识为内容不当。对此问题建立两个类别:侮辱类和非侮辱类，使用1和0分别表示。2.6.1.2开发流程收集数据:可以使用任何方法准备数据:从文本中构建词向量分析数据:检查词条确保解析的正确性训练算法:从词向量计算概率测试算法:根据现实情况修改分类器使用算法:对社区留言板言论进行分类收集数据:可以使用任何方法2.6.1.3构造词表PythondefloadDataSet():\"\"\"创建数据集:return:单词列表postingList,所属类别classVec\"\"\"postingList=[['my','dog','has','flea','problems','help','please'],#[0,0,1,1,1......]['maybe','not','take','him','to','dog','park','stupid'],['my','dalmation','is','so','cute','I','love','him'],['stop','posting','stupid','worthless','garbage'],['mr','licks','ate','my','steak','how','to','stop','him'],['quit','buying','worthless','dog','food','stupid']]classVec=[0,1,0,1,0,1]#1isabusive,0notreturnpostingList,classVec12345678910111213defloadDataSet():    \"\"\"    创建数据集    :return:单词列表postingList,所属类别classVec    \"\"\"    postingList=[['my','dog','has','flea','problems','help','please'],#[0,0,1,1,1......]                  ['maybe','not','take','him','to','dog','park','stupid'],                  ['my','dalmation','is','so','cute','I','love','him'],                  ['stop','posting','stupid','worthless','garbage'],                  ['mr','licks','ate','my','steak','how','to','stop','him'],                  ['quit','buying','worthless','dog','food','stupid']]    classVec=[0,1,0,1,0,1]  #1isabusive,0not    returnpostingList,classVec2.6.1.4准备数据:从文本中构建词向量PythondefcreateVocabList(dataSet):\"\"\"获取所有单词的集合:paramdataSet:数据集:return:所有单词的集合(即不含重复元素的单词列表)\"\"\"vocabSet=set([])#createemptysetfordocumentindataSet:#操作符|用于求两个集合的并集vocabSet=vocabSet|set(document)#unionofthetwosetsreturnlist(vocabSet)defsetOfWords2Vec(vocabList,inputSet):\"\"\"遍历查看该单词是否出现，出现该单词则将该单词置1:paramvocabList:所有单词集合列表:paraminputSet:输入数据集:return:匹配列表[0,1,0,1...]，其中1与0表示词汇表中的单词是否出现在输入的数据集中\"\"\"#创建一个和词汇表等长的向量，并将其元素都设置为0returnVec=[0]*len(vocabList)#[0,0......]#遍历文档中的所有单词，如果出现了词汇表中的单词，则将输出的文档向量中的对应值设为1forwordininputSet:ifwordinvocabList:returnVec[vocabList.index(word)]=1else:print\"theword:%sisnotinmyVocabulary!\"%wordreturnreturnVec1234567891011121314151617181920212223242526272829defcreateVocabList(dataSet):    \"\"\"    获取所有单词的集合    :paramdataSet:数据集    :return:所有单词的集合(即不含重复元素的单词列表)    \"\"\"    vocabSet=set([])  #createemptyset    fordocumentindataSet:        #操作符|用于求两个集合的并集        vocabSet=vocabSet|set(document)  #unionofthetwosets    returnlist(vocabSet)  defsetOfWords2Vec(vocabList,inputSet):    \"\"\"    遍历查看该单词是否出现，出现该单词则将该单词置1    :paramvocabList:所有单词集合列表    :paraminputSet:输入数据集    :return:匹配列表[0,1,0,1...]，其中1与0表示词汇表中的单词是否出现在输入的数据集中    \"\"\"    #创建一个和词汇表等长的向量，并将其元素都设置为0    returnVec=[0]*len(vocabList)#[0,0......]    #遍历文档中的所有单词，如果出现了词汇表中的单词，则将输出的文档向量中的对应值设为1    forwordininputSet:        ifwordinvocabList:            returnVec[vocabList.index(word)]=1        else:            print\"theword:%sisnotinmyVocabulary!\"%word    returnreturnVec2.6.1.5分析数据:检查词条确保解析的正确性检查函数执行情况，检查词表，不出现重复单词，需要的话，可以对其进行排序。Python>>>listOPosts,listClasses=bayes.loadDataSet()>>>myVocabList=bayes.createVocabList(listOPosts)>>>myVocabList['cute','love','help','garbage','quit','I','problems','is','park','stop','flea','dalmation','licks','food','not','him','buying','posting','has','worthless','ate','to','maybe','please','dog','how','stupid','so','take','mr','steak','my']123456>>>listOPosts,listClasses=bayes.loadDataSet()>>>myVocabList=bayes.createVocabList(listOPosts)>>>myVocabList['cute','love','help','garbage','quit','I','problems','is','park','stop','flea','dalmation','licks','food','not','him','buying','posting','has','worthless','ate','to','maybe','please','dog','how','stupid','so','take','mr','steak','my']检查函数有效性。例如：myVocabList中索引为2的元素是什么单词？应该是是help。该单词在第一篇文档中出现了，现在检查一下看看它是否出现在第四篇文档中。Python>>>bayes.setOfWords2Vec(myVocabList,listOPosts[0])[0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0,0,0,0,0,1]>>>bayes.setOfWords2Vec(myVocabList,listOPosts[3])[0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,1,0,0,0,0,0]12345>>>bayes.setOfWords2Vec(myVocabList,listOPosts[0])[0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0,0,0,0,0,1] >>>bayes.setOfWords2Vec(myVocabList,listOPosts[3])[0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,1,0,0,0,0,0]2.6.1.6训练算法:从词向量计算概率现在已经知道了一个词是否出现在一篇文档中，也知道该文档所属的类别。接下来我们重写贝叶斯准则，将之前的x,y替换为w.粗体的w表示这是一个向量，即它由多个值组成。在这个例子中，数值个数与词汇表中的词个数相同。我们使用上述公式，对每个类计算该值，然后比较这两个概率值的大小。首先可以通过类别i(侮辱性留言或者非侮辱性留言)中的文档数除以总的文档数来计算概率p(ci)。接下来计算p(w|ci)，这里就要用到朴素贝叶斯假设。如果将w展开为一个个独立特征，那么就可以将上述概率写作p(w0,w1,w2…wn|ci)。这里假设所有词都互相独立，该假设也称作条件独立性假设（例如A和B两个人抛骰子，概率是互不影响的，也就是相互独立的，A抛2点的同时B抛3点的概率就是1/6*1/6），它意味着可以使用p(w0|ci)p(w1|ci)p(w2|ci)…p(wn|ci)来计算上述概率，这样就极大地简化了计算的过程。2.6.1.7朴素贝叶斯分类器训练函数Pythondef_trainNB0(trainMatrix,trainCategory):\"\"\"训练数据原版:paramtrainMatrix:文件单词矩阵[[1,0,1,1,1....],[],[]...]:paramtrainCategory:文件对应的类别[0,1,1,0....]，列表长度等于单词矩阵数，其中的1代表对应的文件是侮辱性文件，0代表不是侮辱性矩阵:return:\"\"\"#文件数numTrainDocs=len(trainMatrix)#单词数numWords=len(trainMatrix[0])#侮辱性文件的出现概率，即trainCategory中所有的1的个数，#代表的就是多少个侮辱性文件，与文件的总数相除就得到了侮辱性文件的出现概率pAbusive=sum(trainCategory)/float(numTrainDocs)#构造单词出现次数列表p0Num=zeros(numWords)#[0,0,0,.....]p1Num=zeros(numWords)#[0,0,0,.....]#整个数据集单词出现总数p0Denom=0.0p1Denom=0.0foriinrange(numTrainDocs):#是否是侮辱性文件iftrainCategory[i]==1:#如果是侮辱性文件，对侮辱性文件的向量进行加和p1Num+=trainMatrix[i]#[0,1,1,....]+[0,1,1,....]->[0,2,2,...]#对向量中的所有元素进行求和，也就是计算所有侮辱性文件中出现的单词总数p1Denom+=sum(trainMatrix[i])else:p0Num+=trainMatrix[i]p0Denom+=sum(trainMatrix[i])#类别1，即侮辱性文档的[P(F1|C1),P(F2|C1),P(F3|C1),P(F4|C1),P(F5|C1)....]列表#即在1类别下，每个单词出现的概率p1Vect=p1Num/p1Denom#[1,2,3,5]/90->[1/90,...]#类别0，即正常文档的[P(F1|C0),P(F2|C0),P(F3|C0),P(F4|C0),P(F5|C0)....]列表#即在0类别下，每个单词出现的概率p0Vect=p0Num/p0Denomreturnp0Vect,p1Vect,pAbusive1234567891011121314151617181920212223242526272829303132333435363738def_trainNB0(trainMatrix,trainCategory):    \"\"\"    训练数据原版    :paramtrainMatrix:文件单词矩阵[[1,0,1,1,1....],[],[]...]    :paramtrainCategory:文件对应的类别[0,1,1,0....]，列表长度等于单词矩阵数，其中的1代表对应的文件是侮辱性文件，0代表不是侮辱性矩阵    :return:    \"\"\"    #文件数    numTrainDocs=len(trainMatrix)    #单词数    numWords=len(trainMatrix[0])    #侮辱性文件的出现概率，即trainCategory中所有的1的个数，    #代表的就是多少个侮辱性文件，与文件的总数相除就得到了侮辱性文件的出现概率    pAbusive=sum(trainCategory)/float(numTrainDocs)    #构造单词出现次数列表    p0Num=zeros(numWords)#[0,0,0,.....]    p1Num=zeros(numWords)#[0,0,0,.....]     #整个数据集单词出现总数    p0Denom=0.0    p1Denom=0.0    foriinrange(numTrainDocs):        #是否是侮辱性文件        iftrainCategory[i]==1:            #如果是侮辱性文件，对侮辱性文件的向量进行加和            p1Num+=trainMatrix[i]#[0,1,1,....]+[0,1,1,....]->[0,2,2,...]            #对向量中的所有元素进行求和，也就是计算所有侮辱性文件中出现的单词总数            p1Denom+=sum(trainMatrix[i])        else:            p0Num+=trainMatrix[i]            p0Denom+=sum(trainMatrix[i])    #类别1，即侮辱性文档的[P(F1|C1),P(F2|C1),P(F3|C1),P(F4|C1),P(F5|C1)....]列表    #即在1类别下，每个单词出现的概率    p1Vect=p1Num/p1Denom#[1,2,3,5]/90->[1/90,...]    #类别0，即正常文档的[P(F1|C0),P(F2|C0),P(F3|C0),P(F4|C0),P(F5|C0)....]列表    #即在0类别下，每个单词出现的概率    p0Vect=p0Num/p0Denom    returnp0Vect,p1Vect,pAbusive2.6.1.8测试算法:根据现实情况修改分类器http://www.cnblogs.com/apache…1赞3收藏2评论"], "art_url": ["http://python.jobbole.com/88717/"]}
{"art_img": ["http://pytlab.org/assets/images/blog_img/2017-09-19-%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95%E4%B8%AD%E5%87%A0%E7%A7%8D%E4%B8%8D%E5%90%8C%E9%80%89%E6%8B%A9%E7%AE%97%E5%AD%90%E7%9A%84%E6%AF%94%E8%BE%83/flowchart.png"], "art_title": ["遗传算法中几种不同选择算子及Python实现"], "art_create_time": ["2017/09/19"], "art_content": ["本文作者：伯乐在线-iPytLab。未经作者许可，禁止转载！欢迎加入伯乐在线专栏作者。前言本文对遗传算法中的几种选择策略进行了总结,其中包括:ProportionateRouletteWheelSelectionLinearRankingSelectionExponentialRankingSelectionTournamentSelection对于每种选择策略我都使用Python进行了相应的实现并以内置插件的形式整合进了本人所写的遗传算法框架GAFT中。对需要使用遗传算法优化问题以及学习遗传算法的童鞋可以作为参考.项目链接:GitHub: https://github.com/PytLab/gaftPyPI: https://pypi.python.org/pypi/gaft遗传算法中的选择算子遗传算法(geneticalgorithms,GAs)是一种自适应的启发式搜索算法,它模仿达尔文进化论中的“适者生存”的原则,最终获取优化目标的最优解。下图描述了一个简单的遗传算法流程:对于种群中需要进行杂交的物种选择方法有很多，而且选择一种合适的选择策略对于遗传算法整体性能的影响将是很大的。如果一个选择算法选择多样性降低，便会导致种群过早的收敛到局部最优点而不是我们想要的全局最优点，也就是所谓的”早熟”。而选择策略过于发散则会导致算法难以收敛到最优点。因此在这两点中我们需要进行平衡才能使遗传算法以一种高效的方式收敛到全局最优点。GAFT框架中的算子插件GAFT是我根据自己需求开发的一个遗传算法框架，相关介绍的博客可以参见《GAFT-一个使用Python实现的遗传算法框架》,《使用MPI并行化遗传算法框架GAFT》。该框架提供了插件接口，用户可以通过自定义算子以及on-the-fly分析插件来放到gaft框架中运行遗传算法流程对目标问题进行优化。本部分我稍微介绍下gaft关于遗传算子相关接口规范，以及编写能用于gaft的算子的编写方法。在gaft中遗传算子的编写都是需要继承框架内置的基类，然后根据基类提供的接口，实现自己的算子。其中基类的定义都在/gaft/plugin_interfaces/operators/目录下，下面我以选择算子为例，介绍下接口。gaft中选择算子的基类为GASelection，其中在遗传算法流程中会调用该类实例的select方法，进而根据算子中的相关选择策略完成从种群中选取一对物种作为父亲和母亲产生子代。基类的定义为：PythonclassGASelection(metaclass=SelectionMeta):'''Classforprovidinganinterfacetoeasilyextendthebehaviorofselectionoperation.'''defselect(self,population,fitness):'''Calledwhenweneedtoselectparentsfromapopulationtolaterbreeding.:parampopulation:Thecurrentpopulation.:typepopulation:GAPopulation:returnparents:Twoselectedindividualsforcrossover.:typeparents:TupleoftowGAIndividualobjects.'''raiseNotImplementedError1234567891011121314classGASelection(metaclass=SelectionMeta):    '''    Classforprovidinganinterfacetoeasilyextendthebehaviorofselection    operation.    '''    defselect(self,population,fitness):        '''        Calledwhenweneedtoselectparentsfromapopulationtolaterbreeding.        :parampopulation:Thecurrentpopulation.        :typepopulation:GAPopulation        :returnparents:Twoselectedindividualsforcrossover.        :typeparents:TupleoftowGAIndividualobjects.        '''        raiseNotImplementedErrorselect的方法的参数为当前种群population以及相应的适应度函数fitness，其中population需要是GAPopulation对象，fitness也必须是callable的对象。当然，这些在Python这种动态类型语言中貌似看起来有些鸡肋，但是为了能够更加规范使用者，我利用Python的元类在实例化类对象的时候对接口的实现以及接口的参数类型加以限制。具体的实现都在/gaft/plugin_interfaces/metaclasses.py中，有兴趣的童鞋可以看看实现方法。具体自定义算子的编写方法我将在下一部分同选择策略一起贴出来。不同的选择策略本部分我主要对四种不同的选择策略进行总结并加以gaft插件形式的Python实现。选择算子决定了哪些个体将会从种群中被选择出来用于繁衍下一代种群中的新个体。其主要的原则就是:thebetterisanindividual;thehigherisitschanceofbeingaparent选择算子在遗传算法迭代中将适应度函数引入进来，因为适应度函数式标定一个个体是否足够“好”的重要标准。但是选择过程又不能仅仅完全依赖于适应度函数，因为一个种群中的最优物种并不一定是在全局最优点附近。因此我们也应该给相对来说并那么“好”的个体一点机会让他们繁衍后代,避免“早熟”。ProportionateRouletteWheelSelection此轮盘赌选择策略，是最基本的选择策略之一，种群中的个体被选中的概率与个体相应的适应度函数的值成正比。我们需要将种群中所有个体的适应度值进行累加然后归一化，最终通过随机数对随机数落在的区域对应的个体进行选取，类似赌场里面的旋转的轮盘。每个个体ai被选中的概率为:好了，下面可以将此算法写成一个可以gaft中执行的算子。Pythonfromrandomimportrandomfrombisectimportbisect_rightfromitertoolsimportaccumulatefrom...plugin_interfaces.operators.selectionimportGASelectionclassRouletteWheelSelection(GASelection):def__init__(self):'''Selectionoperatorwithfitnessproportionateselection(FPS)orso-calledroulette-wheelselectionimplementation.'''passdefselect(self,population,fitness):'''SelectapairofparentusingFPSalgorithm.'''#Normalizefitnessvaluesforallindividuals.fit=[fitness(indv)forindvinpopulation.individuals]min_fit=min(fit)fit=[(i-min_fit)foriinfit]#Createroulettewheel.sum_fit=sum(fit)wheel=list(accumulate([i/sum_fitforiinfit]))#Selectafatherandamother.father_idx=bisect_right(wheel,random())father=population[father_idx]mother_idx=(father_idx+1)%len(wheel)mother=population[mother_idx]returnfather,mother12345678910111213141516171819202122232425262728293031323334fromrandomimportrandomfrombisectimportbisect_rightfromitertoolsimportaccumulate from...plugin_interfaces.operators.selectionimportGASelection classRouletteWheelSelection(GASelection):    def__init__(self):        '''        Selectionoperatorwithfitnessproportionateselection(FPS)or        so-calledroulette-wheelselectionimplementation.        '''        pass     defselect(self,population,fitness):        '''        SelectapairofparentusingFPSalgorithm.        '''        #Normalizefitnessvaluesforallindividuals.        fit=[fitness(indv)forindvinpopulation.individuals]        min_fit=min(fit)        fit=[(i-min_fit)foriinfit]         #Createroulettewheel.        sum_fit=sum(fit)        wheel=list(accumulate([i/sum_fitforiinfit]))         #Selectafatherandamother.        father_idx=bisect_right(wheel,random())        father=population[father_idx]        mother_idx=(father_idx+1)%len(wheel)        mother=population[mother_idx]         returnfather,mother过程主要分为下面几个:继承GASelection类实现select方法select的参数为GAPopulation实例和适应度函数根据算法选择出两个需要繁衍的物种并返回即可TournamentSelection由于算法执行的效率以及易实现的的特点，锦标赛选择算法是遗传算法中最流行的选择策略。在本人的实际应用中的确此策略比基本的轮盘赌效果要好些。他的策略也很直观，就是我们再整个种群中抽取n个个体，让他们进行竞争(锦标赛)，抽取其中的最优的个体。参加锦标赛的个体个数成为tournamentsize。通常当n=2便是最常使用的大小，也称作BinaryTournamentSelection.TournamentSelection的优势:更小的复杂度O(n)易并行化处理不易陷入局部最优点不需要对所有的适应度值进行排序处理下图显示了n=3的TournamentSelection的过程:可以开始写成自定义算子在gaft运行了:Pythonfromrandomimportsamplefrom...plugin_interfaces.operators.selectionimportGASelectionclassTournamentSelection(GASelection):def__init__(self,tournament_size=2):'''SelectionoperatorusingTournamentStrategywithtournamentsizeequalstotwobydefault.'''self.tournament_size=tournament_sizedefselect(self,population,fitness):'''SelectapairofparentusingTournamentstrategy.'''#Competitionfunction.complete=lambdacompetitors:max(competitors,key=fitness)#Checkvalidityoftournamentsize.ifself.tournament_size>=len(population):msg='Tournamentsize({})islargerthanpopulationsize({})'raiseValueError(msg.format(self.tournament_size,len(population)))#Pickwinnersoftwogroupsasparent.competitors_1=sample(population.individuals,self.tournament_size)competitors_2=sample(population.individuals,self.tournament_size)father,mother=complete(competitors_1),complete(competitors_2)returnfather,mother123456789101112131415161718192021222324252627282930fromrandomimportsample from...plugin_interfaces.operators.selectionimportGASelection classTournamentSelection(GASelection):    def__init__(self,tournament_size=2):        '''        SelectionoperatorusingTournamentStrategywithtournamentsizeequals        totwobydefault.        '''        self.tournament_size=tournament_size     defselect(self,population,fitness):        '''        SelectapairofparentusingTournamentstrategy.        '''        #Competitionfunction.        complete=lambdacompetitors:max(competitors,key=fitness)         #Checkvalidityoftournamentsize.        ifself.tournament_size>=len(population):            msg='Tournamentsize({})islargerthanpopulationsize({})'            raiseValueError(msg.format(self.tournament_size,len(population)))         #Pickwinnersoftwogroupsasparent.        competitors_1=sample(population.individuals,self.tournament_size)        competitors_2=sample(population.individuals,self.tournament_size)        father,mother=complete(competitors_1),complete(competitors_2)         returnfather,motherLinearRankingSelection下面两个介绍的选择策略都是基于排序的选择策略，上面提到的第一种基本轮盘赌选择算法，有一个缺点，就是如果一个个体的适应度值为0的话，则被选中的概率将会是0,这个个体将不能产生后代。于是我们需要一种基于排序的算法，来给每个个体安排相应的选中概率。在LinearRankingSelection中，种群中的个体首先根据适应度的值进行排序，然后给所有个体赋予一个序号，最好的个体为N,被选中的概率为Pmax,最差的个体序号为1,被选中的概率为Pmin，于是其他的在他们中间的个体的概率便可以根据如下公式得到:实现代码:Pythonfromrandomimportrandomfromitertoolsimportaccumulatefrombisectimportbisect_rightfrom...plugin_interfaces.operators.selectionimportGASelectionclassLinearRankingSelection(GASelection):def__init__(self,pmin=0.1,pmax=0.9):'''SelectionoperatorusingLinearRankingselectionmethod.Reference:BakerJE.Adaptiveselectionmethodsforgeneticalgorithms[C]//ProceedingsofanInternationalConferenceonGeneticAlgorithmsandtheirapplications.1985:101-111.'''#Selectionprobabilitiesfortheworstandbestindividuals.self.pmin,self.pmax=pmin,pmaxdefselect(self,population,fitness):'''Selectapairofparentindividualsusinglinearrankingmethod.'''#Individualnumber.NP=len(population)#Addranktoallindividualsinpopulation.sorted_indvs=sorted(population.individuals,key=fitness,reverse=True)#Assignselectionprobabilitieslinearly.#NOTE:Heretherankibelongsto{1,...,N}p=lambdai:(self.pmin+(self.pmax-self.pmin)*(i-1)/(NP-1))probabilities=[self.pmin]+[p(i)foriinrange(2,NP)]+[self.pmax]#Normalizeprobabilities.psum=sum(probabilities)wheel=list(accumulate([p/psumforpinprobabilities]))#Selectparents.father_idx=bisect_right(wheel,random())father=population[father_idx]mother_idx=(father_idx+1)%len(wheel)mother=population[mother_idx]returnfather,mother123456789101112131415161718192021222324252627282930313233343536373839404142fromrandomimportrandomfromitertoolsimportaccumulatefrombisectimportbisect_right from...plugin_interfaces.operators.selectionimportGASelection classLinearRankingSelection(GASelection):    def__init__(self,pmin=0.1,pmax=0.9):        '''        SelectionoperatorusingLinearRankingselectionmethod.        Reference:BakerJE.Adaptiveselectionmethodsforgenetic        algorithms[C]//ProceedingsofanInternationalConferenceonGenetic        Algorithmsandtheirapplications.1985:101-111.        '''        #Selectionprobabilitiesfortheworstandbestindividuals.        self.pmin,self.pmax=pmin,pmax     defselect(self,population,fitness):        '''        Selectapairofparentindividualsusinglinearrankingmethod.        '''        #Individualnumber.        NP=len(population)        #Addranktoallindividualsinpopulation.        sorted_indvs=sorted(population.individuals,key=fitness,reverse=True)         #Assignselectionprobabilitieslinearly.        #NOTE:Heretherankibelongsto{1,...,N}        p=lambdai:(self.pmin+(self.pmax-self.pmin)*(i-1)/(NP-1))        probabilities=[self.pmin]+[p(i)foriinrange(2,NP)]+[self.pmax]         #Normalizeprobabilities.        psum=sum(probabilities)        wheel=list(accumulate([p/psumforpinprobabilities]))         #Selectparents.        father_idx=bisect_right(wheel,random())        father=population[father_idx]        mother_idx=(father_idx+1)%len(wheel)        mother=population[mother_idx]         returnfather,motherExponentialRankingSelection类似上面的LinearRanking选择策略，这种指数排序便是在确定每个个体的选择概率的时候使用了指数形式的表达式,其中c为底数，满足0<c<1:实现代码:Pythonfromrandomimportrandomfromitertoolsimportaccumulatefrombisectimportbisect_rightfrom...plugin_interfaces.operators.selectionimportGASelectionclassExponentialRankingSelection(GASelection):def__init__(self,base=0.5):'''SelectionoperatorusingExponentialRankingselectionmethod.:parambase:Thebaseofexponent:typebase:floatinrange(0.0,1.0)'''ifnot(0.0<base<1.0):raiseValueError('Thebaseofexponentcmustinrange(0.0,1.0)')self.base=basedefselect(self,population,fitness):'''Selectapairofparentindividualsusingexponentialrankingmethod.'''#Individualnumber.NP=len(population)#NOTE:Heretherankibelongsto{1,...,N}p=lambdai:self.base**(NP-i)probabilities=[p(i)foriinrange(1,NP+1)]#Normalizeprobabilities.psum=sum(probabilities)wheel=list(accumulate([p/psumforpinprobabilities]))#Selectparents.father_idx=bisect_right(wheel,random())father=population[father_idx]mother_idx=(father_idx+1)%len(wheel)mother=population[mother_idx]returnfather,mother1234567891011121314151617181920212223242526272829303132333435fromrandomimportrandomfromitertoolsimportaccumulatefrombisectimportbisect_right from...plugin_interfaces.operators.selectionimportGASelection classExponentialRankingSelection(GASelection):    def__init__(self,base=0.5):        '''        SelectionoperatorusingExponentialRankingselectionmethod.        :parambase:Thebaseofexponent        :typebase:floatinrange(0.0,1.0)        '''        ifnot(0.0<base<1.0):            raiseValueError('Thebaseofexponentcmustinrange(0.0,1.0)')        self.base=base     defselect(self,population,fitness):        '''        Selectapairofparentindividualsusingexponentialrankingmethod.        '''        #Individualnumber.        NP=len(population)        #NOTE:Heretherankibelongsto{1,...,N}        p=lambdai:self.base**(NP-i)        probabilities=[p(i)foriinrange(1,NP+1)]        #Normalizeprobabilities.        psum=sum(probabilities)        wheel=list(accumulate([p/psumforpinprobabilities]))        #Selectparents.        father_idx=bisect_right(wheel,random())        father=population[father_idx]        mother_idx=(father_idx+1)%len(wheel)        mother=population[mother_idx]        returnfather,mother总结本文对于遗传算法中四种不同的选择策略进行了介绍和总结，同时对于本文所写的遗传算法框架的自定义算子接口进行了简要的介绍，针对本文中的选择策略分别根据接口的要求实现了相应的算子，这些算子也作为GAFT框架的内置算子放入到GAFT中，对于使用GAFT的童鞋可以直接拿来使用。参考Shukla,Anupriya,HariMohanPandey,andDeeptiMehrotra.“Comparativereviewofselectiontechniquesingeneticalgorithm.”FuturisticTrendsonComputationalAnalysisandKnowledgeManagement(ABLAZE),2015InternationalConferenceon.IEEE,2015.打赏支持我写出更多好文章，谢谢！打赏作者打赏支持我写出更多好文章，谢谢！1赞2收藏评论关于作者：iPytLab喜欢写程序的计算化学狗，Python/C/C++/Fortran,个人博客http://pytlab.org个人主页·我的文章·22·"], "art_url": ["http://python.jobbole.com/88608/"]}
{"art_img": ["http://jbcdn2.b0.upaiyun.com/2017/09/533a271bc27ca7198850b80934d96e97.png"], "art_title": ["用 Scikit-Learn 和 Pandas 学习线性回归"], "art_create_time": ["2017/09/14"], "art_content": ["原文出处：刘建平Pinard   对于想深入了解线性回归的童鞋，这里给出一个完整的例子，详细学完这个例子，对用scikit-learn来运行线性回归，评估模型不会有什么问题了。1.获取数据，定义问题没有数据，当然没法研究机器学习啦。:)这里我们用UCI大学公开的机器学习数据来跑线性回归。数据的介绍在这： http://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant数据的下载地址在这： http://archive.ics.uci.edu/ml/machine-learning-databases/00294/里面是一个循环发电场的数据，共有9568个样本数据，每个数据有5列，分别是:AT（温度）,V（压力）,AP（湿度）,RH（压强）,PE（输出电力)。我们不用纠结于每项具体的意思。我们的问题是得到一个线性的关系，对应PE是样本输出，而AT/V/AP/RH这4个是样本特征，机器学习的目的就是得到一个线性回归模型，即:PE=θ_0+θ_1*AT+θ_2*V+θ_3*AP+θ_4*RH而需要学习的，就是θ_0、θ_1、θ_2、θ_3、θ_4这5个参数。2.整理数据下载后的数据可以发现是一个压缩文件，解压后可以看到里面有一个xlsx文件，我们先用excel把它打开，接着“另存为“”csv格式，保存下来，后面我们就用这个csv来运行线性回归。打开这个csv可以发现数据已经整理好，没有非法数据，因此不需要做预处理。但是这些数据并没有归一化，也就是转化为均值0，方差1的格式。也不用我们搞，后面scikit-learn在线性回归时会先帮我们把归一化搞定。好了，有了这个csv格式的数据，我们就可以大干一场了。3. 用pandas来读取数据我们先打开ipythonnotebook,新建一个notebook。当然也可以直接在python的交互式命令行里面输入，不过还是推荐用notebook。下面的例子和输出我都是在notebook里面跑的。先把要导入的库声明了：importmatplotlib.pyplotasplt%matplotlibinlineimportnumpyasnpimportpandasaspdfromsklearnimportdatasets,linear_model123456importmatplotlib.pyplotasplt%matplotlibinline importnumpyasnpimportpandasaspdfromsklearnimportdatasets,linear_model接着我们就可以用pandas读取数据了：#read_csv里面的参数是csv在你电脑上的路径，此处csv文件放在notebook运行目录下面的CCPP目录里data=pd.read_csv('.\\CCPP\\ccpp.csv')12#read_csv里面的参数是csv在你电脑上的路径，此处csv文件放在notebook运行目录下面的CCPP目录里data=pd.read_csv('.\\CCPP\\ccpp.csv')测试下读取数据是否成功：#读取前五行数据，如果是最后五行，用data.tail()data.head()12#读取前五行数据，如果是最后五行，用data.tail()data.head()运行结果应该如下，看到下面的数据，说明pandas读取数据成功：ATVAPRHPE08.3440.771010.8490.01480.48123.6458.491011.4074.20445.75229.7456.901007.1541.91438.76319.0749.691007.2276.79453.09411.8040.661017.1397.20464.434. 准备运行算法的数据我们看看数据的维度：data.shape1data.shape结果是(9568,5)。说明我们有9568个样本，每个样本有5列。现在我们开始准备样本特征X，我们用AT，V，AP和RH这4个列作为样本特征。X=data[['AT','V','AP','RH']]X.head()12X=data[['AT','V','AP','RH']]X.head()可以看到X的前五条输出如下：ATVAPRH08.3440.771010.8490.01123.6458.491011.4074.20229.7456.901007.1541.91319.0749.691007.2276.79411.8040.661017.1397.20接着我们准备样本输出y，我们用PE作为样本输出。y=data[['PE']]y.head()12y=data[['PE']]y.head()可以看到y的前五条输出如下：PE0480.481445.752438.763453.094464.435.划分训练集和测试集我们把X和y的样本组合划分成两部分，一部分是训练集，一部分是测试集，代码如下：fromsklearn.cross_validationimporttrain_test_splitX_train,X_test,y_train,y_test=train_test_split(X,y,random_state=1)12fromsklearn.cross_validationimporttrain_test_splitX_train,X_test,y_train,y_test=train_test_split(X,y,random_state=1)查看下训练集和测试集的维度：printX_train.shapeprinty_train.shapeprintX_test.shapeprinty_test.shape1234printX_train.shapeprinty_train.shapeprintX_test.shapeprinty_test.shape结果如下：(7176,4)(7176,1)(2392,4)(2392,1)1234(7176,4)(7176,1)(2392,4)(2392,1)可以看到75%的样本数据被作为训练集，25%的样本被作为测试集。6.运行scikit-learn的线性模型终于到了临门一脚了，我们可以用scikit-learn的线性模型来拟合我们的问题了。scikit-learn的线性回归算法使用的是最小二乘法来实现的。代码如下：fromsklearn.linear_modelimportLinearRegressionlinreg=LinearRegression()linreg.fit(X_train,y_train)123fromsklearn.linear_modelimportLinearRegressionlinreg=LinearRegression()linreg.fit(X_train,y_train)拟合完毕后，我们看看我们的需要的模型系数结果：printlinreg.intercept_printlinreg.coef_12printlinreg.intercept_printlinreg.coef_输出如下：[447.06297099][[-1.97376045-0.232290860.0693515-0.15806957]]12[447.06297099][[-1.97376045-0.23229086  0.0693515  -0.15806957]]这样我们就得到了在步骤1里面需要求得的5个值。也就是说PE和其他4个变量的关系如下：PE=447.06297099-1.97376045*AT-0.23229086*V+0.0693515*AP-0.15806957*RH7.模型评价我们需要评估我们的模型的好坏程度，对于线性回归来说，我们一般用均方差（MeanSquaredError,MSE）或者均方根差(RootMeanSquaredError,RMSE)在测试集上的表现来评价模型的好坏。我们看看我们的模型的MSE和RMSE，代码如下：#模型拟合测试集y_pred=linreg.predict(X_test)fromsklearnimportmetrics#用scikit-learn计算MSEprint\"MSE:\",metrics.mean_squared_error(y_test,y_pred)#用scikit-learn计算RMSEprint\"RMSE:\",np.sqrt(metrics.mean_squared_error(y_test,y_pred))1234567#模型拟合测试集y_pred=linreg.predict(X_test)fromsklearnimportmetrics#用scikit-learn计算MSEprint\"MSE:\",metrics.mean_squared_error(y_test,y_pred)#用scikit-learn计算RMSEprint\"RMSE:\",np.sqrt(metrics.mean_squared_error(y_test,y_pred))输出如下：MSE:20.0804012021RMSE:4.4811160665712MSE:20.0804012021RMSE:4.48111606657得到了MSE或者RMSE，如果我们用其他方法得到了不同的系数，需要选择模型时，就用MSE小的时候对应的参数。比如这次我们用AT，V，AP这3个列作为样本特征。不要RH，输出仍然是PE。代码如下：X=data[['AT','V','AP']]y=data[['PE']]X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=1)fromsklearn.linear_modelimportLinearRegressionlinreg=LinearRegression()linreg.fit(X_train,y_train)#模型拟合测试集y_pred=linreg.predict(X_test)fromsklearnimportmetrics#用scikit-learn计算MSEprint\"MSE:\",metrics.mean_squared_error(y_test,y_pred)#用scikit-learn计算RMSEprint\"RMSE:\",np.sqrt(metrics.mean_squared_error(y_test,y_pred))12345678910111213X=data[['AT','V','AP']]y=data[['PE']]X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=1)fromsklearn.linear_modelimportLinearRegressionlinreg=LinearRegression()linreg.fit(X_train,y_train)#模型拟合测试集y_pred=linreg.predict(X_test)fromsklearnimportmetrics#用scikit-learn计算MSEprint\"MSE:\",metrics.mean_squared_error(y_test,y_pred)#用scikit-learn计算RMSEprint\"RMSE:\",np.sqrt(metrics.mean_squared_error(y_test,y_pred))输出如下：MSE:23.2089074701RMSE:4.8175623991912MSE:23.2089074701RMSE:4.81756239919可以看出，去掉RH后，模型拟合的没有加上RH的好，MSE变大了。8.交叉验证我们可以通过交叉验证来持续优化模型，代码如下，我们采用10折交叉验证，即cross_val_predict中的cv参数为10：X=data[['AT','V','AP','RH']]y=data[['PE']]fromsklearn.model_selectionimportcross_val_predictpredicted=cross_val_predict(linreg,X,y,cv=10)#用scikit-learn计算MSEprint\"MSE:\",metrics.mean_squared_error(y,predicted)#用scikit-learn计算RMSEprint\"RMSE:\",np.sqrt(metrics.mean_squared_error(y,predicted))12345678X=data[['AT','V','AP','RH']]y=data[['PE']]fromsklearn.model_selectionimportcross_val_predictpredicted=cross_val_predict(linreg,X,y,cv=10)#用scikit-learn计算MSEprint\"MSE:\",metrics.mean_squared_error(y,predicted)#用scikit-learn计算RMSEprint\"RMSE:\",np.sqrt(metrics.mean_squared_error(y,predicted))输出如下：MSE:20.7955974619RMSE:4.5602190146912MSE:20.7955974619RMSE:4.56021901469可以看出，采用交叉验证模型的MSE比第6节的大，主要原因是我们这里是对所有折的样本做测试集对应的预测值的MSE，而第6节仅仅对25%的测试集做了MSE。两者的先决条件并不同。9.画图观察结果这里画图真实值和预测值的变化关系，离中间的直线y=x直接越近的点代表预测损失越低。代码如下：fig,ax=plt.subplots()ax.scatter(y,predicted)ax.plot([y.min(),y.max()],[y.min(),y.max()],'k--',lw=4)ax.set_xlabel('Measured')ax.set_ylabel('Predicted')plt.show()123456fig,ax=plt.subplots()ax.scatter(y,predicted)ax.plot([y.min(),y.max()],[y.min(),y.max()],'k--',lw=4)ax.set_xlabel('Measured')ax.set_ylabel('Predicted')plt.show()输出的图像如下:以上就是用scikit-learn和pandas学习线性回归的过程，希望可以对初学者有所帮助。1赞5收藏评论"], "art_url": ["http://python.jobbole.com/88597/"]}
{"art_img": ["http://jbcdn2.b0.upaiyun.com/2015/02/86a1298ce44ca9807871520b81767e6f.gif"], "art_title": ["为什么你应该学 Python ？"], "art_create_time": ["2017/09/20"], "art_content": ["本文由伯乐在线-逆旅翻译，黄利民校稿。未经许可，禁止转载！英文出处：iluxonchik。欢迎加入翻译组。引言第一次接触Python是在一节编程入门课上。其实，在此之前了解过它，所以在上课之前我对它的语法已经很熟悉了，但在上课之前我没有用它做过真正的项目。尽管对它没有太大兴趣，但我认为把它介绍给人们去学习编程还是很好的。我对它不是不喜欢，而是一种“无所谓”的态度。原因很简单：它里面有太多“魔法”。C和Java这些语言，对底层的行为描述的很清晰，Python则完全相反。另外，Python结构松散：写大型复杂程序时，遇到规则严谨的程序结构体（比如每个文件一个公共类），比其他语言（比如Java）要费些力气。但是，在这些方面Python给了你很大的自由。另一件事是严格的编码风格和调试：因为Python是解释型语言，查找问题不太容易：如果C语言有语法错误，编译器会直接停止编译，但在解释型语言中，直到执行到问题行，问题才会被发现。试着在需要整数的时候传一个字符串？cc会马上提醒你，Python解释器却对此一点都不介意（虽然有工具可以发现这个问题，比如mypy，但我讨论的是通用的Python）。我提到的这些问题是解释型语言的通病，并非Python独有，但这些是我不喜欢它的主要原因。还有一个烦人的问题是强制缩进。我们老师（很优秀）认为这是好事情，因为“它强制我们形成简洁的代码风格”。确实如此，但还是有点烦，当代码没有按预期执行时，你分析代码想要找出bug，它却无影无踪，过了很长时间之后你发现if语句那一行有一个多余的空格。我曾经和同事聊过Python，告诉他为什么我之前对这个语言不感冒，他笑着问我“问什么不喜欢Python呢？因为它读起来很像英语？”。是的。因为这个语言做了很多底层的工作，有时候会不清楚发生了什么。举个读文件的例子，假设你想一行一行读取文件内容并打印出来。C会这么做：C#include<stdio>intmain(void){FILE*fp;charbuff[256];//assumingalinewon'tcontainmorethan256charsfp=fopen(\"hello.txt\",\"r\");while(fgets(buff,256,fp)){printf(\"%s\",buff);}fclose(fp);return0;}1234567891011121314#include<stdio> intmain(void){    FILE*fp;    charbuff[256];//assumingalinewon'tcontainmorethan256chars    fp=fopen(\"hello.txt\",\"r\");     while(fgets(buff,256,fp)){        printf(\"%s\",buff);    }     fclose(fp);    return0;}python这么做：Pythonwithopen('hello.txt')asf:forlineinf:print(line)123withopen('hello.txt')asf:    forlineinf:        print(line)现在，很多人会认为这是python的优势，然而，第一个例子中，干了什么一目了然：获取一个文件指针从文件读取每一行数据到缓存中，打印缓存中的内容关闭文件流python的例子中看不到这些，它是一种“魔法般的”过程。现在，有人认为这是好事，因为将程序员与底层实现细节隔离（我同意这个说法），但我想知道到底发生了什么。有趣的是，我以上提到的缺点，我现在认为都是优点。为了公平起见，我强调，Python里边没有魔法，如果你多了解一点，你会发现真的没有，有的只是语言解释代码的方式，从这点来看，我发现它挺有意思的。如果你也这么觉得，我建议你深入了解它的工作机制，如果有东西像魔法，就找出来到底发生了什么，事情就会变得清晰，魔法就变成了便利。我的认识发生很大的变化，尤其是我决定使用Python后，事实上我现在是Python的死忠！现在你也许会想我将会在哪里说服你学Python是个好主意，不要担心，马上就到。作为引言的结尾，我想说明，这只是我对这个语言的个人感受，只是个人偏好。我没有试图以“如果你用Python，你就不是真正的程序员（实际上，我不这么认为）”的理由劝说人们学C。当有人问我他们的入门语言应该选哪个，我通常建议他们选Python，基于我上边提到的“缺点”的原因。我的感觉来源于我的兴趣，我曾经在做一些很底层的东西，你能想到，Python并不适用。Python语言精粹在借用了JavaScript畅销书《JavaScript语言精粹》作为本节标题后，我们开始讨论本文的主题：为什么你(没错，就是你！)应该学Python。1、通用脚本语言这是我使用Python的主要原因。我曾经和很多人做过很多项目，不同的人用不同的系统。就我而言，我经常在windows系统和linux系统之间切换。举一个实际的例子，有一个项目，我写了项目的自动测试脚本，结果发现只有我能用，因为是用PowerShell写的，而我是项目中唯一使用Windows的。当时同事们自然认为bash是最好的，我还向他们解释PowerShell遵循一种不同的模式并且有它的强项（例如，它提供了.NET框架接口），它是面向对象的脚本语言，和bash完全不一样。现在我不想讨论哪个更好，因为这不是本文的重点。那么这个问题怎么解决呢？嗯…现在，是否有一种脚本语言可以在所有主流平台上运行呢？你猜对了，它就是Python。除了可以在主流平台上运行，它还是开箱即用的脚本语言。标准库包含不少实用程序，提供了独立于系统的常用接口。举一个简洁明了的例子，假设你想获取文件夹下所有文件的文件名，然后对其进行处理，在UNIX下，你要这么做：forfin*;doecho\"Processing$ffile...\";done1forfin*;doecho\"Processing$ffile...\";done用PowerShell做类似的事情：Get-ChildItem\".\"|Foreach-Object{$name=$_.NameWrite-Output\"Processing$($name)file...\"}12345Get-ChildItem\".\"|Foreach-Object{    $name=$_.Name    Write-Output\"Processing$($name)file...\"}AnequivalentfunctionalityinPythoncanbeachievedwith:python这么做：fromosimportlistdirforfinlistdir('.'):print('Processing{}file...'.format(f))1234fromosimportlistdir forfinlistdir('.'):    print('Processing{}file...'.format(f))现在我认为，Python除了可以跑在Linux，MacOSX和Windows上，它也很易读。上边例子中的脚本很简单，在复杂的例子中不同语言的易读性差异会更明显。就像我之前提到的，Python自带了许多强大的库用来取代shell脚本，你会发现，最有用的是：os–提供系统无关功能，比如文件目录和文件读写。subprocess–产生新进程、与输入输出流和返回代码交互。可以用它来启动系统已安装的程序，但请记住如果你担心脚本的可移植性，这不是最好的选择。shutil–提供对文件和文件集合的高级操作。argparse–解析命令行参数，构建命令行接口。好了，假设你get到了重点，跨平台和易读性听起来挺不错的，但是你真的喜欢类UNIXshell类似的语法怎么办？告诉你个好消息，鱼和熊掌可以兼得！看看Plumbum，它是一个Python模块，它的座右铭是“再也不写shell脚本”。它模仿了shell语法，同时保持了跨平台。不要完全抛弃shell脚本即使Python可以完全取代shell脚本，但也不是必须这么做，因为Python脚本天生适合Unix命令行理念，你要做的就是让它们从sys.stdin(标准输入)读数据，向sys.stdout(标准输出)写数据。举个例子，假设你有一个文件，每行有一个单词，你想知道每个单词在文中出现的次数。这种情况就没必要全部是用Python，我们可以使用cat命令和我们的脚本，称它为namecount.py一起来完成这个任务。假设有一个文件，名为names.txt，内容如下：catdogmousebirdcatcatdog1234567catdogmousebirdcatcatdog现在使用我们的脚本：$>catnames.txt|namecount.py1$>catnames.txt|namecount.pyPowershell:$>Get-Contentnames.txt|pythonnamecount.py1$>Get-Contentnames.txt|pythonnamecount.py期望的输出如下（顺序可能会变化）：bird1mouse1cat3dog21234bird1mouse1cat3dog2namecount.py源码:#!/usr/bin/envpython3importsysdefcount_names():names={}fornameinsys.stdin.readlines():name=name.strip()ifnameinnames:names[name]+=1else:names[name]=1forname,countinnames.items():sys.stdout.write(\"{0}\\t{1}\\n\".format(name,count))if__name__==\"__main__\":count_names()123456789101112131415161718#!/usr/bin/envpython3importsys defcount_names():    names={}    fornameinsys.stdin.readlines():        name=name.strip()         ifnameinnames:            names[name]+=1        else:            names[name]=1                forname,countinnames.items():        sys.stdout.write(\"{0}\\t{1}\\n\".format(name,count)) if__name__==\"__main__\":    count_names()无序的信息可读性差，你可能想按单词出现的次数对其排序，让我们试试。我们要用管道输出文件内容供内建命令处理。按数字降序排序，我们要做的就是$>catnames.txt|namecount.py|sort-rn。如果使用PowerShell应该这样：$>Get-Contentnames.txt|pythonnamecount.py|Sort-Object{[int]$_.split()[-1]}-Descending（你可能听到了Unixer的吐槽声了，PowerShell怎么这么繁琐）。这回我们的输出是确定的，如下所示：cat3dog2bird1mouse11234cat3dog2bird1mouse1（旁注：如果你用PowerShell，cat是Get-Content的别名，sort是Sort_object的别名，所以以上命令可以写成：$>catnames.txt|pythonnamecount.py和$>catnames.txt|pythonnamecount.py|sort{[int]$_.split()[-1]}-Descending）但愿我成功说服你python是你某些脚本的替代品，你不必完全抛弃shell脚本，因为你可以将Python融合到你现有的工作流和工具箱中，还可以从它跨平台，更好的可读性，还有丰富的库中获益（后面会讲）。2、大量优秀的库Python有非常丰富的库。我的意思是，几乎任何事都有库（有趣的是：如果你在你的Python解释器中输入importantigravity，在浏览器中打开xkdc漫画的页面，是不是很酷？）。我不是很推崇堆叠模块式的编程，但你不必这样。因为有太多的库，不表示你都要使用。我也不喜欢堆叠模块（它有点像CBSE），我在了解它们之后才使用。例如，我决定研究马尔科夫链，我想了一个项目：抓取一个艺术家的所有歌词，建立一个马尔科夫链，然后从其中生成歌曲。这个项目的目的是生成的歌曲应该能反映出艺术家的风格。所以我到处找相关的东西，搞出了lyricst项目（这只是个样品，还不成熟，只是一个测试项目，如我所言，我只是随便搞了一下，没想深入。如果你想玩的话，它包含有命令行界面和示例的说明文档）。我认为，最好的找歌词的地方是RAPGenius，因为它很活跃，经常更新。为了获取艺术家所有的歌词，我必须从网站上爬，然后处理HTML。幸运的是，Python很适合做网络爬虫，它有强大的库像BeautifulSoup可以处理HTML。所以我是这么做的，先使用BeautifulSoup从网页中抽取我需要的信息（就是歌词）然后用这些信息构建马尔科夫链。当然我曾经想用正则表达式构建自己的HTML解析器，但是这个库的存在让我更关注项目的最终目的：把玩马尔科夫链，让它更有趣，比方说，从文件中读取些内容出来。3、用来做渗透测试很强大如果你在作渗透测试或仅仅是喜欢玩玩，Python是你的好帮手！由于Python在所有LInux和MACOS机器上都有安装，还有丰富的库，完善的语法，还是一门脚本语言，让它很适合干这个。另一个我为什么决定使用Python的原因（除了我之前提到的）是我对安全很感兴趣，Python是用来做渗透测试的完美选择。我在第一次进入领域是通过Scapy（或Scapy3k，python3），我印象很深。Scapy能够创建、监听、解析数据包。它的API很简单，文档也很完善。你可以很容易的创建不同层的数据（我指的是OSI模型）或者捕获它们对其进行分析或修改。你甚至可以导出pcap文件用Wireshark打开。虽然除了抓包还能做很多事情，还有很多其他的库也可以，但我在这里不会涉及，因为这不是本文的重点而且要展开讲的话需要一篇文章。有人可能会说，“哦，太棒了，但我感兴趣的是Windows设备，里边不会自带Python”。别当心，你可以用py2exe把你的脚本编译成.exe文件。文件可能会有点大（取决于你是用的库的数量），但这不是重点。如果你很好奇，请参考listofPythonpentestingtools。文末我还推荐了几本书。4、黑客的语言Python是可塑性很强的语言。你可以用各种方法改造它。可参见《alteringthewayimportswork》和《messingwithclassesbeforetheyarecreated》这两篇文章。这只是一些例子。也让它成为强大的脚本语言（在第一节有说）适合做渗透测试（第三节），因为它给了你很大的自由。我不想讲太多，但我会讲述它让我惊讶的地方。当时，我在做一个网络爬虫(Python很适合干这个！)，我用的其中一个工具是BeautifulSoup。这是我用来学习Python的项目之一。Beautifulsoup处理HTML的语法清晰直观，原因是在自定义行为方面，Python给了你很大的自由。了解一番API后，发现有“魔法”。和这种情况类似：Pythonfrombs4importBeautifulSoupsoup=BeautifulSoup('<pclass=\"someclass\">Hello</p>','html.parser')soup.p1234frombs4importBeautifulSoup soup=BeautifulSoup('<pclass=\"someclass\">Hello</p>','html.parser')soup.p上面的代码利用第一个字符串参数创建了一个BeautifulsSoup实例，第二个参数表示我想使用Python自带的HTML解析器（BeautifulSoup可以搭配多种解析器）。soup.p返回一个Tag(bs4.element.Tag)对象，表示将作为第一个参数。以上代码的输出是：XHTML<pclass=\"someclass\">Hello</p>1<pclass=\"someclass\">Hello</p>现在你可能会想，你说的魔法在哪？马上就来。魔法在于上面的代码可以被修改为任何标签，甚至可以是自定义的。它意味着下面的代码也可以正常运行：Pythonfrombs4importBeautifulSoupsoup=BeautifulSoup('<foobarfooclass=\"someclass\">Hello</foobarfoo>','html.parser')soup.foobarfoo1234frombs4importBeautifulSoupsoup=BeautifulSoup('<foobarfooclass=\"someclass\">Hello</foobarfoo>','html.parser')soup.foobarfooTheoutputisthefollowing:输出如下：<foobarfooclass=\"someclass\">Hello</foobarfoo>1<foobarfooclass=\"someclass\">Hello</foobarfoo>当我发现这样也能运行，我的反应是“怎么回事？”。因为，第一个例子很容易实现，我的意思是最直接的方法是为每一个HTML标签定义一个属性（实例变量），在解析过程中如果找到了，就赋值给它们。但是这对第二种情况不适用，不可能对所有的字符串定义属性。我想知道它是怎么实现的，所以我打开BeautifulSoups源代码开始寻找。我没有发现任何命名为p的属性，这一点也不奇怪，解析函数没有对其赋值。谷歌一番后，我找到了答案：魔法方法。什么是魔法方法，为什么要叫这个名字？事实上，魔法方法是给你的类赋予魔法的方法。这种方法通常前后有两条下划线（例如__init__()）,在Python文档的DataModelmodelsection有对它的说明。真正让BeautifulSoup拥有这个功能的魔法方法是__getattr__(self,name)（self在python中指向实例，和Java中的this类似）。如果去查看文档，你会发现第一段如下：如果在属性常见地方找不到属性时，比如既不是实例属性，又没有在self类树中找到，则调用该方法（object.__getattr__(self,name)）。参数name就是属性名这个方法应当返回（计算过的）属性值或抛出AttributeError异常。当你尝试访问一个不存在的属性，对象的__getattr__(self,name)方法会被调用，将返回一个以name作为名字的属性的字符串。举个例子。假设你有一个Person类，拥有first_name属性。我们给使用者访问和name相同属性的内容的能力。下面是代码：classPerson(object):def__init__(self,first_name):self.first_name=first_namedef__getattr__(self,name):if(name=='name'):returnself.first_nameraiseAttributeError('Personobjecthasnoattribute\\'{}\\''.format(name))12345678classPerson(object):    def__init__(self,first_name):        self.first_name=first_name     def__getattr__(self,name):        if(name=='name'):            returnself.first_name        raiseAttributeError('Personobjecthasnoattribute\\'{}\\''.format(name))我们在终端运行代码：person=Person('Jason')>>>person.first_name'Jason'>>>person.name'Jason'>>>person.abcTraceback(mostrecentcalllast):File\"<stdin>\",line1,in<module>File\"<stdin>\",line7,in__getattr__AttributeError:Personobjecthasnoattribute'abc'123456789101112person=Person('Jason')>>>person.first_name'Jason' >>>person.name'Jason' >>>person.abcTraceback(mostrecentcalllast):  File\"<stdin>\",line1,in<module>  File\"<stdin>\",line7,in__getattr__AttributeError:Personobjecthasnoattribute'abc'这意味着我们能凭空构造实例属性，是不是很棒？所以你可以偷偷的让你的Dog除了汪汪叫之外，还会喵喵叫：classDog(object):defbark(self):print('Ruff,ruff!')def__getattr__(self,name):if(name=='meow'):returnlambda:print('Meeeeeeow')raiseAttributeError('Idon\\'tknowwhatyou\\'retalkingabout...')12345678classDog(object):    defbark(self):        print('Ruff,ruff!')        def__getattr__(self,name):        if(name=='meow'):          returnlambda:  print('Meeeeeeow')        raiseAttributeError('Idon\\'tknowwhatyou\\'retalkingabout...')>>>snoop=Dog()>>>snoop.bark()Ruff,ruff!>>>snoop.meow()Meeeeeeow1234567>>>snoop=Dog() >>>snoop.bark()Ruff,ruff! >>>snoop.meow()Meeeeeeow你可以在没有reflection的情况下，随意添加新属性。object.__dict__是（字典）[https://docs.python.org/3.5/library/stdtypes.html#typesmapping]包含object的属性和它们的值（注意我说的是object.dict,object是一个实例，还有一个class.dict,是类的属性的字典)。意思是：classDog(object):def__init__(self):self.name='DoggyDogg'1234classDog(object):        def__init__(self):        self.name='DoggyDogg'等价于：classDog(object):def__init__(self):self.__dict__['name']='DoggyDogg'1234classDog(object):        def__init__(self):        self.__dict__['name']='DoggyDogg'两者输出是一样的：snoop=Dog()>>>snoop.name'DoggyDogg'1234snoop=Dog() >>>snoop.name'DoggyDogg'到这里你会想，是挺好的，但是有什么用呢？答案很简单：magicalAPIs。你有没有用过一些Python库让你感觉像魔法？这是让它们变的有”魔法”的一种情况。虽然一旦你懂了底层发生的事情,就会发现没有魔法。如果你还想了解更多，可以查看文档中的DescriptionProtocol。Python的面向对象Python的面向对象有点奇怪。例如，类中没有私有变量和方法。所以你想在类中创建一个实例变量或私有方法，你必须遵守规则：一个下划线 (_)表示私有变量和方法。两个下划线(__) 表示的变量和方法，它们的名字会被修改。举个例子，假设你有如下类：classFoo(object):def__init__(self):self.public='public'self._private='public'self.__secret='secret'12345classFoo(object):    def__init__(self):        self.public='public'        self._private='public'        self.__secret='secret'转到解释器：>>>foo=Foo()>>>foo.public'public'>>>foo._private'public'>>>foo.__secretTraceback(mostrecentcalllast):File\"<stdin>\",line1,in<module>AttributeError:'Foo'objecthasnoattribute'__secret'123456789>>>foo=Foo()>>>foo.public'public'>>>foo._private'public'>>>foo.__secretTraceback(mostrecentcalllast):  File\"<stdin>\",line1,in<module>AttributeError:'Foo'objecthasnoattribute'__secret'如你所见，你可以访问_private变量，但是最后一个例子发生了什么，它是否意味着有两个下划线的变量是真正的私有变量？答案是NO，它们的名字被改变了，实际上，它被Python替换成了_Foo_secret。如果你想访问的话，你仍然可以访问：>>>foo._Foo__secret'secret'12>>>foo._Foo__secret'secret'然而，PEP8建议只在父类中使用双下划线来避免属性名冲突。“PEP”，表示“PythonEnhancementProposal”，它用来描述Python特性或作用。如果你想要添加一个新特性，你可以创建一个PEP，这样可以让整个社区可以看到并讨论。你可以在这里了解更多的PEPs。可见，Python很信任程序员。我不会再深入讲OO了，因为它需要单独一篇文章（甚至是一系列）来讲解。我确实想给你提个醒，Python的OO可不像Java语言那么自然，你需要慢慢适应，但你知道吗，它只是做事的方法不同而已。举个例子，它没有抽象类，你必须使用装饰器来实现这个行为。结语希望这篇文章，能够给你一个学习Python的理由。这篇文章来自一个为过去说了Python的坏话而愧疚，如今在到处宣传Python的人。我先申明一点，这只是个人喜好问题，当有人问我先学哪门语言时，我通常推荐Python。如果你还没决定，那就给它一次机会！用上一两个小时，多读些关于它的东西。如果你喜欢从书上学习，我也会帮你，看看《FluentPython》，下节还有更多。书籍推荐我兑现了诺言，这一节推荐书籍。我会尽量保持简短一些，只包含一些我读过的书籍。《FluentPython》——一本讲Python3的好书。无论你是新手、熟手还是高手都值得一读。包含了Python的来龙去脉。《WebScrapingWithPython》——标题已经说明了一切，讲如何用Python来做网络爬虫。你会探索如何爬网上的内容，解析HTML等。我觉得这本书对爬虫领域的新手和熟手很有帮助。即使你之前从没用过Python，你也可以看懂。它没有涉及任何高级主题。《BlackHatPython》——这个有趣！你可以创建反弹SSHshell，木马等等！如果你想知道Python如何做渗透测试，请一定要读它。注意它使用的是Python2，我有一个仓库，用的是Python3。《ViolentPython:ACookbookforHackers,ForensicAnalysts,PenetrationTestersandSecurityEngineers》——比上面的主题要多，你会学到如何写一个常见的用于实战的渗透测试，取证分析和安全脚本。打赏支持我翻译更多好文章，谢谢！打赏译者打赏支持我翻译更多好文章，谢谢！2赞11收藏5评论关于作者：逆旅aprogrammer,Doone'slevelbestandleavetheresttoGod'swill.Ilovethissaying.个人主页·我的文章·17"], "art_url": ["http://python.jobbole.com/88622/"]}
{"art_img": ["http://img.blog.csdn.net/20171111215334992"], "art_title": ["三生万物：决策树"], "art_create_time": ["2017/11/16"], "art_content": ["本文作者：伯乐在线-翱翔的翱。未经作者许可，禁止转载！欢迎加入伯乐在线专栏作者。一、概述不知怎么回事，提到决策树我就想起”道生一，一生二，二生三，三生万物“这句话，大概是因为决策树从一个根节点慢慢“长”成一棵树，也要经历“一生二，二生三”的过程。决策树本质上就是一种二叉树，根据特定的标准不停的分成左右两个子树，直到符合某种条件停止。树算法解释性强、简单直观以及接近人的决策方式使它成为流行的机器学习算法之一。当决策树与装袋法(Bag)、提升法(Boosting)结合之后，可以成为更强大的算法。决策树按响应值的类型大致分为分类树和回归树，实现决策树的方法也很多，比如CART、ID3、C4.5等等，本文将对CART这种算法进行介绍。二、算法一棵树要长成要解决两方面的问题，一是如何分，二是何时停。这两点对于分类和回归略有区别，先说如何分，对于定量变量一般是将小于某个值的数据划分为左子树，大于等于某个值的划分为右子树；对于定性变量一般是将等于某个值的划分为左子树，不等于某个值的划分为右子树。那么什么才是一个好的划分呢？分类树大致分为两种，一种是按纯度(Purity)，纯度是通过基尼系数(GiniIndex)进行定义的，基尼系数越小，纯度越大，那么划分效果越好。基尼系数的计算方法如下式所示： G=∑k=1Kp^k(1−p^k)p^k代表第k类所占比例，当p^k接近0或1时，基尼系数会很小。另一种标准是互熵(Cross-entropy)，互熵的定义如下： D=−∑k=1Kp^klogp^k由定义可以看到，和基尼系数类似，当p^k接近0或1时，互熵也很小，划分的效果也越好。回归树则根据残差平方和RSS： RSS=1N∑i=1N(yi−y¯)2y¯代表平均响应值，可以看到这实际上就是方差，我们都知道方差是衡量数据变异性的量，因此RSS越小表示回归模型效果越好。注意上面的纯度、互熵以及残差平方和均是树的一个分枝上的值，总的值要对左右分枝进行加权平均，例如基尼系数的最终值应该这样计算， Gtotal=NleftNGleft+NrightNGrightN表示总的样本数，Nleft，Nright分别代表左分枝和右分枝的样本数，互熵和残差平方和的计算方式类似。说了如何分，那什么时候停呢？一般的惯例是子树中的预测变量或响应值都一样了就可以停止分裂了。有时候这个条件可能有些苛刻，这时候可以设置一个NodeSize值，表示叶子节点包含的最小的样本数。分裂过程中如果一个子树的样本数小于等于这个值就停止分裂，分类数取数目最多的那个类，回归树取响应的均值。说了这么多，下面举个例子，来演示下决策树算法，比如这里有一份城市和农村儿童身高数据，注意这里的数据都是我杜撰的，只是为了演示决策树的算法。如果已知一个儿童身高和性别，如何判断所处的区域？身高性别地区100男城市90女城市90男农村80女农村下面尝试根据基尼系数来构造一个分类树，第一次分裂：身高身高身高性别=男：2/4x(1/2x1/2+1/2x1/2)+2/4x(1/2x1/2+1/2x1/2)=1/2性别=女：2/4x(1/2x1/2+1/2x1/2)+2/4x(1/2x1/2+1/2x1/2)=1/2可以看到前面两个都是1/3，选择哪一个都行，这里我选择第一个最小值：“身高左子树身高性别地区90女城市90男农村80女农村右子树身高性别地区100男城市第二次分裂：由于右面的子树只有一条数据，因此只需计算左边子树的基尼系数，身高身高性别=女：2/3x(1/2x1/2+1/2x1/2)+1/3x(1x0)=1/3性别=男：1/3x(1x0)+2/3x(1/2x1/2+1/2x1/2)=1/3同上选择第一个最低值“身高左子树身高性别地区80女农村右子树身高性别地区90女城市90男农村第三次分裂：同理，左边子树只有一条数据，只需计算右子树身高性别=女：1/2x(1x0)+1/2x(1x0)=0性别=男：1/2x(1x0)+1/2x(1x0)=0选择“性别=女”这个条件，至此所有的子树的响应值都是唯一的，停止分裂。最终这个分类树的样子大概如下，三、树的剪枝其实树的剪枝就是正则化，剪枝一般分为两种：一种称为预剪枝，通过设置NodeSize的大小来达到控制树的分枝个数的目的，这种方式简单易用，但有短视的风险；另一种称为后剪枝，原理是让树充分“生长”，然后尝试合并树的分枝，通过对比合并前后错误率是否降低来决定是否真得合并，这种方式效果较前一种好，但是实现稍微复杂一些。四、说了就练俗话说，光说不练假把式，下面我用R语言实现一个决策树，并尝试分析两个实际的数据集。1、鸢尾花(iris)数据集，这个数据集包括五个变量：花萼长度(Sepal.Length)，花萼宽度(Sepal.Width)，花瓣长度(Petal.Length)，花瓣宽度(Petal.Width)，种类(Species)，下面尝试使用花萼长度(Sepal.Length)和花萼宽度(Sepal.Width)这两个变量来预测鸢尾花的种类(Species)。为了简便，我采用的是预剪枝的方式。那么选择多大的NodeSize合适呢？关于这个问题通常的方法就是交叉验证(Cross-validation)。下图是采用10折交叉验证(k-foldcross-validation)得到的错误率,可以看到，当NodeSize为40的时候测试集的错误率Eout最低，从另一个方面也可以看到如果不进行剪枝，Eout约为0.4，比剪枝后的错误率高了将近0.2。从下面的第一张图也可以直观的看到当NodeSize从小到大增加时，分类边界(DecisionBoundary)从过拟合(Overfit)到欠拟合(Underfit)的变化趋势。第二张图是根据交叉验证得到的最佳分类边界，它和NodeSize为30的分类边界非常相似。最终的错误率约为0.2，从上面第二张图可以看到versicolor和virginica这两类的鸢尾花有些数据在二维空间完全重合在了一起，仅仅依靠花萼长度(Sepal.Length)，花萼宽度(Sepal.Width)这两个变量是无法把它们分开的，这个时候单纯的增加样本数无法进一步提高模型的质量，这个时候最好去寻找新的变量，事实上，当加上花瓣长度(Petal.Length)，花瓣宽度(Petal.Width)这两个变量时，预测的错误率可以降低到0.06左右。树的样子如下，[L]和[R]分别代表左右分枝。2、上面是个分类问题，那么再看一个回归问题。北京二手房这个数据集有13个特征，下面使用决策树根据房子的区域(area)、是否学区(school)、是否有地铁(subway)、总价(num)这四个变量来预测房价(price)。同样，祭出我们的法宝交叉验证得到一个合适的NodeSize，如下所示，对于回归,我采用了决策系数R2作为衡量模型效果的标准，由于R2是越大越好，且0得到的决策系数R2约为0.7，也就是区域、是否学区、是否有地铁、总价这四个变量解释了70%房价变异。由这个相对误差图可以看出大部分的数据都落在了0附近，实际上有20275条数据落在[-0.2,0.2]，28379条数据落在[-0.5,0.5]。那么，那些误差比较大的都是些什么数据呢？Python下面的数据为相对误差大于3的arearegionzonemetersdirectionconflooryearschoolsubwaytaxnumprice海淀东小营甲1号5室2厅350南西北旺二手房低楼层1998无学区无地铁非免税2808000朝阳北苑家园望春园1室0厅36南北北苑二手房地下室2008无学区无地铁非免税205556昌平香堂文化新村二期5室4厅460南北昌平其它二手房低楼层2010无学区无地铁非免税2204783昌平东亚上北中心1室0厅738北回龙观二手房地下室2007无学区无地铁非免税3705012123456下面的数据为相对误差大于3的arearegionzonemetersdirectionconflooryearschoolsubwaytaxnumprice海淀东小营甲1号5室2厅350南西北旺二手房低楼层1998无学区无地铁非免税280  8000朝阳北苑家园望春园1室0厅36南北北苑二手房地下室2008无学区无地铁非免税  20  5556昌平香堂文化新村二期5室4厅460南北昌平其它二手房低楼层2010无学区无地铁非免税2204783昌平东亚上北中心1室0厅738北回龙观二手房地下室2007无学区无地铁非免税370  5012感觉这些数据好像异常数据，北京还有低于1万的房价？！五、总结当一个小小的种子慢慢成长为一颗参天大树，独霸森林一方，常常让人感受生命的强大，而决策树算法同样让人惊叹，易于实现又足够灵活，既能用于分类又能用于回归，也在机器学习领域赢得了一席之地。本文简单介绍了决策树的算法和剪枝，在此基础上用R实现了一个决策树，并在两个数据集上进行了测验，证实了决策树的能力。打赏支持我写出更多好文章，谢谢！打赏作者打赏支持我写出更多好文章，谢谢！任选一种支付方式1赞3收藏2评论关于作者：翱翔的翱Javaisagoodboy!个人主页·我的文章·6·"], "art_url": ["http://python.jobbole.com/88858/"]}
{"art_img": ["http://jbcdn2.b0.upaiyun.com/2017/08/10921e07716aca4930a41a292bb8953a.png"], "art_title": ["标签传播算法（Label Propagation）及 Python 实现"], "art_create_time": ["2017/08/31"], "art_content": ["原文出处：zouxy09   众所周知，机器学习可以大体分为三大类：监督学习、非监督学习和半监督学习。监督学习可以认为是我们有非常多的labeled标注数据来train一个模型，期待这个模型能学习到数据的分布，以期对未来没有见到的样本做预测。那这个性能的源头–训练数据，就显得非常感觉。你必须有足够的训练数据，以覆盖真正现实数据中的样本分布才可以，这样学习到的模型才有意义。那非监督学习就是没有任何的labeled数据，就是平时所说的聚类了，利用他们本身的数据分布，给他们划分类别。而半监督学习，顾名思义就是处于两者之间的，只有少量的labeled数据，我们试图从这少量的labeled数据和大量的unlabeled数据中学习到有用的信息。一、半监督学习半监督学习（Semi-supervisedlearning）发挥作用的场合是：你的数据有一些有label，一些没有。而且一般是绝大部分都没有，只有少许几个有label。半监督学习算法会充分的利用unlabeled数据来捕捉我们整个数据的潜在分布。它基于三大假设：Smoothness平滑假设：相似的数据具有相同的label。Cluster聚类假设：处于同一个聚类下的数据具有相同label。Manifold流形假设：处于同一流形结构下的数据具有相同label。例如下图，只有两个labeled数据，如果直接用他们来训练一个分类器，例如LR或者SVM，那么学出来的分类面就是左图那样的。如果现实中，这个数据是右图那边分布的话，猪都看得出来，左图训练的这个分类器烂的一塌糊涂、惨不忍睹。因为我们的labeled训练数据太少了，都没办法覆盖我们未来可能遇到的情况。但是，如果右图那样，把大量的unlabeled数据（黑色的）都考虑进来，有个全局观念，牛逼的算法会发现，哎哟，原来是两个圈圈（分别处于两个圆形的流形之上）！那算法就很聪明，把大圈的数据都归类为红色类别，把内圈的数据都归类为蓝色类别。因为，实践中，labeled数据是昂贵，很难获得的，但unlabeled数据就不是了，写个脚本在网上爬就可以了，因此如果能充分利用大量的unlabeled数据来辅助提升我们的模型学习，这个价值就非常大。半监督学习算法有很多，下面我们介绍最简单的标签传播算法（labelpropagation），最喜欢简单了，哈哈。二、标签传播算法标签传播算法（labelpropagation）的核心思想非常简单：相似的数据应该具有相同的label。LP算法包括两大步骤：1）构造相似矩阵；2）勇敢的传播吧。2.1、相似矩阵构建LP算法是基于Graph的，因此我们需要先构建一个图。我们为所有的数据构建一个图，图的节点就是一个数据点，包含labeled和unlabeled的数据。节点i和节点j的边表示他们的相似度。这个图的构建方法有很多，这里我们假设这个图是全连接的，节点i和节点j的边权重为：这里，α是超参。还有个非常常用的图构建方法是knn图，也就是只保留每个节点的k近邻权重，其他的为0，也就是不存在边，因此是稀疏的相似矩阵。2.2、LP算法标签传播算法非常简单：通过节点之间的边传播label。边的权重越大，表示两个节点越相似，那么label越容易传播过去。我们定义一个NxN的概率转移矩阵P：Pij表示从节点i转移到节点j的概率。假设有C个类和L个labeled样本，我们定义一个LxC的label矩阵YL，第i行表示第i个样本的标签指示向量，即如果第i个样本的类别是j，那么该行的第j个元素为1，其他为0。同样，我们也给U个unlabeled样本一个UxC的label矩阵YU。把他们合并，我们得到一个NxC的softlabel矩阵F=[YL;YU]。softlabel的意思是，我们保留样本i属于每个类别的概率，而不是互斥性的，这个样本以概率1只属于一个类。当然了，最后确定这个样本i的类别的时候，是取max也就是概率最大的那个类作为它的类别的。那F里面有个YU，它一开始是不知道的，那最开始的值是多少？无所谓，随便设置一个值就可以了。千呼万唤始出来，简单的LP算法如下：执行传播：F=PF重置F中labeled样本的标签：FL=YL重复步骤1）和2）直到F收敛。步骤1）就是将矩阵P和矩阵F相乘，这一步，每个节点都将自己的label以P确定的概率传播给其他节点。如果两个节点越相似（在欧式空间中距离越近），那么对方的label就越容易被自己的label赋予，就是更容易拉帮结派。步骤2）非常关键，因为labeled数据的label是事先确定的，它不能被带跑，所以每次传播完，它都得回归它本来的label。随着labeled数据不断的将自己的label传播出去，最后的类边界会穿越高密度区域，而停留在低密度的间隔中。相当于每个不同类别的labeled样本划分了势力范围。2.3、变身的LP算法我们知道，我们每次迭代都是计算一个softlabel矩阵F=[YL;YU]，但是YL是已知的，计算它没有什么用，在步骤2）的时候，还得把它弄回来。我们关心的只是YU，那我们能不能只计算YU呢？Yes。我们将矩阵P做以下划分：这时候，我们的算法就一个运算：迭代上面这个步骤直到收敛就ok了，是不是很cool。可以看到FU不但取决于labeled数据的标签及其转移概率，还取决了unlabeled数据的当前label和转移概率。因此LP算法能额外运用unlabeled数据的分布特点。这个算法的收敛性也非常容易证明，具体见参考文献[1]。实际上，它是可以收敛到一个凸解的：所以我们也可以直接这样求解，以获得最终的YU。但是在实际的应用过程中，由于矩阵求逆需要O(n3)的复杂度，所以如果unlabeled数据非常多，那么I–PUU矩阵的求逆将会非常耗时，因此这时候一般选择迭代算法来实现。三、LP算法的Python实现Python环境的搭建就不啰嗦了，可以参考前面的博客。需要额外依赖的库是经典的numpy和matplotlib。代码中包含了两种图的构建方法：RBF和KNN指定。同时，自己生成了两个toy数据库：两条长形形状和两个圈圈的数据。第四部分我们用大点的数据库来做实验，先简单的可视化验证代码的正确性，再前线。算法代码：#***************************************************************************#*#*Description:labelpropagation#*Author:ZouXiaoyi(zouxy09@qq.com)#*Date:2015-10-15#*HomePage:http://blog.csdn.net/zouxy09#*#**************************************************************************importtimeimportnumpyasnp#returnkneighborsindexdefnavie_knn(dataSet,query,k):numSamples=dataSet.shape[0]##step1:calculateEuclideandistancediff=np.tile(query,(numSamples,1))-dataSetsquaredDiff=diff**2squaredDist=np.sum(squaredDiff,axis=1)#sumisperformedbyrow##step2:sortthedistancesortedDistIndices=np.argsort(squaredDist)ifk>len(sortedDistIndices):k=len(sortedDistIndices)returnsortedDistIndices[0:k]#buildabiggraph(normalizedweightmatrix)defbuildGraph(MatX,kernel_type,rbf_sigma=None,knn_num_neighbors=None):num_samples=MatX.shape[0]affinity_matrix=np.zeros((num_samples,num_samples),np.float32)ifkernel_type=='rbf':ifrbf_sigma==None:raiseValueError('Youshouldinputasigmaofrbfkernel!')foriinxrange(num_samples):row_sum=0.0forjinxrange(num_samples):diff=MatX[i,:]-MatX[j,:]affinity_matrix[i][j]=np.exp(sum(diff**2)/(-2.0*rbf_sigma**2))row_sum+=affinity_matrix[i][j]affinity_matrix[i][:]/=row_sumelifkernel_type=='knn':ifknn_num_neighbors==None:raiseValueError('Youshouldinputakofknnkernel!')foriinxrange(num_samples):k_neighbors=navie_knn(MatX,MatX[i,:],knn_num_neighbors)affinity_matrix[i][k_neighbors]=1.0/knn_num_neighborselse:raiseNameError('Notsupportkerneltype!Youcanuseknnorrbf!')returnaffinity_matrix#labelpropagationdeflabelPropagation(Mat_Label,Mat_Unlabel,labels,kernel_type='rbf',rbf_sigma=1.5,\\knn_num_neighbors=10,max_iter=500,tol=1e-3):#initializenum_label_samples=Mat_Label.shape[0]num_unlabel_samples=Mat_Unlabel.shape[0]num_samples=num_label_samples+num_unlabel_sampleslabels_list=np.unique(labels)num_classes=len(labels_list)MatX=np.vstack((Mat_Label,Mat_Unlabel))clamp_data_label=np.zeros((num_label_samples,num_classes),np.float32)foriinxrange(num_label_samples):clamp_data_label[i][labels[i]]=1.0label_function=np.zeros((num_samples,num_classes),np.float32)label_function[0:num_label_samples]=clamp_data_labellabel_function[num_label_samples:num_samples]=-1#graphconstructionaffinity_matrix=buildGraph(MatX,kernel_type,rbf_sigma,knn_num_neighbors)#starttopropagationiter=0;pre_label_function=np.zeros((num_samples,num_classes),np.float32)changed=np.abs(pre_label_function-label_function).sum()whileiter<max_iterandchanged>tol:ifiter%1==0:print\"--->Iteration%d/%d,changed:%f\"%(iter,max_iter,changed)pre_label_function=label_functioniter+=1#propagationlabel_function=np.dot(affinity_matrix,label_function)#clamplabel_function[0:num_label_samples]=clamp_data_label#checkconvergechanged=np.abs(pre_label_function-label_function).sum()#getterminatelabelofunlabeleddataunlabel_data_labels=np.zeros(num_unlabel_samples)foriinxrange(num_unlabel_samples):unlabel_data_labels[i]=np.argmax(label_function[i+num_label_samples])returnunlabel_data_labels123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101#***************************************************************************  #*  #*Description:labelpropagation  #*Author:ZouXiaoyi(zouxy09@qq.com)  #*Date:  2015-10-15  #*HomePage:http://blog.csdn.net/zouxy09  #*  #**************************************************************************    importtime  importnumpyasnp    #returnkneighborsindex  defnavie_knn(dataSet,query,k):      numSamples=dataSet.shape[0]        ##step1:calculateEuclideandistance      diff=np.tile(query,(numSamples,1))-dataSet      squaredDiff=diff**2      squaredDist=np.sum(squaredDiff,axis=1)#sumisperformedbyrow        ##step2:sortthedistance      sortedDistIndices=np.argsort(squaredDist)      ifk>len(sortedDistIndices):          k=len(sortedDistIndices)        returnsortedDistIndices[0:k]      #buildabiggraph(normalizedweightmatrix)  defbuildGraph(MatX,kernel_type,rbf_sigma=None,knn_num_neighbors=None):      num_samples=MatX.shape[0]      affinity_matrix=np.zeros((num_samples,num_samples),np.float32)      ifkernel_type=='rbf':          ifrbf_sigma==None:              raiseValueError('Youshouldinputasigmaofrbfkernel!')          foriinxrange(num_samples):              row_sum=0.0              forjinxrange(num_samples):                  diff=MatX[i,:]-MatX[j,:]                  affinity_matrix[i][j]=np.exp(sum(diff**2)/(-2.0*rbf_sigma**2))                  row_sum+=affinity_matrix[i][j]              affinity_matrix[i][:]/=row_sum      elifkernel_type=='knn':          ifknn_num_neighbors==None:              raiseValueError('Youshouldinputakofknnkernel!')          foriinxrange(num_samples):              k_neighbors=navie_knn(MatX,MatX[i,:],knn_num_neighbors)              affinity_matrix[i][k_neighbors]=1.0/knn_num_neighbors      else:          raiseNameError('Notsupportkerneltype!Youcanuseknnorrbf!')            returnaffinity_matrix      #labelpropagation  deflabelPropagation(Mat_Label,Mat_Unlabel,labels,kernel_type='rbf',rbf_sigma=1.5,\\                      knn_num_neighbors=10,max_iter=500,tol=1e-3):      #initialize      num_label_samples=Mat_Label.shape[0]      num_unlabel_samples=Mat_Unlabel.shape[0]      num_samples=num_label_samples+num_unlabel_samples      labels_list=np.unique(labels)      num_classes=len(labels_list)            MatX=np.vstack((Mat_Label,Mat_Unlabel))      clamp_data_label=np.zeros((num_label_samples,num_classes),np.float32)      foriinxrange(num_label_samples):          clamp_data_label[i][labels[i]]=1.0            label_function=np.zeros((num_samples,num_classes),np.float32)      label_function[0:num_label_samples]=clamp_data_label      label_function[num_label_samples:num_samples]=-1            #graphconstruction      affinity_matrix=buildGraph(MatX,kernel_type,rbf_sigma,knn_num_neighbors)            #starttopropagation      iter=0;pre_label_function=np.zeros((num_samples,num_classes),np.float32)      changed=np.abs(pre_label_function-label_function).sum()      whileiter<max_iterandchanged>tol:          ifiter%1==0:              print\"--->Iteration%d/%d,changed:%f\"%(iter,max_iter,changed)          pre_label_function=label_function          iter+=1                    #propagation          label_function=np.dot(affinity_matrix,label_function)                    #clamp          label_function[0:num_label_samples]=clamp_data_label                    #checkconverge          changed=np.abs(pre_label_function-label_function).sum()            #getterminatelabelofunlabeleddata      unlabel_data_labels=np.zeros(num_unlabel_samples)      foriinxrange(num_unlabel_samples):          unlabel_data_labels[i]=np.argmax(label_function[i+num_label_samples])            returnunlabel_data_labels测试代码：#***************************************************************************#*#*Description:labelpropagation#*Author:ZouXiaoyi(zouxy09@qq.com)#*Date:2015-10-15#*HomePage:http://blog.csdn.net/zouxy09#*#**************************************************************************importtimeimportmathimportnumpyasnpfromlabel_propagationimportlabelPropagation#showdefshow(Mat_Label,labels,Mat_Unlabel,unlabel_data_labels):importmatplotlib.pyplotaspltforiinrange(Mat_Label.shape[0]):ifint(labels[i])==0:plt.plot(Mat_Label[i,0],Mat_Label[i,1],'Dr')elifint(labels[i])==1:plt.plot(Mat_Label[i,0],Mat_Label[i,1],'Db')else:plt.plot(Mat_Label[i,0],Mat_Label[i,1],'Dy')foriinrange(Mat_Unlabel.shape[0]):ifint(unlabel_data_labels[i])==0:plt.plot(Mat_Unlabel[i,0],Mat_Unlabel[i,1],'or')elifint(unlabel_data_labels[i])==1:plt.plot(Mat_Unlabel[i,0],Mat_Unlabel[i,1],'ob')else:plt.plot(Mat_Unlabel[i,0],Mat_Unlabel[i,1],'oy')plt.xlabel('X1');plt.ylabel('X2')plt.xlim(0.0,12.)plt.ylim(0.0,12.)plt.show()defloadCircleData(num_data):center=np.array([5.0,5.0])radiu_inner=2radiu_outer=4num_inner=num_data/3num_outer=num_data-num_innerdata=[]theta=0.0foriinrange(num_inner):pho=(theta%360)*math.pi/180tmp=np.zeros(2,np.float32)tmp[0]=radiu_inner*math.cos(pho)+np.random.rand(1)+center[0]tmp[1]=radiu_inner*math.sin(pho)+np.random.rand(1)+center[1]data.append(tmp)theta+=2theta=0.0foriinrange(num_outer):pho=(theta%360)*math.pi/180tmp=np.zeros(2,np.float32)tmp[0]=radiu_outer*math.cos(pho)+np.random.rand(1)+center[0]tmp[1]=radiu_outer*math.sin(pho)+np.random.rand(1)+center[1]data.append(tmp)theta+=1Mat_Label=np.zeros((2,2),np.float32)Mat_Label[0]=center+np.array([-radiu_inner+0.5,0])Mat_Label[1]=center+np.array([-radiu_outer+0.5,0])labels=[0,1]Mat_Unlabel=np.vstack(data)returnMat_Label,labels,Mat_UnlabeldefloadBandData(num_unlabel_samples):#Mat_Label=np.array([[5.0,2.],[5.0,8.0]])#labels=[0,1]#Mat_Unlabel=np.array([[5.1,2.],[5.0,8.1]])Mat_Label=np.array([[5.0,2.],[5.0,8.0]])labels=[0,1]num_dim=Mat_Label.shape[1]Mat_Unlabel=np.zeros((num_unlabel_samples,num_dim),np.float32)Mat_Unlabel[:num_unlabel_samples/2,:]=(np.random.rand(num_unlabel_samples/2,num_dim)-0.5)*np.array([3,1])+Mat_Label[0]Mat_Unlabel[num_unlabel_samples/2:num_unlabel_samples,:]=(np.random.rand(num_unlabel_samples/2,num_dim)-0.5)*np.array([3,1])+Mat_Label[1]returnMat_Label,labels,Mat_Unlabel#mainfunctionif__name__==\"__main__\":num_unlabel_samples=800#Mat_Label,labels,Mat_Unlabel=loadBandData(num_unlabel_samples)Mat_Label,labels,Mat_Unlabel=loadCircleData(num_unlabel_samples)##Notice:whenuse'rbf'asourkernel,thechoiceofhyperparameter'sigma'isveryimport!Itshouldbe##choseaccordingtoyourdataset,specificthedistanceoftwodatapoints.Ithinkitshouldensurethat##eachpointhasabout10knnorw_i,jislargeenough.Italsoinfluencethespeedofconverge.So,maybe##'knn'kernelisbetter!#unlabel_data_labels=labelPropagation(Mat_Label,Mat_Unlabel,labels,kernel_type='rbf',rbf_sigma=0.2)unlabel_data_labels=labelPropagation(Mat_Label,Mat_Unlabel,labels,kernel_type='knn',knn_num_neighbors=10,max_iter=400)show(Mat_Label,labels,Mat_Unlabel,unlabel_data_labels)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101#***************************************************************************  #*  #*Description:labelpropagation  #*Author:ZouXiaoyi(zouxy09@qq.com)  #*Date:  2015-10-15  #*HomePage:http://blog.csdn.net/zouxy09  #*  #**************************************************************************    importtime  importmath  importnumpyasnp  fromlabel_propagationimportlabelPropagation    #show  defshow(Mat_Label,labels,Mat_Unlabel,unlabel_data_labels):      importmatplotlib.pyplotasplt            foriinrange(Mat_Label.shape[0]):          ifint(labels[i])==0:                plt.plot(Mat_Label[i,0],Mat_Label[i,1],'Dr')            elifint(labels[i])==1:                plt.plot(Mat_Label[i,0],Mat_Label[i,1],'Db')          else:              plt.plot(Mat_Label[i,0],Mat_Label[i,1],'Dy')            foriinrange(Mat_Unlabel.shape[0]):          ifint(unlabel_data_labels[i])==0:                plt.plot(Mat_Unlabel[i,0],Mat_Unlabel[i,1],'or')            elifint(unlabel_data_labels[i])==1:                plt.plot(Mat_Unlabel[i,0],Mat_Unlabel[i,1],'ob')          else:              plt.plot(Mat_Unlabel[i,0],Mat_Unlabel[i,1],'oy')            plt.xlabel('X1');plt.ylabel('X2')      plt.xlim(0.0,12.)      plt.ylim(0.0,12.)      plt.show()        defloadCircleData(num_data):      center=np.array([5.0,5.0])      radiu_inner=2      radiu_outer=4      num_inner=num_data/3      num_outer=num_data-num_inner            data=[]      theta=0.0      foriinrange(num_inner):          pho=(theta%360)*math.pi/180          tmp=np.zeros(2,np.float32)          tmp[0]=radiu_inner*math.cos(pho)+np.random.rand(1)+center[0]          tmp[1]=radiu_inner*math.sin(pho)+np.random.rand(1)+center[1]          data.append(tmp)          theta+=2            theta=0.0      foriinrange(num_outer):          pho=(theta%360)*math.pi/180          tmp=np.zeros(2,np.float32)          tmp[0]=radiu_outer*math.cos(pho)+np.random.rand(1)+center[0]          tmp[1]=radiu_outer*math.sin(pho)+np.random.rand(1)+center[1]          data.append(tmp)          theta+=1            Mat_Label=np.zeros((2,2),np.float32)      Mat_Label[0]=center+np.array([-radiu_inner+0.5,0])      Mat_Label[1]=center+np.array([-radiu_outer+0.5,0])      labels=[0,1]      Mat_Unlabel=np.vstack(data)      returnMat_Label,labels,Mat_Unlabel      defloadBandData(num_unlabel_samples):      #Mat_Label=np.array([[5.0,2.],[5.0,8.0]])      #labels=[0,1]      #Mat_Unlabel=np.array([[5.1,2.],[5.0,8.1]])            Mat_Label=np.array([[5.0,2.],[5.0,8.0]])      labels=[0,1]      num_dim=Mat_Label.shape[1]      Mat_Unlabel=np.zeros((num_unlabel_samples,num_dim),np.float32)      Mat_Unlabel[:num_unlabel_samples/2,:]=(np.random.rand(num_unlabel_samples/2,num_dim)-0.5)*np.array([3,1])+Mat_Label[0]      Mat_Unlabel[num_unlabel_samples/2:num_unlabel_samples,:]=(np.random.rand(num_unlabel_samples/2,num_dim)-0.5)*np.array([3,1])+Mat_Label[1]      returnMat_Label,labels,Mat_Unlabel      #mainfunction  if__name__==\"__main__\":      num_unlabel_samples=800      #Mat_Label,labels,Mat_Unlabel=loadBandData(num_unlabel_samples)      Mat_Label,labels,Mat_Unlabel=loadCircleData(num_unlabel_samples)            ##Notice:whenuse'rbf'asourkernel,thechoiceofhyperparameter'sigma'isveryimport!Itshouldbe      ##choseaccordingtoyourdataset,specificthedistanceoftwodatapoints.Ithinkitshouldensurethat      ##eachpointhasabout10knnorw_i,jislargeenough.Italsoinfluencethespeedofconverge.So,maybe      ##'knn'kernelisbetter!      #unlabel_data_labels=labelPropagation(Mat_Label,Mat_Unlabel,labels,kernel_type='rbf',rbf_sigma=0.2)      unlabel_data_labels=labelPropagation(Mat_Label,Mat_Unlabel,labels,kernel_type='knn',knn_num_neighbors=10,max_iter=400)      show(Mat_Label,labels,Mat_Unlabel,unlabel_data_labels)该注释的，代码都注释的，有看不明白的，欢迎交流。不同迭代次数时候的结果如下：是不是很漂亮的传播过程？！在数值上也是可以看到随着迭代的进行逐渐收敛的，迭代的数值变化过程如下：--->Iteration0/400,changed:1602.000000--->Iteration1/400,changed:6.300182--->Iteration2/400,changed:5.129996--->Iteration3/400,changed:4.301994--->Iteration4/400,changed:3.819295--->Iteration5/400,changed:3.501743--->Iteration6/400,changed:3.277122--->Iteration7/400,changed:3.105952--->Iteration8/400,changed:2.967030--->Iteration9/400,changed:2.848606--->Iteration10/400,changed:2.743997--->Iteration11/400,changed:2.649270--->Iteration12/400,changed:2.562057--->Iteration13/400,changed:2.480885--->Iteration14/400,changed:2.404774--->Iteration15/400,changed:2.333075--->Iteration16/400,changed:2.265301--->Iteration17/400,changed:2.201107--->Iteration18/400,changed:2.140209--->Iteration19/400,changed:2.082354--->Iteration20/400,changed:2.027376--->Iteration21/400,changed:1.975071--->Iteration22/400,changed:1.925286--->Iteration23/400,changed:1.877894--->Iteration24/400,changed:1.832743--->Iteration25/400,changed:1.789721--->Iteration26/400,changed:1.748706--->Iteration27/400,changed:1.709593--->Iteration28/400,changed:1.672284--->Iteration29/400,changed:1.636668--->Iteration30/400,changed:1.602668--->Iteration31/400,changed:1.570200--->Iteration32/400,changed:1.539179--->Iteration33/400,changed:1.509530--->Iteration34/400,changed:1.481182--->Iteration35/400,changed:1.454066--->Iteration36/400,changed:1.428120--->Iteration37/400,changed:1.403283--->Iteration38/400,changed:1.379502--->Iteration39/400,changed:1.356734--->Iteration40/400,changed:1.334906--->Iteration41/400,changed:1.313983--->Iteration42/400,changed:1.293921--->Iteration43/400,changed:1.274681--->Iteration44/400,changed:1.256214--->Iteration45/400,changed:1.238491--->Iteration46/400,changed:1.221474--->Iteration47/400,changed:1.205126--->Iteration48/400,changed:1.189417--->Iteration49/400,changed:1.174316--->Iteration50/400,changed:1.159804--->Iteration51/400,changed:1.145844--->Iteration52/400,changed:1.132414--->Iteration53/400,changed:1.119490--->Iteration54/400,changed:1.107032--->Iteration55/400,changed:1.095054--->Iteration56/400,changed:1.083513--->Iteration57/400,changed:1.072397--->Iteration58/400,changed:1.061671--->Iteration59/400,changed:1.051324--->Iteration60/400,changed:1.041363--->Iteration61/400,changed:1.031742--->Iteration62/400,changed:1.022459--->Iteration63/400,changed:1.013494--->Iteration64/400,changed:1.004836--->Iteration65/400,changed:0.996484--->Iteration66/400,changed:0.988407--->Iteration67/400,changed:0.980592--->Iteration68/400,changed:0.973045--->Iteration69/400,changed:0.965744--->Iteration70/400,changed:0.958682--->Iteration71/400,changed:0.951848--->Iteration72/400,changed:0.945227--->Iteration73/400,changed:0.938820--->Iteration74/400,changed:0.932608--->Iteration75/400,changed:0.926590--->Iteration76/400,changed:0.920765--->Iteration77/400,changed:0.915107--->Iteration78/400,changed:0.909628--->Iteration79/400,changed:0.904309--->Iteration80/400,changed:0.899143--->Iteration81/400,changed:0.894122--->Iteration82/400,changed:0.889259--->Iteration83/400,changed:0.884530--->Iteration84/400,changed:0.879933--->Iteration85/400,changed:0.875464--->Iteration86/400,changed:0.871121--->Iteration87/400,changed:0.866888--->Iteration88/400,changed:0.862773--->Iteration89/400,changed:0.858783--->Iteration90/400,changed:0.854879--->Iteration91/400,changed:0.851084--->Iteration92/400,changed:0.847382--->Iteration93/400,changed:0.843779--->Iteration94/400,changed:0.840274--->Iteration95/400,changed:0.836842--->Iteration96/400,changed:0.833501--->Iteration97/400,changed:0.830240--->Iteration98/400,changed:0.827051--->Iteration99/400,changed:0.823950--->Iteration100/400,changed:0.820906--->Iteration101/400,changed:0.817946--->Iteration102/400,changed:0.815053--->Iteration103/400,changed:0.812217--->Iteration104/400,changed:0.809437--->Iteration105/400,changed:0.806724--->Iteration106/400,changed:0.804076--->Iteration107/400,changed:0.801480--->Iteration108/400,changed:0.798937--->Iteration109/400,changed:0.796448--->Iteration110/400,changed:0.794008--->Iteration111/400,changed:0.791612--->Iteration112/400,changed:0.789282--->Iteration113/400,changed:0.786984--->Iteration114/400,changed:0.784728--->Iteration115/400,changed:0.782516--->Iteration116/400,changed:0.780355--->Iteration117/400,changed:0.778216--->Iteration118/400,changed:0.776139--->Iteration119/400,changed:0.774087--->Iteration120/400,changed:0.772072--->Iteration121/400,changed:0.770085--->Iteration122/400,changed:0.768146--->Iteration123/400,changed:0.766232--->Iteration124/400,changed:0.764356--->Iteration125/400,changed:0.762504--->Iteration126/400,changed:0.760685--->Iteration127/400,changed:0.758889--->Iteration128/400,changed:0.757135--->Iteration129/400,changed:0.755406123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130--->Iteration0/400,changed:1602.000000  --->Iteration1/400,changed:6.300182  --->Iteration2/400,changed:5.129996  --->Iteration3/400,changed:4.301994  --->Iteration4/400,changed:3.819295  --->Iteration5/400,changed:3.501743  --->Iteration6/400,changed:3.277122  --->Iteration7/400,changed:3.105952  --->Iteration8/400,changed:2.967030  --->Iteration9/400,changed:2.848606  --->Iteration10/400,changed:2.743997  --->Iteration11/400,changed:2.649270  --->Iteration12/400,changed:2.562057  --->Iteration13/400,changed:2.480885  --->Iteration14/400,changed:2.404774  --->Iteration15/400,changed:2.333075  --->Iteration16/400,changed:2.265301  --->Iteration17/400,changed:2.201107  --->Iteration18/400,changed:2.140209  --->Iteration19/400,changed:2.082354  --->Iteration20/400,changed:2.027376  --->Iteration21/400,changed:1.975071  --->Iteration22/400,changed:1.925286  --->Iteration23/400,changed:1.877894  --->Iteration24/400,changed:1.832743  --->Iteration25/400,changed:1.789721  --->Iteration26/400,changed:1.748706  --->Iteration27/400,changed:1.709593  --->Iteration28/400,changed:1.672284  --->Iteration29/400,changed:1.636668  --->Iteration30/400,changed:1.602668  --->Iteration31/400,changed:1.570200  --->Iteration32/400,changed:1.539179  --->Iteration33/400,changed:1.509530  --->Iteration34/400,changed:1.481182  --->Iteration35/400,changed:1.454066  --->Iteration36/400,changed:1.428120  --->Iteration37/400,changed:1.403283  --->Iteration38/400,changed:1.379502  --->Iteration39/400,changed:1.356734  --->Iteration40/400,changed:1.334906  --->Iteration41/400,changed:1.313983  --->Iteration42/400,changed:1.293921  --->Iteration43/400,changed:1.274681  --->Iteration44/400,changed:1.256214  --->Iteration45/400,changed:1.238491  --->Iteration46/400,changed:1.221474  --->Iteration47/400,changed:1.205126  --->Iteration48/400,changed:1.189417  --->Iteration49/400,changed:1.174316  --->Iteration50/400,changed:1.159804  --->Iteration51/400,changed:1.145844  --->Iteration52/400,changed:1.132414  --->Iteration53/400,changed:1.119490  --->Iteration54/400,changed:1.107032  --->Iteration55/400,changed:1.095054  --->Iteration56/400,changed:1.083513  --->Iteration57/400,changed:1.072397  --->Iteration58/400,changed:1.061671  --->Iteration59/400,changed:1.051324  --->Iteration60/400,changed:1.041363  --->Iteration61/400,changed:1.031742  --->Iteration62/400,changed:1.022459  --->Iteration63/400,changed:1.013494  --->Iteration64/400,changed:1.004836  --->Iteration65/400,changed:0.996484  --->Iteration66/400,changed:0.988407  --->Iteration67/400,changed:0.980592  --->Iteration68/400,changed:0.973045  --->Iteration69/400,changed:0.965744  --->Iteration70/400,changed:0.958682  --->Iteration71/400,changed:0.951848  --->Iteration72/400,changed:0.945227  --->Iteration73/400,changed:0.938820  --->Iteration74/400,changed:0.932608  --->Iteration75/400,changed:0.926590  --->Iteration76/400,changed:0.920765  --->Iteration77/400,changed:0.915107  --->Iteration78/400,changed:0.909628  --->Iteration79/400,changed:0.904309  --->Iteration80/400,changed:0.899143  --->Iteration81/400,changed:0.894122  --->Iteration82/400,changed:0.889259  --->Iteration83/400,changed:0.884530  --->Iteration84/400,changed:0.879933  --->Iteration85/400,changed:0.875464  --->Iteration86/400,changed:0.871121  --->Iteration87/400,changed:0.866888  --->Iteration88/400,changed:0.862773  --->Iteration89/400,changed:0.858783  --->Iteration90/400,changed:0.854879  --->Iteration91/400,changed:0.851084  --->Iteration92/400,changed:0.847382  --->Iteration93/400,changed:0.843779  --->Iteration94/400,changed:0.840274  --->Iteration95/400,changed:0.836842  --->Iteration96/400,changed:0.833501  --->Iteration97/400,changed:0.830240  --->Iteration98/400,changed:0.827051  --->Iteration99/400,changed:0.823950  --->Iteration100/400,changed:0.820906  --->Iteration101/400,changed:0.817946  --->Iteration102/400,changed:0.815053  --->Iteration103/400,changed:0.812217  --->Iteration104/400,changed:0.809437  --->Iteration105/400,changed:0.806724  --->Iteration106/400,changed:0.804076  --->Iteration107/400,changed:0.801480  --->Iteration108/400,changed:0.798937  --->Iteration109/400,changed:0.796448  --->Iteration110/400,changed:0.794008  --->Iteration111/400,changed:0.791612  --->Iteration112/400,changed:0.789282  --->Iteration113/400,changed:0.786984  --->Iteration114/400,changed:0.784728  --->Iteration115/400,changed:0.782516  --->Iteration116/400,changed:0.780355  --->Iteration117/400,changed:0.778216  --->Iteration118/400,changed:0.776139  --->Iteration119/400,changed:0.774087  --->Iteration120/400,changed:0.772072  --->Iteration121/400,changed:0.770085  --->Iteration122/400,changed:0.768146  --->Iteration123/400,changed:0.766232  --->Iteration124/400,changed:0.764356  --->Iteration125/400,changed:0.762504  --->Iteration126/400,changed:0.760685  --->Iteration127/400,changed:0.758889  --->Iteration128/400,changed:0.757135  --->Iteration129/400,changed:0.755406四、LP算法MPI并行实现这里，我们测试的是LP的变身版本。从公式，我们可以看到，第二项PULYL迭代过程并没有发生变化，所以这部分实际上从迭代开始就可以计算好，从而避免重复计算。不过，不管怎样，LP算法都要计算一个UxU的矩阵PUU和一个UxC矩阵FU的乘积。当我们的unlabeled数据非常多，而且类别也很多的时候，计算是很慢的，同时占用的内存量也非常大。另外，构造Graph需要计算两两的相似度，也是O(n2)的复杂度，当我们数据的特征维度很大的时候，这个计算量也是非常客观的。所以我们就得考虑并行处理了。而且最好是能放到集群上并行。那如何并行呢？对算法的并行化，一般分为两种：数据并行和模型并行。数据并行很好理解，就是将数据划分，每个节点只处理一部分数据，例如我们构造图的时候，计算每个数据的k近邻。例如我们有1000个样本和20个CPU节点，那么就平均分发，让每个CPU节点计算50个样本的k近邻，然后最后再合并大家的结果。可见这个加速比也是非常可观的。模型并行一般发生在模型很大，无法放到单机的内存里面的时候。例如庞大的深度神经网络训练的时候，就需要把这个网络切开，然后分别求解梯度，最后有个leader的节点来收集大家的梯度，再反馈给大家去更新。当然了，其中存在更细致和高效的工程处理方法。在我们的LP算法中，也是可以做模型并行的。假如我们的类别数C很大，把类别数切开，让不同的CPU节点处理，实际上就相当于模型并行了。那为啥不切大矩阵PUU，而是切小点的矩阵FU，因为大矩阵PUU没法独立分块，并行的一个原则是处理必须是独立的。矩阵FU依赖的是所有的U，而把PUU切开分发到其他节点的时候，每次FU的更新都需要和其他的节点通信，这个通信的代价是很大的（实际上，很多并行系统没法达到线性的加速度的瓶颈是通信！线性加速比是，我增加了n台机器，速度就提升了n倍）。但是对类别C也就是矩阵FU切分，就不会有这个问题，因为他们的计算是独立的。只是决定样本的最终类别的时候，将所有的FU收集回来求max就可以了。所以，在下面的代码中，是同时包含了数据并行和模型并行的雏形的。另外，还值得一提的是，我们是迭代算法，那决定什么时候迭代算法停止？除了判断收敛外，我们还可以让每迭代几步，就用测试label测试一次结果，看模型的整体训练性能如何。特别是判断训练是否过拟合的时候非常有效。因此，代码中包含了这部分内容。好了，代码终于来了。大家可以搞点大数据库来测试，如果有MPI集群条件的话就更好了。下面的代码依赖numpy、scipy（用其稀疏矩阵加速计算）和mpi4py。其中mpi4py需要依赖openmpi和Cpython，可以参考我之前的博客进行安装。#***************************************************************************#*#*Description:labelpropagation#*Author:ZouXiaoyi(zouxy09@qq.com)#*Date:2015-10-15#*HomePage:http://blog.csdn.net/zouxy09#*#**************************************************************************importos,sys,timeimportnumpyasnpfromscipy.sparseimportcsr_matrix,lil_matrix,eyeimportoperatorimportcPickleaspickleimportmpi4py.MPIasMPI##GlobalvariablesforMPI##instanceforinvokingMPIrelatedfunctionscomm=MPI.COMM_WORLD#thenoderankinthewholecommunitycomm_rank=comm.Get_rank()#thesizeofthewholecommunity,i.e.,thetotalnumberofworkingnodesintheMPIclustercomm_size=comm.Get_size()#loadmnistdatasetdefload_MNIST():importgzipf=gzip.open(\"mnist.pkl.gz\",\"rb\")train,val,test=pickle.load(f)f.close()Mat_Label=train[0]labels=train[1]Mat_Unlabel=test[0]groundtruth=test[1]labels_id=[0,1,2,3,4,5,6,7,8,9]returnMat_Label,labels,labels_id,Mat_Unlabel,groundtruth#returnkneighborsindexdefnavie_knn(dataSet,query,k):numSamples=dataSet.shape[0]##step1:calculateEuclideandistancediff=np.tile(query,(numSamples,1))-dataSetsquaredDiff=diff**2squaredDist=np.sum(squaredDiff,axis=1)#sumisperformedbyrow##step2:sortthedistancesortedDistIndices=np.argsort(squaredDist)ifk>len(sortedDistIndices):k=len(sortedDistIndices)returnsortedDistIndices[0:k]#buildabiggraph(normalizedweightmatrix)#sparseUx(U+L)matrixdefbuildSubGraph(Mat_Label,Mat_Unlabel,knn_num_neighbors):num_unlabel_samples=Mat_Unlabel.shape[0]data=[];indices=[];indptr=[0]Mat_all=np.vstack((Mat_Label,Mat_Unlabel))values=np.ones(knn_num_neighbors,np.float32)/knn_num_neighborsforiinxrange(num_unlabel_samples):k_neighbors=navie_knn(Mat_all,Mat_Unlabel[i,:],knn_num_neighbors)indptr.append(np.int32(indptr[-1])+knn_num_neighbors)indices.extend(k_neighbors)data.append(values)returncsr_matrix((np.hstack(data),indices,indptr))#buildabiggraph(normalizedweightmatrix)#sparseUx(U+L)matrixdefbuildSubGraph_MPI(Mat_Label,Mat_Unlabel,knn_num_neighbors):num_unlabel_samples=Mat_Unlabel.shape[0]local_data=[];local_indices=[];local_indptr=[0]Mat_all=np.vstack((Mat_Label,Mat_Unlabel))values=np.ones(knn_num_neighbors,np.float32)/knn_num_neighborssample_offset=np.linspace(0,num_unlabel_samples,comm_size+1).astype('int')foriinrange(sample_offset[comm_rank],sample_offset[comm_rank+1]):k_neighbors=navie_knn(Mat_all,Mat_Unlabel[i,:],knn_num_neighbors)local_indptr.append(np.int32(local_indptr[-1])+knn_num_neighbors)local_indices.extend(k_neighbors)local_data.append(values)data=np.hstack(comm.allgather(local_data))indices=np.hstack(comm.allgather(local_indices))indptr_tmp=comm.allgather(local_indptr)indptr=[]foriinrange(len(indptr_tmp)):ifi==0:indptr.extend(indptr_tmp[i])else:last_indptr=indptr[-1]del(indptr[-1])indptr.extend(indptr_tmp[i]+last_indptr)returncsr_matrix((np.hstack(data),indices,indptr),dtype=np.float32)#labelpropagationdefrun_label_propagation_sparse(knn_num_neighbors=20,max_iter=100,tol=1e-4,test_per_iter=1):#loaddataandgraphprint\"Processor%d/%dloadinggraphfile...\"%(comm_rank,comm_size)#Mat_Label,labels,Mat_Unlabel,groundtruth=loadFourBandData()Mat_Label,labels,labels_id,Mat_Unlabel,unlabel_data_id=load_MNIST()ifcomm_size>len(labels_id):raiseValueError(\"Sorry,theprocessorsmustbelessthanthenumberofclasses\")#affinity_matrix=buildSubGraph(Mat_Label,Mat_Unlabel,knn_num_neighbors)affinity_matrix=buildSubGraph_MPI(Mat_Label,Mat_Unlabel,knn_num_neighbors)#getsomeparametersnum_classes=len(labels_id)num_label_samples=len(labels)num_unlabel_samples=Mat_Unlabel.shape[0]affinity_matrix_UL=affinity_matrix[:,0:num_label_samples]affinity_matrix_UU=affinity_matrix[:,num_label_samples:num_label_samples+num_unlabel_samples]ifcomm_rank==0:print\"Have%dlabeledimages,%dunlabeledimagesand%dclasses\"%(num_label_samples,num_unlabel_samples,num_classes)#dividelabel_function_Uandlabel_function_Ltoallprocessorsclass_offset=np.linspace(0,num_classes,comm_size+1).astype('int')#initializelocallabel_function_Ulocal_start_class=class_offset[comm_rank]local_num_classes=class_offset[comm_rank+1]-local_start_classlocal_label_function_U=eye(num_unlabel_samples,local_num_classes,0,np.float32,format='csr')#initializelocallabel_function_Llocal_label_function_L=lil_matrix((num_label_samples,local_num_classes),dtype=np.float32)foriinxrange(num_label_samples):class_off=int(labels[i])-local_start_classifclass_off>=0andclass_off<local_num_classes:local_label_function_L[i,class_off]=1.0local_label_function_L=local_label_function_L.tocsr()local_label_info=affinity_matrix_UL.dot(local_label_function_L)print\"Processor%d/%dhastoprocess%dclasses...\"%(comm_rank,comm_size,local_label_function_L.shape[1])#starttopropagationiter=1;changed=100.0;evaluation(num_unlabel_samples,local_start_class,local_label_function_U,unlabel_data_id,labels_id)whileTrue:pre_label_function=local_label_function_U.copy()#propagationlocal_label_function_U=affinity_matrix_UU.dot(local_label_function_U)+local_label_info#checkconvergelocal_changed=abs(pre_label_function-local_label_function_U).sum()changed=comm.reduce(local_changed,root=0,op=MPI.SUM)status='RUN'test=Falseifcomm_rank==0:ifiter%1==0:norm_changed=changed/(num_unlabel_samples*num_classes)print\"--->Iteration%d/%d,changed:%f\"%(iter,max_iter,norm_changed)ifiter>=max_iterorchanged<tol:status='STOP'print\"**************Iterationover!****************\"ifiter%test_per_iter==0:test=Trueiter+=1test=comm.bcast(testifcomm_rank==0elseNone,root=0)status=comm.bcast(statusifcomm_rank==0elseNone,root=0)ifstatus=='STOP':breakiftest==True:evaluation(num_unlabel_samples,local_start_class,local_label_function_U,unlabel_data_id,labels_id)evaluation(num_unlabel_samples,local_start_class,local_label_function_U,unlabel_data_id,labels_id)defevaluation(num_unlabel_samples,local_start_class,local_label_function_U,unlabel_data_id,labels_id):#getlocallabelwithmaxscoreifcomm_rank==0:print\"Starttocombinelocalresult...\"local_max_score=np.zeros((num_unlabel_samples,1),np.float32)local_max_label=np.zeros((num_unlabel_samples,1),np.int32)foriinxrange(num_unlabel_samples):local_max_label[i,0]=np.argmax(local_label_function_U.getrow(i).todense())local_max_score[i,0]=local_label_function_U[i,local_max_label[i,0]]local_max_label[i,0]+=local_start_class#gathertheresultsfromalltheprocessorsifcomm_rank==0:print\"Starttogatherresultsfromallprocessors\"all_max_label=np.hstack(comm.allgather(local_max_label))all_max_score=np.hstack(comm.allgather(local_max_score))#getterminatelabelofunlabeleddataifcomm_rank==0:print\"Starttoanalysistheresults...\"right_predict_count=0foriinxrange(num_unlabel_samples):ifi%1000==0:print\"***\",all_max_score[i]max_idx=np.argmax(all_max_score[i])max_label=all_max_label[i,max_idx]ifint(unlabel_data_id[i])==int(labels_id[max_label]):right_predict_count+=1accuracy=float(right_predict_count)*100.0/num_unlabel_samplesprint\"Have%dsamples,accuracy:%.3f%%!\"%(num_unlabel_samples,accuracy)if__name__=='__main__':run_label_propagation_sparse(knn_num_neighbors=20,max_iter=30)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207#***************************************************************************  #*  #*Description:labelpropagation  #*Author:ZouXiaoyi(zouxy09@qq.com)  #*Date:  2015-10-15  #*HomePage:http://blog.csdn.net/zouxy09  #*  #**************************************************************************    importos,sys,time  importnumpyasnp  fromscipy.sparseimportcsr_matrix,lil_matrix,eye  importoperator  importcPickleaspickle  importmpi4py.MPIasMPI    #  #  GlobalvariablesforMPI  #    #instanceforinvokingMPIrelatedfunctions  comm=MPI.COMM_WORLD  #thenoderankinthewholecommunity  comm_rank=comm.Get_rank()  #thesizeofthewholecommunity,i.e.,thetotalnumberofworkingnodesintheMPIcluster  comm_size=comm.Get_size()    #loadmnistdataset  defload_MNIST():      importgzip      f=gzip.open(\"mnist.pkl.gz\",\"rb\")      train,val,test=pickle.load(f)      f.close()            Mat_Label=train[0]      labels=train[1]      Mat_Unlabel=test[0]      groundtruth=test[1]      labels_id=[0,1,2,3,4,5,6,7,8,9]        returnMat_Label,labels,labels_id,Mat_Unlabel,groundtruth    #returnkneighborsindex  defnavie_knn(dataSet,query,k):      numSamples=dataSet.shape[0]        ##step1:calculateEuclideandistance      diff=np.tile(query,(numSamples,1))-dataSet      squaredDiff=diff**2      squaredDist=np.sum(squaredDiff,axis=1)#sumisperformedbyrow        ##step2:sortthedistance      sortedDistIndices=np.argsort(squaredDist)      ifk>len(sortedDistIndices):          k=len(sortedDistIndices)      returnsortedDistIndices[0:k]      #buildabiggraph(normalizedweightmatrix)  #sparseUx(U+L)matrix  defbuildSubGraph(Mat_Label,Mat_Unlabel,knn_num_neighbors):      num_unlabel_samples=Mat_Unlabel.shape[0]      data=[];indices=[];indptr=[0]      Mat_all=np.vstack((Mat_Label,Mat_Unlabel))      values=np.ones(knn_num_neighbors,np.float32)/knn_num_neighbors      foriinxrange(num_unlabel_samples):          k_neighbors=navie_knn(Mat_all,Mat_Unlabel[i,:],knn_num_neighbors)          indptr.append(np.int32(indptr[-1])+knn_num_neighbors)          indices.extend(k_neighbors)          data.append(values)      returncsr_matrix((np.hstack(data),indices,indptr))      #buildabiggraph(normalizedweightmatrix)  #sparseUx(U+L)matrix  defbuildSubGraph_MPI(Mat_Label,Mat_Unlabel,knn_num_neighbors):      num_unlabel_samples=Mat_Unlabel.shape[0]      local_data=[];local_indices=[];local_indptr=[0]      Mat_all=np.vstack((Mat_Label,Mat_Unlabel))      values=np.ones(knn_num_neighbors,np.float32)/knn_num_neighbors      sample_offset=np.linspace(0,num_unlabel_samples,comm_size+1).astype('int')      foriinrange(sample_offset[comm_rank],sample_offset[comm_rank+1]):          k_neighbors=navie_knn(Mat_all,Mat_Unlabel[i,:],knn_num_neighbors)          local_indptr.append(np.int32(local_indptr[-1])+knn_num_neighbors)          local_indices.extend(k_neighbors)          local_data.append(values)      data=np.hstack(comm.allgather(local_data))      indices=np.hstack(comm.allgather(local_indices))      indptr_tmp=comm.allgather(local_indptr)      indptr=[]      foriinrange(len(indptr_tmp)):          ifi==0:              indptr.extend(indptr_tmp[i])          else:              last_indptr=indptr[-1]              del(indptr[-1])              indptr.extend(indptr_tmp[i]+last_indptr)      returncsr_matrix((np.hstack(data),indices,indptr),dtype=np.float32)      #labelpropagation  defrun_label_propagation_sparse(knn_num_neighbors=20,max_iter=100,tol=1e-4,test_per_iter=1):      #loaddataandgraph      print\"Processor%d/%dloadinggraphfile...\"%(comm_rank,comm_size)      #Mat_Label,labels,Mat_Unlabel,groundtruth=loadFourBandData()      Mat_Label,labels,labels_id,Mat_Unlabel,unlabel_data_id=load_MNIST()      ifcomm_size>len(labels_id):          raiseValueError(\"Sorry,theprocessorsmustbelessthanthenumberofclasses\")      #affinity_matrix=buildSubGraph(Mat_Label,Mat_Unlabel,knn_num_neighbors)      affinity_matrix=buildSubGraph_MPI(Mat_Label,Mat_Unlabel,knn_num_neighbors)            #getsomeparameters      num_classes=len(labels_id)      num_label_samples=len(labels)      num_unlabel_samples=Mat_Unlabel.shape[0]        affinity_matrix_UL=affinity_matrix[:,0:num_label_samples]      affinity_matrix_UU=affinity_matrix[:,num_label_samples:num_label_samples+num_unlabel_samples]        ifcomm_rank==0:          print\"Have%dlabeledimages,%dunlabeledimagesand%dclasses\"%(num_label_samples,num_unlabel_samples,num_classes)            #dividelabel_function_Uandlabel_function_Ltoallprocessors      class_offset=np.linspace(0,num_classes,comm_size+1).astype('int')            #initializelocallabel_function_U      local_start_class=class_offset[comm_rank]      local_num_classes=class_offset[comm_rank+1]-local_start_class      local_label_function_U=eye(num_unlabel_samples,local_num_classes,0,np.float32,format='csr')            #initializelocallabel_function_L      local_label_function_L=lil_matrix((num_label_samples,local_num_classes),dtype=np.float32)      foriinxrange(num_label_samples):          class_off=int(labels[i])-local_start_class          ifclass_off>=0andclass_off<local_num_classes:              local_label_function_L[i,class_off]=1.0      local_label_function_L=local_label_function_L.tocsr()      local_label_info=affinity_matrix_UL.dot(local_label_function_L)      print\"Processor%d/%dhastoprocess%dclasses...\"%(comm_rank,comm_size,local_label_function_L.shape[1])            #starttopropagation      iter=1;changed=100.0;      evaluation(num_unlabel_samples,local_start_class,local_label_function_U,unlabel_data_id,labels_id)      whileTrue:          pre_label_function=local_label_function_U.copy()                    #propagation          local_label_function_U=affinity_matrix_UU.dot(local_label_function_U)+local_label_info                    #checkconverge          local_changed=abs(pre_label_function-local_label_function_U).sum()          changed=comm.reduce(local_changed,root=0,op=MPI.SUM)          status='RUN'          test=False          ifcomm_rank==0:              ifiter%1==0:                  norm_changed=changed/(num_unlabel_samples*num_classes)                  print\"--->Iteration%d/%d,changed:%f\"%(iter,max_iter,norm_changed)              ifiter>=max_iterorchanged<tol:                  status='STOP'                  print\"**************Iterationover!****************\"              ifiter%test_per_iter==0:                  test=True              iter+=1          test=comm.bcast(testifcomm_rank==0elseNone,root=0)          status=comm.bcast(statusifcomm_rank==0elseNone,root=0)          ifstatus=='STOP':              break          iftest==True:              evaluation(num_unlabel_samples,local_start_class,local_label_function_U,unlabel_data_id,labels_id)      evaluation(num_unlabel_samples,local_start_class,local_label_function_U,unlabel_data_id,labels_id)      defevaluation(num_unlabel_samples,local_start_class,local_label_function_U,unlabel_data_id,labels_id):      #getlocallabelwithmaxscore      ifcomm_rank==0:          print\"Starttocombinelocalresult...\"      local_max_score=np.zeros((num_unlabel_samples,1),np.float32)      local_max_label=np.zeros((num_unlabel_samples,1),np.int32)      foriinxrange(num_unlabel_samples):          local_max_label[i,0]=np.argmax(local_label_function_U.getrow(i).todense())          local_max_score[i,0]=local_label_function_U[i,local_max_label[i,0]]          local_max_label[i,0]+=local_start_class                #gathertheresultsfromalltheprocessors      ifcomm_rank==0:          print\"Starttogatherresultsfromallprocessors\"      all_max_label=np.hstack(comm.allgather(local_max_label))      all_max_score=np.hstack(comm.allgather(local_max_score))            #getterminatelabelofunlabeleddata      ifcomm_rank==0:          print\"Starttoanalysistheresults...\"          right_predict_count=0          foriinxrange(num_unlabel_samples):              ifi%1000==0:                  print\"***\",all_max_score[i]              max_idx=np.argmax(all_max_score[i])              max_label=all_max_label[i,max_idx]              ifint(unlabel_data_id[i])==int(labels_id[max_label]):                  right_predict_count+=1          accuracy=float(right_predict_count)*100.0/num_unlabel_samples          print\"Have%dsamples,accuracy:%.3f%%!\"%(num_unlabel_samples,accuracy)      if__name__=='__main__':      run_label_propagation_sparse(knn_num_neighbors=20,max_iter=30)五、参考资料[1]Semi-SupervisedLearningwithGraphs.pdf1赞3收藏评论"], "art_url": ["http://python.jobbole.com/88467/"]}
{"art_img": ["http://jbcdn2.b0.upaiyun.com/2017/08/c7a2b2dc9d96e05848eee5262c928c4a.png"], "art_title": ["Python 爬虫(七)-- Scrapy 模拟登录"], "art_create_time": ["2017/08/31"], "art_content": ["原文出处：Andrew_liu   1.Cookie原理HTTP是无状态的面向连接的协议,为了保持连接状态,引入了Cookie机制Cookie是http消息头中的一种属性，包括：Cookie名字（Name）Cookie的值（Value）Cookie的过期时间（Expires/Max-Age）Cookie作用路径（Path）Cookie所在域名（Domain），使用Cookie进行安全连接（Secure）。前两个参数是Cookie应用的必要条件，另外，还包括Cookie大小（Size，不同浏览器对Cookie个数及大小限制是有差异的）。更详细的cookie2.模拟登陆这次主要爬取的网站是知乎爬取知乎就需要登陆的,通过之前的python内建库,可以很容易的实现表单提交现在就来看看如何通过Scrapy实现表单提交首先查看登陆时的表单结果,依然像前面使用的技巧一样,故意输错密码,方面抓到登陆的网页头部和表单(我使用的Chrome自带的开发者工具中的Network功能)表单截图查看抓取到的表单可以发现有四个部分:邮箱和密码就是个人登陆的邮箱和密码rememberme字段表示是否记住账号第一个字段是_xsrf,猜测是一种验证机制现在只有_xsrf不知道,猜想这个验证字段肯定会实现在请求网页的时候发送过来,那么我们查看当前网页的源码(鼠标右键然后查看网页源代码,或者直接用快捷键)查询网页源码发现我们的猜测是正确的那么现在就可以来写表单登陆功能了Pythondefstart_requests(self):return[Request(\"https://www.zhihu.com/login\",callback=self.post_login)]#重写了爬虫类的方法,实现了自定义请求,运行成功后会调用callback回调函数#FormRequesetdefpost_login(self,response):print'Preparinglogin'#下面这句话用于抓取请求网页后返回网页中的_xsrf字段的文字,用于成功提交表单xsrf=Selector(response).xpath('//input[@name=\"_xsrf\"]/@value').extract()[0]printxsrf#FormRequeset.from_response是Scrapy提供的一个函数,用于post表单#登陆成功后,会调用after_login回调函数return[FormRequest.from_response(response,formdata={'_xsrf':xsrf,'email':'123456','password':'123456'},callback=self.after_login)]12345678910111213141516171819defstart_requests(self):        return[Request(\"https://www.zhihu.com/login\",callback=self.post_login)]  #重写了爬虫类的方法,实现了自定义请求,运行成功后会调用callback回调函数     #FormRequeset    defpost_login(self,response):        print'Preparinglogin'        #下面这句话用于抓取请求网页后返回网页中的_xsrf字段的文字,用于成功提交表单        xsrf=Selector(response).xpath('//input[@name=\"_xsrf\"]/@value').extract()[0]        printxsrf        #FormRequeset.from_response是Scrapy提供的一个函数,用于post表单        #登陆成功后,会调用after_login回调函数        return[FormRequest.from_response(response,                              formdata={                            '_xsrf':xsrf,                            'email':'123456',                            'password':'123456'                            },                            callback=self.after_login                            )]其中主要的功能都在函数的注释中说明3.Cookie的保存为了能使用同一个状态持续的爬取网站,就需要保存cookie,使用cookie保存状态,Scrapy提供了cookie处理的中间件,可以直接拿来使用CookiesMiddleware这个cookie中间件保存追踪web服务器发出的cookie,并将这个cookie在接来下的请求的时候进行发送Scrapy官方的文档中给出了下面的代码范例:Pythonfori,urlinenumerate(urls):yieldscrapy.Request(\"http://www.example.com\",meta={'cookiejar':i},callback=self.parse_page)defparse_page(self,response):#dosomeprocessingreturnscrapy.Request(\"http://www.example.com/otherpage\",meta={'cookiejar':response.meta['cookiejar']},callback=self.parse_other_page)123456789fori,urlinenumerate(urls):    yieldscrapy.Request(\"http://www.example.com\",meta={'cookiejar':i},        callback=self.parse_page) defparse_page(self,response):    #dosomeprocessing    returnscrapy.Request(\"http://www.example.com/otherpage\",        meta={'cookiejar':response.meta['cookiejar']},        callback=self.parse_other_page)那么可以对我们的爬虫类中方法进行修改,使其追踪cookiePython#重写了爬虫类的方法,实现了自定义请求,运行成功后会调用callback回调函数defstart_requests(self):return[Request(\"https://www.zhihu.com/login\",meta={'cookiejar':1},callback=self.post_login)]#添加了meta#FormRequeset出问题了defpost_login(self,response):print'Preparinglogin'#下面这句话用于抓取请求网页后返回网页中的_xsrf字段的文字,用于成功提交表单xsrf=Selector(response).xpath('//input[@name=\"_xsrf\"]/@value').extract()[0]printxsrf#FormRequeset.from_response是Scrapy提供的一个函数,用于post表单#登陆成功后,会调用after_login回调函数return[FormRequest.from_response(response,#\"http://www.zhihu.com/login\",meta={'cookiejar':response.meta['cookiejar']},#注意这里cookie的获取headers=self.headers,formdata={'_xsrf':xsrf,'email':'123456','password':'123456'},callback=self.after_login,dont_filter=True)]1234567891011121314151617181920212223#重写了爬虫类的方法,实现了自定义请求,运行成功后会调用callback回调函数    defstart_requests(self):        return[Request(\"https://www.zhihu.com/login\",meta={'cookiejar':1},callback=self.post_login)]  #添加了meta     #FormRequeset出问题了    defpost_login(self,response):        print'Preparinglogin'        #下面这句话用于抓取请求网页后返回网页中的_xsrf字段的文字,用于成功提交表单        xsrf=Selector(response).xpath('//input[@name=\"_xsrf\"]/@value').extract()[0]        printxsrf        #FormRequeset.from_response是Scrapy提供的一个函数,用于post表单        #登陆成功后,会调用after_login回调函数        return[FormRequest.from_response(response,  #\"http://www.zhihu.com/login\",                            meta={'cookiejar':response.meta['cookiejar']},#注意这里cookie的获取                            headers=self.headers,                            formdata={                            '_xsrf':xsrf,                            'email':'123456',                            'password':'123456'                            },                            callback=self.after_login,                            dont_filter=True                            )]4.伪装头部有时候登陆网站需要进行头部伪装,比如增加防盗链的头部,还有模拟服务器登陆,这些都在前面的爬虫知识中提到过Headers为了保险,我们可以在头部中填充更多的字段,如下Pythonheaders={\"Accept\":\"*/*\",\"Accept-Encoding\":\"gzip,deflate\",\"Accept-Language\":\"en-US,en;q=0.8,zh-TW;q=0.6,zh;q=0.4\",\"Connection\":\"keep-alive\",\"Content-Type\":\"application/x-www-form-urlencoded;charset=UTF-8\",\"User-Agent\":\"Mozilla/5.0(Macintosh;IntelMacOSX10_10_1)AppleWebKit/537.36(KHTML,likeGecko)Chrome/38.0.2125.111Safari/537.36\",\"Referer\":\"http://www.zhihu.com/\"}123456789headers={    \"Accept\":\"*/*\",    \"Accept-Encoding\":\"gzip,deflate\",    \"Accept-Language\":\"en-US,en;q=0.8,zh-TW;q=0.6,zh;q=0.4\",    \"Connection\":\"keep-alive\",    \"Content-Type\":\"application/x-www-form-urlencoded;charset=UTF-8\",    \"User-Agent\":\"Mozilla/5.0(Macintosh;IntelMacOSX10_10_1)AppleWebKit/537.36(KHTML,likeGecko)Chrome/38.0.2125.111Safari/537.36\",    \"Referer\":\"http://www.zhihu.com/\"    }在scrapy中Request和FormRequest初始化的时候都有一个headers字段,可以自定义头部,这样我们可以添加headers字段形成最终版的登陆函数Python#!/usr/bin/envpython#-*-coding:utf-8-*-fromscrapy.contrib.spidersimportCrawlSpider,Rulefromscrapy.selectorimportSelectorfromscrapy.contrib.linkextractors.sgmlimportSgmlLinkExtractorfromscrapy.httpimportRequest,FormRequestfromzhihu.itemsimportZhihuItemclassZhihuSipder(CrawlSpider):name=\"zhihu\"allowed_domains=[\"www.zhihu.com\"]start_urls=[\"http://www.zhihu.com\"]rules=(Rule(SgmlLinkExtractor(allow=('/question/\\d+#.*?',)),callback='parse_page',follow=True),Rule(SgmlLinkExtractor(allow=('/question/\\d+',)),callback='parse_page',follow=True),)headers={\"Accept\":\"*/*\",\"Accept-Encoding\":\"gzip,deflate\",\"Accept-Language\":\"en-US,en;q=0.8,zh-TW;q=0.6,zh;q=0.4\",\"Connection\":\"keep-alive\",\"Content-Type\":\"application/x-www-form-urlencoded;charset=UTF-8\",\"User-Agent\":\"Mozilla/5.0(Macintosh;IntelMacOSX10_10_1)AppleWebKit/537.36(KHTML,likeGecko)Chrome/38.0.2125.111Safari/537.36\",\"Referer\":\"http://www.zhihu.com/\"}#重写了爬虫类的方法,实现了自定义请求,运行成功后会调用callback回调函数defstart_requests(self):return[Request(\"https://www.zhihu.com/login\",meta={'cookiejar':1},callback=self.post_login)]#FormRequeset出问题了defpost_login(self,response):print'Preparinglogin'#下面这句话用于抓取请求网页后返回网页中的_xsrf字段的文字,用于成功提交表单xsrf=Selector(response).xpath('//input[@name=\"_xsrf\"]/@value').extract()[0]printxsrf#FormRequeset.from_response是Scrapy提供的一个函数,用于post表单#登陆成功后,会调用after_login回调函数return[FormRequest.from_response(response,#\"http://www.zhihu.com/login\",meta={'cookiejar':response.meta['cookiejar']},headers=self.headers,#注意此处的headersformdata={'_xsrf':xsrf,'email':'1095511864@qq.com','password':'123456'},callback=self.after_login,dont_filter=True)]defafter_login(self,response):forurlinself.start_urls:yieldself.make_requests_from_url(url)defparse_page(self,response):problem=Selector(response)item=ZhihuItem()item['url']=response.urlitem['name']=problem.xpath('//span[@class=\"name\"]/text()').extract()printitem['name']item['title']=problem.xpath('//h2[@class=\"zm-item-titlezm-editable-content\"]/text()').extract()item['description']=problem.xpath('//div[@class=\"zm-editable-content\"]/text()').extract()item['answer']=problem.xpath('//div[@class=\"zm-editable-contentclearfix\"]/text()').extract()returnitem1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768#!/usr/bin/envpython#-*-coding:utf-8-*-fromscrapy.contrib.spidersimportCrawlSpider,Rulefromscrapy.selectorimportSelectorfromscrapy.contrib.linkextractors.sgmlimportSgmlLinkExtractorfromscrapy.httpimportRequest,FormRequestfromzhihu.itemsimportZhihuItem   classZhihuSipder(CrawlSpider):    name=\"zhihu\"    allowed_domains=[\"www.zhihu.com\"]    start_urls=[        \"http://www.zhihu.com\"    ]    rules=(        Rule(SgmlLinkExtractor(allow=('/question/\\d+#.*?',)),callback='parse_page',follow=True),        Rule(SgmlLinkExtractor(allow=('/question/\\d+',)),callback='parse_page',follow=True),    )    headers={    \"Accept\":\"*/*\",    \"Accept-Encoding\":\"gzip,deflate\",    \"Accept-Language\":\"en-US,en;q=0.8,zh-TW;q=0.6,zh;q=0.4\",    \"Connection\":\"keep-alive\",    \"Content-Type\":\"application/x-www-form-urlencoded;charset=UTF-8\",    \"User-Agent\":\"Mozilla/5.0(Macintosh;IntelMacOSX10_10_1)AppleWebKit/537.36(KHTML,likeGecko)Chrome/38.0.2125.111Safari/537.36\",    \"Referer\":\"http://www.zhihu.com/\"    }     #重写了爬虫类的方法,实现了自定义请求,运行成功后会调用callback回调函数    defstart_requests(self):        return[Request(\"https://www.zhihu.com/login\",meta={'cookiejar':1},callback=self.post_login)]     #FormRequeset出问题了    defpost_login(self,response):        print'Preparinglogin'        #下面这句话用于抓取请求网页后返回网页中的_xsrf字段的文字,用于成功提交表单        xsrf=Selector(response).xpath('//input[@name=\"_xsrf\"]/@value').extract()[0]        printxsrf        #FormRequeset.from_response是Scrapy提供的一个函数,用于post表单        #登陆成功后,会调用after_login回调函数        return[FormRequest.from_response(response,  #\"http://www.zhihu.com/login\",                            meta={'cookiejar':response.meta['cookiejar']},                            headers=self.headers,  #注意此处的headers                            formdata={                            '_xsrf':xsrf,                            'email':'1095511864@qq.com',                            'password':'123456'                            },                            callback=self.after_login,                            dont_filter=True                            )]     defafter_login(self,response):        forurlinself.start_urls:            yieldself.make_requests_from_url(url)     defparse_page(self,response):        problem=Selector(response)        item=ZhihuItem()        item['url']=response.url        item['name']=problem.xpath('//span[@class=\"name\"]/text()').extract()        printitem['name']        item['title']=problem.xpath('//h2[@class=\"zm-item-titlezm-editable-content\"]/text()').extract()        item['description']=problem.xpath('//div[@class=\"zm-editable-content\"]/text()').extract()        item['answer']=problem.xpath('//div[@class=\"zm-editable-contentclearfix\"]/text()').extract()        returnitem5.Item类和抓取间隔完整的知乎爬虫代码链接Pythonfromscrapy.itemimportItem,FieldclassZhihuItem(Item):#definethefieldsforyouritemherelike:#name=scrapy.Field()url=Field()#保存抓取问题的urltitle=Field()#抓取问题的标题description=Field()#抓取问题的描述answer=Field()#抓取问题的答案name=Field()#个人用户的名称1234567891011fromscrapy.itemimportItem,Field  classZhihuItem(Item):    #definethefieldsforyouritemherelike:    #name=scrapy.Field()    url=Field()  #保存抓取问题的url    title=Field()  #抓取问题的标题    description=Field()  #抓取问题的描述    answer=Field()  #抓取问题的答案    name=Field()  #个人用户的名称设置抓取间隔,访问由于爬虫的过快抓取,引发网站的发爬虫机制,在setting.py中设置PythonBOT_NAME='zhihu'SPIDER_MODULES=['zhihu.spiders']NEWSPIDER_MODULE='zhihu.spiders'DOWNLOAD_DELAY=0.25#设置下载间隔为250ms12345BOT_NAME='zhihu' SPIDER_MODULES=['zhihu.spiders']NEWSPIDER_MODULE='zhihu.spiders'DOWNLOAD_DELAY=0.25  #设置下载间隔为250ms更多设置可以查看官方文档抓取结果(只是截取了其中很少一部分)Python...'url':'http://www.zhihu.com/question/20688855/answer/16577390'}2014-12-1923:24:15+0800[zhihu]DEBUG:Crawled(200)<GEThttp://www.zhihu.com/question/20688855/answer/15861368>(referer:http://www.zhihu.com/question/20688855/answer/19231794)[]2014-12-1923:24:15+0800[zhihu]DEBUG:Scrapedfrom<200http://www.zhihu.com/question/20688855/answer/15861368>{'answer':[u'\\u9009\\u4f1a\\u8ba1\\u8fd9\\u4e2a\\u4e13\\u4e1a\\uff0c\\u8003CPA\\uff0c\\u5165\\u8d22\\u52a1\\u8fd9\\u4e2a\\u884c\\u5f53\\u3002\\u8fd9\\u4e00\\u8def\\u8d70\\u4e0b\\u6765\\uff0c\\u6211\\u53ef\\u4ee5\\u5f88\\u80af\\u5b9a\\u7684\\u544a\\u8bc9\\u4f60\\uff0c\\u6211\\u662f\\u771f\\u7684\\u559c\\u6b22\\u8d22\\u52a1\\uff0c\\u70ed\\u7231\\u8fd9\\u4e2a\\u884c\\u4e1a\\uff0c\\u56e0\\u6b64\\u575a\\u5b9a\\u4e0d\\u79fb\\u5730\\u5728\\u8fd9\\u4e2a\\u884c\\u4e1a\\u4e2d\\u8d70\\u4e0b\\u53bb\\u3002',u'\\u4e0d\\u8fc7\\u4f60\\u8bf4\\u6709\\u4eba\\u4ece\\u5c0f\\u5c31\\u559c\\u6b22\\u8d22\\u52a1\\u5417\\uff1f\\u6211\\u89c9\\u5f97\\u51e0\\u4e4e\\u6ca1\\u6709\\u5427\\u3002\\u8d22\\u52a1\\u7684\\u9b45\\u529b\\u5728\\u4e8e\\u4f60\\u771f\\u6b63\\u61c2\\u5f97\\u5b83\\u4e4b\\u540e\\u3002',u'\\u901a\\u8fc7\\u5b83\\uff0c\\u4f60\\u53ef\\u4ee5\\u5b66\\u4e60\\u4efb\\u4f55\\u4e00\\u79cd\\u5546\\u4e1a\\u7684\\u7ecf\\u8425\\u8fc7\\u7a0b\\uff0c\\u4e86\\u89e3\\u5176\\u7eb7\\u7e41\\u5916\\u8868\\u4e0b\\u7684\\u5b9e\\u7269\\u6d41\\u3001\\u73b0\\u91d1\\u6d41\\uff0c\\u751a\\u81f3\\u4f60\\u53ef\\u4ee5\\u638c\\u63e1\\u5982\\u4f55\\u53bb\\u7ecf\\u8425\\u8fd9\\u79cd\\u5546\\u4e1a\\u3002',u'\\u5982\\u679c\\u5bf9\\u4f1a\\u8ba1\\u7684\\u8ba4\\u8bc6\\u4ec5\\u4ec5\\u505c\\u7559\\u5728\\u505a\\u5206\\u5f55\\u8fd9\\u4e2a\\u5c42\\u9762\\uff0c\\u5f53\\u7136\\u4f1a\\u89c9\\u5f97\\u67af\\u71e5\\u65e0\\u5473\\u3002\\u5f53\\u4f60\\u5bf9\\u5b83\\u7684\\u8ba4\\u8bc6\\u8fdb\\u5165\\u5230\\u6df1\\u5c42\\u6b21\\u7684\\u65f6\\u5019\\uff0c\\u4f60\\u81ea\\u7136\\u5c31\\u4f1a\\u559c\\u6b22\\u4e0a\\u5b83\\u4e86\\u3002\\n\\n\\n'],'description':[u'\\u672c\\u4eba\\u5b66\\u4f1a\\u8ba1\\u6559\\u80b2\\u4e13\\u4e1a\\uff0c\\u6df1\\u611f\\u5176\\u67af\\u71e5\\u4e4f\\u5473\\u3002\\n\\u5f53\\u521d\\u662f\\u51b2\\u7740\\u5e08\\u8303\\u4e13\\u4e1a\\u62a5\\u7684\\uff0c\\u56e0\\u4e3a\\u68a6\\u60f3\\u662f\\u6210\\u4e3a\\u4e00\\u540d\\u8001\\u5e08\\uff0c\\u4f46\\u662f\\u611f\\u89c9\\u73b0\\u5728\\u666e\\u901a\\u521d\\u9ad8\\u4e2d\\u8001\\u5e08\\u5df2\\u7ecf\\u8d8b\\u4e8e\\u9971\\u548c\\uff0c\\u800c\\u987a\\u6bcd\\u4eb2\\u5927\\u4eba\\u7684\\u610f\\u9009\\u4e86\\u8fd9\\u4e2a\\u4e13\\u4e1a\\u3002\\u6211\\u559c\\u6b22\\u4e0a\\u6559\\u80b2\\u5b66\\u7684\\u8bfe\\uff0c\\u5e76\\u597d\\u7814\\u7a76\\u5404\\u79cd\\u6559\\u80b2\\u5fc3\\u7406\\u5b66\\u3002\\u4f46\\u4f1a\\u8ba1\\u8bfe\\u4f3c\\u4e4e\\u662f\\u4e3b\\u6d41\\u3001\\u54ce\\u3002\\n\\n\\u4e00\\u76f4\\u4e0d\\u559c\\u6b22\\u94b1\\u4e0d\\u94b1\\u7684\\u4e13\\u4e1a\\uff0c\\u6240\\u4ee5\\u5f88\\u597d\\u5947\\u5927\\u5bb6\\u9009\\u4f1a\\u8ba1\\u4e13\\u4e1a\\u5230\\u5e95\\u662f\\u51fa\\u4e8e\\u4ec0\\u4e48\\u76ee\\u7684\\u3002\\n\\n\\u6bd4\\u5982\\u8bf4\\u5b66\\u4e2d\\u6587\\u7684\\u4f1a\\u8bf4\\u4ece\\u5c0f\\u559c\\u6b22\\u770b\\u4e66\\uff0c\\u4f1a\\u6709\\u4ece\\u5c0f\\u559c\\u6b22\\u4f1a\\u8ba1\\u501f\\u554a\\u8d37\\u554a\\u7684\\u7684\\u4eba\\u5417\\uff1f'],'name':[],'title':[u'\\n\\n',u'\\n\\n'],'url':'http://www.zhihu.com/question/20688855/answer/15861368'}...1234567891011121314...'url':'http://www.zhihu.com/question/20688855/answer/16577390'}2014-12-1923:24:15+0800[zhihu]DEBUG:Crawled(200)<GEThttp://www.zhihu.com/question/20688855/answer/15861368>(referer:http://www.zhihu.com/question/20688855/answer/19231794)[]2014-12-1923:24:15+0800[zhihu]DEBUG:Scrapedfrom<200http://www.zhihu.com/question/20688855/answer/15861368>    {'answer':[u'\\u9009\\u4f1a\\u8ba1\\u8fd9\\u4e2a\\u4e13\\u4e1a\\uff0c\\u8003CPA\\uff0c\\u5165\\u8d22\\u52a1\\u8fd9\\u4e2a\\u884c\\u5f53\\u3002\\u8fd9\\u4e00\\u8def\\u8d70\\u4e0b\\u6765\\uff0c\\u6211\\u53ef\\u4ee5\\u5f88\\u80af\\u5b9a\\u7684\\u544a\\u8bc9\\u4f60\\uff0c\\u6211\\u662f\\u771f\\u7684\\u559c\\u6b22\\u8d22\\u52a1\\uff0c\\u70ed\\u7231\\u8fd9\\u4e2a\\u884c\\u4e1a\\uff0c\\u56e0\\u6b64\\u575a\\u5b9a\\u4e0d\\u79fb\\u5730\\u5728\\u8fd9\\u4e2a\\u884c\\u4e1a\\u4e2d\\u8d70\\u4e0b\\u53bb\\u3002',                u'\\u4e0d\\u8fc7\\u4f60\\u8bf4\\u6709\\u4eba\\u4ece\\u5c0f\\u5c31\\u559c\\u6b22\\u8d22\\u52a1\\u5417\\uff1f\\u6211\\u89c9\\u5f97\\u51e0\\u4e4e\\u6ca1\\u6709\\u5427\\u3002\\u8d22\\u52a1\\u7684\\u9b45\\u529b\\u5728\\u4e8e\\u4f60\\u771f\\u6b63\\u61c2\\u5f97\\u5b83\\u4e4b\\u540e\\u3002',                u'\\u901a\\u8fc7\\u5b83\\uff0c\\u4f60\\u53ef\\u4ee5\\u5b66\\u4e60\\u4efb\\u4f55\\u4e00\\u79cd\\u5546\\u4e1a\\u7684\\u7ecf\\u8425\\u8fc7\\u7a0b\\uff0c\\u4e86\\u89e3\\u5176\\u7eb7\\u7e41\\u5916\\u8868\\u4e0b\\u7684\\u5b9e\\u7269\\u6d41\\u3001\\u73b0\\u91d1\\u6d41\\uff0c\\u751a\\u81f3\\u4f60\\u53ef\\u4ee5\\u638c\\u63e1\\u5982\\u4f55\\u53bb\\u7ecf\\u8425\\u8fd9\\u79cd\\u5546\\u4e1a\\u3002',                u'\\u5982\\u679c\\u5bf9\\u4f1a\\u8ba1\\u7684\\u8ba4\\u8bc6\\u4ec5\\u4ec5\\u505c\\u7559\\u5728\\u505a\\u5206\\u5f55\\u8fd9\\u4e2a\\u5c42\\u9762\\uff0c\\u5f53\\u7136\\u4f1a\\u89c9\\u5f97\\u67af\\u71e5\\u65e0\\u5473\\u3002\\u5f53\\u4f60\\u5bf9\\u5b83\\u7684\\u8ba4\\u8bc6\\u8fdb\\u5165\\u5230\\u6df1\\u5c42\\u6b21\\u7684\\u65f6\\u5019\\uff0c\\u4f60\\u81ea\\u7136\\u5c31\\u4f1a\\u559c\\u6b22\\u4e0a\\u5b83\\u4e86\\u3002\\n\\n\\n'],    'description':[u'\\u672c\\u4eba\\u5b66\\u4f1a\\u8ba1\\u6559\\u80b2\\u4e13\\u4e1a\\uff0c\\u6df1\\u611f\\u5176\\u67af\\u71e5\\u4e4f\\u5473\\u3002\\n\\u5f53\\u521d\\u662f\\u51b2\\u7740\\u5e08\\u8303\\u4e13\\u4e1a\\u62a5\\u7684\\uff0c\\u56e0\\u4e3a\\u68a6\\u60f3\\u662f\\u6210\\u4e3a\\u4e00\\u540d\\u8001\\u5e08\\uff0c\\u4f46\\u662f\\u611f\\u89c9\\u73b0\\u5728\\u666e\\u901a\\u521d\\u9ad8\\u4e2d\\u8001\\u5e08\\u5df2\\u7ecf\\u8d8b\\u4e8e\\u9971\\u548c\\uff0c\\u800c\\u987a\\u6bcd\\u4eb2\\u5927\\u4eba\\u7684\\u610f\\u9009\\u4e86\\u8fd9\\u4e2a\\u4e13\\u4e1a\\u3002\\u6211\\u559c\\u6b22\\u4e0a\\u6559\\u80b2\\u5b66\\u7684\\u8bfe\\uff0c\\u5e76\\u597d\\u7814\\u7a76\\u5404\\u79cd\\u6559\\u80b2\\u5fc3\\u7406\\u5b66\\u3002\\u4f46\\u4f1a\\u8ba1\\u8bfe\\u4f3c\\u4e4e\\u662f\\u4e3b\\u6d41\\u3001\\u54ce\\u3002\\n\\n\\u4e00\\u76f4\\u4e0d\\u559c\\u6b22\\u94b1\\u4e0d\\u94b1\\u7684\\u4e13\\u4e1a\\uff0c\\u6240\\u4ee5\\u5f88\\u597d\\u5947\\u5927\\u5bb6\\u9009\\u4f1a\\u8ba1\\u4e13\\u4e1a\\u5230\\u5e95\\u662f\\u51fa\\u4e8e\\u4ec0\\u4e48\\u76ee\\u7684\\u3002\\n\\n\\u6bd4\\u5982\\u8bf4\\u5b66\\u4e2d\\u6587\\u7684\\u4f1a\\u8bf4\\u4ece\\u5c0f\\u559c\\u6b22\\u770b\\u4e66\\uff0c\\u4f1a\\u6709\\u4ece\\u5c0f\\u559c\\u6b22\\u4f1a\\u8ba1\\u501f\\u554a\\u8d37\\u554a\\u7684\\u7684\\u4eba\\u5417\\uff1f'],    'name':[],    'title':[u'\\n\\n',u'\\n\\n'],    'url':'http://www.zhihu.com/question/20688855/answer/15861368'}...6.存在问题Rule设计不能实现全网站抓取,只是设置了简单的问题的抓取Xpath设置不严谨,需要重新思考Unicode编码应该转换成UTF-81赞12收藏1评论"], "art_url": ["http://python.jobbole.com/88478/"]}
{"art_img": ["http://jbcdn2.b0.upaiyun.com/2015/02/591d8b55a524f825dd29a22b8df70000.jpg"], "art_title": ["Python线性分类模型简介"], "art_create_time": ["2017/09/01"], "art_content": ["本文由伯乐在线-bound翻译，小米云豆粥校稿。未经许可，禁止转载！英文出处：www.pyimagesearch.com。欢迎加入翻译组。在过去几周中，我们开始对机器学习有了更多的了解，也认识到机器学习在机器视觉、图像分类和深度学习领域的重要作用。我们已经看到卷积神经网络，如LetNet，可以用于对MNIST数据集的手写字迹进行分类。我们使用了k-NN算法来识别一张图片中是否含有猫或狗，并且我们也已经学习了如何调参来优化模型，进而提高分类精度。然而，还有一个重要的机器学习的算法我们尚未涉及：这个算法非常容易构建，并能很自然地扩展到神经网络和卷积神经网络中。是什么算法呢？它是一个简单的线性分类器，并且由于其算法很直观，被认为是更多高级的机器学习和深度学习算法的基石。继续阅读来加深你对线性分类器的认识，以及如何使用它们进行图像分类。Python线性分类模型简介本教程的前半部分主要关注线性分类有关的基本原理和数学知识。总的来说，线性分类指的是那些真正从训练数据中“学习”的有参分类算法。在这里，我提供了一个真正的线性分类实现代码，以及一个用scikit-learn对一张图片中的内容分类的例子。4大参数化学习和线性分类的组件我已经多次使用“参数化”,但它到底是什么意思？简而言之：参数化是确定模型必要参数的过程。在机器学习的任务中，参数化根据以下几个方面来确定面对的问题：数据：这是我们将要学习的输入数据。这些数据包括了数据点（例如，特征向量，颜色矩阵，原始像素特征等）和它们对应的标签。评分函数：一个函数接收输入数据，然后将数据匹配到类标签上。例如，我们输入特征向量后，评分函数就开始处理这些数据，调用某个函数f（比如我们的评分函数），最后返回预测的分类标签。损失函数：损失函数可以量化预测的类标签与真实的类标签之间的匹配程度。这两个标签集合之间的相似度越高，损失就越小（即分类精度越高）。我们的目标是最小化损失函数，相应就能提高分类精度。权重矩阵：权重矩阵，通常标记为W，它是分类器实际优化的权重或参数。我们根据评分函数的输出以及损失函数，调整并优化权重矩阵，提高分类精度。注意：取决于你使用的模型的种类，参数可能会多的多。但是在最底层，你会经常遇到4个参数化学习的基本模块。一旦确定了这4个关键组件，我们就可以使用优化方法来找到评分函数的一组参数W，使得损失函数达到最小值（同时提升对数据的分类准确度）接下来，我们就将看到如何利用这些组件，搭建一个将输入数据转化为真实预测值的分类器。线性分类器：从图片到标签在这部分中，我们将更多的从数学角度研究参数模型到机器学习。首先，我们需要数据。假设训练数据集（图片或者压缩后的特征向量）标记为，每个图或特征向量都有对应的类标签。我们用  和表示有N个D维（特征向量的长度）的数据点，划分到K个唯一的类别中。为了这些表达式更具体点，参考我们以前的教程：基于提取后的颜色矩阵，使用knn分类器识别图片中的猫和狗。这份数据集中，总共有N=25，000张图片，每张图片的特征都是一个3D颜色直方图，其中每个管道有8个桶。这样产出的特征向量有D=8x8x8=512个元素。最后，我们有k=2个类别标签，一个是“狗”，另一个是“猫”。有了这些变量后，我们必须定义一个评分函数f，将特征向量映射到类标签的打分上。正如本篇博客标题所说，我们将使用一个简单的线性映射：我们假设每个都由一个形状为[Dx1]的单列向量所表示。我们在本例中要再次使用颜色直方图，不过如果我们使用的是原始像素粒度，那可以直接把图片中的像素压扁到一个单列向量中。我们的权重矩阵W形状为[KxD]（类别标签数乘以特征向量的维数）最后，偏置矩阵b,大小为[Kx1]。实质上，偏置矩阵可以让我们的评分函数向着一个或另一个方向“提升”，而不会真正影响权重矩阵W，这点往往对学习的成功与否非常关键。回到Kaggle的猫和狗例子中，每个都表示为512维颜色直方图，因此的形状是[512x1]。权重矩阵W的形状为[2x512]，而偏置矩阵b为[2x1]。下面展示的是线性分类的评分函数f在上图的左侧是原始输入图片，我们将从中提取特征。在本例中，我们计算的是一个512维的颜色直方图，也可以用其他一些特征表示方式（包括原始像素密度），但是对于这个例子，我们就只用颜色分布，即直方图来表示xi。然后我们有了权重矩阵W，有2行（每个类标签一行）和512列（每一列都是特征向量中的条目）将W和xi点乘后，再加上大小为[2x1]的偏置矩阵bi。最后就得到了右边的两个值：猫和狗标签各自的分数。看着上面的公式，你可以确信输入xi和yi都是固定的，没法修改。我们当然可以通过不同的特征提取技术来得到不同的xi，但是一旦特征抽取后，这些值就不会再改变了。实际上，我们唯一能控制的参数就是权重矩阵W以及偏置向量b。因此，我们的目标是利用评分函数和损失函数去优化权重和偏置向量，来提升分类的准确度。如何优化权重矩阵则取决于我们的损失函数，但通常会涉及梯度下降的某种形式。我们会在以后的博客中重温优化和损失函数的概念，不过现在只要简单理解为给定了一个评分函数后，我们还需要定义一个损失函数，来告诉我们对于输入数据的预测有多“好”。参数学习和线性分类的优点利用参数学习有两个主要的优点，正如我上面详述的方法：一旦我们训练完了模型，就可以丢掉输入数据而只保留权重矩阵W和偏置向量b。这大大减少了模型的大小，因为我们只需要存储两个向量集合（而非整个训练集）。对新的测试数据分类很快。为了执行分类，我们要做的只是点乘W和xi，然后再加上偏置b。这样做远比将每个测试点和整个训练集比较（比如像knn算法那样）快的多。既然我们理解了线性分类的原理，就一起看下如何在python，opencv和scikit-learn中实现。使用python，opencv和scikit-learn对图片线性分类就像在之前的例子Kaggle猫vs狗数据集和knn算法中，我们将从数据集中提取颜色直方图，不过和前面例子不同的是，我们将用一个线性分类器，而非knn。准确地说，我们将使用线性支持向量机（SVM），即在n维空间中的数据之间构造一个最大间隔分离超平面。这个分离超平面的目标是将类别为i的所有（或者在给定的容忍度下，尽可能多）的样本分到超平面的一边，而所有类别非i的样本分到另一边。关于支持向量机的具体描述已经超出本博客的范围。（不过在PyImageSearchGuruscourse有详细描述）同时，只需知道我们的线性SVM使用了和本博客“线性分类器：从图片到标签”部分中相似的评分函数，然后使用损失函数，用于确定最大分离超平面来对数据点分类（同样，我们将在以后的博客中讲述损失函数）。我们从打开一个新文件开始，命名为linear_classifier.py，然后插入以下代码：Python#importthenecessarypackagesfromsklearn.preprocessingimportLabelEncoderfromsklearn.svmimportLinearSVCfromsklearn.metricsimportclassification_reportfromsklearn.cross_validationimporttrain_test_splitfromimutilsimportpathsimportnumpyasnpimportargparseimportimutilsimportcv2importos1234567891011#importthenecessarypackagesfromsklearn.preprocessingimportLabelEncoderfromsklearn.svmimportLinearSVCfromsklearn.metricsimportclassification_reportfromsklearn.cross_validationimporttrain_test_splitfromimutilsimportpathsimportnumpyasnpimportargparseimportimutilsimportcv2importos从第2行至11行导入了必须的python包。我们要使用scikit-learn库，因此如果你还没安装的话，跟着这些步骤，确保将其安装到你机器上。我们还将使用我的imutils包，用于方便处理图像的一系列函数。如果你还没有安装imutils，那就让pip安装。Shell$pipinstallimutils1$pipinstallimutils现在我们定义extract_color_histogram 函数，用于提取和量化输入图片的内容：Pythondefextract_color_histogram(image,bins=(8,8,8)):#extracta3DcolorhistogramfromtheHSVcolorspaceusing#thesuppliednumberof`bins`perchannelhsv=cv2.cvtColor(image,cv2.COLOR_BGR2HSV)hist=cv2.calcHist([hsv],[0,1,2],None,bins,[0,180,0,256,0,256])#handlenormalizingthehistogramifweareusingOpenCV2.4.Xifimutils.is_cv2():hist=cv2.normalize(hist)#otherwise,perform\"inplace\"normalizationinOpenCV3(I#personallyhatethewaythisisdoneelse:cv2.normalize(hist,hist)#returntheflattenedhistogramasthefeaturevectorreturnhist.flatten()123456789101112131415161718defextract_color_histogram(image,bins=(8,8,8)):#extracta3DcolorhistogramfromtheHSVcolorspaceusing#thesuppliednumberof`bins`perchannelhsv=cv2.cvtColor(image,cv2.COLOR_BGR2HSV)hist=cv2.calcHist([hsv],[0,1,2],None,bins,[0,180,0,256,0,256])#handlenormalizingthehistogramifweareusingOpenCV2.4.Xifimutils.is_cv2():hist=cv2.normalize(hist)#otherwise,perform\"inplace\"normalizationinOpenCV3(I#personallyhatethewaythisisdoneelse:cv2.normalize(hist,hist)#returntheflattenedhistogramasthefeaturevectorreturnhist.flatten()这个函数接收一个输入image ，将其转化为HSV颜色空间，然后利用为每个通道提供的bins，计算3D颜色直方图。利用cv2.calcHist函数计算出颜色直方图后，将其归一化后返回给调用函数。有关extract_color_histogram方法更详细的描述，参阅这篇博客。接着，我们从命令行解析参数，并初始化几个变量:Python#importthenecessarypackagesfromsklearn.preprocessingimportLabelEncoderfromsklearn.svmimportLinearSVCfromsklearn.metricsimportclassification_reportfromsklearn.cross_validationimporttrain_test_splitfromimutilsimportpathsimportnumpyasnpimportargparseimportimutilsimportcv2importosdefextract_color_histogram(image,bins=(8,8,8)):#extracta3DcolorhistogramfromtheHSVcolorspaceusing#thesuppliednumberof`bins`perchannelhsv=cv2.cvtColor(image,cv2.COLOR_BGR2HSV)hist=cv2.calcHist([hsv],[0,1,2],None,bins,[0,180,0,256,0,256])#handlenormalizingthehistogramifweareusingOpenCV2.4.Xifimutils.is_cv2():hist=cv2.normalize(hist)#otherwise,perform\"inplace\"normalizationinOpenCV3(I#personallyhatethewaythisisdoneelse:cv2.normalize(hist,hist)#returntheflattenedhistogramasthefeaturevectorreturnhist.flatten()#constructtheargumentparseandparsetheargumentsap=argparse.ArgumentParser()ap.add_argument(\"-d\",\"--dataset\",required=True,help=\"pathtoinputdataset\")args=vars(ap.parse_args())#grabthelistofimagesthatwe'llbedescribingprint(\"[INFO]describingimages...\")imagePaths=list(paths.list_images(args[\"dataset\"]))#initializethedatamatrixandlabelslistdata=[]labels=[]1234567891011121314151617181920212223242526272829303132333435363738394041424344#importthenecessarypackagesfromsklearn.preprocessingimportLabelEncoderfromsklearn.svmimportLinearSVCfromsklearn.metricsimportclassification_reportfromsklearn.cross_validationimporttrain_test_splitfromimutilsimportpathsimportnumpyasnpimportargparseimportimutilsimportcv2importos defextract_color_histogram(image,bins=(8,8,8)):#extracta3DcolorhistogramfromtheHSVcolorspaceusing#thesuppliednumberof`bins`perchannelhsv=cv2.cvtColor(image,cv2.COLOR_BGR2HSV)hist=cv2.calcHist([hsv],[0,1,2],None,bins,[0,180,0,256,0,256]) #handlenormalizingthehistogramifweareusingOpenCV2.4.Xifimutils.is_cv2():hist=cv2.normalize(hist) #otherwise,perform\"inplace\"normalizationinOpenCV3(I#personallyhatethewaythisisdoneelse:cv2.normalize(hist,hist) #returntheflattenedhistogramasthefeaturevectorreturnhist.flatten() #constructtheargumentparseandparsetheargumentsap=argparse.ArgumentParser()ap.add_argument(\"-d\",\"--dataset\",required=True,help=\"pathtoinputdataset\")args=vars(ap.parse_args()) #grabthelistofimagesthatwe'llbedescribingprint(\"[INFO]describingimages...\")imagePaths=list(paths.list_images(args[\"dataset\"])) #initializethedatamatrixandlabelslistdata=[]labels=[]33行到36行解析命令行参数。我们这里只需要一个简单的开关，—dataset是kaggle猫vs狗数据集的路径。然后我们将25000张图片保存的磁盘位置赋值给imagePaths，跟着初始化一个data矩阵，存储提取后的特征向量和类别labels。说到提取特征，我们接着这样做：Python#loopovertheinputimagesfor(i,imagePath)inenumerate(imagePaths):#loadtheimageandextracttheclasslabel(assumingthatour#pathastheformat:/path/to/dataset/{class}.{image_num}.jpgimage=cv2.imread(imagePath)label=imagePath.split(os.path.sep)[-1].split(\".\")[0]#extractacolorhistogramfromtheimage,thenupdatethe#datamatrixandlabelslisthist=extract_color_histogram(image)data.append(hist)labels.append(label)#showanupdateevery1,000imagesifi>0andi%1000==0:print(\"[INFO]processed{}/{}\".format(i,len(imagePaths)))12345678910111213141516#loopovertheinputimagesfor(i,imagePath)inenumerate(imagePaths):#loadtheimageandextracttheclasslabel(assumingthatour#pathastheformat:/path/to/dataset/{class}.{image_num}.jpgimage=cv2.imread(imagePath)label=imagePath.split(os.path.sep)[-1].split(\".\")[0]#extractacolorhistogramfromtheimage,thenupdatethe#datamatrixandlabelslisthist=extract_color_histogram(image)data.append(hist)labels.append(label)#showanupdateevery1,000imagesifi>0andi%1000==0:print(\"[INFO]processed{}/{}\".format(i,len(imagePaths)))在47行，我们开始对输入的imagePaths进行遍历，对于每个imagePath，我们从磁盘中加载image，提取类别label，然后通过计算颜色直方图来量化图片。然后我们更新data和labels各自的列表。目前，我们的labels是一个字符串列表，如“狗”或者“猫”。但是，很多scikit-learn中的机器学习算法倾向于将labels编码为整数，每个标签有一个唯一的数字。使用LabelEncoder类可以很方便的将类别标签从字符串转变为整数：Python#importthenecessarypackagesfromsklearn.preprocessingimportLabelEncoderfromsklearn.svmimportLinearSVC123#importthenecessarypackagesfromsklearn.preprocessingimportLabelEncoderfromsklearn.svmimportLinearSVC调用了.fit_transform方法后，现在我们的labels表示为整数列表。代码的最后一部分将数据划分为训练和测试两组、训练线性SVM、评估模型。Python#partitionthedataintotrainingandtestingsplits,using75%#ofthedatafortrainingandtheremaining25%fortestingprint(\"[INFO]constructingtraining/testingsplit...\")(trainData,testData,trainLabels,testLabels)=train_test_split(np.array(data),labels,test_size=0.25,random_state=42)#trainthelinearregressionclasifierprint(\"[INFO]trainingLinearSVMclassifier...\")model=LinearSVC()model.fit(trainData,trainLabels)#evaluatetheclassifierprint(\"[INFO]evaluatingclassifier...\")predictions=model.predict(testData)print(classification_report(testLabels,predictions,target_names=le.classes_))12345678910111213141516#partitionthedataintotrainingandtestingsplits,using75%#ofthedatafortrainingandtheremaining25%fortestingprint(\"[INFO]constructingtraining/testingsplit...\")(trainData,testData,trainLabels,testLabels)=train_test_split(np.array(data),labels,test_size=0.25,random_state=42)#trainthelinearregressionclasifierprint(\"[INFO]trainingLinearSVMclassifier...\")model=LinearSVC()model.fit(trainData,trainLabels)#evaluatetheclassifierprint(\"[INFO]evaluatingclassifier...\")predictions=model.predict(testData)print(classification_report(testLabels,predictions,target_names=le.classes_))70和71行构造了训练集和测试集。我们将75%的数据用于训练，剩下的25%用于测试。我们将使用scikit-learn库实现的LinearSVC（75和76行）来训练线性SVM。最后，80到82行评估我们的模型，显示一个格式整齐的报告，来说明模型的执行情况。需要注意的一点是，我故意没有进行调参，只是为了让这个例子简单，容易理解。不过，既然提到了，我就把LinearSVC 分类器的调参作为练习留个读者。可以参考我以前的k-NN分类器调参博客。评估线性分类器为了测试我们的线性分类器，确保你已经下载了：1.本博客中的源代码，可以使用教程底部的“下载”部分。2.kaggle猫vs狗数据集有了代码和数据集之后，你可以执行如下命令：Shell$pythonlinear_classifier.py--datasetkaggle_dogs_vs_cats1$pythonlinear_classifier.py--datasetkaggle_dogs_vs_cats特征提取过程大约要花费1-3分钟不等，具体时间根据机器的速度。之后，训练并评估我们的线性SVM：正如上图所示，我们的分类精度有64%，大致接近本教程中调参后的knn算法精度。注意：对线性SVM调参可以得到更高的分类精度，为了使教程稍微短一点，而且不至于太复杂，我简单地省略了这个步骤。此外，我们不仅得到了和knn相同的分类精度，模型的测试时间也快的多，只需要将权重矩阵和数据集进行点乘（高度优化后），然后是一个简单的加法。我们也可以在训练完成后丢弃训练集，只保留权重矩阵W和偏置向量b，从而大大精简了模型表示。总结在今天的博客中，我讨论了参数学习和线性分类的基础概念。虽然线性分类器比较简单，但它被视为更多高级的机器学习和深度学习算法的基石，并能很自然地扩展到神经网络和卷积神经网络中。你看，卷积神经网络可以将原始像素映射到类别标签，类似于我们在本教程中所作的那些，只是评分函数f更复杂，并且参数更多。参数学习的一大好处就是可以在训练完毕之后，丢弃原训练数据。我们可以只用从数据中学到的参数（比如，权重矩阵和偏置向量）来进行分类。这使得分类变得非常高效，因为（1）我们不需要像knn那样在模型中存储一份训练数据的拷贝（2）我们不用将测试图片一个一个的和训练图片进行比较（一个O(N)的操作，并且当数据集很大时就变得非常麻烦）简而言之，这个方法明显更快，只需一个简单的点乘和加法。非常简洁，不是吗？最后，我们在kaggle狗vs猫的数据集上，利用Python,OpenCV,和scikit-learn进行线性分类。从数据集提取出颜色直方图之后，我们在特征向量上训练一个线性支持向量机，并且得到64%的分类精度，这已经很不错了，因为（1）颜色直方图并非狗和猫特征化的最佳选择（2）我们没有对线性SVM进行调参。到这里，我们开始理解了构建神经网络、卷积神经网络和深度学习模型的基本模块，但还有很长一段路要走。首先，我们需要更详细地了解损失函数，特别是如何使用损失函数来优化权重矩阵以获得更准确的预测。未来的博客文章将更详细地介绍这些概念。打赏支持我翻译更多好文章，谢谢！打赏译者打赏支持我翻译更多好文章，谢谢！1赞3收藏评论关于作者：bound简介还没来得及写:）个人主页·我的文章·11"], "art_url": ["http://python.jobbole.com/88487/"]}
{"art_img": ["http://jbcdn2.b0.upaiyun.com/2015/02/edecceebedd0d04aa17bccba430ddcaf.jpg"], "art_title": ["如何为使用 Python 语言而辩论"], "art_create_time": ["2017/08/31"], "art_content": ["原文出处：TALL,SNARKYCANADIAN   译文出处：开源中国   最近我写了一篇关于我为什么不担心Python流失用户的文章。几分钟之后有人问我Python的用法(usage)，而这篇文章没有提及，但却是一个让人深思的问题。我们看到，使用Python的用户很可能在未来保持高位，但是Python是否会被用到尽可能多的项目中是不能保证的；用户(users)数目很多而且稳定，但是项目中Python的用处(use)并不确定。这篇文章的用意是帮助表明Python仍然对大多数软件项目是切实可行的。我不担心把Python推销给反对其他动态语言(如Ruby)的人，因为我认为这些争论与个人喜好有关。这篇文章是讲给那些推销静态类型语言的人。具体上，这篇文章是针对Go的，但也可以是其他任何静态类型语言。”为什么Go?”，你可能会问。因为Go实际上在获取Python的用户。当2003到2005年间Python的增长曲线是个曲棍球棒时，Python还不是被推下山巅的王者，而是个弱者。传统上，Python从Java之类的语言阵营中获得用户，并且留住了他们(我不想谈C++用户，因为通常他们有严格的性能需求，需要一个系统语言，或者是性能成瘾者，并且需要好好恢复)。但是Go的情况不太一样。如今Python是使用最多的语言之一，而不再是弱者了。一旦在静态类型语言社区中出现一门语言，它的生产效率/性能的取舍相当好，那便足以说服一些Python的程序员选择Go而不再是Python了。如今的Go首先我应该说，Go是目前我第二喜欢的语言。如果今天我要启动一个项目，但不能说服人们使用Python，那我会提议使用Go。不要误解我在本文中说Go是门不好的语言。这篇文章的要点是说服其他人，Python是生产率/性能取舍游戏中Go之外切实可行的替代方案，而不是表达Go是门不好的语言。认为这篇文章是反Go的，那就是你的个人想法，而且不应该这样认为。我应该说，我偶尔在工作中使用Go，并有点想关注这门语言的社区。既然我不能仅凭想象就成为Go专家，但这番话并不是仅从文档或者博客中提取出来的。但是由于我是Python开发团队的一份子，无论我如何试图表现得公平，固有的偏见某种程度上还是有的。那么，带着这些警告，我们来看下Go提供给开发者什么。生产率我看待Go的方式是，使用你最喜欢的编程语言，移除那些难于加速生产率的特性，就是Go。静态类型的影响被降到最小，因为通常只有在API边界时你才会面对它。结构类型同样使事情变得简单(把它认为是鸭子类型)。语法并不笨拙(虽然它使用了花括号)。不要认为Go是C/C++去掉不安全的特性，加上生产率更高的东西，不然你会很失望(比如，“为什么我不能使用make()内置函数，也不能像map类型一样对返回值进行计数”，这种看待Go的方式是错误的；这就是为什么C++开发者没有转到Go的原因)。快速编译也使开发周期更像一个动态语言，而不是一个需要编译的语言。而且事实上有些人喜欢没有异常机制带来的冗长，因为这促使你处理每种异常情形而不是(意外地)忽略它们(这是贯穿Go初始系统语言设计的实例)。还有，这门语言本身相当短小易记，并有严格的前向兼容性要求(forward-compatibilityrequirements)(你不可能更快地获得泛型)，大体上使用Go来编码是件很愉快的事情。由于是静态类型，Go可以很容易地获得工具支持(它对之前以此为设计目标的语言也有帮助)。Go确保核心工具跟随Go本身提供，也是明智之举。gofmt强制执行Go风格的规则，并允许通过用户自定义的规则来重构代码(“采用制表符缩进”不再是问题，因为这意味着你可以随心所欲地设置编辑器来代表制表符，然后gofmt将其转换为普通制表符以适用VCS)。gofix会更新代码以跟最新发布的版本保持一致。goget获取依赖并安装。Go最后一个生产率功能是它静态编译所有东西，使部署更简单。如果你使用容器来开发和部署，这也不算什么。只有当你发布单个文件的命令行工具，而不是一组依赖和你自己的代码时，这才算得上事。性能就性能来说，Go做的很好。很难指出任何基准能准确的证明Go总是最快的选择，甚至计算机语言基准游戏中一些基准证明CPython3是最快的。但是通常情况下可以认为对于你的任何工作来说Go已经足够快了。Go真正出色的地方是并发性(concurrency)。要注意并发代码并不是通常误解的并行(parallelized)代码;并发代码仍然可以是单线程的，仅仅在任务切换方面更加简单/出色。Go通过使用goroutine使连续并发的代码执行起来绝对的简单。如果你不想使用共享内存的方式(虽然也同样支持)，该语言提供的通信管道允许以非常简洁的消息传递方式进行并发编程。将所有特征整合进此语言中成为尽可能使用该语言开发并发代码的又一原因。换句话说，Go程序运行很快，该语言尽力使你在合理的方式上获得该效果。如今的Python如果顺利的话我已经让你相信Go是一种优秀的编程语言，除非因为其他原因，一些人不会认为我在整篇文章对Go的描述很糟糕。现在我们讨论一下Python的生产率/性能是怎么样的。生产率首先也是最重要的，Python非常容易学习。这也是为什么在当前高评价的美国大学中将Python作为首选的教学语言。这相当于该语言拥有成熟稳定的新程序员的来源以及更容易培训其他程序员。我想，要说服别人只用几行Python代码就会完成很多工作这并不难(Go/Python3比较显示Python每次都比Go使用更少的代码完成相同的工作)。所以我会坚持认为使用Python会更高产，即使和Go相比，这不会有人反对。通常大家反对Python的地方是在工具支持方面。但是如果你注意到我指出的Go相关的支持工具，fmt,fix,和get,Python社区也有对等的工具。对遵循PEP8的风格格式化(styleformating),可以在提交检查时使用pep8，或者如果想要更多gofmt风格的自动重写可以使用autopep8。对用于重构的gofix或gofmt，你可以说2to3也可以完成同样的功能。对于goget,Python有pip。我们有venv/virtualenv或cx_Freeze这样的代码冻结工具(跟其他一样，位于容器之上?ontopofcontainerizationlikeanythingelse)，而不是静态编译的二进制包。甚至有贯穿项目的代码分析工具如pylint。说Python因为缺少工具支持而不能用于大型项目，这种观点对我来说是很肤浅的。如果说有哪方面Python完全做的好，那就一定是它丰富的第三方扩展库和相应的工具可供使用，就像在PyPI上面看到的那样（我相信肯定有人忍不住要争论说，“并不是所有的第三方库都能够在Python3上面运行啊”，事实确实如此，然而，这些第三方扩展库对Python3的支持已经相当好了，而且还在继续改善中，所以我不会太在意这个争论，另外，你可以同时使用Python2/3两个版本进行编码，不需要关心针对哪个版本）。看一下godoc.org，上面显示Go也并不缺少社区支持，Pytho之所以能够拥有更多可用的第三方库仅仅是因为它的年龄，这个状态也会继续持续。性能因为Python已经存在很久，且变得如此庞大，简单地去说“Python是足够快的”不能说明整个的情况，那是因为有各种各样的实现加速的方式。但是在深入到VM级别的选项之后，意味着Python的stdlib提供了获得加速的选项。举例来说，concurrent.futures是尴尬地执行并行代码的方式，这种方式是极其简单的。而在Python3.3中，新的asyncio编写了异步代码。它没有像Go那样被集成进语言，在Python中的并发程序设计是可行的，且在方式上也未必是那么痛苦的。但是最好的办法是，你可以在选择的VM里改变Python代码的性能。CPython+Cython如果你在使用C拓展模块，CPython就会使你最好的选择(可能你不知道这个术语，CPython是你可以在python.org获得的解释器)。对大多数的情况而言性能至少合理些–因为某些原因，一些人认为Python开发团队不关心性能，这个一个谎言–而且即将会成为新的特性，因为CPython同时担当着语言规范的作用。如果认为你的一些内循环代码确实需要提高些速度，Cython是CPython的选择。Cython会尽可能的将你的Python代码编译成C拓展代码。有若干种支持的方法可以产生更好的C代码，所以这取决于你需要怎样的Cython特性。Cython同时也使写出C拓展模块更加简单(但要继续读下去，除了CPython还有其他的选择)。PyPy+cffi如果你不依赖于已存在的C拓展模块，PyPy会给你提供总体上最好的性能。它的JIT非常好而且它的团队欢迎受到使用CPython并且运行更快的代码的挑战，因为他们痛恨在speed.pypy.org中显示的那么慢。实话说，除非PyPy不支持你真的想用的那个版本的Python–因为PyPy确实会落后2个版本,比如pypy3现在支持Python3.2然而3.4是最新的CPython发布版;它们期盼在这个问题上能得到帮助(donation)–我只能考虑不使用PyPy因为你依赖于已存在的C拓展模块C(numpy是最常见的问题，虽然PyPyislookingfordonations可修复这个问题)。但这不意味着，如果你想封装一些C代码就用不了PyPy。PyPy项目还有另外一个子项目cffi，这个项目的目的是使Python代码也可以利用封装的C代码。使用cffi的关键好处在于，一旦你使用了cffi，C代码就可以用于CPython和PyPy(我认为IronPython和Jython也在添加对cffi的支持)。所以如果你在封装C代码，我强烈建议你看下cffi，而不是手动写C扩展模块或者使用Cython，这样你有更好的Python实现的支持，还能使用PyPy。Numba如果你在做数值相关的工作，你肯定应该考虑Numba这个选择。在科学计算上，经济学家注意到了它的性能。虽然在普通的Python编程上，它不能帮到什么忙，但是如果在Python非常强大的科学计算栈中用到了numpy或者其他模块，Numba使用LLVM来进行JIT肯定会有帮助。未来的Python考虑到所有内容，Python肯定不是停滞不前的(Go也没有，比如它们都在忙于用Go重写编译器和将连接器以外的东西转移进编译器来获得更快的编译速度）。Python的未来看起来还是光明的。生产率Python是一种在进化的语言。不像Go，Python乐于改变该语言，甚至是以永远不再向后兼容的方式。这意味着Python会比Go更快的速度变得更加高效(虽然在Go2开发之前Go的团队对该语言进化持哪种观点还是未知的)。在工具方面，标准化的函数注解是为了声明类型。这是在PyCon2014语言峰会期间提出的，针对函数参数和返回值，里面提到有大量的项目现在想要有一种声明预期类型的方法，使用函数注解考虑到了在某些方面的标准化，最终可能对标准库函数也会是有用的。在pytypedecl的邮件列表上的讨论还没有开始，但是我知道PEP大概是要开始了。不仅仅是对像Cython和Numba这样的项目在什么地方使用打印信息，还包括在诸如代码分析，重构等的时候使用。性能长远看来，有两个项目可以帮助提升Python的性能。一个是新的Python虚拟机Pyston。尽管Pyston刚出现时间不长，但它的目标是要使用LLVM的JIT(是的这不由得让人想起UnladenSwallow，然而LLVM的JIT已经比它在2009年时好很多了，所以这个项目还是颇有希望取得好效果的)。其实PyPy-STM才是真正能够让我兴奋的项目，”STM”表示”软件事务性内存”，它基本上是允许Python丢弃GIL的。PyPy-STM此时的性能比PyPy要慢大约1.2-3倍，这样的表现已经相当不错了。目前他们正在寻找资助，来继续这项工作，要实现这个目标：使得带有两个线程的PyPy-STM值得在PyPy上普遍运行。在黑暗中做出选择希望这篇博客传达的不是一个总结，而是一个关于生产力/性能折衷的方案。Python已经清晰地拥有了强大的生产力辅助并且没有哪个领域表现不佳，这仍是我选择的语言。如果你发现自己有可能选择Python项目以外的东西，请一定要停下来思考没有使用Python所带来的生产力损失，然后看看你有各种选项让Python加快执行，这样你再去做一个全面的关于Python是否可以为你的项目工作的选择。1赞收藏1评论"], "art_url": ["http://python.jobbole.com/88461/"]}
{"art_img": ["http://jbcdn2.b0.upaiyun.com/2012/02/python-logo.png"], "art_title": ["创建成功的Python项目"], "art_create_time": ["2012/02/02"], "art_content": ["英文原文：CreatesuccessfulPythonprojects，编译：Elaine.Ye创建一个成功的开源Python项目所涉及的并不仅仅是编写有用的代码，与其相关的还有社区的参与、越来越多的合作机会、技艺以及支持等。探索最佳的做法有助于你创建出自己的成功项目。开源Python项目的生态系统丰富多样，这使得您能够站在巨人的肩膀上来开发下一个开源项目。此外，这意味着存在一系列的社区规范和最佳做法，通过遵守这些约定并把这些做法应用到项目中，你可以为自己的软件赢得更广范围的采用。本文涵盖了一些在构建大型和小型的项目时都运作得很好的实践做法，这些项目都已经赢得了广泛的用户群体。这里给出的这些建议的都是合理的、有其意义的，不过，因为结果可能会有所不同，所以不必把它们当成严格的教条来遵守。首先我们来讨论一下，解耦的过程如何能够带来一个更强健的社区，在编写、维护和支持开源软件等各方面都带来更大的生产能力。合作（collaboration）和互助（cooperation）的对比在DjangoCon2011大会期间，DavidEaves作了基调发言，雄辩地表达了这样的想法，即尽管合作（collaboration）和互助（cooperation）有着类似的定义，但还是有着细微的差别：“我认为，和作（collaboration）不同于互助（cooperation），其需要项目涉及到的各方通力来解决问题。”Eaves接着又给出了一整篇文章，专门说明GitHub如何成为革新开源的运作方式——特别是在社区管理方面的运作方式的推动力。在“HowGitHubSavedOpenSource”这篇文章（参见参考资料）中，Eaves说到：“我相信，当捐献者能够以低交易成本的互助方式来参与，并且高交易成本的合作尽可能的少时，开源项目的运作能达到最好。开源的高明之处就在于其不需要一个工作组来共同讨论每个问题以及解决问题，而是恰恰相反。”他接着谈到了分支（forking）的价值所在，以及其如何通过在参与者之间启用低成本的互助来降低合作的高成本，这些参与者能够在无需批准的情况下推进项目。这种分支把需要的合作搁置起来，直到解决方案已经做好了合并的准备为止，如此来支持更快速的动态的实验。你可以以类似的方式来打造自己的项目，目标是相同的：在编写、管理和支持项目的整个期间，增加低成本的互助，同时尽可能减少代价高昂的合作。编写从一张白纸开始，创建一些新鲜的东西，制造一些有创意的东西——或仅仅是一些与现有的略不同的东西，没有什么事情比得上启动一个新的项目并和全世界分享你的努力成果让人感觉更好的了。与维护不同，在编写代码时，你是在创造新的东西而不是在修改或是修正已有的东西。编写和构思一个项目除了是一门科学之外还是一种艺术形式，其他人会看到实现的情况并会对代码的质量作出判断，而你的名字将会永远和它连在一起。因此。了解工匠的心态以及据此来编写软件的方法是很重要的。编写新的项目不仅仅是意味着生成代码：项目的创建和构思包括了编写有着精美风格的让人乐于阅读的代码、在适当的时候为验证项目中的功能创建测试代码，以及制作详尽的有帮助的文档。技艺工艺（craft）一般是指艺术行业或是职业需要特殊的技能来手工制作一些东西，通常是小规模生产的物理器件。就软件工匠关注的更多的是质量而非数量这一意义而言，你可以延伸这一定义，把它应用在软件上。对于工匠来说，产品具有吸引力而非只是功用是很重要的。具体来说，在软件中，工匠要努力确保代码的干净和美观、应用编程接口（API）的悦目，以及文档和测试用例能够给用户带来是在使用坚实的产品进行工作的这种感受。在这种心态下工作，对于心灵来说是一种奖赏，也是在制作开源软件时能感受到诸多享受的原因：你不再受困于回应最后期限、客户以及其他的外部需求，而是按照自己的时间来，享受制作一些美好事物的乐趣。代码风格和规范检查Python的增强建议（PythonEnhancementProposal，PEP）8（参见参考资料）是一个详细的Python风格指南，你应该基于该指南来建立自己的Python项目（或至少是基于你的项目的风格指南）。不是非要教条地采用PEP8，不过你的工作成果越接近PEP8规范，其他的Python开发者就越容易提交以标准的Python社区风格实现的整洁的补丁包。除了风格的一致性之外，在捕捉诸如缺失导入和未定义变量一类的错误方面，代码规范（linting）的概念也是很有作用的。除了风格检查器会帮助你进行检查，找出违背了默认规则或是自定义规则的代码之外，现还有一些规范器（linter）或是一些工具，最常用到的一些实用程序是：1.pyflakes2.pylint3.pep8请参阅参考资料获得到这些工具的链接。无论你选择遵从的是哪一种约定，如果这些约定偏离了PEP8的话，我建议文档化它们，以便让那些想要为你的项目做贡献的人了解你所采用的编码风格，显式的说明要好于隐含不语。pyflakes是一个特别有用的规范器，它很好地平衡了有用的功能、捕捉和标出错误这两方面，不会过度地揪住微小的古怪做法不放。下面是一个在某个Python项目上使用pyflakes的示例会话：Python$pyflakeskaleokaleo/forms.py:1:'form'importedbutunusedkaleo/forms.py:4:undefinedname'forms'kaleo/forms.py:6:undefinedname'forms'1234$pyflakeskaleokaleo/forms.py:1:'form'importedbutunusedkaleo/forms.py:4:undefinedname'forms'kaleo/forms.py:6:undefinedname'forms'立刻，该工具告诉了我有一个import的输入错误，查看文件kaleo/forms.py，我发现：Python;html-script:false]1:fromdjangoimportform2:3:classInviteForm(forms.Form):4:email_address=forms.EmailField()1234;html-script:false]1:fromdjangoimportform2:3:classInviteForm(forms.Form):4:email_address=forms.EmailField()从内容中可看出来，要把第1行改为fromdjangoimportforms。测试在项目中提供验证代码有效性的测试始终是一件好事，以此来防止回归被忽视，以及在某些情况下作为一种文档形式，通过阅读其中的测试代码可以让其他人知道你的库API是如何工作的。话虽如此，但我不会根据项目是否包括测试用例或是完成这些测试的方式来判断项目的完整性或可行性。测试用例的存在并不能保证代码的质量，这可能是一个有争议的观点，但我相信，完全没有测试比去测一些错误的东西要来得好一些。在编写测试代码时，考虑为每个测试单元给出各种输入是一件很重要的事情。文档不过，与测试不同的是，你可以根据项目文档的质量和广博性来判断项目的质量和技艺水平。用与创作和维护代码相同的方式来创作和维护稳定，编写良好的并且是有深度的文档会鼓励捐献者效仿你的做法，使你的项目变得更易于为用户接受。使用诸如Sphinx和ReadtheDocs一类的工具（参见参考资料），你可以发布及时更新的、外观极为不错的文档。使用这些工具是一件简单的事情，也就是写一些文字内容并并推送提交。习惯于尽可能地使用commit来提交文档的变更是很适当的一种做法。维护在PythonPackageIndex（PyPI）上发布了第一个版本，并通过各种Tweet消息和博客文章公布该版本的消息，开始有了一些使用者之后，你就需要在任何后续的创作活动中加入维护方面的考虑了。用户会报告错误、要求添加功能、提一些文档中没有明显涉及到问题，诸如此类等等。有些事情你会选择不去处理，给出一些权变措施；但其他的一些问题，你会打算或是修正文档或是修正代码。使用诸如git一类的分布式版本控制系统（distributedversioncontrolsystem，DVCS）并常常发布开发者包，这种做法可以大大简化维护工作，使之变成一件不再是烦人的事情。源控制有许多可用的DVCS，其中就包括了git和mercurial（参见参考资料），无论你选择的是哪一个控制系统，请确保它提供了源控制功能，这种功能赋予你这样的能力，可以让用户分支你的项目，然后自己来解决其中的错误。进行变更的速率取决于许多因素，一个关键的因素是目标受众（例如，其他开发者、非技术型的最终用户）。如果你的项目是针对开发者来编写的，那么鼓励通过拉请求（pullrequest）来报告错误或是请求功能之类的做法可以真正地做到降低维护者的负担。这种做法还提升了社区的归属感，因为大家都把他们的捐献合并到了将来的版本中。开发构建你会希望尽早地以及经常性地发布开发版本，在每次有一组附加的补丁包出来之后都会发布版本，如此多次。这会让其他在工作中使用你的项目的开发者能够更容易地针对项目中的最新更改来运行。越多的人在不同的情况下使用这些代码，那么一旦到发布一个新的稳定版本的时候，该版本就会有越高的质量。支持支持是和维护相随的，参与并构建一个由用户和捐献者组成的社区至关重要。赋予其他人通过支持来帮助你的权利，你就是在增强项目的全面合作因素，在项目的规模方面提供更好的伸缩性，以及自然而然地增加了解决用户问题的做法。为了达到该目的，请确保提供多种渠道来增加接触的机会，让用户更加容易地与你接洽以及参与到项目中。可选的沟通渠道包括IRC、邮件列表以及诸如Twitter一类的社交媒体汇聚点。IRC在诸如freenode一类的IRC平台上设置一个沟通频道是一个好主意，我就为自己的项目设置了一个：nashvegas；除了我之外只有一个用户，虽然这种情况很少有，但我的IRC客户端还是悄无声息地运行在后台。当偶尔有用户提问时，我能够只花很少的交易成本就以一种比通过邮件要动态得多的方式来做出响应。邮件列表对于大多数的开源项目来说，有一个用于支持的邮件列表并在捐献者之间讨论开发进程是一种标准的做法。我的建议是，把支持放在一个邮件列表中，只有在内容已经变得太多，彼此影响到了各小组的讨论的时候，才把它分成“用户”列表和“开发”列表。Twitter为项目开设一个Twitter帐户，大家可以在这里与你快速地讨论工作。Twitter帐户还是一个可以作为发布项目消息的好地方。结束语给Python社区中的开源软件编写并捐献代码是一种有趣且有益的体验。在增加低成本互助机会的同时侧重于减少高成本的合作，这种做法有助于项目与活跃的捐献者一起成长。在开源领域，就你的项目来说，你有大把的自由来成为一个能工巧匠，充分利用这一点并享受它。把关注的重点放在一致的代码风格、坚实的测试和编写良好的文档上，以此来提高项目被用户和其他开发者采用的几率。此外，要利用DVCS，关注拉请求，经常性地发布开发版本。最后还有一点就是，你可以提供多种支持渠道，以及允许社区协助你提供这种支持，通过这些做法来进一步提升项目的采用率并促进项目的成长。 参考资料1.阅读MarkPilgrim的DiveintoPython，获取关于该语言的一个介绍。2.欲了解更多关于打包Python项目方面的信息，可以读一下 AguidetoPythonpackaging（PatrickAltman，developerWorks，2011年10月）这篇文章。3.阅读更多DavidEaves的博客文章： WikisandOpenSource:CollaborativeorCooperative? 和HowGitHubSavedOpenSource.4.在潜心进行下一个Python项目之前，请确保已了解PEP8，Python代码的这一“官方”风格指南。5.浏览一下我的项目nashvegas的GitHub页面，以此来做为一个使用DVCS的Python项目的例子。6.看一看PyPI。7.了解更多关于分布式Python模块方面的内容。8. developerWorks开源专区提供了丰富的关于开源工具和使用开源技术方面的信息。9.在Twitter上关注developerWorks。10.在EasyandbeautifuldocumentationwithSphinx (AlfredoDeza，developerWorks，2011年11月)一文中了解更多关于Sphinx的内容。1赞1收藏评论"], "art_url": ["http://python.jobbole.com/12649/"]}
{"art_img": ["/wp-content/uploads/vb/765-thumb_dip.jpg"], "art_title": ["9本免费的Python编程书"], "art_create_time": ["2013/04/02"], "art_content": ["原文出处：Linuxtoy   本文将向各位推荐9本免费的Python语言编程书籍，希望对你学习Python编程有所帮助。1.AByteofPythonbySwaroopCH十分简明的Python教程。“无论您刚接触电脑还是一个有经验的程序员，本书都将有助您学习使用Python语言。”包含Python2.x和Python3.0两个版本。2.DiveIntoPythonbyMarkPilgrim本书“是为有经验的程序员编写的一本Python书”，具有英文版、中文版等多个版本。在去年9月，该书还针对Python3进行了更新，新书名为《DiveIntoPython3》。3.BuildingSkillsinPythonbyStevenF.Lott这本500多页的书包含42章，通过一系列的练习来帮助你学习Python编程。本书有HTML和PDF格式。4.PythonStandardLibrarybyFredrikLundh该书对Python标准库进行了介绍，并提供范例脚本参考。你可以在这里找到。5.HowtoThinkLikeaComputerScientistbyJeffreyElkner,AllenB.Downey,andChrisMeyers此书通过Python语言来教你如何进行编程。在线阅读。6.InventYourOwnComputerGameswithPythonbyAlbertSweigart通过其它的书学习编程，你可能会感觉枯燥，但这一本不会。该书通过创建游戏的方式来教你学习Python编程，非常有趣。可在线阅读或下载PDF。7.TextProcessinginPythonbyDavidMertz如果你想用Python进行文本处理的话，那么这本书将为你提供全面而有用的参考。TXT格式下载。8.TheDefinitiveGuidetoDjango:WebDevelopmentDoneRightbyAdrianHolovaty,JacobKaplan-Moss关于Django这个Web框架的权威指南。在线版。9.TheDefinitiveGuidetoPylonsJamesGardner另一个Web框架Pylons的权威指南。在线阅读。1赞1收藏4评论"], "art_url": ["http://python.jobbole.com/765/"]}
{"art_img": ["http://jbcdn2.b0.upaiyun.com/2012/07/Python-code-performance-optimization-techniques.png"], "art_title": ["Python 代码性能优化技巧"], "art_create_time": ["2013/04/06"], "art_content": ["来源：张颖@developerworks代码优化能够让程序运行更快，它是在不改变程序运行结果的情况下使得程序的运行效率更高，根据80/20原则，实现程序的重构、优化、扩展以及文档相关的事情通常需要消耗80%的工作量。优化通常包含两方面的内容：减小代码的体积，提高代码的运行效率。改进算法，选择合适的数据结构一个良好的算法能够对性能起到关键作用，因此性能改进的首要点是对算法的改进。在算法的时间复杂度排序上依次是：O(1)->O(lgn)->O(nlgn)->O(n^2)->O(n^3)->O(n^k)->O(k^n)->O(n!)因此如果能够在时间复杂度上对算法进行一定的改进，对性能的提高不言而喻。但对具体算法的改进不属于本文讨论的范围，读者可以自行参考这方面资料。下面的内容将集中讨论数据结构的选择。●字典(dictionary)与列表(list)Python字典中使用了hashtable，因此查找操作的复杂度为O(1)，而list实际是个数组，在list中，查找需要遍历整个list，其复杂度为O(n)，因此对成员的查找访问等操作字典要比list更快。清单1.代码dict.pyPythonfromtimeimporttimet=time()list=['a','b','is','python','jason','hello','hill','with','phone','test','dfdf','apple','pddf','ind','basic','none','baecr','var','bana','dd','wrd']#list=dict.fromkeys(list,True)printlistfilter=[]foriinrange(1000000):forfindin['is','hat','new','list','old','.']:iffindnotinlist:filter.append(find)print\"totalruntime:\"printtime()-t12345678910111213fromtimeimporttimet=time()list=['a','b','is','python','jason','hello','hill','with','phone','test','dfdf','apple','pddf','ind','basic','none','baecr','var','bana','dd','wrd']#list=dict.fromkeys(list,True)printlistfilter=[]foriinrange(1000000):forfindin['is','hat','new','list','old','.']:iffindnotinlist:filter.append(find)print\"totalruntime:\"printtime()-t上述代码运行大概需要16.09seconds。如果去掉行#list=dict.fromkeys(list,True)的注释，将list转换为字典之后再运行，时间大约为8.375seconds，效率大概提高了一半。因此在需要多数据成员进行频繁的查找或者访问的时候，使用dict而不是list是一个较好的选择。●集合(set)与列表(list)set的union，intersection，difference操作要比list的迭代要快。因此如果涉及到求list交集，并集或者差的问题可以转换为set来操作。清单2.求list的交集：Pythonfromtimeimporttimet=time()lista=[1,2,3,4,5,6,7,8,9,13,34,53,42,44]listb=[2,4,6,9,23]intersection=[]foriinrange(1000000):forainlista:forbinlistb:ifa==b:intersection.append(a)print\"totalruntime:\"printtime()-t12345678910111213fromtimeimporttimet=time()lista=[1,2,3,4,5,6,7,8,9,13,34,53,42,44]listb=[2,4,6,9,23]intersection=[]foriinrange(1000000):forainlista:forbinlistb:ifa==b:intersection.append(a) print\"totalruntime:\"printtime()-t上述程序的运行时间大概为：totalruntime:38.4070000648清单3.使用set求交集Pythonfromtimeimporttimet=time()lista=[1,2,3,4,5,6,7,8,9,13,34,53,42,44]listb=[2,4,6,9,23]intersection=[]foriinrange(1000000):list(set(lista)&set(listb))print\"totalruntime:\"printtime()-t123456789fromtimeimporttimet=time()lista=[1,2,3,4,5,6,7,8,9,13,34,53,42,44]listb=[2,4,6,9,23]intersection=[]foriinrange(1000000):list(set(lista)&set(listb))print\"totalruntime:\"printtime()-t改为set后程序的运行时间缩减为8.75，提高了4倍多，运行时间大大缩短。读者可以自行使用表1其他的操作进行测试。表1.set常见用法语法                      操作             说明set(list1)|set(list2)      union           包含list1和list2所有数据的新集合set(list1)&set(list2)      intersection     包含list1和list2中共同元素的新集合set(list1)–set(list2)      difference       在list1中出现但不在list2中出现的元素的集合对循环的优化对循环的优化所遵循的原则是尽量减少循环过程中的计算量，有多重循环的尽量将内层的计算提到上一层。下面通过实例来对比循环优化后所带来的性能的提高。程序清单4中，如果不进行循环优化，其大概的运行时间约为132.375。清单4.为进行循环优化前Pythonfromtimeimporttimet=time()lista=[1,2,3,4,5,6,7,8,9,10]listb=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.01]foriinrange(1000000):forainrange(len(lista)):forbinrange(len(listb)):x=lista[a]+listb[b]print\"totalruntime:\"printtime()-t12345678910fromtimeimporttimet=time()lista=[1,2,3,4,5,6,7,8,9,10]listb=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.01]foriinrange(1000000):forainrange(len(lista)):forbinrange(len(listb)):x=lista[a]+listb[b]print\"totalruntime:\"printtime()-t现在进行如下优化，将长度计算提到循环外，range用xrange代替，同时将第三层的计算lista[a]提到循环的第二层。清单5.循环优化后Pythonfromtimeimporttimet=time()lista=[1,2,3,4,5,6,7,8,9,10]listb=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.01]len1=len(lista)len2=len(listb)foriinxrange(1000000):forainxrange(len1):temp=lista[a]forbinxrange(len2):x=temp+listb[b]print\"totalruntime:\"printtime()-t12345678910111213fromtimeimporttimet=time()lista=[1,2,3,4,5,6,7,8,9,10]listb=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.01]len1=len(lista)len2=len(listb)foriinxrange(1000000):forainxrange(len1):temp=lista[a]forbinxrange(len2):x=temp+listb[b]print\"totalruntime:\"printtime()-t上述优化后的程序其运行时间缩短为102.171999931。在清单4中lista[a]被计算的次数为1000000*10*10，而在优化后的代码中被计算的次数为1000000*10，计算次数大幅度缩短，因此性能有所提升。充分利用Lazyif-evaluation的特性python中条件表达式是lazyevaluation的，也就是说如果存在条件表达式ifxandy，在x为false的情况下y表达式的值将不再计算。因此可以利用该特性在一定程度上提高程序效率。清单6.利用Lazyif-evaluation的特性Pythonfromtimeimporttimet=time()abbreviations=['cf.','e.g.','ex.','etc.','fig.','i.e.','Mr.','vs.']foriinrange(1000000):forwin('Mr.','Hat','is','chasing','the','black','cat','.'):ifwinabbreviations:#ifw[-1]=='.'andwinabbreviations:passprint\"totalruntime:\"printtime()-t12345678910fromtimeimporttimet=time()abbreviations=['cf.','e.g.','ex.','etc.','fig.','i.e.','Mr.','vs.']foriinrange(1000000):forwin('Mr.','Hat','is','chasing','the','black','cat','.'):ifwinabbreviations:#ifw[-1]=='.'andwinabbreviations:passprint\"totalruntime:\"printtime()-t在未进行优化之前程序的运行时间大概为8.84，如果使用注释行代替第一个if，运行的时间大概为6.17。字符串的优化python中的字符串对象是不可改变的，因此对任何字符串的操作如拼接，修改等都将产生一个新的字符串对象，而不是基于原字符串，因此这种持续的copy会在一定程度上影响python的性能。对字符串的优化也是改善性能的一个重要的方面，特别是在处理文本较多的情况下。字符串的优化主要集中在以下几个方面：1、在字符串连接的使用尽量使用join()而不是+：在代码清单7中使用+进行字符串连接大概需要0.125s，而使用join缩短为0.016s。因此在字符的操作上join比+要快，因此要尽量使用join而不是+。清单7.使用join而不是+连接字符串Python<spanstyle=\"font-family:Monaco,Consolas,'AndaleMono','DejaVuSansMono',monospace;font-style:normal;\">fromtimeimporttime</span>t=time()s=\"\"list=['a','b','b','d','e','f','g','h','i','j','k','l','m','n']foriinrange(10000):forsubstrinlist:s+=substrprint\"totalruntime:\"printtime()-t12345678910<spanstyle=\"font-family:Monaco,Consolas,'AndaleMono','DejaVuSansMono',monospace;font-style:normal;\">fromtimeimporttime</span> t=time()s=\"\"list=['a','b','b','d','e','f','g','h','i','j','k','l','m','n']foriinrange(10000):forsubstrinlist:s+=substrprint\"totalruntime:\"printtime()-t同时要避免：Pythons=&quot;&quot;forxinlist:s+=func(x)123s=&quot;&quot;forxinlist:    s+=func(x)而是要使用：Pythonslist=[func(elt)foreltinsomelist]s=&quot;&quot;.join(slist)12slist=[func(elt)foreltinsomelist]s=&quot;&quot;.join(slist)2、当对字符串可以使用正则表达式或者内置函数来处理的时候，选择内置函数。如str.isalpha()，str.isdigit()，str.startswith((‘x’,‘yz’))，str.endswith((‘x’,‘yz’))3、对字符进行格式化比直接串联读取要快，因此要使用Pythonout=\"%s%s%s%s\"%(head,prologue,query,tail)1out=\"%s%s%s%s\"%(head,prologue,query,tail)而避免Pythonout=\"\"+head+prologue+query+tail+\"\"1out=\"\"+head+prologue+query+tail+\"\"使用列表解析（listcomprehension）和生成器表达式（generatorexpression）列表解析要比在循环中重新构建一个新的list更为高效，因此我们可以利用这一特性来提高运行的效率。Pythonfromtimeimporttimet=time()list=['a','b','is','python','jason','hello','hill','with','phone','test','dfdf','apple','pddf','ind','basic','none','baecr','var','bana','dd','wrd']total=[]foriinrange(1000000):forwinlist:total.append(w)print\"totalruntime:\"printtime()-t12345678910fromtimeimporttimet=time()list=['a','b','is','python','jason','hello','hill','with','phone','test','dfdf','apple','pddf','ind','basic','none','baecr','var','bana','dd','wrd']total=[]foriinrange(1000000):forwinlist:total.append(w)print\"totalruntime:\"printtime()-t使用列表解析：Pythonforiinrange(1000000):a=[wforwinlist]12foriinrange(1000000):a=[wforwinlist]上述代码直接运行大概需要17s，而改为使用列表解析后，运行时间缩短为9.29s。将近提高了一半。生成器表达式则是在2.4中引入的新内容，语法和列表解析类似，但是在大数据量处理时，生成器表达式的优势较为明显，它并不创建一个列表，只是返回一个生成器，因此效率较高。在上述例子上中代码a=[wforwinlist]修改为a=(wforwinlist)，运行时间进一步减少，缩短约为2.98s。其他优化技巧1、如果需要交换两个变量的值使用a,b=b,a而不是借助中间变量t=a;a=b;b=t；Python&gt;&gt;&gt;fromtimeitimportTimer&gt;&gt;&gt;Timer(&quot;t=a;a=b;b=t&quot;,&quot;a=1;b=2&quot;).timeit()0.25154118749729365&gt;&gt;&gt;Timer(&quot;a,b=b,a&quot;,&quot;a=1;b=2&quot;).timeit()0.17156677734181258&gt;&gt;&gt;123456&gt;&gt;&gt;fromtimeitimportTimer&gt;&gt;&gt;Timer(&quot;t=a;a=b;b=t&quot;,&quot;a=1;b=2&quot;).timeit()0.25154118749729365&gt;&gt;&gt;Timer(&quot;a,b=b,a&quot;,&quot;a=1;b=2&quot;).timeit()0.17156677734181258&gt;&gt;&gt;2、在循环的时候使用xrange而不是range；使用xrange可以节省大量的系统内存，因为xrange()在序列中每次调用只产生一个整数元素。而range()將直接返回完整的元素列表，用于循环时会有不必要的开销。在python3中xrange不再存在，里面range提供一个可以遍历任意长度的范围的iterator。3、使用局部变量，避免”global”关键字。python访问局部变量会比全局变量要快得多，因此可以利用这一特性提升性能。4、ifdoneisnotNone比语句ifdone!=None更快，读者可以自行验证；5、在耗时较多的循环中，可以把函数的调用改为内联的方式；6、使用级联比较“x<y<z”而不是“x<yandy<z”；7、while1要比whileTrue更快（当然后者的可读性更好）；8、buildin函数通常较快，add(a,b)要优于a+b。定位程序性能瓶颈对代码优化的前提是需要了解性能瓶颈在什么地方，程序运行的主要时间是消耗在哪里，对于比较复杂的代码可以借助一些工具来定位，python内置了丰富的性能分析工具，如profile,cProfile与hotshot等。其中Profiler是python自带的一组程序，能够描述程序运行时候的性能，并提供各种统计帮助用户定位程序的性能瓶颈。Python标准模块提供三种profilers:cProfile,profile以及hotshot。profile的使用非常简单，只需要在使用之前进行import即可。具体实例如下：清单8.使用profile进行性能分析PythonimportprofiledefprofileTest():Total=1;foriinrange(10):Total=Total*(i+1)printTotalreturnTotalif__name__==\"__main__\":profile.run(\"profileTest()\")123456789importprofiledefprofileTest():    Total=1;    foriinrange(10):        Total=Total*(i+1)        printTotal    returnTotalif__name__==\"__main__\":    profile.run(\"profileTest()\")程序的运行结果如下：图1.性能分析结果其中输出每列的具体解释如下：●ncalls：表示函数调用的次数；●tottime：表示指定函数的总的运行时间，除掉函数中调用子函数的运行时间；●percall：（第一个percall）等于tottime/ncalls；●cumtime：表示该函数及其所有子函数的调用运行的时间，即函数开始调用到返回的时间；●percall：（第二个percall）即函数运行一次的平均时间，等于cumtime/ncalls；●filename:lineno(function)：每个函数调用的具体信息；如果需要将输出以日志的形式保存，只需要在调用的时候加入另外一个参数。如profile.run(“profileTest()”,”testprof”)。对于profile的剖析数据，如果以二进制文件的时候保存结果的时候，可以通过pstats模块进行文本报表分析，它支持多种形式的报表输出，是文本界面下一个较为实用的工具。使用非常简单：Pythonimportpstatsp=pstats.Stats('testprof')p.sort_stats(\"name\").print_stats()123importpstatsp=pstats.Stats('testprof')p.sort_stats(\"name\").print_stats()其中sort_stats()方法能够对剖分数据进行排序，可以接受多个排序字段，如sort_stats(‘name’,‘file’)将首先按照函数名称进行排序，然后再按照文件名进行排序。常见的排序字段有calls(被调用的次数)，time（函数内部运行时间），cumulative（运行的总时间）等。此外pstats也提供了命令行交互工具，执行python–mpstats后可以通过help了解更多使用方式。对于大型应用程序，如果能够将性能分析的结果以图形的方式呈现，将会非常实用和直观，常见的可视化工具有Gprof2Dot，visualpytune，KCacheGrind等，读者可以自行查阅相关官网，本文不做详细讨论。Python性能优化工具Python性能优化除了改进算法，选用合适的数据结构之外，还有几种关键的技术，比如将关键python代码部分重写成C扩展模块，或者选用在性能上更为优化的解释器等，这些在本文中统称为优化工具。python有很多自带的优化工具，如Psyco，Pypy，Cython，Pyrex等，这些优化工具各有千秋，本节选择几种进行介绍。Psycopsyco是一个just-in-time的编译器，它能够在不改变源代码的情况下提高一定的性能，Psyco将操作编译成有点优化的机器码，其操作分成三个不同的级别，有”运行时”、”编译时”和”虚拟时”变量。并根据需要提高和降低变量的级别。运行时变量只是常规Python解释器处理的原始字节码和对象结构。一旦Psyco将操作编译成机器码，那么编译时变量就会在机器寄存器和可直接访问的内存位置中表示。同时python能高速缓存已编译的机器码以备今后重用，这样能节省一点时间。但Psyco也有其缺点，其本身运行所占内存较大。目前psyco已经不在python2.7中支持，而且不再提供维护和更新了，对其感兴趣的可以参考 http://psyco.sourceforge.net/PypyPyPy表示“用Python实现的Python”，但实际上它是使用一个称为RPython的Python子集实现的，能够将Python代码转成C，.NET，Java等语言和平台的代码。PyPy集成了一种即时(JIT)编译器。和许多编译器，解释器不同，它不关心Python代码的词法分析和语法树。因为它是用Python语言写的，所以它直接利用Python语言的CodeObject.。CodeObject是Python字节码的表示，也就是说，PyPy直接分析Python代码所对应的字节码,，这些字节码即不是以字符形式也不是以某种二进制格式保存在文件中，而在Python运行环境中。目前版本是1.8.支持不同的平台安装，windows上安装Pypy需要先下载https://bitbucket.org/pypy/pypy/downloads/pypy-1.8-win32.zip，然后解压到相关的目录，并将解压后的路径添加到环境变量path中即可。在命令行运行pypy，如果出现如下错误：”没有找到MSVCR100.dll,因此这个应用程序未能启动，重新安装应用程序可能会修复此问题”，则还需要在微软的官网上下载VS2010runtimelibraries解决该问题。具体地址为http://www.microsoft.com/download/en/details.aspx?displaylang=en&id=5555安装成功后在命令行里运行pypy，输出结果如下：PythonC:\\DocumentsandSettings\\Administrator>pypyPython2.7.2(0e28b379d8b3,Feb092012,18:31:47)[PyPy1.8.0withMSCv.150032bit]onwin32Type\"help\",\"copyright\",\"credits\"or\"license\"formoreinformation.Andnowforsomethingcompletelydifferent:``PyPyisvast,andcontainsmultitudes''>>>>1234567C:\\DocumentsandSettings\\Administrator>pypyPython2.7.2(0e28b379d8b3,Feb092012,18:31:47)[PyPy1.8.0withMSCv.150032bit]onwin32Type\"help\",\"copyright\",\"credits\"or\"license\"formoreinformation.Andnowforsomethingcompletelydifferent:``PyPyisvast,andcontainsmultitudes''>>>>以清单5的循环为例子，使用python和pypy分别运行，得到的运行结果分别如下：PythonC:\\DocumentsandSettings\\Administrator\\桌面\\doc\\python>pypyloop.pytotalruntime:8.42199993134C:\\DocumentsandSettings\\Administrator\\桌面\\doc\\python>pythonloop.pytotalruntime:106.391000032123456C:\\DocumentsandSettings\\Administrator\\桌面\\doc\\python>pypyloop.pytotalruntime:8.42199993134C:\\DocumentsandSettings\\Administrator\\桌面\\doc\\python>pythonloop.pytotalruntime:106.391000032 可见使用pypy来编译和运行程序，其效率大大的提高。CythonCython是用python实现的一种语言，可以用来写python扩展，用它写出来的库都可以通过import来载入，性能上比python的快。cython里可以载入python扩展(比如importmath)，也可以载入c的库的头文件(比如:cdefexternfrom“math.h”)，另外也可以用它来写python代码。将关键部分重写成C扩展模块LinuxCpython的安装：第一步：下载Python[root@v5254085f259cpython]#wget-Nhttp://cython.org/release/Cython-0.15.1.zip--2012-04-1622:08:35--http://cython.org/release/Cython-0.15.1.zipResolvingcython.org...128.208.160.197Connectingtocython.org|128.208.160.197|:80...connected.HTTPrequestsent,awaitingresponse...200OKLength:2200299(2.1M)[application/zip]Savingto:`Cython-0.15.1.zip&#039;100%[======================================&gt;]2,200,2991.96M/sin1.1s2012-04-1622:08:37(1.96MB/s)-`Cython-0.15.1.zip&#039;saved[2200299/2200299]123456789[root@v5254085f259cpython]#wget-Nhttp://cython.org/release/Cython-0.15.1.zip--2012-04-1622:08:35--http://cython.org/release/Cython-0.15.1.zipResolvingcython.org...128.208.160.197Connectingtocython.org|128.208.160.197|:80...connected.HTTPrequestsent,awaitingresponse...200OKLength:2200299(2.1M)[application/zip]Savingto:`Cython-0.15.1.zip&#039;100%[======================================&gt;]2,200,2991.96M/sin1.1s2012-04-1622:08:37(1.96MB/s)-`Cython-0.15.1.zip&#039;saved[2200299/2200299] 第二步：解压Python[root@v5254085f259cpython]#unzip-oCython-0.15.1.zip1[root@v5254085f259cpython]#unzip-oCython-0.15.1.zip第三步：安装Pythonpythonsetup.pyinstall1pythonsetup.pyinstall安装完成后直接输入cython，如果出现如下内容则表明安装成功。Python[root@v5254085f259Cython-0.15.1]#cythonCython(http://cython.org)isacompilerforcodewrittenintheCythonlanguage.CythonisbasedonPyrexbyGregEwing.Usage:cython[options]sourcefile.{pyx,py}...Options:-V,--versionDisplayversionnumberofcythoncompiler-l,--create-listingWriteerrormessagestoalistingfile-I,--include-dir<directory>Searchforincludefilesinnameddirectory(multipleincludedirectoriesareallowed).-o,--output-file<filename>SpecifynameofgeneratedCfile-t,--timestampsOnlycompilenewersourcefiles-f,--forceCompileallsourcefiles(overridesimplied-t)-q,--quietDon'tprintmodulenamesinrecursivemode-v,--verboseBeverbose,printfilenamesonmultiplecompilation-p,--embed-positionsIfspecified,thepositionsinCythonfilesofeachfunctiondefinitionisembeddedinitsdocstring.--cleanup<level>Releaseinternedobjectsonpythonexit,formemorydebugging.Levelindicatesaggressiveness,default0releasesnothing.-w,--working<directory>SetstheworkingdirectoryforCython(thedirectorymodulesaresearchedfrom)--gdbOutputdebuginformationforcygdb-D,--no-docstringsStripdocstringsfromthecompiledmodule.-a,--annotateProduceacolorizedHTMLversionofthesource.--line-directivesProduce#linedirectivespointingtothe.pyxsource--cplusOutputaC++ratherthanCfile.--embed[=<method_name>]Generateamain()functionthatembedsthePythoninterpreter.-2CompilebasedonPython-2syntaxandcodesemantics.-3CompilebasedonPython-3syntaxandcodesemantics.--fast-failAbortthecompilationonthefirsterror--warning-error,-WerrorMakeallwarningsintoerrors--warning-extra,-WextraEnableextrawarnings-X,--directive<name>=<value>[,<name=value,...]Overridesacompilerdirective1234567891011121314151617181920212223242526272829303132333435363738394041[root@v5254085f259Cython-0.15.1]#cythonCython(http://cython.org)isacompilerforcodewrittenintheCythonlanguage.  CythonisbasedonPyrexbyGregEwing. Usage:cython[options]sourcefile.{pyx,py}... Options:  -V,--version                  Displayversionnumberofcythoncompiler  -l,--create-listing          Writeerrormessagestoalistingfile  -I,--include-dir<directory>  Searchforincludefilesinnameddirectory                                (multipleincludedirectoriesareallowed).  -o,--output-file<filename>  SpecifynameofgeneratedCfile  -t,--timestamps              Onlycompilenewersourcefiles  -f,--force                    Compileallsourcefiles(overridesimplied-t)  -q,--quiet                    Don'tprintmodulenamesinrecursivemode  -v,--verbose                  Beverbose,printfilenamesonmultiplecompilation  -p,--embed-positions          Ifspecified,thepositionsinCythonfilesofeach  functiondefinitionisembeddedinitsdocstring.  --cleanup<level>  Releaseinternedobjectsonpythonexit,formemorydebugging.    Levelindicatesaggressiveness,default0releasesnothing.  -w,--working<directory>  SetstheworkingdirectoryforCython(thedirectorymodulesaresearchedfrom)  --gdbOutputdebuginformationforcygdb  -D,--no-docstrings              Stripdocstringsfromthecompiledmodule.  -a,--annotate              ProduceacolorizedHTMLversionofthesource.  --line-directives              Produce#linedirectivespointingtothe.pyxsource  --cplus              OutputaC++ratherthanCfile.  --embed[=<method_name>]              Generateamain()functionthatembedsthePythoninterpreter.  -2          CompilebasedonPython-2syntaxandcodesemantics.  -3          CompilebasedonPython-3syntaxandcodesemantics.  --fast-fail    Abortthecompilationonthefirsterror  --warning-error,-Werror      Makeallwarningsintoerrors  --warning-extra,-Wextra      Enableextrawarnings  -X,--directive<name>=<value>  [,<name=value,...]Overridesacompilerdirective其他平台上的安装可以参考文档：http://docs.cython.org/src/quickstart/install.htmlCython代码与python不同，必须先编译，编译一般需要经过两个阶段，将pyx文件编译为.c文件，再将.c文件编译为.so文件。编译有多种方法：●通过命令行编译：假设有如下测试代码，使用命令行编译为.c文件。Pythondefsum(inta,intb):printa+b[root@v5254085f259test]#cythonsum.pyx[root@v5254085f259test]#lstotal764drwxr-xr-x2rootroot4096Apr1702:45.4drwxr-xr-x4rootroot4096Apr1622:20..4-rw-r--r--1rootroot35Apr1702:45160-rw-r--r--1rootroot55169Apr1702:45sum.c4-rw-r--r--1rootroot35Apr1702:45sum.pyx1234567891011defsum(inta,intb):        printa+b [root@v5254085f259test]#cythonsum.pyx[root@v5254085f259test]#lstotal764drwxr-xr-x2rootroot  4096Apr1702:45.4drwxr-xr-x4rootroot  4096Apr1622:20..4-rw-r--r--1rootroot    35Apr1702:45160-rw-r--r--1rootroot55169Apr1702:45sum.c4-rw-r--r--1rootroot    35Apr1702:45sum.pyx在linux上利用gcc编译为.so文件：Python[root@v5254085f259test]#gcc-shared-pthread-fPIC-fwrapv-O2-Wall-fno-strict-aliasing-I/usr/include/python2.4-osum.sosum.c[root@v5254085f259test]#lstotal964drwxr-xr-x2rootroot4096Apr1702:47.4drwxr-xr-x4rootroot4096Apr1622:20..4-rw-r--r--1rootroot35Apr1702:45160-rw-r--r--1rootroot55169Apr1702:45sum.c4-rw-r--r--1rootroot35Apr1702:45sum.pyx20-rwxr-xr-x1rootroot20307Apr1702:47sum.so12345678910[root@v5254085f259test]#gcc-shared-pthread-fPIC-fwrapv-O2-Wall-fno-strict-aliasing-I/usr/include/python2.4-osum.sosum.c[root@v5254085f259test]#lstotal964drwxr-xr-x2rootroot  4096Apr1702:47.4drwxr-xr-x4rootroot  4096Apr1622:20..4-rw-r--r--1rootroot    35Apr1702:45160-rw-r--r--1rootroot55169Apr1702:45sum.c4-rw-r--r--1rootroot    35Apr1702:45sum.pyx20-rwxr-xr-x1rootroot20307Apr1702:47sum.soPython●使用distutils编译1●使用distutils编译建立一个setup.py的脚本：Pythonfromdistutils.coreimportsetupfromdistutils.extensionimportExtensionfromCython.Distutilsimportbuild_extext_modules=[Extension(\"sum\",[\"sum.pyx\"])]setup(name='sumapp',cmdclass={'build_ext':build_ext},ext_modules=ext_modules)[root@v5254085f259test]#pythonsetup.pybuild_ext--inplacerunningbuild_extcythoningsum.pyxtosum.cbuilding'sum'extensiongcc-pthread-fno-strict-aliasing-fPIC-g-O2-DNDEBUG-g-fwrapv-O3-Wall-Wstrict-prototypes-fPIC-I/opt/ActivePython-2.7/include/python2.7-csum.c-obuild/temp.linux-x86_64-2.7/sum.ogcc-pthread-sharedbuild/temp.linux-x86_64-2.7/sum.o-o/root/cpython/test/sum.so123456789101112131415161718192021fromdistutils.coreimportsetupfromdistutils.extensionimportExtensionfromCython.Distutilsimportbuild_ext ext_modules=[Extension(\"sum\",[\"sum.pyx\"])] setup(    name='sumapp',    cmdclass={'build_ext':build_ext},    ext_modules=ext_modules) [root@v5254085f259test]#  pythonsetup.pybuild_ext--inplacerunningbuild_extcythoningsum.pyxtosum.cbuilding'sum'extensiongcc-pthread-fno-strict-aliasing-fPIC-g-O2-DNDEBUG-g-fwrapv-O3-Wall-Wstrict-prototypes-fPIC-I/opt/ActivePython-2.7/include/python2.7  -csum.c-obuild/temp.linux-x86_64-2.7/sum.ogcc-pthread-sharedbuild/temp.linux-x86_64-2.7/sum.o-o/root/cpython/test/sum.so编译完成之后可以导入到python中使用：Python[root@v5254085f259test]#pythonActivePython2.7.2.5(ActiveStateSoftwareInc.)basedonPython2.7.2(default,Jun242011,11:24:26)[GCC4.0.220051125(RedHat4.0.2-8)]onlinux2Type\"help\",\"copyright\",\"credits\"or\"license\"formoreinformation.>>>importpyximport;pyximport.install()>>>importsum>>>sum.sum(1,3)12345678[root@v5254085f259test]#pythonActivePython2.7.2.5(ActiveStateSoftwareInc.)basedonPython2.7.2(default,Jun242011,11:24:26)[GCC4.0.220051125(RedHat4.0.2-8)]onlinux2Type\"help\",\"copyright\",\"credits\"or\"license\"formoreinformation.>>>importpyximport;pyximport.install()>>>importsum>>>sum.sum(1,3)下面来进行一个简单的性能比较：清单9.Cython测试代码Pythonfromtimeimporttimedeftest(intn):cdefinta=0cdefintiforiinxrange(n):a+=ireturnat=time()test(10000000)print\"totalruntime:\"printtime()-t123456789101112fromtimeimporttimedeftest(intn):        cdefinta=0        cdefinti        foriinxrange(n):                a+=i        returna t=time()test(10000000)print\"totalruntime:\"printtime()-t测试结果：Python[GCC4.0.220051125(RedHat4.0.2-8)]onlinux2Type&quot;help&quot;,&quot;copyright&quot;,&quot;credits&quot;or&quot;license&quot;formoreinformation.&gt;&gt;&gt;importpyximport;pyximport.install()&gt;&gt;&gt;importctesttotalruntime:0.00714015960693123456[GCC4.0.220051125(RedHat4.0.2-8)]onlinux2Type&quot;help&quot;,&quot;copyright&quot;,&quot;credits&quot;or&quot;license&quot;formoreinformation.&gt;&gt;&gt;importpyximport;pyximport.install()&gt;&gt;&gt;importctesttotalruntime:0.00714015960693清单10.Python测试代码Pythonfromtimeimporttimedeftest(n):a=0;foriinxrange(n):a+=ireturnat=time()test(10000000)print\"totalruntime:\"printtime()-t[root@v5254085f259test]#pythontest.pytotalruntime:0.971596002579123456789101112131415fromtimeimporttimedeftest(n):        a=0;        foriinxrange(n):                a+=i        returna t=time()test(10000000)print\"totalruntime:\"printtime()-t [root@v5254085f259test]#pythontest.pytotalruntime:0.971596002579 从上述对比可以看到使用Cython的速度提高了将近100多倍。 总结本文初步探讨了python常见的性能优化技巧以及如何借助工具来定位和分析程序的性能瓶颈，并提供了相关可以进行性能优化的工具或语言，希望能够更相关人员一些参考。1赞2收藏评论"], "art_url": ["http://python.jobbole.com/24197/"]}
{"art_img": ["http://jbcdn2.b0.upaiyun.com/2012/05/import_python.jpg"], "art_title": ["Python编程中需要注意的一些事"], "art_create_time": ["2013/04/15"], "art_content": ["本文由伯乐在线-刘志军翻译。未经许可，禁止转载！英文出处：SatyajitRanjeev。欢迎加入翻译组。围绕一门语言，学习它的文化精髓，能让你成为一名更优秀的程序员。如果你还没读过Python之禅(ZenofPython)，那么打开Python的命令提示符输入importthis，列表中的每一项你都可以在这里找到相对应的例子。（Credit： itswater）吸引我注意力的一条是：优雅胜于丑陋(Beautifulisbetterthanugly)看下面例子：一个带有数字参数的list函数其功能是返回参数中的奇数可以分开写：Python#-----------------------------------------------------------------------halve_evens_only=lambdanums:map(lambdai:i/2,\\filter(lambdai:noti%2,nums))#-----------------------------------------------------------------------defhalve_evens_only(nums):return[i/2foriinnumsifnoti%2]123456  #-----------------------------------------------------------------------  halve_evens_only=lambdanums:map(lambdai:i/2,\\  filter(lambdai:noti%2,nums))  #-----------------------------------------------------------------------defhalve_evens_only(nums):      return[i/2foriinnumsifnoti%2]记住Python中那些非常简单的事两个变量的交换：Pythona,b=b,a1a,b=b,a参数在切片操作中的步骤，如：Pythona=[1,2,3,4,5]>>>a[::2]#以步长为2的增量迭代整个list对象[1,3,5]123      a=[1,2,3,4,5]      >>>a[::2]  #以步长为2的增量迭代整个list对象      [1,3,5]Python一个特殊的例子`x[::-1]`用来反转x的实用语法。1一个特殊的例子`x[::-1]`用来反转x的实用语法。1Python>>>a[::-1][5,4,3,2,1]12    >>>a[::-1]      [5,4,3,2,1]不要用可变对象作为默认参数值(Don’tusemutableasdefaults)Pythondeffunction(x,l=[]):#不要这么干deffunction(x,l=None):#更好的一种方式iflisNone:l=[]1234deffunction(x,l=[]):          #不要这么干deffunction(x,l=None):        #更好的一种方式    iflisNone:      l=[]使用iteritems而不是itemsiteriterms使用的是generators，所以当迭代很大的序列是此方法更好1Pythond={1:\"1\",2:\"2\",3:\"3\"}forkey,valind.items()#调用items()后会构建一个完整的list对象forkey,valind.iteritems()#只有在迭代时每请求一次才生成一个值12345d={1:\"1\",2:\"2\",3:\"3\"} forkey,valind.items()      #调用items()后会构建一个完整的list对象 forkey,valind.iteritems()  #只有在迭代时每请求一次才生成一个值此情景和range与xrange的关系相似。使用isinstance而不是type不要这样做：1Pythoniftype(s)==type(\"\"):...iftype(seq)==listor\\type(seq)==tuple:...123iftype(s)==type(\"\"):...iftype(seq)==listor\\    type(seq)==tuple:...应该是这样：1Pythonifisinstance(s,basestring):...ifisinstance(seq,(list,tuple)):...12ifisinstance(s,basestring):...ifisinstance(seq,(list,tuple)):...至于为什么这样做，看这里：http://stackoverflow.com/a/1549854/504262需要注意的是这里使用basestring而不是str是因为你可能会用一个unicode对象去检查是否为string,例如：1Python>>>a=u'aaaa'>>>printisinstance(a,basestring)True>>>printisinstance(a,str)False12345  >>>a=u'aaaa'  >>>printisinstance(a,basestring)  True  >>>printisinstance(a,str)  False因为在Python中3.0以下的版本存在两种字符串类型str和unicodePython       object1       objectPython         |1         |Python      basestring1      basestringPython         /\\1         /\\Python      str unicode1      str unicode学习各种集合(learnthevariouscollections)python有各种各样的容器数据类型，在特定情况下选择python内建的容器如：list和dict。通常更多像如下方式使用：Pythonfreqs={}forcin\"abracadabra\":try:freqs[c]+=1except:freqs[c]=1123456  freqs={}  forcin\"abracadabra\":      try:          freqs[c]+=1      except:          freqs[c]=1一种更好的方案如下：Pythonfreqs={}forcin\"abracadabra\":freqs[c]=freqs.get(c,0)+1123freqs={}  forcin\"abracadabra\":      freqs[c]=freqs.get(c,0)+1一种更好的选择collection类型defautdict：Pythonfromcollectionsimportdefaultdictfreqs=defaultdict(int)forcin\"abracadabra\":freqs[c]+=11234fromcollectionsimportdefaultdictfreqs=defaultdict(int)    forcin\"abracadabra\":        freqs[c]+=1其它集合Pythonnamedtuple()#用指定的域创建元组子类的工厂函数deque#类似list的容器，快速追加以及删除在序列的两端Counter#统计哈希表的dict子类OrderedDict#记录实体添加顺序的dict子类defaultdict#调用工厂方法为key提供缺省值的dict子类12345  namedtuple()      #用指定的域创建元组子类的工厂函数  deque            #类似list的容器，快速追加以及删除在序列的两端  Counter          #统计哈希表的dict子类  OrderedDict            #记录实体添加顺序的dict子类  defaultdict            #调用工厂方法为key提供缺省值的dict子类当创建类时Python的魔术方法：Python__eq__(self,other)#定义相等操作的行为,==.__ne__(self,other)#定义不相等操作的行为,!=.__lt__(self,other)#定义小于操作的行为,<.__gt__(self,other)#定义不大于操作的行为,>.__le__(self,other)#定义小于等于操作的行为,<=.__ge__(self,other)#定义大于等于操作的行为,>=.123456  __eq__(self,other)      #定义相等操作的行为,==.  __ne__(self,other)      #定义不相等操作的行为,!=.  __lt__(self,other)      #定义小于操作的行为,<.  __gt__(self,other)      #定义不大于操作的行为,>.  __le__(self,other)      #定义小于等于操作的行为,<=.  __ge__(self,other)      #定义大于等于操作的行为,>=.条件赋值Pythonx=3if(y==1)else21x=3if(y==1)else2表达式请起来恰恰像：如果y等于1就把3赋值给x,否则把2赋值给x，当然同样可以使用链式条件赋值如果你还有更复杂的条件的话。Pythonx=3if(y==1)else2if(y==-1)else11x=3if(y==1)else2if(y==-1)else1然而到了某个特定的点，它就有点儿过分了。记住，你可以在任何表达式中使用if-else例如：Python(func1ify==1elsefunc2)(arg1,arg2)1(func1ify==1elsefunc2)(arg1,arg2)func1将被调用如果y等于1的话，反之func2被调用。两种情况下，arg1和arg2两个参数都将附带在相应的函数中。类似地，下面这个表达式同样是正确的Pythonx=(class1ify==1elseclass2)(arg1,arg2)1x=(class1ify==1elseclass2)(arg1,arg2)class1和class2是两个类在有必要的时侯使用Ellipsis创建类时，你可以使用__getitem__，让你的类像字典一个工作，拿下面这个类举例来说：PythonclassMyClass(object):def__init__(self,a,b,c,d):self.a,self.b,self.c,self.d=a,b,c,ddef__getitem__(self,item):returngetattr(self,item)x=MyClass(10,12,22,14)12345678  classMyClass(object):      def__init__(self,a,b,c,d):          self.a,self.b,self.c,self.d=a,b,c,d       def__getitem__(self,item):          returngetattr(self,item)   x=MyClass(10,12,22,14)因为有了__getitem__，你就能够通过对象x的x[‘a’]获取a的值，这应该是公认的事实。这个对象通常用于继承Python的切片(slicing)(http://docs.python.org/library/stdtypes.html#bltin-ellipsis-object)，如果添加如下语句：Pythondef__getitem__(self,item):ifitemisEllipsis:return[self.a,self.b,self.c,self.d]else:returngetattr(self,item)12345  def__getitem__(self,item):      ifitemisEllipsis:          return[self.a,self.b,self.c,self.d]      else:          returngetattr(self,item)我们就可以使用x[…]获取的包含所有项的序列Python>>>x=MyClass(11,34,23,12)>>>x[...][11,34,23,12]123  >>>x=MyClass(11,34,23,12)  >>>x[...]  [11,34,23,12]打赏支持我翻译更多好文章，谢谢！打赏译者打赏支持我翻译更多好文章，谢谢！1赞1收藏1评论关于作者：刘志军个人博客：http://foofish.net，微信公众号：Python之禅（ID：VTtalk）个人主页·我的文章·45·"], "art_url": ["http://python.jobbole.com/19835/"]}
{"art_img": ["http://jbcdn2.b0.upaiyun.com/2014/12/5b4b7a9f30c17adabfdb0974c60cc5c1.jpeg"], "art_title": ["给Python初学者的一些技巧"], "art_create_time": ["2013/05/09"], "art_content": ["本文由伯乐在线-刘志军翻译。未经许可，禁止转载！英文出处：MaxBurstein。欢迎加入翻译组。以下是我近些年收集的一些Python实用技巧和工具，希望能对你有所帮助。交换变量Pythonx=6y=5x,y=y,xprintx>>>5printy>>>6123456789x=6y=5 x,y=y,x printx>>>5printy>>>6if语句在行内Pythonprint\"Hello\"ifTrueelse\"World\">>>Hello12print\"Hello\"ifTrueelse\"World\">>>Hello连接下面的最后一种方式在绑定两个不同类型的对象时显得很cool。Pythonnfc=[\"Packers\",\"49ers\"]afc=[\"Ravens\",\"Patriots\"]printnfc+afc>>>['Packers','49ers','Ravens','Patriots']printstr(1)+\"world\">>>1worldprint`1`+\"world\">>>1worldprint1,\"world\">>>1worldprintnfc,1>>>['Packers','49ers']1123456789101112131415nfc=[\"Packers\",\"49ers\"]afc=[\"Ravens\",\"Patriots\"]printnfc+afc>>>['Packers','49ers','Ravens','Patriots'] printstr(1)+\"world\">>>1world print`1`+\"world\">>>1world print1,\"world\">>>1worldprintnfc,1>>>['Packers','49ers']1数字技巧Python#除后向下取整print5.0//2>>>2#2的5次方print2**5>>32123456#除后向下取整print5.0//2>>>2#2的5次方print2**5>>32注意浮点数的除法Pythonprint.3/.1>>>2.9999999999999996print.3//.1>>>2.01234print.3/.1>>>2.9999999999999996print.3//.1>>>2.0数值比较这是我见过诸多语言中很少有的如此棒的简便法Pythonx=2if3>x>1:printx>>>2if1<x>0:printx>>>21234567x=2if3>x>1:  printx>>>2if1<x>0:  printx>>>2同时迭代两个列表Pythonnfc=[\"Packers\",\"49ers\"]afc=[\"Ravens\",\"Patriots\"]forteama,teambinzip(nfc,afc):printteama+\"vs.\"+teamb>>>Packersvs.Ravens>>>49ersvs.Patriots123456nfc=[\"Packers\",\"49ers\"]afc=[\"Ravens\",\"Patriots\"]forteama,teambinzip(nfc,afc):    printteama+\"vs.\"+teamb>>>Packersvs.Ravens>>>49ersvs.Patriots带索引的列表迭代Pythonteams=[\"Packers\",\"49ers\",\"Ravens\",\"Patriots\"]forindex,teaminenumerate(teams):printindex,team>>>0Packers>>>149ers>>>2Ravens>>>3Patriots1234567teams=[\"Packers\",\"49ers\",\"Ravens\",\"Patriots\"]forindex,teaminenumerate(teams):    printindex,team>>>0Packers>>>149ers>>>2Ravens>>>3Patriots列表推导式已知一个列表，我们可以刷选出偶数列表方法：Pythonnumbers=[1,2,3,4,5,6]even=[]fornumberinnumbers:ifnumber%2==0:even.append(number)12345numbers=[1,2,3,4,5,6]even=[]fornumberinnumbers:    ifnumber%2==0:        even.append(number)转变成如下：Pythonnumbers=[1,2,3,4,5,6]even=[numberfornumberinnumbersifnumber%2==0]12numbers=[1,2,3,4,5,6]even=[numberfornumberinnumbersifnumber%2==0]是不是很牛呢，哈哈。字典推导和列表推导类似，字典可以做同样的工作：Pythonteams=[\"Packers\",\"49ers\",\"Ravens\",\"Patriots\"]print{key:valueforvalue,keyinenumerate(teams)}>>>{'49ers':1,'Ravens':2,'Patriots':3,'Packers':0}123teams=[\"Packers\",\"49ers\",\"Ravens\",\"Patriots\"]print{key:valueforvalue,keyinenumerate(teams)}>>>{'49ers':1,'Ravens':2,'Patriots':3,'Packers':0}初始化列表的值Pythonitems=[0]*3printitems>>>[0,0,0]123items=[0]*3printitems>>>[0,0,0]列表转换为字符串Pythonteams=[\"Packers\",\"49ers\",\"Ravens\",\"Patriots\"]print\",\".join(teams)>>>'Packers,49ers,Ravens,Patriots'123teams=[\"Packers\",\"49ers\",\"Ravens\",\"Patriots\"]print\",\".join(teams)>>>'Packers,49ers,Ravens,Patriots'从字典中获取元素我承认try/except代码并不雅致，不过这里有一种简单方法，尝试在字典中查找key，如果没有找到对应的alue将用第二个参数设为其变量值。Pythondata={'user':1,'name':'Max','three':4}try:is_admin=data['admin']exceptKeyError:is_admin=False12345data={'user':1,'name':'Max','three':4}try:  is_admin=data['admin']exceptKeyError:  is_admin=FalsePython替换诚这样：1替换诚这样：Pythondata={'user':1,'name':'Max','three':4}is_admin=data.get('admin',False)12data={'user':1,'name':'Max','three':4}is_admin=data.get('admin',False)获取列表的子集有时，你只需要列表中的部分元素，这里是一些获取列表子集的方法。Pythonx=[1,2,3,4,5,6]#前3个printx[:3]>>>[1,2,3]#中间4个printx[1:5]>>>[2,3,4,5]#最后3个printx[-3:]>>>[4,5,6]#奇数项printx[::2]>>>[1,3,5]#偶数项printx[1::2]>>>[2,4,6]12345678910111213141516x=[1,2,3,4,5,6]#前3个printx[:3]>>>[1,2,3]#中间4个printx[1:5]>>>[2,3,4,5]#最后3个printx[-3:]>>>[4,5,6]#奇数项printx[::2]>>>[1,3,5]#偶数项printx[1::2]>>>[2,4,6]60个字符解决FizzBuzz前段时间JeffAtwood推广了一个简单的编程练习叫FizzBuzz，问题引用如下：写一个程序，打印数字1到100，3的倍数打印“Fizz”来替换这个数，5的倍数打印“Buzz”，对于既是3的倍数又是5的倍数的数字打印“FizzBuzz”。这里就是一个简短的，有意思的方法解决这个问题：Pythonforxinrange(101):print\"fizz\"[x%3*4::]+\"buzz\"[x%5*4::]orx1forxinrange(101):print\"fizz\"[x%3*4::]+\"buzz\"[x%5*4::]orx集合除了python内置的数据类型外，在collection模块同样还包括一些特别的用例，在有些场合Counter非常实用。如果你参加过在这一年的FacebookHackerCup，你甚至也能找到他的实用之处。PythonfromcollectionsimportCounterprintCounter(\"hello\")>>>Counter({'l':2,'h':1,'e':1,'o':1})123fromcollectionsimportCounterprintCounter(\"hello\")>>>Counter({'l':2,'h':1,'e':1,'o':1}) 迭代工具和collections库一样，还有一个库叫itertools，对某些问题真能高效地解决。其中一个用例是查找所有组合，他能告诉你在一个组中元素的所有不能的组合方式Pythonfromitertoolsimportcombinationsteams=[\"Packers\",\"49ers\",\"Ravens\",\"Patriots\"]forgameincombinations(teams,2):printgame>>>('Packers','49ers')>>>('Packers','Ravens')>>>('Packers','Patriots')>>>('49ers','Ravens')>>>('49ers','Patriots')>>>('Ravens','Patriots')12345678910fromitertoolsimportcombinationsteams=[\"Packers\",\"49ers\",\"Ravens\",\"Patriots\"]forgameincombinations(teams,2):    printgame>>>('Packers','49ers')>>>('Packers','Ravens')>>>('Packers','Patriots')>>>('49ers','Ravens')>>>('49ers','Patriots')>>>('Ravens','Patriots')False==True比起实用技术来说这是一个很有趣的事，在python中，True和False是全局变量，因此：PythonFalse=TrueifFalse:print\"Hello\"else:print\"World\">>>Hello123456False=TrueifFalse:  print\"Hello\"else:  print\"World\">>>Hello如果你还有任何很酷的奇技淫巧，可以在下面留言，感谢阅读。打赏支持我翻译更多好文章，谢谢！打赏译者打赏支持我翻译更多好文章，谢谢！3赞23收藏19评论关于作者：刘志军个人博客：http://foofish.net，微信公众号：Python之禅（ID：VTtalk）个人主页·我的文章·45·"], "art_url": ["http://python.jobbole.com/32748/"]}
{"art_img": ["http://jbcdn2.b0.upaiyun.com/2017/08/6a0105126666cfc3204945b1fcd51626.png"], "art_title": ["选择一个 Python Web 框架：Django vs Flask vs Pyramid"], "art_create_time": ["2017/08/31"], "art_content": ["原文出处：RyanBrown   译文出处：开源中国   Pyramid,Django,和Flask都是优秀的框架，为项目选择其中的哪一个都是伤脑筋的事。我们将会用三种框架实现相同功能的应用来更容易的对比三者。也可以直接跳到框架实战（FrameworksinAction）章节查看代码（code）。1简介世界上可选的基于Python的web框架有很多。Django,Flask,Pyramid,Tornado,Bottle,Diesel,Pecan,Falcon等等，都在争取开发者支持。作为一开发者从一堆选择中筛选出一个来完成项目将会成为下一个大工程。我们今天专注于Flask,Pyramid,和Django。它们涵盖了从小微项目到企业级的web服务。为了更容易在三者中作出选择（至少更了解它们），我们将用每一个框架构建同样的应用并比较它们的代码，对于每一个方法我们会高亮显示它的优点和缺点。如果你只想要代码，直接跳到框架实战章节（FrameworksinAction），或者查看其在Github上的代码。Flask是一个面向简单需求小型应用的“微框架（microframework）”。Pyramid和Django都是面向大型应用的，但是有不同的拓展性和灵活性。Pyramid的目的是更灵活，能够让开发者为项目选择合适的工具。这意味着开发者能够选择数据库、URL结构、模板类型等等。Django目的是囊括web应用的所有内容，所以开发者只需要打开箱子开始工作，将Django的模块拉进箱子中。Django包括一个开箱即用的ORM，而Pyramid和Flask让开发者自己选择如何或者是否存储他们的数据。到目前为止对于非Django的web应用来说最流行的ORM是SQLAlchemy，同时还有多种其他选择，从DynamoDB和MongoDB到简单本地存储的LevelDB或朴实的SQLite。Pyramid被设计为可使用任何数据持久层，甚至是还没有开发出来的。2、关于框架Django的”batteriesincluded”特性让开发者不需要提前为他们的应用程序基础设施做决定，因为他们知道Python已经深入到了web应用当中。Django已经内建了模板、表单、路由、认证、基本数据库管理等等。比较起来，Pyramid包括路由和认证，但是模板和数据库管理需要额外的库。前面为Flask和Pyramidapps选择组件的额外工作给那些使用案例不适用标准ORM的开发者提供了更多的灵活性，同样也给使用不同工作流和模版化系统的开发者们带来了灵活性。Flask，作为三个框架里面最稚气的一个，开始于2010年年中。Pyramid框架是从Pylons项目开始的，在2010年底获得Pyramid这个名字，虽然在2005年就已经发布了第一个版本。Django2006年发布了第一个版本，就在Pylons项目（最后叫Pyramid）开始之后。Pyramid和Django都是非常成熟的框架，积累了众多插件和扩展以满足难以置信的巨大需求。虽然Flask历史相对更短，但它能够学习之前出现的框架并且把注意力放在了微小项目上。它大多数情况被使用在一些只有一两个功能的小型项目上。例如httpbin，一个简单的（但很强大的）调试和测试HTTP库的项目。3.社区最具活力的社区当属Django，其有80,000个StackOverflow问题和一系列来自开发者和优秀用户的良好的博客。Flask和Pyramid社区并没有那么大，但它们的社区在邮件列表和IRC上相当活跃。StackOverflow上仅有5,000个相关的标签，Flask比Django小了15倍。在Github上，它们的star近乎相当，Django有11,300个，Flask有10,900个。三个框架都使用的是BSD衍生的协议。Flask和Django的协议是BSD3条款，Pyramid的RepozePublicLicenseRPL是BSD协议4条款的衍生。4.BootstrappingDjango和Pyramid都内建bootstrapping工具。Flask没有包含类似的工具，因为Flask的目标用户不是那种试图构建大型MVC应用的人。4.1FlaskFlask的helloworld应用非常的简单，仅仅单个Python文件的7行代码就够了。Python#fromhttp://flask.pocoo.org/tutorialfromflaskimportFlaskapp=Flask(__name__)@app.route(\"/\")#takenoteofthisdecoratorsyntax,it'sacommonpatterndefhello():return\"HelloWorld!\"if__name__==\"__main__\":app.run()12345678910#fromhttp://flask.pocoo.org/tutorialfromflaskimportFlaskapp=Flask(__name__) @app.route(\"/\")#takenoteofthisdecoratorsyntax,it'sacommonpatterndefhello():    return\"HelloWorld!\" if__name__==\"__main__\":    app.run()这是Flask没有bootstrapping工具的原因：没有它们的需求。从Flask主页上的HelloWorld特性看，没有构建Pythonweb应用经验的开发者可以立即开始hacking。对于各部分需要更多分离的项目，Flask有blueprints。例如，你可以将所有用户相关的函数放在users.py中，将销售相关的函数放在ecommerce.py中，然后在site.py中添加引用它们来结构化你的Flask应用。我们不会深入这个功能，因为它超出了我们展示demo应用的需求。4.2PyramidPyramid的bootstrapping工具叫pcreate，是Pyramid的组成部分.之前的Paste工具套装提供了bootstrapping，但是从那之后被Pyramid专用工具链替代了。Python$pcreate-sstarterhello_pyramid#JustmakeaPyramidproject1$pcreate-sstarterhello_pyramid#JustmakeaPyramidprojectPyramid比Flask适用于更大更复杂的应用程序.因为这一点,它的bootstrapping工具创建更大的项目骨架.Pyramid同样加入了基本的配置文件，一个例子模版和用于将程序打包上传到PythonPackageIndex的所有文件。Pythonhello_pyramid├──CHANGES.txt├──development.ini├──MANIFEST.in├──production.ini├──hello_pyramid│├──__init__.py│├──static││├──pyramid-16x16.png││├──pyramid.png││├──theme.css││└──theme.min.css│├──templates││└──mytemplate.pt│├──tests.py│└──views.py├──README.txt└──setup.py123456789101112131415161718hello_pyramid├──CHANGES.txt├──development.ini├──MANIFEST.in├──production.ini├──hello_pyramid│  ├──__init__.py│  ├──static│  │  ├──pyramid-16x16.png│  │  ├──pyramid.png│  │  ├──theme.css│  │  └──theme.min.css│  ├──templates│  │  └──mytemplate.pt│  ├──tests.py│  └──views.py├──README.txt└──setup.py作为最后描述的框架，Pyramid的bootstrapper非常灵活.不局限于一个默认的程序;pcreate可以使用任意数量的项目模版.包括我们上面用到的pcreate里面的”starter”的模版,还有SQLAlchemy-，ZODB-支持scaffold项目.在PyPi可以发现已经为GoogleAppEngine,jQueryMobile,Jinja2templating,modernfrontendframeworks做好的scaffolds，还有更多~4.3DjangoDjango也有自己的bootstrap工具,内置在django-admin中.Pythondjango-adminstartprojecthello_djangodjango-adminstartapphowdy#makeanapplicationwithinourproject12django-adminstartprojecthello_djangodjango-adminstartapphowdy#makeanapplicationwithinourprojectDjango跟Pyramid区别在于:Django由多个应用程序组成一个项目,而Pyramid以及Flask项目是包含View和Model单一应用程序.理论上,Flask和Pyramid的项目允许存在多个project/app,不过在默认配置中只能有一个.Pythonhello_django├──hello_django│├──__init__.py│├──settings.py│├──urls.py│└──wsgi.py├──howdy│├──admin.py│├──__init__.py│├──migrations││└──__init__.py│├──models.py│├──tests.py│└──views.py└──manage.py123456789101112131415hello_django├──hello_django│  ├──__init__.py│  ├──settings.py│  ├──urls.py│  └──wsgi.py├──howdy│  ├──admin.py│  ├──__init__.py│  ├──migrations│  │  └──__init__.py│  ├──models.py│  ├──tests.py│  └──views.py└──manage.pyDjango默认只在项目中创建空白的model和模板文件,供新手参考的示范代码不多.此外,开发者在发布应用程序的时候,还要自己配置,这也是个麻烦.bootstrap工具的缺点是没有指导开发者如何打包应用.对于那些没有经验的新手来说,第一次部署应用将是个很头疼的问题.像django-oscar这样的大社区,项目都是打包好了,放在PyPi上供大家安装.但是Github上面的小项目缺少统一的打包方式.5模板一个Python应用能够响应HTTP请求将是一个伟大的开端，但是有可能你的大多数用户是没有兴趣使用curl与你的web应用交互的。幸运的是，这三个竞争者提供了使用自定义信息填充HTML的方法，以便让大伙们能够享受时髦的Bootstrap前端。模板让你能够直接向页面注入动态信息，而不是采用AJAX。你只需要一次请求就可以获取整个页面以及所有的动态数据，这对用户体验来说是很好的。这对于手机网站来说尤其重要，因为一次请求花费的时间会更长。所有的模板选项依赖于“上下文环境（context）”，其为模板转换为HTML提供了动态信息。模板的最简单的例子是填充已登录用户的名字以正确的迎接他们。也可以用AJAX获取这种动态信息，但是用一整个调用来填写用户的名字有点过头了，而同时模板又是这么的简单。5.1Django我们使用的例子正如写的那么简单，假设我们有一个包含了用户名的funllname属性的user对象。在Python中我们这样向模板中传递当前用户：Pythondefa_view(request):#gettheloggedinuser#...domorethingsreturnrender_to_response(\"view.html\",{\"user\":cur_user})1234567defa_view(request):    #gettheloggedinuser    #...domorethings    returnrender_to_response(        \"view.html\",        {\"user\":cur_user}    )拥有这个模板的上下文很简单，传入一个Python对象的字典和模板使用的数据结构。现在我们需要在页面上渲染他们的名字，以防页面忘了他们是谁。Python<!--view.html--><divclass=\"top-barrow\"><divclass=\"col-md-10\"><!--moretopbarthingsgohere--></div>{%ifuser%}<divclass=\"col-md-2whoami\">Youareloggedinas{{user.fullname}}</div>{%endif%}</div>1234567891011<!--view.html--><divclass=\"top-barrow\">  <divclass=\"col-md-10\">  <!--moretopbarthingsgohere-->  </div>  {%ifuser%}  <divclass=\"col-md-2whoami\">    Youareloggedinas{{user.fullname}}  </div>  {%endif%}</div>首先，你会注意到这个{%ifuser%}概念。在Django模板中，{%用来控制循环和条件的声明。这里的ifuser声明是为了防止那些不是用户的情况。匿名用户不应该在页面头部看到“你已经登录”的字样。在if块内，你可以看到，包含名字非常的简单，只要用{{}}包含着我们要插入的属性就可以了。{{是用来向模板插入真实值的，如{{user.fullname}}。模板的另一个常用情况是展示一组物品，如一个电子商务网站的存货清单页面。Pythondefbrowse_shop(request):#getitemsreturnrender_to_response(\"browse.html\",{\"inventory\":all_items})123456defbrowse_shop(request):    #getitems    returnrender_to_response(        \"browse.html\",        {\"inventory\":all_items}    )在模板中，我们使用同样的{%来循环清单中的所有条目，并填入它们各自的页面地址。Python{%forwidgetininventory%}<li><ahref=\"/widget/{{widget.slug}}/\">{{widget.displayname}}</a></li>{%endfor%}123{%forwidgetininventory%}    <li><ahref=\"/widget/{{widget.slug}}/\">{{widget.displayname}}</a></li>{%endfor%}为了做大部分常见的模板任务，Django可以仅仅使用很少的结构来完成目标，因此很容易上手。5.2FlaskFlask默认使用受Django启发的Jinja2模板语言，但也可以配置来使用另一门语言。不应该抱怨一个仓促的程序员分不清Django和Jinja模板。事实是，上面的Django例子在Jinja2也有效。为了不去重复相同的例子，我们来看下Jinja2比Django模板更具表现力的地方。Jinja和Django模板都提够了过滤的特性，即传入的列表会在展示前通过一个函数。一个拥有博文类别属性的博客，可以利用过滤特性，在一个用逗号分割的列表中展示博文的类别。Python<!--Django--><divclass=\"categories\">Categories:{{post.categories|join:\",\"}}</div><!--nowinJinja--><divclass=\"categories\">Categories:{{post.categories|join(\",\")}}</div>12345<!--Django--><divclass=\"categories\">Categories:{{post.categories|join:\",\"}}</div> <!--nowinJinja--><divclass=\"categories\">Categories:{{post.categories|join(\",\")}}</div>在Jinja模板语言中，可以向过滤器传入任意数量的参数，因为Jinja把它看成是使用括号包含参数的Python函数的一个调用。Django使用冒号来分割过滤器的名字和过滤参数，这限制了参数的数目只能为一。Jinjia和Django的for循环有点类似。我们来看看他们的不同。在Jinjia2中，for-else-endfor结构能遍历一个列表，同时也处理了没有项的情况。Python{%foritemininventory%}<divclass=\"display-item\">{{item.render()}}</div>{%else%}<divclass=\"display-warn\"><h3>Noitemsfound</h3><p>Tryanothersearch,maybe?</p></div>{%endfor%}12345678{%foritemininventory%}<divclass=\"display-item\">{{item.render()}}</div>{%else%}<divclass=\"display-warn\"><h3>Noitemsfound</h3><p>Tryanothersearch,maybe?</p></div>{%endfor%}Django版的这个功能是一样的，但是是用for-empty-endfor而不是for-else-endfor。Python{%foritemininventory%}<divclass=\"display-item\">{{item.render}}</div>{%empty%}<divclass=\"display-warn\"><h3>Noitemsfound</h3><p>Tryanothersearch,maybe?</p></div>{%endfor%}12345678{%foritemininventory%}<divclass=\"display-item\">{{item.render}}</div>{%empty%}<divclass=\"display-warn\"><h3>Noitemsfound</h3><p>Tryanothersearch,maybe?</p></div>{%endfor%}除了语法上的不同，Jinja2通过执行环境和高级特性提供了更多的控制。例如，它可以关闭危险的特性以安全的执行不受信任的模板，或者提前编译模板以确保它们的合法性。5.3Pyramid与Flask类似，Pyramid支持多种模板语言（包括Jinja2和Mako），但是默认只附带一个。Pyramid使用Chameleon,一个ZPT(ZopePageTemplate)模板语言的实现。我们来回头看看第一个例子，添加用户的名字到网站的顶栏。Python代码除了明确调用了render_template函数外其他看起来都差不多。Python@view_config(renderer='templates/home.pt')defmy_view(request):#dostuff...return{'user':user}1234@view_config(renderer='templates/home.pt')defmy_view(request):    #dostuff...    return{'user':user}但是我们的模板看起来有些不同。ZPT是一个基于XML得模板标准，所以我们使用了类XSLT语句来操作数据。Python<divclass=\"top-barrow\"><divclass=\"col-md-10\"><!--moretopbarthingsgohere--></div><divtal:condition=\"user\"tal:content=\"string:Youareloggedinas${user.fullname}\"class=\"col-md-2whoami\"></div></div>123456789<divclass=\"top-barrow\">  <divclass=\"col-md-10\">  <!--moretopbarthingsgohere-->  </div>  <divtal:condition=\"user\"      tal:content=\"string:Youareloggedinas${user.fullname}\"      class=\"col-md-2whoami\">  </div></div>Chameleon对于模板操作有三种不同的命名空间。TAL（模板属性语言）提供了基本的条件语句，字符串的格式化，以及填充标签内容。上面的例子只用了TAL来完成相关工作。对于更多高级任务，就需要TALES和METAL。TALES（模板属性表达式语法的语言）提供了像高级字符串格式化，Python表达式评估，以及导入表达式和模板的表达式。METAL（宏扩展模板属性语言）是Chameleon模板最强大的（和复杂的）一部分。宏是可扩展的，并能被定义为带有槽且当宏被调用时可以被填充。6.利用框架行动起来对于各个框架，我们将通过制作一个叫做wut4lunch的应用来了解，这个应用是告诉整个互联网你午饭吃了什么的社交网络。很自由的一个起始想法，完全可以随意改变。应用将有一个简单的接口，允许用户提交他们午饭的内容，并看到其他用户吃的什么的列表。主页完成后将看起来像这样。6.1使用Flask的Demo应用最短的实现用了34行Python代码和一个22行的Jinja模板。首先，我们有些管理类的任务要做，比如初始化我们的应用并拉近我们的ORM。PythonfromflaskimportFlask#Forthisexamplewe'lluseSQLAlchemy,apopularORMthatsupportsa#varietyofbackendsincludingSQLite,MySQL,andPostgreSQLfromflask.ext.sqlalchemyimportSQLAlchemyapp=Flask(__name__)#We'lljustuseSQLiteheresowedon'tneedanexternaldatabaseapp.config['SQLALCHEMY_DATABASE_URI']='sqlite:///test.db'db=SQLAlchemy(app)1234567891011fromflaskimportFlask #Forthisexamplewe'lluseSQLAlchemy,apopularORMthatsupportsa#varietyofbackendsincludingSQLite,MySQL,andPostgreSQLfromflask.ext.sqlalchemyimportSQLAlchemy app=Flask(__name__)#We'lljustuseSQLiteheresowedon'tneedanexternaldatabaseapp.config['SQLALCHEMY_DATABASE_URI']='sqlite:///test.db' db=SQLAlchemy(app)现在我们看下我们的模型，这将和另两个样例基本一样。PythonclassLunch(db.Model):\"\"\"Asinglelunch\"\"\"id=db.Column(db.Integer,primary_key=True)submitter=db.Column(db.String(63))food=db.Column(db.String(255))12345classLunch(db.Model):    \"\"\"Asinglelunch\"\"\"    id=db.Column(db.Integer,primary_key=True)    submitter=db.Column(db.String(63))    food=db.Column(db.String(255))哇，相当简单。最难的部分是找到合适的SQLAlchemy数据类型，选择数据库中String域的长度。使用我们的模型也超级简单，这在于我们将要看到SQLAlchemy查询语法。构建我们的提交表单也很简单。在引入Flask-WTForms和正确的域类型后，你可以看到表单看起来有点像我们的模型。主要的区别在于新的提交按钮和食物与提交者姓名域的提示。应用中的SECRET_KEY域是被WTForms用来创建CSRF符号的。它也被itsdangerous（Flask内包含）用来设置cookies和其他数据。Pythonfromflask.ext.wtfimportFormfromwtforms.fieldsimportStringField,SubmitFieldapp.config['SECRET_KEY']='please,tellnobody'classLunchForm(Form):submitter=StringField(u'Hi,mynameis')food=StringField(u'andIate')#submitbuttonwillread\"sharemylunch!\"submit=SubmitField(u'sharemylunch!')12345678910fromflask.ext.wtfimportFormfromwtforms.fieldsimportStringField,SubmitField app.config['SECRET_KEY']='please,tellnobody' classLunchForm(Form):    submitter=StringField(u'Hi,mynameis')    food=StringField(u'andIate')    #submitbuttonwillread\"sharemylunch!\"    submit=SubmitField(u'sharemylunch!')让表单在浏览器中显示意味着模板要有它。我们像下面那样传递进去。Pythonfromflaskimportrender_template@app.route(\"/\")defroot():lunches=Lunch.query.all()form=LunchForm()returnrender_template('index.html',form=form,lunches=lunches)1234567fromflaskimportrender_template @app.route(\"/\")defroot():    lunches=Lunch.query.all()    form=LunchForm()    returnrender_template('index.html',form=form,lunches=lunches)好了，发生了什么？我们得到已经用Lunch.query.all()提交的午餐列表，并实例化一个表单，让用户提交他们自己的美食之旅。为了简化，变量使用相同的名字出入模板，但这不是必须的。Python<html><title>Wut4Lunch</title><b>Whatarepeopleeating?</b><p>Wut4Lunchisthelatestsocialnetworkwhereyoucantellallyourfriendsaboutyournoontimerepast!</p>123456<html><title>Wut4Lunch</title><b>Whatarepeopleeating?</b> <p>Wut4Lunchisthelatestsocialnetworkwhereyoucantellallyourfriendsaboutyournoontimerepast!</p>这就是模板的真实情况，我们在已经吃过的午餐中循环，并在<ul>中展示他们。这几乎与我们前面看到的循环例子一样。Python<ul>{%forlunchinlunches%}<li><strong>{{lunch.submitter|safe}}</strong>justate<strong>{{lunch.food|safe}}</strong>{%else%}<li><em>Nobodyhaseatenlunch,youmustallbestarving!</em></li>{%endfor%}</ul><b>WhatareYOUeating?</b><formmethod=\"POST\"action=\"/new\">{{form.hidden_tag()}}{{form.submitter.label}}{{form.submitter(size=40)}}<br/>{{form.food.label}}{{form.food(size=50)}}<br/>{{form.submit}}</form></html>12345678910111213141516171819<ul>{%forlunchinlunches%}<li><strong>{{lunch.submitter|safe}}</strong>justate<strong>{{lunch.food|safe}}</strong>{%else%}<li><em>Nobodyhaseatenlunch,youmustallbestarving!</em></li>{%endfor%}</ul> <b>WhatareYOUeating?</b> <formmethod=\"POST\"action=\"/new\">    {{form.hidden_tag()}}    {{form.submitter.label}}{{form.submitter(size=40)}}    <br/>    {{form.food.label}}{{form.food(size=50)}}    <br/>    {{form.submit}}</form></html>模板的<form>部分仅仅渲染我们在root()视图中传入模板的WTForm对象的表单标签和输入。当表单提交时，它将向/new提交一个POST请求，这个请求会被下面的函数处理。Pythonfromflaskimporturl_for,redirect@app.route(u'/new',methods=[u'POST'])defnewlunch():form=LunchForm()ifform.validate_on_submit():lunch=Lunch()form.populate_obj(lunch)db.session.add(lunch)db.session.commit()returnredirect(url_for('root'))1234567891011fromflaskimporturl_for,redirect @app.route(u'/new',methods=[u'POST'])defnewlunch():    form=LunchForm()    ifform.validate_on_submit():        lunch=Lunch()        form.populate_obj(lunch)        db.session.add(lunch)        db.session.commit()    returnredirect(url_for('root'))在验证了表单数据后，我们把内容放入我们Model对象中，并提交到数据库。一旦我们在数据库中存了午餐，它将在人们吃过的午餐列表中出现。Pythonif__name__==\"__main__\":db.create_all()#makeoursqlalchemytablesapp.run()123if__name__==\"__main__\":    db.create_all()  #makeoursqlalchemytables    app.run()最后，我们只需做（非常）少量的工作来让应用运行起来。使用SQLAlchemy，我们可以创建存储午餐的表，然后开始运行我们写的路径管理就行了。6.2测试Django版APPDjango版wut4lunch和Flask版有点像，但是在Django项目中被分到了好几个文件中。首先，我们看看最相似的部分：数据库模型。它和SQLAlchemy版本的唯一不同之处是声明保存文本的数据库字段有轻微的语法区别。Python#fromwut4lunch/models.pyfromdjango.dbimportmodelsclassLunch(models.Model):submitter=models.CharField(max_length=63)food=models.CharField(max_length=255)123456#fromwut4lunch/models.pyfromdjango.dbimportmodels classLunch(models.Model):    submitter=models.CharField(max_length=63)    food=models.CharField(max_length=255)在表单系统上。不像Flask，我们可以用Django内建的表单系统。它看起来非常像我们在Flask中使用的WTFroms模块，只是语法有点不同。Pythonfromdjangoimportformsfromdjango.httpimportHttpResponsefromdjango.shortcutsimportrender,redirectfrom.modelsimportLunch#Createyourviewshere.classLunchForm(forms.Form):\"\"\"Formobject.LooksalotliketheWTFormsFlaskexample\"\"\"submitter=forms.CharField(label='Yourname')food=forms.CharField(label='Whatdidyoueat?')123456789101112fromdjangoimportformsfromdjango.httpimportHttpResponsefromdjango.shortcutsimportrender,redirect from.modelsimportLunch #Createyourviewshere. classLunchForm(forms.Form):    \"\"\"Formobject.LooksalotliketheWTFormsFlaskexample\"\"\"    submitter=forms.CharField(label='Yourname')    food=forms.CharField(label='Whatdidyoueat?')现在我们只需要构造一个LunchForm实例传递到我们的模板。Pythonlunch_form=LunchForm(auto_id=False)defindex(request):lunches=Lunch.objects.all()returnrender(request,'wut4lunch/index.html',{'lunches':lunches,'form':lunch_form,})123456789101112lunch_form=LunchForm(auto_id=False) defindex(request):    lunches=Lunch.objects.all()    returnrender(        request,        'wut4lunch/index.html',        {            'lunches':lunches,            'form':lunch_form,        }    )render函数是Djangoshortcut，以接受请求、模板路径和一个上下文的dict。与Flask的render_template类似，它也接受接入请求。Pythondefnewlunch(request):l=Lunch()l.submitter=request.POST['submitter']l.food=request.POST['food']l.save()returnredirect('home')123456defnewlunch(request):    l=Lunch()    l.submitter=request.POST['submitter']    l.food=request.POST['food']    l.save()    returnredirect('home')保存表单应答到数据库是不一样的，Django调用模型的.save()方法以及处理会话管理而不是用全局数据库会话。干净利落！Django提供了一些优雅的特性，让我们管理用户提交的午餐，因此我们可以删除那些不合适的午餐信息。Flask和Pyramid没有自动提供这些功能，而在创建一个Django应用时不需要写另一个管理页面当然也是其一个特性。开发者的时间可不免费啊！我们所要做的就是告诉Django-admin我们的模型，是在wut5lunch/admin.py中添加两行。Pythonfromwut4lunch.modelsimportLunchadmin.site.register(Lunch)12fromwut4lunch.modelsimportLunchadmin.site.register(Lunch)Bam。现在我们可以添加删除一些条目，而无需额外的工作。最后，让我们看下主页模板的不同之处。Python<ul>{%forlunchinlunches%}<li><strong>{{lunch.submitter}}</strong>justate<strong>{{lunch.food}}</strong></li>{%empty%}<em>Nobodyhaseatenlunch,youmustallbestarving!</em>{%endfor%}</ul>1234567<ul>{%forlunchinlunches%}<li><strong>{{lunch.submitter}}</strong>justate<strong>{{lunch.food}}</strong></li>{%empty%}<em>Nobodyhaseatenlunch,youmustallbestarving!</em>{%endfor%}</ul>Django拥有方便的快捷方式，在你的页面中引用其他的视图。url标签可以使你重建应用中的URLs，而不需破坏视图。这个是因为url标签会主动查询视图中的URL。Python<formaction=\"{%url'newlunch'%}\"method=\"post\">{%csrf_token%}{{form.as_ul}}<inputtype=\"submit\"value=\"Iatethis!\"/></form>12345<formaction=\"{%url'newlunch'%}\"method=\"post\">  {%csrf_token%}  {{form.as_ul}}  <inputtype=\"submit\"value=\"Iatethis!\"/></form>表单被不同的语法渲染，我们需要人工在表单主体中添加CSRFtoken，但这些区别更多的是装饰6.3测试Pyramid版App最后，我们看看用Pyramid实现的同样的程序。与Django和Flask的最大不同是模板。只需要对Jinja2做很小的改动就足以解决我们在Django中的问题。这次不是这样的，Pyramid的Chameleon模板的语法更容易让人联想到XSLT而不是别的。Python<!--pyramid_wut4lunch/templates/index.pt--><divtal:condition=\"lunches\"><ul><divtal:repeat=\"lunchlunches\"tal:omit-tag=\"\"><lital:content=\"string:${lunch.submitter}justate${lunch.food}\"/></div></ul></div><divtal:condition=\"not:lunches\"><em>Nobodyhaseatenlunch,youmustallbestarving!</em></div>1234567891011<!--pyramid_wut4lunch/templates/index.pt--><divtal:condition=\"lunches\">  <ul>    <divtal:repeat=\"lunchlunches\"tal:omit-tag=\"\">      <lital:content=\"string:${lunch.submitter}justate${lunch.food}\"/>    </div>  </ul></div><divtal:condition=\"not:lunches\">  <em>Nobodyhaseatenlunch,youmustallbestarving!</em></div>与Django模板类似，缺少for-else-endfor结构使得逻辑稍微的更清晰了。这种情况下，我们以if-for和if-not-for语句块结尾以提供同样的功能。使用{{或{%来控制结构和条件的Django以及AngularJS类型的模板让使用XHTML标签的模板显得很外行。Chameleon模板类型的一大好处是你所选择的编辑器可以正确的使语法高亮，因为模板是有些得XHTML。对于Django和Flask模板来说，你的编辑器需要能够正确的支持这些模板语言高亮显示。Python<b>WhatareYOUeating?</b><formmethod=\"POST\"action=\"/newlunch\">Name:${form.text(\"submitter\",size=40)}<br/>Whatdidyoueat?${form.text(\"food\",size=40)}<br/><inputtype=\"submit\"value=\"Iatethis!\"/></form></html>12345678910<b>WhatareYOUeating?</b> <formmethod=\"POST\"action=\"/newlunch\">  Name:${form.text(\"submitter\",size=40)}  <br/>  Whatdidyoueat?${form.text(\"food\",size=40)}  <br/>  <inputtype=\"submit\"value=\"Iatethis!\"/></form></html>Pyramid中表单得转换稍微更细致些，因为pytamid_simpleform不像Django表单的form.as_ul函数那样可以自动转换所有的表单字段。现在我们看看什么返回给应用。首先，定义我们需要得表单并呈现我们的主页。Python#pyramid_wut4lunch/views.pyclassLunchSchema(Schema):submitter=validators.UnicodeString()food=validators.UnicodeString()@view_config(route_name='home',renderer='templates/index.pt')defhome(request):lunches=DBSession.query(Lunch).all()form=Form(request,schema=LunchSchema())return{'lunches':lunches,'form':FormRenderer(form)}1234567891011#pyramid_wut4lunch/views.pyclassLunchSchema(Schema):    submitter=validators.UnicodeString()    food=validators.UnicodeString() @view_config(route_name='home',            renderer='templates/index.pt')defhome(request):    lunches=DBSession.query(Lunch).all()    form=Form(request,schema=LunchSchema())    return{'lunches':lunches,'form':FormRenderer(form)}获取午餐的查询语法和Flask的很相似，这是因为这两个demo应用使用了流行的SQLAlchemyORM来提供持久存储。在Pyramid中，允许你直接返回模板上下文的字典，而不是要调用特殊的render函数。@view_config装饰器自动将返回的上下文传入要渲染的模板。避免调用render方法使得Pyramid写的函数更加容易测试，因为它们返回的数据没有被模板渲染对象掩盖。Python@view_config(route_name='newlunch',renderer='templates/index.pt',request_method='POST')defnewlunch(request):l=Lunch(submitter=request.POST.get('submitter','nobody'),food=request.POST.get('food','nothing'),)withtransaction.manager:DBSession.add(l)raiseexc.HTTPSeeOther('/')12345678910111213@view_config(route_name='newlunch',            renderer='templates/index.pt',            request_method='POST')defnewlunch(request):    l=Lunch(        submitter=request.POST.get('submitter','nobody'),        food=request.POST.get('food','nothing'),    )     withtransaction.manager:        DBSession.add(l)     raiseexc.HTTPSeeOther('/')从Pyramid的请求对象中更加容易得到表单数据，因为在我们获取时会自动将表单POST数据解析成dict。为了阻止同一时间多并发的请求数据库，ZopeTransactions模块提供了上下文管理器，对写入逻辑事物的数据库进行分组，并阻止应用的线程在各个改变时互相影响，这在你的视图共享一个全局session并接收到大量通信的情况下将会是个问题。7.总结Pyramid是三个中最灵活的。它可以用于小的应用，正如我们所见，但它也支撑着有名的网站如Dropbox。开源社区如Fedora选择它开发应用，如他们社区中的徽章系统，从项目工具中接受事件的信息，并向用户奖励成就类型的徽章。对于Pyramid的一个最常见的抱怨是，它提供了这么多的选项，以至于用它开始一个新项目很吓人。目前最流行的框架是Django，使用它的网站列表也令人印象深刻。Bitbucket，Pinterest，Instagram，以及Onion完全或部分使用Django。对于有常见需求的网站，Django是非常理智的选择，也因此它成为中大型网站应用的流行选择。Flask对于那些开发小项目、需要快速制作一个简单的Python支撑的网站的开发者很有用。它提供小型的统一工具，或者在已有的API上构建的简单网络接口。可以快速开发需要简单web接口并不怎么配置的后端项目使用Flask将会在前端获益，如jitviewer提供了一个web接口来检测PyPyjust-in-time的编译日志。这三个框架都能解决我们简单的需求，我们已经看到了它们的不同。这些区别不仅仅是装饰性的，它们将会改变你设计产品的方法，以及添加新特性和修复的速度。因为我们的例子很小，我们看到Flask的闪光点，以及Django在小规模应用上的笨重。Pyramid的灵活并未体现出来，因为我们的要求是一样的，但在真实场景中，新的需求会常常出现。7.1致谢标题图像的logo来自与Flask、Django和Pyramid项目网站。这篇文章非常感谢它的评阅者，RemyDeCausemaker，RossDelinger和LiamMiddlebrook，忍受了许多初期的草稿。这篇文章的当前样式来自于AdamChainz、bendwarn、SergerMaertens、TomLeo和wichert的评论和修正（名字按字母表顺序）。1赞3收藏评论"], "art_url": ["http://python.jobbole.com/88447/"]}
{"art_img": ["http://jbcdn2.b0.upaiyun.com/2011/11/book-logo.jpg"], "art_title": ["25本免费的Python电子书"], "art_create_time": ["2013/06/21"], "art_content": ["本文由伯乐在线-伯乐翻译。未经许可，禁止转载！英文出处：sangkrit。欢迎加入翻译组。Python是一种面向对象、直译式计算机编程语言，具有近二十年的发展历史，成熟且稳定。它包含了一组完善而且容易理解的标准库，能够轻松完成很多常见的任务。它的语法简捷和清晰，尽量使用无异义的英语单词，与其它大多数程序设计语言使用大括号不一样，它使用缩进来定义语句块。Python可以和C/C++语言整合在一起，也能支持命令式程序设计、面向对象程序设计、函数式编程、面向侧面程序设计、泛型编程多种编程范式。（摘自维基百科Python词条）Python的一些重要特性简单：Python是一种代表简单主义思想的语言。阅读一个良好的Python程序就感觉像是在读英语一样。它使你能够专注于解决问题而不是去搞明白语言本身。易学：Python极其容易上手，因为Python有极其简单的说明文档。速度快：Python的底层是用C语言写的，很多标准库和第三方库也都是用C写的，运行速度非常快。免费、开源：Python是FLOSS（自由/开放源码软件）之一。使用者可以自由地发布这个软件的拷贝、阅读它的源代码、对它做改动、把它的一部分用于新的自由软件中。FLOSS是基于一个团体分享知识的概念。高层语言：用Python语言编写程序的时候无需考虑诸如如何管理你的程序使用的内存一类的底层细节。可移植性：由于它的开源本质，Python已经被移植在许多平台上（经过改动使它能够工作在不同平台上）。这些平台包括Linux、Windows、FreeBSD、Macintosh、Solaris、OS/2、Amiga、AROS、AS/400、BeOS、OS/390、z/OS、PalmOS、QNX、VMS、Psion、AcomRISCOS、VxWorks、PlayStation、SharpZaurus、WindowsCE、PocketPC、Symbian以及Google基于linux开发的android平台。解释性：一个用编译性语言比如C或C++写的程序可以从源文件（即C或C++语言）转换到一个你的计算机使用的语言（二进制代码，即0和1）。这个过程通过编译器和不同的标记、选项完成。运行程序的时候，连接/转载器软件把你的程序从硬盘复制到内存中并且运行。而Python语言写的程序不需要编译成二进制代码。你可以直接从源代码运行程序。在计算机内部，Python解释器把源代码转换成称为字节码的中间形式，然后再把它翻译成计算机使用的机器语言并运行。这使得使用Python更加简单。也使得Python程序更加易于移植。面向对象：Python既支持面向过程的编程也支持面向对象的编程。在“面向过程”的语言中，程序是由过程或仅仅是可重用代码的函数构建起来的。在“面向对象”的语言中，程序是由数据和功能组合而成的对象构建起来的。可扩展性：如果需要一段关键代码运行得更快或者希望某些算法不公开，可以部分程序用C或C++编写，然后在Python程序中使用它们。可嵌入性：可以把Python嵌入C/C++程序，从而向程序用户提供脚本功能。丰富的库：Python标准库确实很庞大。它可以帮助处理各种工作，包括正则表达式、文档生成、单元测试、线程、数据库、网页浏览器、CGI、FTP、电子邮件、XML、XML-RPC、HTML、WAV文件、密码系统、GUI（图形用户界面）、Tk和其他与系统有关的操作。这被称作Python的“功能齐全”理念。除了标准库以外，还有许多其他高质量的库，如wxPython、Twisted和Python图像库等等。（摘自百度百科Python词条）下面是sangkrit收集整理的25本免费的Python电子书。如果你是Python新手，并且不知该先看哪本，sangkrit是建议从第12本开始。对于Python新手应该从哪本开始，如果各位朋友有不同看法，欢迎在评论中留言。ThinkStatsDiveIntoPythonAByteOfPythonThinkComplexityDiveIntoPython3DJANGOTUTORIALBuildingSkillsInOOPPyramidForHumansFlaskMicroframeworkBuildingSkillsInPythonKivyProgrammingGuideSnakeWranglingForKidsAnIntroductionToPythonProgrammezAvecPython2ProgrammezAvecPython 3PythonModuleOfTheWeekLearnPythonTheHardWayTheStandardPythonLibraryBuildingSkillsInProgrammingPythonScientificLectureNotesMakingGamesWithPython&PygamePython101(anintroductiontopython)HowToThinkLikeAComputerScientistNaturalLanguageProcessingWithPythonProgrammingComputerVisionWithPython1赞14收藏6评论关于作者：伯乐简介还没来得及写:）个人主页·我的文章·4"], "art_url": ["http://python.jobbole.com/29281/"]}
{"art_img": ["http://jbcdn2.b0.upaiyun.com/2012/12/ungood-guido-van-rossum-python.jpg"], "art_title": ["Slashdot对Python之父的采访"], "art_create_time": ["2013/06/20"], "art_content": ["本文由伯乐在线-贱圣OMG翻译。未经许可，禁止转载！英文出处：Slashdot。欢迎加入翻译组。伯乐在线导读：Python之父GuidovanRossum在2013年1月正式从Google离职后并正式加入Dropbox。2013年8月19日，Slashdot网站发起了一个对Guido的访谈主题帖，网友在评论中提问。8月25日，Slashdot在另外一个帖子中汇总了“面向对象、函数式编程、PyPy、Python3”等问题和回复。该汇总帖现已由 贱圣OMG 翻译成中文。（如果其他朋友也有不错的原创或译文，可以尝试提交到伯乐在线。）从Google到Dropboxnurhussein提问：“Hi，是什么促使离开Google去Dropbox的？你之前在Google主要做什么？以后在Dropbox会做什么？”Guido：在Google呆了7年之后，我已经准备好生活里有一些变化，而这时Dropbox的工作机会正好契合了我的想法。以较高的层次来看，我的工作仍然没有什么变化：花费一半时间来做作为Python的BDFL需要做的事情在公司里作为一名普通的工程师（不是一名经理或者团队领导者）做什么代码审查，架构和设计工作处理很多email用Python来完成我的很多工作一些细节当然是不同的。我在Google只做了两件事：最开始的两年我从事在线代码审查工具Mondrian的开发。这个工具从来没有被开源，但是它促使了Rietveld的产生，它被Python，Go和Chromium社区使用。在我加入GoogleAppEngine后，我做了很多不同的事情，大部分是Python方面的事情。我Python的最后一个大项目是一个新的Python数据库API，NDB。我已经来Dropbox7个月了，我在这主要的工作是设计Dropbox数据存储API。用到这个词来描述这个数据存储有点讽刺，但是不是我的错——Dropbox数据存储和GoogleAppEngine数据存储有一点重叠。更讽刺的是，即使我做了如此多的设计工作，用Python完成了两个原型，但是我们上个月发布的SDK里面只支持Java，Object-C和JavaScript。不过我正在完善它，这次采访拖累了我的进度。 为什么Python避开了一些常见的面向对象风格由i_ate_god提问：“接口，虚类，私有成员，等等…为什么Python没有这些特性”Guido：我能想到的有两个原因：你并不是真的需要它们，并且如果没有编译时的类型检查会很难实现。Python是作为一个臭鼬工厂的项目开始做的（没有被管理层支持和鼓励但也没有阻止），并且我希望能够快点出一些成果。这指引我移除了一些不是真正需要或者继续的特性；这也让我进行运行时的所有类型检查，它限制了Python能够支持的特性。我也不是面向对象的忠实信徒——我只是想要一个简单的语言，它因为意外或多或少地变得有一些面向对象。在现代的Python中，已经有了和这些语言特性类似的实现，但运行效果并不怎么好，或者导致大量的运行开销，所以，这些特性通常是需要避免使用的（不过，这些特性还是有一些用处，有些人也热衷去使用它们）。函数式语言由ebno-10db提问：“有些人提出，Python是，至少一部分，是一种函数式语言。你不同意，我也是。只是有一些map和filter类型函数并不会让它成为函数式语言。以我的理解，这些函数是被一些思念list的人加到库里的，并且你已经尝试了几次去掉它们。总的来说，你不是一个函数式编程的粉丝，至少从Python上来看不是。问题：你是否感觉函数式编程方法总的来说不是特别有用，或者它不是十分适合Python？很希望听到你不同方面的原因。”Guido：我并不是把一个想法做到极致的信徒，我试着在设计选择的时候走实用主义的路子（但不是“太”实用主义）。我会衡量现实代码的可读性和可用性。有些地方map()和filter()是适合的，但是另一方面Python有列表推导。我不再讨厌reduce()，因为我曾经只用(a)来实现sum()，或者用(b)可读性不好。所以我们添加了内建的sum()，将reduce()移除出内建函数，移到了一个工具函数里。我对函数式语言的看法，就是它们都用非常强大的编译器，比如Haskell。对这样的一个编译器，函数式泛型是非常有用的，因为它让大量的转变成为可能，包括并行化。但是Python解释器并不清楚你的代码的含义，这也是很有用的。所以，我不认为把一下函数式的思想加入Python是合理的，因为这些在函数式语言里是很有用的，但是不适合Python，并且这会让代码对不使用函数式编程的人非常不具有可读性（这里指的是大部分程序员）。我也不认为现在函数式语言的成果已经让它准备好成为主流。不可否认的是，我对于Haskell一些相关的领域并不是很了解，但是任何没有Haskell流行的语言都有它的实际用处，我也没有听过有别的函数式语言比Haskell更流行。对于Haskell，我认为让很多编译器技术得到证明是非常棒的，但是它的“纯净”会是它被人接受的最大障碍。它的单一让它对于大部分人是不适合的。 多行lambda表达式由NeverWorker1提问：“对于Python，有一个最常见的抱怨就是它的对于lambda表达式的限制，也就是说一行里不能赋值。很明显，Python对空格的处理是导致这样的主要原因。我已经花了一些事件思考实现多行lambda表达式的可能性，然后我能想出的最好方法是硬塞进一些不用的符号，比如C语言风格的大括号，这样最多有点乱。有没有更好的方法，你觉得这个功能会被添加上吗？”Guido：真的？我基本上从来没听到过那些抱怨，除了在Slashdot采访里提问题的人。这确实是更好的方法，这里使用def关键字在本地作用域定义一个正规的函数。这个被定义的函数对象变成了一个本地变量，而这根使用lambda是相同的语义，除非这里用到了一个本地变量，并且这里没有任何语法的限制。例如，以下两种写法的语言是相同的：Pythondefmake_adder(n):__defadder(x):____returnx+n__returnadder1234defmake_adder(n):__defadder(x):____returnx+n__returnadder然后这是使用lambda的表达式：Pythondefmake_adder(n):__returnlambdax:x+n12defmake_adder(n):__returnlambdax:x+nAndrewKoenig有一次向我指出了在一种场景下，lambda是非常适合的，那就是你有你个很长的list或者dict包括很多lambda表达式，因此如果你想不用lambda实现的话，那么定义一大堆函数，给它们命名，然后用list或dict里的名称来引用它们就会让你受不了。但是，在那种情况下，lambda表达式是足够简单的，如果你有一些异常，在list或dict之前使用def才是一种好的妥协。 PyPy由Btrot69提问：“你觉得PyPy代表未来的发展方向吗？你是否对此表示怀疑？如果是，为什么？”Guido：我对此仍然持怀疑态度，有两个原因：（1）它们还不支持Python3。（2）还有很多扩展模块不能很好的支持。但是我希望它们能修复那些问题。作为PyPy项目的竞争者，Jython和IronPython会让CPython项目保持其发展势头。 浏览器运行Python？多年以来，曾经尝试几次创建一个沙箱版本的Python，使之能够运行在浏览器上。主要是因为Javascript的问题。而现在针对Javascript做的工作，我们有了一个很好的替代品CoffeeScript——那现在是不是已经是时候来实现让Python运行在浏览器里的功能了？Guido：我在1995年就放弃了这件事。并且请不要把Python编译成Javascript。它们的语义非常不同，结果是你用Javascript写了一个Python运行时，它会让运行变得太慢。 Python3由MetalliQaZ 提问：“你对目前向Python3的迁移的迁移感觉怎么样？从一个用户的角度来看，一些流行的库的转变还差得很远，而这阻碍着这种过渡。在我的专业所及的地方，基本上我用的所有系统都没有安装3.x解释器。事实上，2.7也很少，我想听听你的看法。”Guido：很好奇你在哪工作。我同意向Python3的迁移会持续很长时间，但是如果你的系统还没用上2.7版本的话，那就真是有点古老了！在我离开Google的时候，所有向Python2.7过渡的工作全部完成了（在前几年已经成功的从2.4迁移到2.6），在Dropbox这里，客户端和服务器端都是用的2.7。这两个公司都在考虑Python3的问题了。再来说向Python3的迁移，我实际上是相当乐观的。很多流行的库都开始着手做这件事。它确实会持续很长时间，但也有很多进展，过几年之后，我希望所有的代码都能迁移到Python3上来。完全根除Python2的使用可能会花更多的时间，但是呢，WindowsXP不也是没完全死掉吗。1赞收藏2评论关于作者：贱圣OMG非专业码农（新浪微博：@贱圣OMG）个人主页·我的文章·22"], "art_url": ["http://python.jobbole.com/47081/"]}
{"art_img": ["http://jbcdn2.b0.upaiyun.com/2017/08/e474befecc86207e10115bad821e0a8a.png"], "art_title": ["一个小时搭建一个全栈 Web 应用框架"], "art_create_time": ["2017/08/25"], "art_content": ["原文出处：AngelaJuvetBranæs   译文出处：疯狂的技术宅   把想法变为现实的能力是空想家与实干家的区别。不管你是在一家跨国公司工作，还是正在为自己的创业公司而努力，那些有能力将创意转化为真正产品的人，都具有宝贵的技能并拥有明显的实力。如果你能在不到一个小时的时间里创建一个全栈的Web应用，那么你就有能力为自己下一个伟大的想法迅速的的创建一个简单的MVP，或者在工作中快速构建一个新的应用程序。本文介绍了创建一个简单的全栈Web应用所需的步骤，其中包括一个Python服务器和一个React前端。你可以轻松的在其基础上进行构建，根据你的实际需求进行修改，或是添加一些其他技术特性，例如Redux。世界在互联网的驱动下，计算机的基本技术和简单工具已经成为现代商业人士的必备技能。本文适合想要学习怎样制作一个简单的基于web的应用程序，并且具备基本编程技能的人。尽管你可以在我的GitHub上找到本文所有的源代码，但是如果你能够从头开始创建这个程序，将会得到最好的学习成果。初始项目设置Python.├──README.md└──fullstack_template/├──server/└──static/├──css/├──dist/├──images/└──js/123456789.├──README.md└──fullstack_template/    ├──server/    └──static/        ├──css/        ├──dist/        ├──images/        └──js/我们将使用npm包管理器来处理Javascript依赖项。Npm是非常棒的，因为它易于使用，有良好的文档支持，有将近50万个包可供使用，以及合理的默认项目设置方案。使用包管理器可以使您的项目依赖项保持最新状态，并能够获取和安装最新的包。让我们初始化项目：Python$cdfullstack_template/static$npminit12$cdfullstack_template/static$npminit在初始化的过程中可以接受默认设置，但是你最好填写自己的程序名称和Git库等参数，结束后会自动在你的static目录下生成一个名为package.json的文件。package.json文件有如下几个作用：跟踪所有的依赖项及其版本。它可是使其他开发人员了解你的项目，比如应用的名称、说明、所有者和所在存储库的位置。可以非常容易的通过npm进行自动化安装、运行和更新。安装和配置WebpackWebpack是一个模块打包器。它可以处理你所有的模块依赖，并生成静态资源。使用模块打包器可以减少浏览器需要加载的模块数量，从而大大缩短了网页的加载时间。演示了Webpack是怎样工作的安装Webpack：Python$npmiwebpack--save-dev1$npmiwebpack--save-dev要使用Webpack，我们需要添加一个Webpack配置文件。这个配置告诉Webpack在哪里可以找到JavaScript和React文件，以及在哪里放置生成的JavaScript包。在static目录中添加一个名为webpack.config.js的文件，下面的内容如下:Pythonconstwebpack=require('webpack');constconfig={entry:__dirname+'/js/index.jsx',output:{path:__dirname+'/dist',filename:'bundle.js',},resolve:{extensions:['.js','.jsx','.css']},};module.exports=config;123456789101112constwebpack=require('webpack');constconfig={    entry:  __dirname+'/js/index.jsx',    output:{        path:__dirname+'/dist',        filename:'bundle.js',    },    resolve:{        extensions:['.js','.jsx','.css']    },};module.exports=config;添加运行命令向package.json文件中添加一些运行命令会是你的开发过程更加顺畅。我总是在自己的package.json文件中添加一些build,dev-build和watch命令。build用于构建生产环境版本,dev-build用于开发时的构建版本，watch的作用和dev-build类似，只不过可以自动监视项目文件是否修改，并且自动重新构建被修改的部分，你只需要刷新浏览器就可以看到改动后的结果。自动化构建你的项目还有一个好处，那就是你不会耗费时间去思考为什么修改了代码却看不到效果，一般遇到这种情况纯粹是因为你忘记了构建它们！以下是我的package.json文件内容：Python{\"name\":\"FullStackTemplate\",\"version\":\"1.0.0\",\"description\":\"ATemplateforcreatingaFullStackWebApplicationusingPython,NPM,WebpackandReact\",\"main\":\"index.js\",\"scripts\":{\"build\":\"webpack-p--progress--configwebpack.config.js\",\"dev-build\":\"webpack--progress-d--configwebpack.config.js\",\"test\":\"echo\\\"Error:notestspecified\\\"&&exit1\",\"watch\":\"webpack--progress-d--configwebpack.config.js--watch\"},\"keywords\":[\"fullstack\",\"template\",\"python\",\"react\",\"npm\",\"webpack\"],\"author\":\"AngelaBranaes\",\"license\":\"MIT\",\"devDependencies\":{\"webpack\":\"^3.0.0\"}}12345678910111213141516171819202122232425{  \"name\":\"FullStackTemplate\",  \"version\":\"1.0.0\",  \"description\":\"ATemplateforcreatingaFullStackWebApplicationusingPython,NPM,WebpackandReact\",  \"main\":\"index.js\",  \"scripts\":{    \"build\":\"webpack-p--progress--configwebpack.config.js\",    \"dev-build\":\"webpack--progress-d--configwebpack.config.js\",    \"test\":\"echo\\\"Error:notestspecified\\\"&&exit1\",    \"watch\":\"webpack--progress-d--configwebpack.config.js--watch\"  },  \"keywords\":[    \"fullstack\",    \"template\",    \"python\",    \"react\",    \"npm\",    \"webpack\"  ],  \"author\":\"AngelaBranaes\",  \"license\":\"MIT\",  \"devDependencies\":{    \"webpack\":\"^3.0.0\"  }}添加Babel支持Babel能够允许我们使用最新的JavaScript特性编码，即便是浏览器还没有支持它们。通过安装ES2015和reactpresets，Babel能够把使用Javascript新特性和Reactjsx的代码转换为与当前浏览器兼容的JavaScript语法。Babel转换JavaScript代码的示例安装Babel：Python$npmibabel-corebabel-loaderbabel-preset-es2015babel-preset-react--save-dev1$npmibabel-corebabel-loaderbabel-preset-es2015babel-preset-react--save-dev添加Babelpresets到package.json文件中：Python\"babel\":{\"presets\":[\"es2015\",\"react\"]},123456\"babel\":{  \"presets\":[    \"es2015\",    \"react\"  ]},在Webpack的配置中添加一条babel-loader规则。注意，我们在规则中排除了node_modules。这可以保证Babel不会尝不会对node模块进行转换，从而不会影响到node程序的加载速度。Pythonmodule:{rules:[{test:/\\.jsx?/,exclude:/node_modules/,use:'babel-loader'}]}123456789module:{  rules:[    {      test:/\\.jsx?/,      exclude:/node_modules/,      use:'babel-loader'    }  ]}创建index.jsx和index.html为了能在浏览器中看到一些东西，我们将创建一个简单的index.html页面，这个页面只显示一个由JavaScript弹出的“HelloWorld!”对话框，以此来证明设置是正确的。在static目录中创建一个index.html文件，并填写下面的代码：Python<!—index.html—><html><head><metacharset=\"utf-8\"><!--LatestcompiledandminifiedbootstrapCSS--><linkrel=\"stylesheet\"href=\"https://maxcdn.bootstrapcdn.com/bootstrap/latest/css/bootstrap.min.css\"><title>CreatingaFull-StackPythonApplicationwithNPM,React.jsandWebpack</title></head><body><divid=\"content\"/><scriptsrc=\"dist/bundle.js\"type=\"text/javascript\"></script></body></html>12345678910111213<!—index.html—><html>  <head>    <metacharset=\"utf-8\">    <!--LatestcompiledandminifiedbootstrapCSS-->    <linkrel=\"stylesheet\"href=\"https://maxcdn.bootstrapcdn.com/bootstrap/latest/css/bootstrap.min.css\">    <title>CreatingaFull-StackPythonApplicationwithNPM,React.jsandWebpack</title>  </head>  <body>    <divid=\"content\"/>    <scriptsrc=\"dist/bundle.js\"type=\"text/javascript\"></script>  </body></html>在static/js目录下创建一个index.jsx文件，并添加下面的代码：Pythonalert(“HelloWorld!”);1alert(“HelloWorld!”);启动一个独立的终端窗口来运行前面创建的Webpackwatch命令，这样当我们在工作时，它可以在后台一直运行。它会在没有编码错误的前提下自动构建你的包。Python$npmrunwatch1$npmrunwatch打开浏览器并访问index.html，应该能够看到弹出一个写着“HelloWorld!”的提示窗口。创建一个简单的React应用首先需要安装React：Python$npmireactreact-dom--save-dev1$npmireactreact-dom--save-dev下一步让我们用一个简单的React应用替换掉前面的index.jsx，并让它加载一个创建在单独的App.js文件中的React类。Python//index.jsximportReactfrom\"react\";importReactDOMfrom\"react-dom\";importAppfrom\"./App\";ReactDOM.render(<App/>,document.getElementById(\"content\"));12345//index.jsximportReactfrom\"react\";importReactDOMfrom\"react-dom\";importAppfrom\"./App\";ReactDOM.render(<App/>,document.getElementById(\"content\"));React类需要在不同的React源码文件中做导出，以方便后面的使用。通常每个文件中只写一个类，并且导出。Python//App.jsximportReactfrom“react”;exportdefaultclassAppextendsReact.Component{render(){return<p>HelloReact!</p>;}}1234567//App.jsximportReactfrom“react”;exportdefaultclassAppextendsReact.Component{  render(){    return<p>HelloReact!</p>;  }}如果我们现在刷新浏览器，页面上将会显示“HelloReact!”，而不再是“HelloWorld!”提示框。配置Python服务关于Python服务器我们将会使用Flask。Flask是小型Python应用的最佳选择之一。“微框架（microframework）”可以使你在短短几分钟内轻松快速的使一个服务跑起来。对于大型应用和某些专业领域，企业通常会使用Pyramid或Django。如果你想在自己的环境中拥有很大的灵活性和能够自定义配置的特性，Pyramid是一个不错的选择。Django则提供了一个全功能的Web框架，同时使你不必为应用的配置花费太多的时间，比如在数据库配置等方面。创建一个新的virtualenv并安装Flask在server目录中创建Flask服务源码文件，添加一个用来返回返回“HelloWorld!”的端点路由“/hello”，再添加一个主页面端点路由“/“用来渲染index.html模版。Python#server.pyfromflaskimportFlask,render_templateapp=Flask(__name__,static_folder=\"../static/dist\",template_folder=\"../static\")@app.route(\"/\")defindex():returnrender_template(\"index.html\")@app.route(\"/hello\")defhello():return\"HelloWorld!”if__name__==\"__main__\":app.run()123456789101112131415#server.pyfromflaskimportFlask,render_template app=Flask(__name__,static_folder=\"../static/dist\",template_folder=\"../static\") @app.route(\"/\")defindex():    returnrender_template(\"index.html\") @app.route(\"/hello\")defhello():    return\"HelloWorld!” if__name__==\"__main__\":app.run()运行python服务：Python$pythonserver.py1$pythonserver.py接下来访问http://localhost:5000/就可以看到react应用提供的的“HelloReact!”提示。访问http://localhost:5000/hello将会看到由Python端点路由返回的“HelloWorld!”恭喜，现在你已经有了一个基本的全栈应用如果你想要学习如何与服务器进行通信，以及怎样使自己的程序更加美观，请等待本文的下半部分：《一个小时搭建一个全栈Web应用框架——界面美化与功能实现》2赞15收藏3评论"], "art_url": ["http://python.jobbole.com/88394/"]}
{"art_img": ["http://jbcdn2.b0.upaiyun.com/2014/12/398ac9dbe309ef51ebbde4bd53df99e3.jpg"], "art_title": ["我常用的 Python 调试工具"], "art_create_time": ["2013/05/10"], "art_content": ["本文由伯乐在线-高磊翻译。未经许可，禁止转载！英文出处：IonelCristianMărieș。欢迎加入翻译组。以下是我做调试或分析时用过的工具的一个概览。如果你知道有更好的工具，请在评论中留言，可以不用很完整的介绍。日志没错，就是日志。再多强调在你的应用里保留足量的日志的重要性也不为过。你应当对重要的内容打日志。如果你的日志打的足够好的话，单看日志你就能发现问题所在。那样可以节省你大量的时间。如果一直以来你都在代码里乱用print语句，马上停下来。换用logging.debug。以后你还可以继续复用，或是全部停用等等。 跟踪有时更好的办法是看执行了哪些语句。你可以使用一些IDE的调试器的单步执行，但你需要明确知道你在找那些语句，否则整个过程会进行地非常缓慢。标准库里面的trace模块，可以打印运行时包含在其中的模块里所有执行到的语句。(就像制作一份项目报告)Pythonpython-mtrace–tracescript.py1python-mtrace–tracescript.py这会产生大量输出（执行到的每一行都会被打印出来，你可能想要用grep过滤那些你感兴趣的模块）.比如：Pythonpython-mtrace–tracescript.py|egrep&#039;^(mod1.py|mod2.py)&#039;1python-mtrace–tracescript.py|egrep&#039;^(mod1.py|mod2.py)&#039; 调试器以下是如今应该人尽皆知的一个基础介绍：Pythonimportpdbpdb.set_trace()#开启pdb提示12importpdbpdb.set_trace()#开启pdb提示或者Pythontry:（一段抛出异常的代码）except:importpdbpdb.pm()#或者pdb.post_mortem()12345try:（一段抛出异常的代码）except:    importpdb    pdb.pm()#或者pdb.post_mortem()或者(输入c开始执行脚本)Pythonpython-mpdbscript.py1python-mpdbscript.py在输入-计算-输出循环(注：REPL，READ-EVAL-PRINT-LOOP的缩写)环境下，可以有如下操作：corcontinueqorquitlorlist,显示当前步帧的源码worwhere,回溯调用过程dordown,后退一步帧（注：相当于回滚）uorup,前进一步帧(回车),重复上一条指令其余的几乎全部指令（还有很少的其他一些命令除外）,在当前步帧上当作python代码进行解析。如果你觉得挑战性还不够的话，可以试下smiley，-它可以给你展示那些变量而且你能使用它来远程追踪程序。 更好的调试器pdb的直接替代者：ipdb(easy_installipdb)–类似ipython(有自动完成，显示颜色等)pudb(easy_installpudb)–基于curses（类似图形界面接口），特别适合浏览源代码 远程调试器安装方式:Pythonsudoapt-getinstallwinpdb1sudoapt-getinstallwinpdb用下面的方式取代以前的pdb.set_trace()：Pythonimportrpdb2rpdb2.start_embedded_debugger(\"secretpassword\")12importrpdb2rpdb2.start_embedded_debugger(\"secretpassword\")现在运行winpdb,文件-关联不喜欢Winpdb?也可以直接包装PDB在TCP之上运行！这样做：PythonimportlogggingclassRdb(pdb.Pdb):\"\"\"Thiswillrunpdbasaephemeraltelnetservice.Onceyouconnectnooneelsecanconnect.Onconstructionthisobjectwillblockexecutiontillaclienthasconnected.Basedonhttps://github.com/tamentis/rpdbIthink...Tousethis::Rdb(4444).set_trace()Thenrun:telnet127.0.0.14444\"\"\"def__init__(self,port=0):self.old_stdout=sys.stdoutself.old_stdin=sys.stdinself.listen_socket=socket.socket(socket.AF_INET,socket.SOCK_STREAM)self.listen_socket.bind(('0.0.0.0',port))ifnotport:logging.critical(\"PDBremotesessionopenon:%s\",self.listen_socket.getsockname())print>>sys.__stderr__,\"PDBremotesessionopenon:\",self.listen_socket.getsockname()sys.stderr.flush()self.listen_socket.listen(1)self.connected_socket,address=self.listen_socket.accept()self.handle=self.connected_socket.makefile('rw')pdb.Pdb.__init__(self,completekey='tab',stdin=self.handle,stdout=self.handle)sys.stdout=sys.stdin=self.handledefdo_continue(self,arg):sys.stdout=self.old_stdoutsys.stdin=self.old_stdinself.handle.close()self.connected_socket.close()self.listen_socket.close()self.set_continue()return1do_c=do_cont=do_continuedefset_trace():\"\"\"OpensaremotePDBonfirstavailableport.\"\"\"rdb=Rdb()rdb.set_trace()123456789101112131415161718192021222324252627282930313233343536373839404142434445464748importloggging classRdb(pdb.Pdb):    \"\"\"    Thiswillrunpdbasaephemeraltelnetservice.Onceyouconnectnoone    elsecanconnect.Onconstructionthisobjectwillblockexecutiontilla    clienthasconnected.     Basedonhttps://github.com/tamentis/rpdbIthink...     Tousethis::         Rdb(4444).set_trace()     Thenrun:telnet127.0.0.14444    \"\"\"    def__init__(self,port=0):        self.old_stdout=sys.stdout        self.old_stdin=sys.stdin        self.listen_socket=socket.socket(socket.AF_INET,socket.SOCK_STREAM)        self.listen_socket.bind(('0.0.0.0',port))        ifnotport:            logging.critical(\"PDBremotesessionopenon:%s\",self.listen_socket.getsockname())            print>>sys.__stderr__,\"PDBremotesessionopenon:\",self.listen_socket.getsockname()            sys.stderr.flush()        self.listen_socket.listen(1)        self.connected_socket,address=self.listen_socket.accept()        self.handle=self.connected_socket.makefile('rw')        pdb.Pdb.__init__(self,completekey='tab',stdin=self.handle,stdout=self.handle)        sys.stdout=sys.stdin=self.handle     defdo_continue(self,arg):        sys.stdout=self.old_stdout        sys.stdin=self.old_stdin        self.handle.close()        self.connected_socket.close()        self.listen_socket.close()        self.set_continue()        return1     do_c=do_cont=do_continue defset_trace():    \"\"\"    OpensaremotePDBonfirstavailableport.    \"\"\"    rdb=Rdb()    rdb.set_trace()只想要一个REPL环境？试试IPython如何？如果你不需要一个完整齐全的调试器，那就只需要用一下的方式启动一个IPython即可：PythonimportIPythonIPython.embed()12importIPythonIPython.embed() 标准linux工具我常常惊讶于它们竟然远未被充分利用。你能用这些工具解决很大范围内的问题：从性能问题（太多的系统调用，内存分配等等）到死锁，网络问题，磁盘问题等等。其中最有用的是最直接的strace，只需要运行sudostrace-p12345或者strace-f指令(-f即同时追踪fork出来的子进程)，这就行了。输出一般会非常大，所以你可能想要把它重定向到一个文件以便作更多的分析（只需要加上&>文件名）。再就是ltrace,有点类似strace，不同的是，它输出的是库函数调用。参数大体相同。还有lsof用来指出你在ltrace/strace中看到的句柄数值的意义。比如：Pythonlsof-p123451lsof-p12345 更好的跟踪使用简单而可以做很多事情-人人都该装上htop！Pythonsudoapt-getinstallhtopsudohtop12sudoapt-getinstallhtopsudohtop现在找到那些你想要的进程，再输入：Pythons-代表系统调用过程(类似strace)L-代表库调用过程(类似ltrace)l-代表lsof123s-代表系统调用过程(类似strace)L-代表库调用过程(类似ltrace)l-代表lsof 监控没有好的持续的服务器监控，但是如果你曾遇到一些很诡异的情况，诸如为什么一切都运行的那么慢，那些系统资源都干什么去了，。。。等这些问题，想弄明白却又无处下手之际，不必动用iotop、iftop、htop、iostat、vmstat这些工具，就用dstat吧！它可以做之前我们提过的大部分工作可以做的事情，而且也许可以做的更好！它会用一种紧凑的，代码高亮的方式（不同于iostat,vmstat）向你持续展示数据，你还经常可以看到过去的数据（不同于iftop、iostop、htop）。只需运行：Pythondstat--cpu--io--mem--net--load--fs--vm--disk-util--disk-tps--freespace--swap--top-io--top-bio-adv1dstat--cpu--io--mem--net--load--fs--vm--disk-util--disk-tps--freespace--swap--top-io--top-bio-adv很可能有一种更简短的方式来写上面这条命令，这是一个相当复杂而又强大的工具，但是这里我只提到了一些基本的内容（安装以及基础的命令）Pythonsudoapt-getinstallgdbpython-dbgzcat/usr/share/doc/python2.7/gdbinit.gz&gt;~/.gdbinit12sudoapt-getinstallgdbpython-dbgzcat/usr/share/doc/python2.7/gdbinit.gz&gt;~/.gdbinit用python2.7-dbg运行程序：Pythonsudogdb-p123451sudogdb-p12345现在使用：Pythonbt-堆栈跟踪（C级别）pystack-python堆栈跟踪,不幸的是你需要有~/.gdbinit并且使用python-dbgc-继续123bt-堆栈跟踪（C级别）pystack-python堆栈跟踪,不幸的是你需要有~/.gdbinit并且使用python-dbgc-继续发生段错误？用faulthandler！python3.3版本以后新增的一个很棒的功能，可以向后移植到python2.x版本。只需要运行下面的语句，你就可以大抵知道什么原因引起来段错误。Pythonimportfaulthandlerfaulthandler.enable()12importfaulthandlerfaulthandler.enable()内存泄露嗯，这种情况下有很多的工具可以使用，其中有一些专门针对WSGI的程序比如Dozer，但是我最喜欢的当然是objgraph。使用简单方便，让人惊讶！它没有集成WSGI或者其他，所以你需要自己去发现运行代码的方法，像下面这样：Pythonimportobjgraphobjs=objgraph.by_type(&quot;Request&quot;)[:15]objgraph.show_backrefs(objs,max_depth=20,highlight=lambdav:vinobjs,123importobjgraphobjs=objgraph.by_type(&quot;Request&quot;)[:15]objgraph.show_backrefs(objs,max_depth=20,highlight=lambdav:vinobjs,Pythonfilename=&quot;/tmp/graph.png&quot;)Graphwrittento/tmp/objgraph-zbdM4z.dot(107nodes)Imagegeneratedas/tmp/graph.png123filename=&quot;/tmp/graph.png&quot;)Graphwrittento/tmp/objgraph-zbdM4z.dot(107nodes)Imagegeneratedas/tmp/graph.png你会得到像这样一张图（注意：它非常大)。你也可以得到一张点输出。 内存使用有时你想少用些内存。更少的内存分配常常可以使程序执行的更快，更好，用户希望内存合适好用)有许多可用的工具，但在我看来最好用的是pytracemalloc。与其他工具相比，它开销非常小（不需要依赖于严重影响速度的sys.settrace）而且输出非常详尽。但安装起来比较痛苦，你需要重新编译python，但有了apt，做起来也非常容易。只需要运行这些命令然后去吃顿午餐或者干点别的：Pythonapt-getsourcepython2.7cdpython2.7-*wget?https://github.com/wyplay/pytracemalloc/raw/master/python2.7_track_free_list.patchpatch-p1&lt;python2.7_track_free_list.patchdebuild-us-uccd..sudodpkg-ipython2.7-minimal_2.7*.debpython2.7-dev_*.deb1234567apt-getsourcepython2.7cdpython2.7-*wget?https://github.com/wyplay/pytracemalloc/raw/master/python2.7_track_free_list.patchpatch-p1&lt;python2.7_track_free_list.patchdebuild-us-uccd..sudodpkg-ipython2.7-minimal_2.7*.debpython2.7-dev_*.deb接着安装pytracemalloc（注意如果你在一个virtualenv虚拟环境下操作，你需要在重新安装python后再次重建–只需要运行virtualenvmyenv）Pythonpipinstallpytracemalloc1pipinstallpytracemalloc现在像下面这样在代码里包装你的应用程序Pythonimporttracemalloc,timetracemalloc.enable()top=tracemalloc.DisplayTop(5000,#logthetop5000locationsfile=open('/tmp/memory-profile-%s'%time.time(),\"w\"))top.show_lineno=Truetry:#codethatneedstobetracedfinally:top.display()1234567891011importtracemalloc,timetracemalloc.enable()top=tracemalloc.DisplayTop(    5000,#logthetop5000locations    file=open('/tmp/memory-profile-%s'%time.time(),\"w\"))top.show_lineno=Truetry:    #codethatneedstobetracedfinally:    top.display()输出会像这样：Python2013-05-3118:05:07:Top5000allocationsperfileandline#1:.../site-packages/billiard/_connection.py:198:size=1288KiB,count=70(+0),average=18KiB#2:.../site-packages/billiard/_connection.py:199:size=1288KiB,count=70(+0),average=18KiB#3:.../python2.7/importlib/__init__.py:37:size=459KiB,count=5958(+0),average=78B#4:.../site-packages/amqp/transport.py:232:size=217KiB,count=6960(+0),average=32B#5:.../site-packages/amqp/transport.py:231:size=206KiB,count=8798(+0),average=24B#6:.../site-packages/amqp/serialization.py:210:size=199KiB,count=822(+0),average=248B#7:.../lib/python2.7/socket.py:224:size=179KiB,count=5947(+0),average=30B#8:.../celery/utils/term.py:89:size=172KiB,count=1953(+0),average=90B#9:.../site-packages/kombu/connection.py:281:size=153KiB,count=2400(+0),average=65B#10:.../site-packages/amqp/serialization.py:462:size=147KiB,count=4704(+0),average=32B12345678910111213141516171819202013-05-3118:05:07:Top5000allocationsperfileandline#1:.../site-packages/billiard/_connection.py:198:size=1288KiB,count=70(+0),average=18KiB#2:.../site-packages/billiard/_connection.py:199:size=1288KiB,count=70(+0),average=18KiB#3:.../python2.7/importlib/__init__.py:37:size=459KiB,count=5958(+0),average=78B#4:.../site-packages/amqp/transport.py:232:size=217KiB,count=6960(+0),average=32B#5:.../site-packages/amqp/transport.py:231:size=206KiB,count=8798(+0),average=24B#6:.../site-packages/amqp/serialization.py:210:size=199KiB,count=822(+0),average=248B#7:.../lib/python2.7/socket.py:224:size=179KiB,count=5947(+0),average=30B#8:.../celery/utils/term.py:89:size=172KiB,count=1953(+0),average=90B#9:.../site-packages/kombu/connection.py:281:size=153KiB,count=2400(+0),average=65B#10:.../site-packages/amqp/serialization.py:462:size=147KiB,count=4704(+0),average=32B…很美，不是吗？补充：更多有关调试的内容见这里。1赞16收藏11评论关于作者：高磊关注技术，爱好技术，许不大成，心仍往之！新浪微博：@kaulie个人主页·我的文章·18"], "art_url": ["http://python.jobbole.com/51062/"]}
{"art_img": ["http://jbcdn2.b0.upaiyun.com/2015/10/bfa0d07e7eb2fac2eb80cd5df9567931.jpg"], "art_title": ["Python 爬虫 (四) --多线程"], "art_create_time": ["2017/08/27"], "art_content": ["原文出处：Andrew_liu   1.thread模块python是支持多线程的,主要是通过thread和threading这两个模块来实现的。python的thread模块是比较底层的模块(或者说轻量级)，python的threading模块是对thread做了一些包装的，可以更加方便的被使用。简要的看一下thread模块中含函数和常量Pythonimportthreadthread.LockType#锁对象的一种,用于线程的同步thread.error#线程的异常thread.start_new_thread(function,args[,kwargs])#创建一个新的线程function:线程执行函数args:线程执行函数的参数,类似为tuple,kwargs:是一个字典返回值:返回线程的标识符thread.exit()#线程退出函数thread.allocate_lock()#生成一个未锁状态的锁对象返回值:返回一个锁对象1234567891011121314importthread thread.LockType  #锁对象的一种,用于线程的同步thread.error  #线程的异常 thread.start_new_thread(function,args[,kwargs])  #创建一个新的线程function:线程执行函数args:线程执行函数的参数,类似为tuple,kwargs:是一个字典返回值:返回线程的标识符 thread.exit()  #线程退出函数thread.allocate_lock()  #生成一个未锁状态的锁对象返回值:返回一个锁对象锁对象的方法Pythonlock.acquire([waitflag])#获取锁无参数时,无条件获取锁,无法获取时,会被阻塞,知道可以锁被释放有参数时,waitflag=0时,表示只有在不需要等待的情况下才获取锁,非零情况与上面相同返回值:　获得锁成功返回True,获得锁失败返回Falselock.release()#释放锁lock.locked()#获取当前锁的状态返回值:如果锁已经被某个线程获取,返回True,否则为False123456789lock.acquire([waitflag])#获取锁无参数时,无条件获取锁,无法获取时,会被阻塞,知道可以锁被释放有参数时,waitflag=0时,表示只有在不需要等待的情况下才获取锁,非零情况与上面相同返回值:　获得锁成功返回True,获得锁失败返回False lock.release()#释放锁 lock.locked()#获取当前锁的状态返回值:如果锁已经被某个线程获取,返回True,否则为False1.1.thread多线程Python#!/usr/bin/envpython#-*-coding:utf-8-*-importthreadimporttimedefprint_time(thread_name,delay):count=0whilecount<5:time.sleep(delay)count+=1print\"%s:%s\"%(thread_name,time.ctime(time.time()))try:thread.start_new_thread(print_time,(\"Thread-1\",2,))thread.start_new_thread(print_time,(\"Thread-2\",4,))except:print\"Error:unabletostartthethread\"whileTrue:pass123456789101112131415161718192021#!/usr/bin/envpython#-*-coding:utf-8-*- importthreadimporttime defprint_time(thread_name,delay):    count=0    whilecount<5:        time.sleep(delay)        count+=1        print\"%s:%s\"%(thread_name,time.ctime(time.time())) try:    thread.start_new_thread(print_time,(\"Thread-1\",2,))    thread.start_new_thread(print_time,(\"Thread-2\",4,))except:    print\"Error:unabletostartthethread\" whileTrue:    pass2.threading模块python的threading模块是对thread做了一些包装的，可以更加方便的被使用。经常和Queue结合使用,Queue模块中提供了同步的、线程安全的队列类，包括FIFO（先入先出)队列Queue，LIFO（后入先出）队列LifoQueue，和优先级队列PriorityQueue。这些队列都实现了锁原语，能够在多线程中直接使用。可以使用队列来实现线程间的同步2.1.常用函数和对象Python#函数threading.active_count()#返回当前线程对象Thread的个数threading.enumerate()#返回当前运行的线程对象Thread(包括后台的)的listthreading.Condition()#返回条件变量对象的工厂函数,主要用户线程的并发threading.current_thread()#返回当前的线程对象Thread,文档后面解释没看懂threading.Lock()#返回一个新的锁对象,是在thread模块的基础上实现的与acquire()和release()结合使用#类threading.Thread#一个表示线程控制的类,这个类常被继承thraeding.Timer#定时器,线程在一定时间后执行threading.ThreadError#引发中各种线程相关异常1234567891011#函数threading.active_count()  #返回当前线程对象Thread的个数threading.enumerate()  #返回当前运行的线程对象Thread(包括后台的)的listthreading.Condition()  #返回条件变量对象的工厂函数,主要用户线程的并发threading.current_thread()  #返回当前的线程对象Thread,文档后面解释没看懂threading.Lock()  #返回一个新的锁对象,是在thread模块的基础上实现的与acquire()和release()结合使用 #类threading.Thread  #一个表示线程控制的类,这个类常被继承thraeding.Timer  #定时器,线程在一定时间后执行threading.ThreadError  #引发中各种线程相关异常2.1.1.Thread对象一般来说，使用线程有两种模式,一种是创建线程要执行的函数,把这个函数传递进Thread对象里，让它来执行.另一种是直接从Thread继承，创建一个新的class，把线程执行的代码放到这个新的class里。常用两种方式运行线程(线程中包含name属性):在构造函数中传入用于线程运行的函数(这种方式更加灵活)在子类中重写threading.Thread基类中run()方法(只重写__init__()和run()方法)创建线程对象后,通过调用start()函数运行线程,然后会自动调用run()方法.　通过设置｀daemon｀属性,可以将线程设置为守护线程Pythonthreading.Thread(group=None,target=None,name=None,args=()kwars={})group:应该为Nonetarget:可以传入一个函数用于run()方法调用,name:线程名默认使用\"Thread-N\"args:元组,表示传入target函数的参数kwargs:字典,传入target函数中关键字参数属性:name#线程表示,没有任何语义doemon#布尔值,如果是守护线程为True,不是为False,主线程不是守护线程,默认threading.Thread.damon=False类方法:run()#用以表示线程活动的方法。start()#启动线程活动。join([time])#等待至线程中止。这阻塞调用线程直至线程的join()方法被调用中止-正常退出或者抛出未处理的异常-或者是可选的超时发生。isAlive():返回线程是否活动的。getName():返回线程名。setName():设置线程名。123456789101112131415161718threading.Thread(group=None,target=None,name=None,args=()kwars={})group:应该为Nonetarget:可以传入一个函数用于run()方法调用,name:线程名默认使用\"Thread-N\"args:元组,表示传入target函数的参数kwargs:字典,传入target函数中关键字参数 属性:name  #线程表示,没有任何语义doemon  #布尔值,如果是守护线程为True,不是为False,主线程不是守护线程,默认threading.Thread.damon=False 类方法:run()  #用以表示线程活动的方法。start()  #启动线程活动。join([time])  #等待至线程中止。这阻塞调用线程直至线程的join()方法被调用中止-正常退出或者抛出未处理的异常-或者是可选的超时发生。isAlive():返回线程是否活动的。getName():返回线程名。setName():设置线程名。范例:Python#!/usr/bin/envpython#-*-coding:utf-8-*-importthreadingimporttimedeftest_thread(count):whilecount>0:print\"count=%d\"%countcount=count-1time.sleep(1)defmain():my_thread=threading.Thread(target=test_thread,args=(10,))my_thread.start()my_thread.join()if__name__=='__main__':main()12345678910111213141516171819#!/usr/bin/envpython#-*-coding:utf-8-*- importthreadingimporttime deftest_thread(count):    whilecount>0:        print\"count=%d\"%count        count=count-1        time.sleep(1) defmain():    my_thread=threading.Thread(target=test_thread,args=(10,))    my_thread.start()    my_thread.join() if__name__=='__main__':    main()2.2.常用多线程写法固定线程运行的函数Python#!/usr/bin/envpython#-*-coding:utf-8-*-importthreading,threadimporttimeclassMyThread(threading.Thread):\"\"\"docstringforMyThread\"\"\"def__init__(self,thread_id,name,counter):super(MyThread,self).__init__()#调用父类的构造函数self.thread_id=thread_idself.name=nameself.counter=counterdefrun(self):print\"Starting\"+self.nameprint_time(self.name,self.counter,5)print\"Exiting\"+self.namedefprint_time(thread_name,delay,counter):whilecounter:time.sleep(delay)print\"%s%s\"%(thread_name,time.ctime(time.time()))counter-=1defmain():#创建新的线程thread1=MyThread(1,\"Thread-1\",1)thread2=MyThread(2,\"Thread-2\",2)#开启线程thread1.start()thread2.start()thread1.join()thread2.join()print\"ExitingMainThread\"if__name__=='__main__':main()12345678910111213141516171819202122232425262728293031323334353637383940414243#!/usr/bin/envpython#-*-coding:utf-8-*- importthreading,threadimporttime  classMyThread(threading.Thread):    \"\"\"docstringforMyThread\"\"\"     def__init__(self,thread_id,name,counter):        super(MyThread,self).__init__()  #调用父类的构造函数        self.thread_id=thread_id        self.name=name        self.counter=counter     defrun(self):        print\"Starting\"+self.name        print_time(self.name,self.counter,5)        print\"Exiting\"+self.name defprint_time(thread_name,delay,counter):    whilecounter:        time.sleep(delay)        print\"%s%s\"%(thread_name,time.ctime(time.time()))        counter-=1 defmain():    #创建新的线程    thread1=MyThread(1,\"Thread-1\",1)    thread2=MyThread(2,\"Thread-2\",2)     #开启线程    thread1.start()    thread2.start()      thread1.join()    thread2.join()    print\"ExitingMainThread\" if__name__=='__main__':    main()外部传入线程运行的函数Python#/usr/bin/envpython#-*-coding:utf-8-*-importthreadingimporttimeclassMyThread(threading.Thread):\"\"\"属性:target:传入外部函数,用户线程调用args:函数参数\"\"\"def__init__(self,target,args):super(MyThread,self).__init__()#调用父类的构造函数self.target=targetself.args=argsdefrun(self):self.target(self.args)defprint_time(counter):whilecounter:print\"counter=%d\"%countercounter-=1time.sleep(1)defmain():my_thread=MyThread(print_time,10)my_thread.start()my_thread.join()if__name__=='__main__':main()1234567891011121314151617181920212223242526272829303132#/usr/bin/envpython#-*-coding:utf-8-*-importthreadingimporttime classMyThread(threading.Thread):    \"\"\"    属性:    target:传入外部函数,用户线程调用    args:函数参数    \"\"\"    def__init__(self,target,args):        super(MyThread,self).__init__()  #调用父类的构造函数        self.target=target        self.args=args     defrun(self):        self.target(self.args) defprint_time(counter):    whilecounter:        print\"counter=%d\"%counter        counter-=1        time.sleep(1) defmain():    my_thread=MyThread(print_time,10)    my_thread.start()    my_thread.join() if__name__=='__main__':    main()2.3.生产者消费者问题试着用python写了一个生产者消费者问题(伪生产者消费者),只是使用简单的锁,感觉有点不太对,下面另一个程序会写出正确的生产者消费者问题Python#!/usr/bin/envpython#-*-coding:utf-8-*-importthread,threadingimporturllib2importtime,randomimportQueueshare_queue=Queue.Queue()#共享队列my_lock=thread.allocate_lock()classProducer(threading.Thread):defrun(self):products=range(5)globalshare_queuewhileTrue:num=random.choice(products)my_lock.acquire()share_queue.put(num)print\"Produce:\",nummy_lock.release()time.sleep(random.random())classConsumer(threading.Thread):defrun(self):globalshare_queuewhileTrue:my_lock.acquire()ifshare_queue.empty():#这里没有使用信号量机制进行阻塞等待,print\"QueueisEmpty...\"my_lock.release()time.sleep(random.random())continuenum=share_queue.get()print\"Consumer:\",nummy_lock.release()time.sleep(random.random())defmain():producer=Producer()consumer=Consumer()producer.start()consumer.start()if__name__=='__main__':main()1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#!/usr/bin/envpython#-*-coding:utf-8-*- importthread,threadingimporturllib2importtime,randomimportQueue share_queue=Queue.Queue()  #共享队列my_lock=thread.allocate_lock()classProducer(threading.Thread):     defrun(self):        products=range(5)        globalshare_queue        whileTrue:            num=random.choice(products)            my_lock.acquire()            share_queue.put(num)            print  \"Produce:\",num            my_lock.release()            time.sleep(random.random()) classConsumer(threading.Thread):     defrun(self):        globalshare_queue        whileTrue:            my_lock.acquire()            ifshare_queue.empty():#这里没有使用信号量机制进行阻塞等待,                print\"QueueisEmpty...\"                  my_lock.release()                time.sleep(random.random())                continue            num=share_queue.get()            print\"Consumer:\",num            my_lock.release()            time.sleep(random.random()) defmain():    producer=Producer()    consumer=Consumer()    producer.start()    consumer.start() if__name__=='__main__':    main()杀死多线程程序方法:使用control+z挂起程序(程序依然在后台,可以使用psaux查看),获得程序的进程号,然后使用kill-9进程号杀死进程参考一篇帖子解决了上述问题,重写了生产者消费者问题程序,参考链接惯例放在最后.使用了wait()和notify()解决当然最简答的方法是直接使用Queue,Queue封装了Condition的行为,如wait(),notify(),acquire(),没看文档就这样,使用了Queue竟然不知道封装了这些函数,继续滚去看文档了Python#!/usr/bin/envpython#-*-coding:utf-8-*-importthreadingimportrandom,time,QueueMAX_SIZE=5SHARE_Q=[]#模拟共享队列CONDITION=threading.Condition()classProducer(threading.Thread):defrun(self):products=range(5)globalSHARE_QwhileTrue:CONDITION.acquire()iflen(SHARE_Q)==5:print\"Queueisfull..\"CONDITION.wait()print\"Consumerhavecomsumedsomething\"product=random.choice(products)SHARE_Q.append(product)print\"Producer:\",productCONDITION.notify()CONDITION.release()time.sleep(random.random())classConsumer(threading.Thread):defrun(self):globalSHARE_QwhileTrue:CONDITION.acquire()ifnotSHARE_Q:print\"QueueisEmpty...\"CONDITION.wait()print\"Producerhaveproductedsomething\"product=SHARE_Q.pop(0)print\"Consumer:\",productCONDITION.notify()CONDITION.release()time.sleep(random.random())defmain():producer=Producer()consumer=Consumer()producer.start()consumer.start()if__name__=='__main__':main()12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#!/usr/bin/envpython#-*-coding:utf-8-*- importthreadingimportrandom,time,Queue MAX_SIZE=5SHARE_Q=[]  #模拟共享队列CONDITION=threading.Condition() classProducer(threading.Thread):     defrun(self):        products=range(5)        globalSHARE_Q        whileTrue:            CONDITION.acquire()            iflen(SHARE_Q)==5:                print\"Queueisfull..\"                CONDITION.wait()                print\"Consumerhavecomsumedsomething\"            product=random.choice(products)            SHARE_Q.append(product)            print\"Producer:\",product            CONDITION.notify()            CONDITION.release()            time.sleep(random.random()) classConsumer(threading.Thread):     defrun(self):        globalSHARE_Q        whileTrue:            CONDITION.acquire()            ifnotSHARE_Q:                print\"QueueisEmpty...\"                CONDITION.wait()                print\"Producerhaveproductedsomething\"            product=SHARE_Q.pop(0)            print\"Consumer:\",product            CONDITION.notify()            CONDITION.release()            time.sleep(random.random()) defmain():    producer=Producer()    consumer=Consumer()    producer.start()    consumer.start() if__name__=='__main__':    main()2.4.简单锁如果只是简单的加锁解锁可以直接使用threading.Lock()生成锁对象,然后使用acquire()和release()方法例如:Python#!/usr/bin/envpython#-*-coding:utf-8-*-importthreadingimporttimeclassMyThread(threading.Thread):def__init__(self,thread_id,name,counter):threading.Thread.__init__(self)self.thread_id=thread_idself.name=nameself.counter=counterdefrun(self):#重写run方法,添加线程执行逻辑,start函数运行会自动执行print\"Starting\"+self.namethreadLock.acquire()#获取所print_time(self.name,self.counter,3)threadLock.release()#释放锁defprint_time(thread_name,delay,counter):whilecounter:time.sleep(delay)print\"%s%s\"%(thread_name,time.ctime(time.time()))counter-=1threadLock=threading.Lock()threads=[]#存放线程对象thread1=MyThread(1,\"Thread-1\",1)thread2=MyThread(2,\"Thread-2\",2)#开启线程thread1.start()thread2.start()fortinthreads:t.join()#等待线程直到终止print\"ExitingMainThread\"12345678910111213141516171819202122232425262728293031323334353637383940#!/usr/bin/envpython#-*-coding:utf-8-*- importthreadingimporttime classMyThread(threading.Thread):     def__init__(self,thread_id,name,counter):        threading.Thread.__init__(self)        self.thread_id=thread_id        self.name=name        self.counter=counter     defrun(self):        #重写run方法,添加线程执行逻辑,start函数运行会自动执行        print  \"Starting\"+self.name        threadLock.acquire()#获取所        print_time(self.name,self.counter,3)        threadLock.release()#释放锁 defprint_time(thread_name,delay,counter):    whilecounter:        time.sleep(delay)        print\"%s%s\"%(thread_name,time.ctime(time.time()))        counter-=1 threadLock=threading.Lock()threads=[]#存放线程对象 thread1=MyThread(1,\"Thread-1\",1)thread2=MyThread(2,\"Thread-2\",2) #开启线程thread1.start()thread2.start() fortinthreads:    t.join()  #等待线程直到终止print\"ExitingMainThread\"2.5.Condition如果是向生产者消费者类似的情形,使用Condition类或者直接使用Queue模块Condition条件变量中有acquire()和release方法用来调用锁的方法,有wait(),notify(),notifyAll()方法,后面是三个方法必须在获取锁的情况下调用,否则产生RuntimeError错误.当一个线程获得锁后,发现没有期望的资源或者状态,就会调用wait()阻塞,并释放已经获得锁,知道期望的资源或者状态发生改变当一个线程获得锁,改变了资源或者状态,就会调用notify()和notifyAll()去通知其他线程,Python#官方文档中提供的生产者消费者模型#Consumeoneitemcv.acquire()whilenotan_item_is_available():cv.wait()get_an_available_item()cv.release()#Produceoneitemcv.acquire()make_an_item_available()cv.notify()cv.release()12345678910111213#官方文档中提供的生产者消费者模型#Consumeoneitemcv.acquire()whilenotan_item_is_available():    cv.wait()get_an_available_item()cv.release() #Produceoneitemcv.acquire()make_an_item_available()cv.notify()cv.release()Python#threading.Condition类thread.Condition([lock])可选参数lock:必须是Lock或者RLock对象,并被作为underlying锁(悲观锁?),否则,会创建一个新的RLock对象作为underlying锁类方法:acquire()#获得锁release()#释放锁wait([timeout])#持续等待直到被notify()或者notifyAll()通知或者超时(必须先获得锁),#wait()所做操作,先释放获得的锁,然后阻塞,知道被notify或者notifyAll唤醒或者超时,一旦被唤醒或者超时,会重新获取锁(应该说抢锁),然后返回notify()#唤醒一个wait()阻塞的线程.notify_all()或者notifyAll()#唤醒所有阻塞的线程1234567891011#threading.Condition类thread.Condition([lock])可选参数lock:必须是Lock或者RLock对象,并被作为underlying锁(悲观锁?),否则,会创建一个新的RLock对象作为underlying锁 类方法:acquire()  #获得锁release()  #释放锁wait([timeout])  #持续等待直到被notify()或者notifyAll()通知或者超时(必须先获得锁),#wait()所做操作,先释放获得的锁,然后阻塞,知道被notify或者notifyAll唤醒或者超时,一旦被唤醒或者超时,会重新获取锁(应该说抢锁),然后返回notify()  #唤醒一个wait()阻塞的线程.notify_all()或者notifyAll()  #唤醒所有阻塞的线程参考程序可以查看上面的生产者消费者程序3.参考链接Producer–consumerproblemPython中的生产者消费者问题Python多线程threading官方文档thread官方文档1赞6收藏3评论"], "art_url": ["http://python.jobbole.com/88411/"]}
{"art_img": ["http://jbcdn2.b0.upaiyun.com/2015/02/2c224095b3bd5c6fee7c908a481292f7.jpg"], "art_title": ["Python :浅析 return 和 finally 共同挖的坑"], "art_create_time": ["2017/08/25"], "art_content": ["原文出处：Lin_R   初识return相信每一个用过Python函数的童鞋,肯定会用过return语句,return顾名思义,就是用来返回值给调用者,例如:Pythondeftest():a=2returnas=test()prints#输出结果2123456789deftest():    a=2    returna s=test()prints #输出结果2对于上面的结果,相信大家都不会感到意外,那么加大点难度,如果在return语句还有代码呢?那句代码会怎样呢?Pythondeftest():a=2returnas=3printss=test()prints#结果是什么?12345678910deftest():    a=2    returna    s=3    prints s=test()prints #结果是什么?老司机肯定一眼就能看出结果,但是对于尚在入门或者对return不很了解的童鞋,可能就会懵逼了~后面的两句代码是否会被执行?答案是:不会执行return正如它的名字那样,当执行这句代码,整个函数都会返回,整个调用就算结束了~所以在return后面的代码,都是不会被执行的!也正因为这个特性,所以有种编码规范叫earlyreturn的编码规范就被倡导。它的意思大概就是:当条件已经满足返回时,就马上返回举个例子来说明:Pythondeftest():a=2ifa>2:result='morethan'else:result='lessthan'returnresults=test()prints12345678910deftest():    a=2    ifa>2:        result='morethan'    else:        result='lessthan'    returnresult s=test()prints上面的代码应该比较容易理解,就是根据a的值,来决定返回的result是什么.这样的编码相信也是大部分童鞋喜欢用的,因为这样比较符合我们直觉,然而,这样写似乎有点浪费,因为当第一个判断结束了,如果结果为真,就应该返回morethan,然后结束函数,否则肯定就是返回lessthan,所以我们可以把代码调整成这样:Pythondeftest():a=2ifa>2:return'morethan'else:return'lessthan's=test()prints123456789deftest():    a=2    ifa>2:        return'morethan'    else:        return'lessthan' s=test()prints甚至是:Pythondeftest():a=2ifa>2:return'morethan'return'lessthan's=test()prints12345678deftest():    a=2    ifa>2:        return'morethan'    return'lessthan' s=test()prints结果都是和第一个写法是一样的!第一次看到这样写法的童鞋,可能会觉得比较难以接受,甚至觉得可读性很差,但是其实这样的写法,我觉得反而会稍微好点.因为:运行的代码数少了,调用方能更快得到结果有利于减少嵌套的层数,便于理解.对于第2点在这需要解释下,很多时候我们写得代码,嵌套很深,都是因为if/else的锅,因为嵌套的if/else比较多,所以导致一堆代码都嵌套得比较深,这样对于其他小伙伴,简直就是灾难,因为他们很可能在阅读这部分代码时,就忘了前面的逻辑….为了更加容易理解,举个代码例子:Pythondeftest():a=2ifa>2:result='not2'else:a+=2ifa<2:result='not2'else:foriinrange(2):print'test~'result='Target!'returnresults=test()prints#输出结果test~test~Target!123456789101112131415161718192021deftest():    a=2    ifa>2:        result='not2'    else:        a+=2        ifa<2:            result='not2'        else:            foriinrange(2):                print'test~'            result='Target!'    returnresult s=test()prints #输出结果test~test~Target!代码简化优化版:Pythondeftest():a=2ifa>2:return'not2'a+=2ifa<2:return'not2'foriinrange(2):print'test~'return'Target!'s=test()prints#输出结果test~test~Target!123456789101112131415161718192021deftest():    a=2    ifa>2:        return'not2'        a+=2    ifa<2:        return'not2'        foriinrange(2):        print'test~'     return'Target!' s=test()prints #输出结果test~test~Target!这样对比这来看,应该能更好地理解为什么说earlyreturn能够减少嵌套的层数吧~有疑问欢迎留言讨论~谈谈深坑刚才花了比较长的篇幅去介绍return,相信看到这里,对于return应该有比较基本的理解了!所以来聊聊更加迷惑的话题:Python当return遇上try..finally,会怎样呢?1当return遇上try..finally,会怎样呢?如果刚才有认真看的话,会注意到一句话,就是:Pythonreturn代表整个函数返回,函数调用算结束1return代表整个函数返回,函数调用算结束但事实真的这样吗?通常这样问,答案一般都不是~~先来看看例子:Pythondeftest():try:a=2returnaexcept:passfinally:print'finally's=test()prints123456789101112deftest():    try:        a=2        returna    except:        pass     finally:        print'finally' s=test()prints可以猜猜这句printa会不会打印?相信很多童鞋都想了一会,然后说不会~然而这个答案是错的,真正的输出是:Pythonfinally212finally2有木有觉得仿佛看见了新大陆,在一开始的例子中,return后面的语句没有被执行,但是在这里,相隔那么远,却依旧没有忘记,这或许就是”真爱”吧!然而就是因为这种”真爱”,总是会让一堆新老司机掉坑里..然后还不知道为毛..为了避免它们再继续借用打着”真爱”的幌子,欺负我们,让我们一起来揭开这”真爱”的真面目!于是,我们得借助偷窥神器:dis,想想都有点小兴奋!Pythonimportdisdeftest():try:a=2returnaexcept:passfinally:print'finally'printdis.dis(test)123456789101112importdisdeftest():    try:        a=2        returna    except:        pass     finally:        print'finally' printdis.dis(test)输出比较长,单独写:Python#输出结果60SETUP_FINALLY28(to31)3SETUP_EXCEPT14(to20)76LOAD_CONST1(2)9STORE_FAST0(a)812LOAD_FAST0(a)15RETURN_VALUE16POP_BLOCK17JUMP_FORWARD7(to27)9>>20POP_TOP21POP_TOP22POP_TOP1023JUMP_FORWARD1(to27)26END_FINALLY>>27POP_BLOCK28LOAD_CONST0(None)13>>31LOAD_CONST2('finally')34PRINT_ITEM35PRINT_NEWLINE36END_FINALLY37LOAD_CONST0(None)40RETURN_VALUE123456789101112131415161718192021222324252627#输出结果  6          0SETUP_FINALLY          28(to31)              3SETUP_EXCEPT            14(to20)   7          6LOAD_CONST              1(2)              9STORE_FAST              0(a)   8          12LOAD_FAST                0(a)            15RETURN_VALUE                    16POP_BLOCK                      17JUMP_FORWARD            7(to27)   9    >>  20POP_TOP                        21POP_TOP                        22POP_TOP             10          23JUMP_FORWARD            1(to27)            26END_FINALLY                >>  27POP_BLOCK                      28LOAD_CONST              0(None) 13    >>  31LOAD_CONST              2('finally')            34PRINT_ITEM                      35PRINT_NEWLINE                  36END_FINALLY                    37LOAD_CONST              0(None)            40RETURN_VALUE这边简单说着这些列所代表的意思:Python第一列是代码在文件的行号第二列字节码的偏移量字节码的名字参数字节码处理参数最终的结果12345第一列是代码在文件的行号第二列字节码的偏移量字节码的名字参数字节码处理参数最终的结果在字节码中可以看到,依次是SETUP_FINALLY和SETUP_EXCEPT,这个对应的就是finally和try,虽然finally在try后面,虽然我们通常帮他们看成一个整体,但是他们在实际上却是分开的…因为我们重点是finally,所以就单单看SETUP_FINALLYPython//ceval.cTARGET(SETUP_FINALLY)_setup_finally:{/*NOTE:Ifyouaddanynewblock-setupopcodesthatarenottry/except/finallyhandlers,youmayneedtoupdatethePyGen_NeedsFinalizing()function.*/PyFrame_BlockSetup(f,opcode,INSTR_OFFSET()+oparg,STACK_LEVEL());DISPATCH();}//fameobject.cvoidPyFrame_BlockSetup(PyFrameObject*f,inttype,inthandler,intlevel){PyTryBlock*b;if(f->f_iblock>=CO_MAXBLOCKS)Py_FatalError(\"XXXblockstackoverflow\");b=&f->f_blockstack[f->f_iblock++];b->b_type=type;b->b_level=level;b->b_handler=handler;}123456789101112131415161718192021222324252627//ceval.cTARGET(SETUP_FINALLY)        _setup_finally:        {            /*NOTE:Ifyouaddanynewblock-setupopcodesthat              arenottry/except/finallyhandlers,youmayneed              toupdatethePyGen_NeedsFinalizing()function.              */             PyFrame_BlockSetup(f,opcode,INSTR_OFFSET()+oparg,                              STACK_LEVEL());            DISPATCH();        }  //fameobject.cvoidPyFrame_BlockSetup(PyFrameObject*f,inttype,inthandler,intlevel){    PyTryBlock*b;    if(f->f_iblock>=CO_MAXBLOCKS)        Py_FatalError(\"XXXblockstackoverflow\");    b=&f->f_blockstack[f->f_iblock++];    b->b_type=type;    b->b_level=level;    b->b_handler=handler;}从上面的代码,很明显就能看出来,SETUP_FINALLY就是调用下PyFrame_BlockSetup去创建一个Block,然后为这个Block设置:b_type(opcode也就是SETUP_FINALLY)b_levelb_handler(INSTR_OFFSET()+oparg)handler可能比较难理解,其实看刚才的dis输出就能看到是哪个,就是13>>31LOAD_CONST2(‘finally’),这个箭头就是告诉我们跳转的位置的,为什么会跳转到这句呢?因为60SETUP_FINALLY28(to31)已经告诉我们将要跳转到31这个位置~~~如果这个搞清楚了,那就再来继续看return,return对应的字节码是:RETURN_VALUE,所以它对应的源码是:Python//ceval.cTARGET_NOARG(RETURN_VALUE){retval=POP();why=WHY_RETURN;gotofast_block_end;}1234567//ceval.cTARGET_NOARG(RETURN_VALUE)        {            retval=POP();            why=WHY_RETURN;            gotofast_block_end;        }原来我们以前理解的return是假return!这个return并没有直接返回嘛,而是将堆栈的值弹出来,赋值个retval,然后将why设置成WHY_RETURN,接着就跑路了!跑到一个叫fast_block_end;的地方~,没办法,为了揭穿真面目,只好掘地三尺了:Pythonwhile(why!=WHY_NOT&&f->f_iblock>0){fast_block_end:while(why!=WHY_NOT&&f->f_iblock>0){/*Peekatthecurrentblock.*/PyTryBlock*b=&f->f_blockstack[f->f_iblock-1];assert(why!=WHY_YIELD);if(b->b_type==SETUP_LOOP&&why==WHY_CONTINUE){why=WHY_NOT;JUMPTO(PyInt_AS_LONG(retval));Py_DECREF(retval);break;}/*Nowwehavetopoptheblock.*/f->f_iblock--;while(STACK_LEVEL()>b->b_level){v=POP();Py_XDECREF(v);}if(b->b_type==SETUP_LOOP&&why==WHY_BREAK){why=WHY_NOT;JUMPTO(b->b_handler);break;}if(b->b_type==SETUP_FINALLY||(b->b_type==SETUP_EXCEPT&&why==WHY_EXCEPTION)||b->b_type==SETUP_WITH){if(why==WHY_EXCEPTION){PyObject*exc,*val,*tb;PyErr_Fetch(&exc,&val,&tb);if(val==NULL){val=Py_None;Py_INCREF(val);}/*Maketherawexceptiondataavailabletothehandler,soaprogramcanemulatethePythonmainloop.Don'tdothisfor'finally'.*/if(b->b_type==SETUP_EXCEPT||b->b_type==SETUP_WITH){PyErr_NormalizeException(&exc,&val,&tb);set_exc_info(tstate,exc,val,tb);}if(tb==NULL){Py_INCREF(Py_None);PUSH(Py_None);}elsePUSH(tb);PUSH(val);PUSH(exc);}else{if(why&(WHY_RETURN|WHY_CONTINUE))PUSH(retval);v=PyInt_FromLong((long)why);PUSH(v);}why=WHY_NOT;JUMPTO(b->b_handler);break;}}/*unwindstack*/1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768while(why!=WHY_NOT&&f->f_iblock>0){            fast_block_end:        while(why!=WHY_NOT&&f->f_iblock>0){            /*Peekatthecurrentblock.*/            PyTryBlock*b=&f->f_blockstack[f->f_iblock-1];             assert(why!=WHY_YIELD);            if(b->b_type==SETUP_LOOP&&why==WHY_CONTINUE){                why=WHY_NOT;                JUMPTO(PyInt_AS_LONG(retval));                Py_DECREF(retval);                break;            }             /*Nowwehavetopoptheblock.*/            f->f_iblock--;             while(STACK_LEVEL()>b->b_level){                v=POP();                Py_XDECREF(v);            }            if(b->b_type==SETUP_LOOP&&why==WHY_BREAK){                why=WHY_NOT;                JUMPTO(b->b_handler);                break;            }            if(b->b_type==SETUP_FINALLY||                (b->b_type==SETUP_EXCEPT&&                why==WHY_EXCEPTION)||                b->b_type==SETUP_WITH){                if(why==WHY_EXCEPTION){                    PyObject*exc,*val,*tb;                    PyErr_Fetch(&exc,&val,&tb);                    if(val==NULL){                        val=Py_None;                        Py_INCREF(val);                    }                    /*Maketherawexceptiondata                      availabletothehandler,                      soaprogramcanemulatethe                      Pythonmainloop.  Don'tdo                      thisfor'finally'.*/                    if(b->b_type==SETUP_EXCEPT||                        b->b_type==SETUP_WITH){                        PyErr_NormalizeException(                            &exc,&val,&tb);                        set_exc_info(tstate,                                    exc,val,tb);                    }                    if(tb==NULL){                        Py_INCREF(Py_None);                        PUSH(Py_None);                    }else                        PUSH(tb);                    PUSH(val);                    PUSH(exc);                }                else{                    if(why&(WHY_RETURN|WHY_CONTINUE))                        PUSH(retval);                    v=PyInt_FromLong((long)why);                    PUSH(v);                }                why=WHY_NOT;                JUMPTO(b->b_handler);                break;            }        }/*unwindstack*/在这需要回顾下刚才的一些知识,刚才我们看了return的代码,看到它将why设置成了WHY_RETURN,所以在这么一大串判断中,它只是走了最后面的else,动作也很简单,就是将刚才return储存的值retval再push压回栈,同时将why转换成long再压回栈,然后有设置了下why,接着就是屁颠屁颠去执行刚才SETUP_FINALLY设置的b_handler代码了~当这这段bhandler代码执行完,就再通过END_FINALLY去做回该做的事,而这里就是,returnretval结论所以,我们应该能知道为什么当我们执行了return代码,为什么finally的代码还会先执行了吧,因为return的本质,就是设置why和retval,然后goto到一个大判断,最后根据why的值去执行对应的操作!所以可以说并不是真的实质性的返回.希望我们往后再用到它们的时候,别再掉坑里!1赞4收藏评论"], "art_url": ["http://python.jobbole.com/88408/"]}
{"art_img": ["http://jbcdn2.b0.upaiyun.com/2012/05/py-vs-ruby.jpg"], "art_title": ["再谈PHP、Python与Ruby"], "art_create_time": ["2013/05/09"], "art_content": ["原文出处：nowamagic   本文整理自知乎。一句话总结简单地总结：假如你想帮他尽快找个活儿，赚到钱，推荐PHP。假如你想让他成为一个高效工程师，推荐Python。假如你想让他爱上他的工作，推荐Ruby。 语言的选择编程语言非常重要，不要认为他们都图灵等价，用起来都一样。实际上，好的语言，带给你的东西是超乎想像的。下面是一些看法：程序员的时间远比机器的时间宝贵：选择开发效率最高的语言吧，不要过于在乎运行性能，如果你开发不出东西，那么跑得多快也没用。优雅的抽象胜于简单的堆砌：这意味着你的代码是最简洁而又充满设计感的，面向对象，容易的闭包，一切都是表达式等等，而最好的抽象是语言本身能够分层，既能够书写语言的语言，比如DSL能力强弱是个指标。才华横溢的社区胜过有难度的招聘：这种语言一定要拥有一个活跃且才华横溢的社区，只要使用她的人，就有一种自豪感和精英意识，在社区里的每个人都自认为比别人强。即使，你不那么容易招聘到程序员，但我们不都知道，一个有才华的程序员胜过100个平庸之辈嘛？选用这种语言，能保证团队里一起编码的每一个人，都是高手。也是最重要的，乐趣胜过一切：灵活，好玩，永远有新发现的魔术般的语言，只有你想不到的，没有你做不到的，对一个真正的hacker来说，这是选择语言唯一的理由，这也是Perl为什么能够长期存活的原因，因为她是hacker为hacker创作的。如果能看完这些观点，相信你能猜出我的选择了:)是的，Ruby是主流语言中我认为最出色的，最高的开发效率，最优雅简洁的抽象能力，最有才的社区（Github为中心），和最大的乐趣。做为一个程序员，是应该去选取不同思路的语言去学习的，而且当然要选择最有代表性的语言，在编程的世界里，主流的思路其实就两种，一是结构化编程，一是函数化编程，面向对象只是结构化编程进化的一个阶段而已，其他各种各样的思路大多数是这两个的分支。 社区的创造力选择一门语言要看这个语言社区的气质，以及你想用他做什么东西。Mac上可以运行那么多种语言，有很多人用git。但却只有Ruby的用户开发出了homebrew这样牛逼的包管理系统，善用了github的诸多特性，真正的把每一个用户都变成了潜在的repo贡献者。我相信这绝对不是巧合。如果同样的条件摆在面前，你做出了一个全新的东西而别人没有，说明了你的创造力比别人要高。作为Python的使用者，我觉得Ruby语法的优劣不太重要。但是从外部看Ruby社区近些年持续的向外输出一些从技术到开发上的理念，不断的有大大小小的很酷的项目从Ruby社区中诞生的出来。使得我觉得Ruby社区是一个很有创新气质的社区。因此如果你学编程是想做出一个很酷的东西，那么我强烈建议你直接学习Ruby。 从Web开发去比较对于Web开发，这三种语言都能胜任，但是还是有不少区别：Ruby的rails框架确实是称得上是快速开发的典范，但是Ruby的语法过于灵活，有些时候对同一个处理会有很多种不同的写法。PHP是老牌的网页脚本语言，相对其他两门而言比较成熟。PHP虽然快速成型，但容易失控，尤其项目稍大的时候。对于Python来说，由于自身的简约设计，可能应用的方面的更多，几乎是可以涉及到IT的各个方面，Web只是它其中一个方面，而且有比较成功的杀手级应用，Youtube就是用Python开发的，服务器的各种原来用shell或者perl的脚本，科学计算，游戏的脚本，甚至在桌面客户端上也能看到Python的佳作，如UliPad，对于初学者来说Python是个不错的选择，在这三者中它的语法是最简洁和清晰的，适合初学者学习。入门语言还是应该选择一个比较严谨的，像Java,Python那样的语言比较合适，从PHP语言不太容易学到OO的思想和设计模式之类的东西，而这些对于一个优秀的PHP程序员来说也是很必要的。这里不是黑PHP，每个语言都有自己的优点，比如PHP入门快，可以增加编程的信心，明白编程那点事后，可以触类旁通的接触其他语言。很多编程者就是从PHP入门的。在编程学习里有这么一个现象：直接推荐Python的人，基本都没深入学习过Ruby。学过Python和Ruby的人，基本都喜欢Ruby。具体使用什么语言，不是唯一的。根据场景是要分的。一个人至少是需要学习3种语言的：一种是工作语言，比如C++，Java，PHP，这三种语言，是互联网的万能药，只要会了，工作是永远都不愁的，大型公司一般都会使用，用来开发一些大型项目。一种是自己使用的语言，比如Python，Ruby，当然，我也推荐Ruby。我也用过python，后来因为工作需要，忍痛放弃了Python，最后，竟然深深喜欢上Ruby了。然后就不再回头了，对于个人使用来说，是要非常注重开发效能的，一门语言，至少能用10年，如果自己选择的语言，可以在开发效能上超越其他语言十分之一，那么在十年内这种收益是非常大的。还有一门是新语言使用，不必深入，但是一定要了解，这有助于比较不同语言的不足，才不至于成为井底之蛙。比如Nodejs,Scala等。 关于Ruby我觉得直接学Ruby也是不错的选择。本来rails框架就自带了server，WEBrick。看着Log做开发效率非常高。如果要部署的话，用passenger也是绝对方便。Views层的模板系统，ERB应该比Python的各种要来得美，而且更加简单。之前用过Django，觉得太重了。Rails可以让你不断的惊讶程序可以这样写的啊，我第一次看到有7.days.ago的时候惊掉了。Rails强调一种DSL，一来符合人们的语言习惯、二来我觉得是一种编程语言的颠覆，我们并不是在用某个特定的语言（比如Ruby）来实现一个功能（就如同是用C还是用Java来写一个编译器），而是我可以在这些语言的基础上定义一种新的语言（类似于lex，yacc这样的词法语法生成器）。看看routes.rb的设置吧，能有多么惊讶，这是程序么，简直就是诗。美不只是在于内容，同样在于形式。Ruby或者说Rails的缺点或许就是学习的曲线太陡，我之前有过MVC的经验，上手RoR还是花了三周的时间，或许也是自己接受能力不强吧，但更确切的问题应该在于Rails的惯用法太多：当然，我在用ActiveRecord拿数据的时候，可以写find_by_sql(“blablabla”)，但是细查Rails的文档，他是提供类似于Joins.Group.Select等等的方法的，姑且不论效率是不是真的会快点，少写一点sql在.rb的文件里面不是会更美一些么。再到后来，偶然又发现有metawhere这种东西，是不是又要忍痛抛弃既往学到的那一堆worksbutnotelegant的东西，义无反顾的投身到metawhere的学习中。当然，如果不追求完美，上手也没有这么恐怖。 关于Pythonpython的优势很多，比如：语法简洁，无需编译。Python语法简洁高效，一句话“人生苦短，我用Python”啥都说明了，高效是Python的特点。强大的数据结构。默认安装的Python开发环境已经附带了很多高级数据类型，如列表、元组、字典、集合、队列等，无需进一步编程就可以使用这些数据类型的操作。使用这些数据类型使得实现抽象的数学概念非常简单。强制缩进，让代码自然显得有条理。插件齐全，可以完成绝大部分的程序设计任务。 小结没有最好的语言，只有最合适的语言。没有糟糕的语言，只有糟糕的程序员。没有一种语言是万能的，只会一种语言是万万不能的。用什么语言不重要，最重要的是效率：开发效率和执行效率。PHP语法很傻，一点都不炫，但凡是学过C、JS、AS等等语言的，很容易上手，因为语法上有很多重合的地方。很多人看不起PHP，说PHP太简单的人，往往是因为对PHP本身不够深入了解，其实PHP里面的东西一般人用到的很少，5.3以上的特性也慢慢让这种语言生动起来，你越是深入，就越能发现它的强大和优势。这里仅仅列出PHP，Python与Ruby的一些情况，让读者更好地挑选学习的语言。语言没优劣之分，Justfollowyourpassion.1赞1收藏23评论"], "art_url": ["http://python.jobbole.com/43792/"]}
{"art_img": ["http://jbcdn2.b0.upaiyun.com/2012/06/python-icon.jpg"], "art_title": ["Python状况：为什么PyPy是Python的未来？"], "art_create_time": ["2013/05/17"], "art_content": ["英文原文：ThePythoncondition.WhyPyPyisthefutureofPython编译：oschinaPython现在已经不仅仅是胶水脚本语言了。不信？看看下面使用Python的成功案例：YouTube–主要由Python编写NASAIndustrialLight&MagicRuns –电影公司OpenStackSage –科学软件及其他(SciPy,PythonXY)WEB框架Django,Pyramid,bottle…修订控制系统其他不错的软件要是你正在寻找快速介绍美丽的Python语言，我推荐 My-Favorite-Python-Things高级语言是主流目前高级语言可以写出简单具有灵活性的代码，所以在快速创建应用程序的时候是一个不错的选择，你不需要花时间来处理各种数据类型 (所有接口示例代码都是为了满足编译处理)，可能有些人就会争论了，这个特性会产生有bug的代码，但是GuidovanRossum说：“生产的代码谁会不经过测试呢”？静态语言在编译时期就能处理一些错误，但这并不能检测出所有的错误。最后你还是得编写测试代码。而有这个时间完全可以为动态语言写出测试代码。此外人们还不能设计一个堪称完美的类型系统，对此JimTreavor写了一些总结。新技术允许我们为动态语言设计一个高效的运行环境 (JavaScriptV8,LuaJIT,Racket,CommonLisp…),这也可以和大型的框架竞争(JVM,.NET,…)所有这一切都使得高级语言越来越流行得在大型企业和日常生活中使用。Python能延续传奇吗？现在Python非常流行，同时它的地位也受到竞争者的挑战。Python有良好的生态系统，也有大型软件和社区支持，但它缺乏其竞争者的高效和先进的运行环境。Python作为胶水语言.正如我在开头说的一个特点，Python很容易连接各种编译库，这是它作为胶水语言在20年前流行的重要原因。但是目前依然活跃的工具已经很老旧了，你必须花大量精力才能使用它们。ctypesc扩展 是邪恶的.它们绑定到Python的特定版本还不能被重复使用.更糟糕的是, CPython2和CPython3的c扩展 API不一样.想想将库移植到Python3会是什么情况吧！Cython –这是被设计用来编写C扩展的.但是我敢确定，使用C扩展是你最后想做的事.Cython是一个需要编译的外部工具.它最终的代码并没有动态行为，但是它的语法还需要学习.Cython不支持类型推断.使用Cpython你不得不去编译.Cython也不是一个标准.它不能作为解释代码来执行. _nuitka_的作者 KayHayen在StaticCompilation–Thatisthepoint总结的非常好.swig, boost –这些是非常容易的,通常修改下C/C++代码就可以了,或者写一些方案文件.相比之下，有很多新的工具能在相同的性能下（甚至超出），更好的处理这些任务。cffi –一个能轻松处理你的c库的包。在接触硬件或者支持其他软件时你会经常做这样的事（像数据库客户端、驱动程序）。尝试下在python里使用它是多么简单吧。你不需要写任何的封装，类型化代码。而且还有CPython和PyPy的支持。bitey将Python作为你代码的核心–胶水语言另一面胶水语言也有另一面。我们来想想底层高性能编程的过程。可能看起来会是下面几个过程：构思很多复杂的底层代码和组织机构代码。很可能是一堆晦涩的泛型代码（为了重用性）。编写胶水语言编译运行极可能会做很多的调试，然后回去修改，考虑到有这么多的底层代码。感谢Python的简便性、脚本语言的本质和大量的工具，将他作为你代码的模板和核心。这就意味着你只需要写最少的底层代码，让Python做剩下的事：生成组织代码和你的底层代码需要的环境。这与以往Lisp的理念一样，代码即数据，代码能够被其他正在执行的代码理解（代码可以作为数据被处理）。因而机器可以理解运行时正在执行的代码，并且去优化它，通过通常的方式就能得到全部的数据信息，而不用像C++那样使用模板。这是C++和其他流行的编程语言所没有的。最终我们有相对更底层的抽象级别，而运行时信息相对更丰富，使得编译器可以：为未知的硬件做特化(编码时)，包括支持的数据类型，以及可用的优化方法。自动调整（tuning）(例如为库提供的数据，如ATLAS…)推送更多的信息给编译器，得到更好的推理。人们不用为数据类型烦恼（运行时环境就已经可以保证快速、正确使用数据类型）于是整个流程就好像这样：想法一点Python代码（最棒的部分），用来构建整个架构。然后是一些底层的代码，同样很棒，因为这些代码没有恶心的模板和上下文代码。事实上，底层的代码也可以通过Python代码生成。运行调试，比起前面的步骤时间更短一些就性能而言，这样的过程相比之前的方法有着更好的前景。这些是已经用到这种方式的：PyPy,cffi,PyOpenCL,PyCUDA,numba,theano…把Python当做一个高速语言有很多方法能用Python写出高速的代码。最流行而且仍在广泛传播的方法是，用底层语言来写应用里最复杂的部分，然后使用，这对python来说无疑是很不幸的事。所有Python里出色的高效的工具都需要许多复杂的c代码，这阻碍了其他的贡献者进来。现在我们想要写出高速而且美观的python代码。有很多工具可以把python代码编译成机器代码，比如：Nuitka,Python2C,Shedskin,pythran。我认为它们都是失败的，当你使用它们的时候，就需要跟动态行为说再见了。他们只支持一部分的python语言，并且离完全支持还有很大距离。我甚至不认为以后他们能做到。另外他们也没有用那些使JIT（Just-In-Time运行时编译执行）的解决方案变得出色的先进的技术和运行时信息。多核编程这方面，ArminsRigo的文章写的很棒，可以参考：MulticoreProgramminginPyPyandCPython解释器的设计为了让下一步的开发更简单，实现动态语言的最佳状态，Python需要一个合适的架构。当前CPython的架构过于简单，因而限制比较大，很难做到像JIT编译器那样的功能。下面是一些在增强CPython解释器性能上的失败的努力：psyco(被PyPy代替)Unladenswallow消除GIL的很多失败的尝试还有一些尝试修复CPython一些缺陷的尝试:Stackless和HotPy，但是Guido(Python之父，仁慈的独裁者)的坚持使得这些项目没有被合并到Python中。（说明一下，HotPy还不是产品级的东西）。CPython最大的问题是他的CAPI，这部分没有很好的设计。其他部分的实现多少都受此影响。我们能做什么？在粘结代码中推进新工具的使用( cffi, bitey)在公共库中停止对CPython的底层属性（CAPI，C扩展）的依赖。作为替代，采用有如下功能的中间工具：cffi –简化对C库的应用cython –编写可移植的C扩展。我并不推荐它用于通常的编程，不过它确实在维护C扩展方面更好一些，也更简单。Cython已经有CPython和PyPy后端。为何PyPy是趋势？PyPy为优化和进一步的语言开发提供了更好的架构。对于大部分Python已有的问题，PyPy已经提供了解决方案：先进的runtime和设计，在此文中作了介绍： TheArchitectureofOpenSourceApplications.速度–PyPy内置的JIT很棒，有时（其实很少）甚至可以与C相提并论。GIL问题–PyPy引入了一个很棒的STM实现，在ArminsRigo的 文章中对此作了介绍。粘合代码–使用cffi可以简单的处理C库，甚至比CPython的ctypes还要快！异步编程。这方面，PyPy内置的greenlet比CPython的C扩展更适合一些。实际上，非堆栈式的概念（也即greenlet）在PyPy中还在继续发展（参看https://ep2012.europython.eu/conference/talks/the-story-of-stackless-python）沙盒技术应用在web和移动中。这里有Dusty的一些文章：PushingPythonPastthePresentPyPy已经支持多平台(x86,64_x86,ARM)PyPy同时还包含了一个优秀的现代的架构，在 JimHuang的演讲 中做了介绍，演讲的要点是：解释性语言的框架用于研究和产品的组件组合(不同的数据模型，垃圾回收–这些可以在具体的应用场景进行改变)构建在基于组件链的功能架构之上(翻译工具链)。每一个步骤都会延续/转换程序模型、引入特征、各种后端（JVM,JavaScript,LLVM,GCCIR等等)。来看一下翻译链的例子：python代码->字节码->函数对象->类型推断->垃圾收集器->JIT包含大量在架构的不同层次开发的现代的优化技术(这个任务可以简化)相信让所有软件支持PyPy需要付出艰巨的努力–需要在现有的库上做很多工作。不过使用新的工具，编写支持PyPy和CPython的软件会比采用C扩展的方式更简单一些（在我们能做什么一节有介绍）。CPython遗留问题现在来说一下CPython依赖性代码的遗留问题(源于它们紧密依赖 c扩展)。这些主要是科学相关的软件(NumPy,SciPy等等)。Python在PyPy成为产品级软件之前早就被用于科学计算（我认为实在2年之前），之后这些软件在工具、代码和社区方面都得到很多发展。这些软件一起构建了一个很棒的平台，通常被用来作为Matlab之类的软件的替换（有人甚至认为是更好的选择）。要实现这些，C扩展是目前唯一的解决方案。现在，这些软件的发展仍然与CPython紧密绑定，因为要让科学计算相关的软件全部支持PyPy，需要大量的工作。近似的方案是使用按需执行的JIT–对特定的函数做修饰，然后动态的把它们编译成机器码并切换到使用c扩展。这个想法不需要重写所有的科学计算的平台，而速度同样快速。采用这种方式的典型项目是numba，这是由ContinuumAnalytics(这家公司主要发布基于python库的强壮的科学计算平台)赞助的。Numba采用这种方式是因为它的快速脚本需要与其他依赖CPython的科学计算代码兼容。numba值得学一下，SciPy会议的numba演讲 是一个很好的阐述。不得不说Python的科学计算社区非常棒。他们非常关注于质量、易用性和推广他们的产品（为此组织了很多会议：SciPy大会，PyData等）。感谢他们让Python成为免费的科学分析平台的首选。这里还需要提一下TravisOliphant，他在社区中付出了很多努力，让整个平台协调一致。关于这些可以看一下这篇博客文章：为何Python是你最终必须学习的编程语言PyPy呢?我希望PyPy当时并没有达到产品级可用。后续关于本文的后续在Reddit上有一个有趣的讨论。这是关于在商业上应用PyPy的优缺点的讨论。讨论者总结了如何使用基于高性能库的PyPy。最重要的是使用PyPy软件栈（原始Python、cffi，等等）可以让维护和优化很简单（例如：延迟计算）。至于缺点，上面提到了，主要与CPython的遗留问题有关。1赞收藏评论"], "art_url": ["http://python.jobbole.com/39757/"]}
{"art_img": ["http://jbcdn2.b0.upaiyun.com/2013/08/Tavis-Rudds-desk.jpg"], "art_title": ["用语音写代码比键盘更快"], "art_create_time": ["2013/05/11"], "art_content": ["本文由伯乐在线-奇风余谷翻译。未经许可，禁止转载！英文出处：ergoemacs。欢迎加入翻译组。这是TavisRudd在2013年Python大会上的演讲，展示了如何使用语音进行编程。他在手部肌肉麻木后无法顺利敲键盘，于是利用语音识别工具来写代码。一开始很慢，后来熟练后，速度比敲键盘还快。 提要“两年前我开发一个叫EmacsPinkie的项目时重复性劳损（Repetitive Strain Injury,RSI）严重，结果我的手变得发麻，我不能再打字或者工作了。绝望之中，我尝试用语言识别。起初我写代码慢得要命，但因为不能打字，我只好坚持下来。经过几个月的词汇调整，还有磕巴地用Python和EmacsLisp编程，我做出了这个能让我用语音编程的系统，比我曾经用手写更快更有效率。在一个快放了的实况演示里，我会用Python创建一个小系统，并且加了点其他的语言作为额外赠送，然后不碰键盘将它部署。演示里的神会在预先安排下露面。我希望能说服你，语音识别不再只是残疾人的帮手，不再只是做平凡无奇的事情。现在它成为一种高效的工具，能造福所有程序员。”TavisRudd的工作台（伯乐在线配图）[youkuid=”XNTk2MTAyMjQ0″]第一个演示，用语音编写 EmacsLisp代码，从9：00开始。 如果你缺乏耐心，这里列下了视频概要：他使用微软Windows下的 DragonNaturallySpeaking 语言识别软件。（他说他没能够让Linux的 CMUSphnix 工作起来。）（尽管他使用Mac，但显然在用VirtualBox跑WIndows。）Dragon软件是用Python开发的。他改了一下，用了ChristoButcher写的PythonSpeech识别扩展库DragonFly。见 https://pypi.python.org/pypi/dragonfly/0.6.5。所以他现在可以定义自己的语音命令。（输入些什么，或者移动鼠标到代码的某个位置，或者键盘快捷键，切换应用程序等等。）他创建了许多简短的特殊/唯一的元音表达式来做不同的事情。例如插入换行，切换Emacs缓冲区，启动终端等。所以当他在语音编程时，你会听到“twip,chirp,slap,derp/ 踢，削，拉，打……”，很有意思。他做了大概2千多种命令。他说他系统做的所有事情都是在Vim或者Emacs里。（在他的演示中，用的好像都是Emacs，作为做其他操作的接口。频繁使用了Emacs的各种特性，包括模板，自动完成，Emacs里的shell。）他演示了用它来写EmacsLisp，Python程序，在Emacs，终端下工作。如此高效，比一般程序员用手在键盘上写都快。他花了3到6个月来习惯这个系统。他的重复性劳损（Repetitive Strain Injury RSI）目前已经完全康复，但他说他仍然在用语音编程，大概占了40%到60%的时间。他说他会在4个月内发布代码。关注他的twitter或github。https://github.com/tavisrudd，https://twitter.com/tavisrudd 他推荐的一款麦克风是 AudixOM-7Microphone，比较贵，198美元。我在2010年用过几个月微软的语音识别系统。微软Windows7自带。对于正常的说话很好，但是不太可能用来编程（即装即用）。在普通的Windows应用程序上工作得很好，尤其是来自微软的应用例如Office，但是在Emacs上行不通。另外一个有趣的语音技术，见《英译汉的实时机器语音翻译》。1赞收藏6评论关于作者：奇风余谷做一个偷懒的程序员。电子邮箱：dut.hww@gmail.com、个人主页：http://deepfish.info、新浪微博：@奇风余谷个人主页·我的文章"], "art_url": ["http://python.jobbole.com/45781/"]}
{"art_img": ["http://jbcdn2.b0.upaiyun.com/2013/05/29221100_r72w.png"], "art_title": ["设置 Sublime Text 的 Python 开发环境"], "art_create_time": ["2013/05/31"], "art_content": ["原文出处：dbader.org   译文出处：开源中国   最近，当我主要使用Python开发环境编辑的时候，我开始越来越多地用到SublinmeText2.这篇文章主要说明了能让Python的编程者使用更方便的一些设置和调整。 为何选择SublimeText?我以前一直是TextMate的忠实用户。这是一个轻量级的、开源的软件，作为OSX的本地应用，具有很好的Mac风格。不过，虽然TextMate是一个很棒的编辑器，有时候还是显得功能不够。我用过一些功能更强的软件，例如 加上Python插件的IntelliJIDEA。我特别喜欢它的debugger和testrunner。不过，一个像IntelliJ这样的全功能的IDE对于中小项目来说还是显得过于庞大。最近几周我开始越来越多的使用 SublimeText。当我将它安装好之后，感觉非常不错。它确实非常快，自动定期的更新，以及更棒的是完全支持跨平台。对我来说，它最终胜过TextMate的地方是Sublime强大的插件子系统。对于Python开发，有不少插件可以让你开发起来更流畅、更有乐趣。我现在仍然在不同的项目之间切换编辑器。不过我发现对应Python开发，Sublime在轻量级的编辑器和全功能的IDE之间有着很好的平衡。字体的选择UbuntuMono 是非常非常不错的字体。前些天我刚从 Menlo 切换过来，这绝对不让人后悔。在我的15寸的MacBook上，UbuntuMono的16号字非常适合。1680×1050的分辨率对于一个边栏加两个编辑器窗口（自动调整到80个字符宽）刚好合适。如果你打算认真的挑选一下字体， slant.co的这篇文章 写的不错。它包含了大部分流行的编程方面的字体的截图及下载链接。安装插件正如之前提到的，Sublime有一个非常丰富的插件系统。而我当前使用的插件如下：PackageControl 在Sublime里直接安装附加插件的包管理器。这是唯一一个你必须手动安装的插件。这边列出的其他所有插件都可以通过PackageControl来安装。也可以通过它来更新已安装过的插件。简单得想做是Sublimepackages的apt-get就行了。ColorScheme–TomorrowNight Colorschemes 决定了编辑器界面语法高亮的字体颜色。这是一个非常酷的暗黑系样式。Theme–SodaDark Themes 影响Sublime界面元素的颜色和风格。这个非常适合TomorrowNight的配色方案。SideBarEnhancements 这个插件提供了侧边栏附加的上下文菜单选项，例如”Newfile”，”NewFloder”等。这些本应当默认就该有的，却没有。AllAutocomplete Sublime默认的自动完成只关注当前文件的单词。这个插件扩展了其自动完成的单词列表到所有打开的文件。SublimeCodeIntel 为部分语言增强自动完成功能，包括了Python。这个插件同时也可以让你跳转到符号定义的地方，通过按住alt并点击符号。非常方便。SublimeREPL 允许你在编辑界面直接运行Python解释器。我倾向于在单独的终端窗口用 bpython 来运行，但有时SublimeREPL是很有帮助的。GitGutter 在编辑器的凹槽区，依照Git，增加小图标来标识一行是否被插入、修改或删除。在GitGutter的readme中有说明如何更改颜色图标来更新你的配色方案文件。Pylinter 这个插件提供了目前我所见到的最好的 pylint 编辑器整合。它自动检查.py文件，无论其何时被保存，并且会直接在编辑界面显示pylint违规。它还有一个快捷方式来禁用局部的pylint检查，通过插入一个#pylint:禁用注释。这个插件对于我确实非常有用。配置文件SublimeText的一个优点就是它的所有配置都是简单的基于JSON的配置文件。这使得你可以很容易的将配置转到另一个系统中。我也见过一些人使用Dropbox自动同步他们所有电脑上的配置。Preferences.sublime-settings 配置了Sublimede的显示和行为.你可以在sublime中通过 Preferences>Settings—User 打开并编辑此文件。我使用如下配置：Python{//Colors&quot;color_scheme&quot;:&quot;Packages/TomorrowColorSchemes/Tomorrow-Night.tmTheme&quot;,&quot;theme&quot;:&quot;SodaDark.sublime-theme&quot;,//Font&quot;font_face&quot;:&quot;UbuntuMono&quot;,&quot;font_size&quot;:16.0,&quot;font_options&quot;:[&quot;subpixel_antialias&quot;,&quot;no_bold&quot;],&quot;line_padding_bottom&quot;:0,&quot;line_padding_top&quot;:0,//Cursorstyle-noblinkingandslightlywiderthandefault&quot;caret_style&quot;:&quot;solid&quot;,&quot;wide_caret&quot;:true,//Editorviewlook-and-feel&quot;draw_white_space&quot;:&quot;all&quot;,&quot;fold_buttons&quot;:false,&quot;highlight_line&quot;:true,&quot;auto_complete&quot;:false,&quot;show_minimap&quot;:false,//Editorbehavior&quot;scroll_past_end&quot;:false,&quot;highlight_modified_tabs&quot;:true,&quot;find_selected_text&quot;:true,//Wordwrapping-followPEP8recommendations&quot;rulers&quot;:[72,79],&quot;word_wrap&quot;:true,&quot;wrap_width&quot;:80,//Whitespace-notabs,trimming,endfileswith\\n&quot;tab_size&quot;:4,&quot;translate_tabs_to_spaces&quot;:true,&quot;trim_trailing_white_space_on_save&quot;:true,&quot;ensure_newline_at_eof_on_save&quot;:true,//Sidebar-excludedistractingfilesandfolders&quot;file_exclude_patterns&quot;:[&quot;.DS_Store&quot;,&quot;*.pid&quot;,&quot;*.pyc&quot;],&quot;folder_exclude_patterns&quot;:[&quot;.git&quot;,&quot;__pycache__&quot;,&quot;env&quot;,&quot;env3&quot;]}123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354{    //Colors    &quot;color_scheme&quot;:&quot;Packages/TomorrowColorSchemes/Tomorrow-Night.tmTheme&quot;,    &quot;theme&quot;:&quot;SodaDark.sublime-theme&quot;,     //Font    &quot;font_face&quot;:&quot;UbuntuMono&quot;,    &quot;font_size&quot;:16.0,    &quot;font_options&quot;:[&quot;subpixel_antialias&quot;,&quot;no_bold&quot;],    &quot;line_padding_bottom&quot;:0,    &quot;line_padding_top&quot;:0,     //Cursorstyle-noblinkingandslightlywiderthandefault    &quot;caret_style&quot;:&quot;solid&quot;,    &quot;wide_caret&quot;:true,     //Editorviewlook-and-feel    &quot;draw_white_space&quot;:&quot;all&quot;,    &quot;fold_buttons&quot;:false,    &quot;highlight_line&quot;:true,    &quot;auto_complete&quot;:false,    &quot;show_minimap&quot;:false,     //Editorbehavior    &quot;scroll_past_end&quot;:false,    &quot;highlight_modified_tabs&quot;:true,    &quot;find_selected_text&quot;:true,     //Wordwrapping-followPEP8recommendations    &quot;rulers&quot;:[72,79],    &quot;word_wrap&quot;:true,    &quot;wrap_width&quot;:80,     //Whitespace-notabs,trimming,endfileswith\\n    &quot;tab_size&quot;:4,    &quot;translate_tabs_to_spaces&quot;:true,    &quot;trim_trailing_white_space_on_save&quot;:true,    &quot;ensure_newline_at_eof_on_save&quot;:true,     //Sidebar-excludedistractingfilesandfolders    &quot;file_exclude_patterns&quot;:    [        &quot;.DS_Store&quot;,        &quot;*.pid&quot;,        &quot;*.pyc&quot;    ],    &quot;folder_exclude_patterns&quot;:    [        &quot;.git&quot;,        &quot;__pycache__&quot;,        &quot;env&quot;,        &quot;env3&quot;    ]}Pylinter.sublime-settings配置了pylinter插件。我使用下面的配置让Pyhton在保存时自动规范，并对违反规范显示图标。Python{//Configurepylint&#039;sbehavior&quot;pylint_rc&quot;:&quot;/Users/daniel/dev/pylintrc&quot;,//Showdifferenticonsforerrors,warnings,etc.&quot;use_icons&quot;:true,//AutomaticallyrunPylinterwhensavingaPythondocument&quot;run_on_save&quot;:true,//Don&#039;thidepylintmessageswhenmovingthecursor&quot;message_stay&quot;:true}12345678910111213{    //Configurepylint&#039;sbehavior    &quot;pylint_rc&quot;:&quot;/Users/daniel/dev/pylintrc&quot;,     //Showdifferenticonsforerrors,warnings,etc.    &quot;use_icons&quot;:true,     //AutomaticallyrunPylinterwhensavingaPythondocument    &quot;run_on_save&quot;:true,     //Don&#039;thidepylintmessageswhenmovingthecursor    &quot;message_stay&quot;:true}按键绑定Sublime的按键绑定也是全部可配置的基于JSON的sublime-keymap配置文件。我修改了一些默认配置以更好的配合我的TextMate/IntelliJ肌肉记忆。你可以完全不修改。如果你想，修改很简单，并可以跨平台使用。我使用如下的绑定：Python[//Rebind&quot;gotofile&quot;tocmd+shift+O{&quot;keys&quot;:[&quot;super+shift+o&quot;],&quot;command&quot;:&quot;show_overlay&quot;,&quot;args&quot;:{&quot;overlay&quot;:&quot;goto&quot;,&quot;show_files&quot;:true}},//Rebindswaplineup/downtocmd+shift+up/down{&quot;keys&quot;:[&quot;super+shift+up&quot;],&quot;command&quot;:&quot;swap_line_up&quot;},{&quot;keys&quot;:[&quot;super+shift+down&quot;],&quot;command&quot;:&quot;swap_line_down&quot;},//Deletealinewithcmd+delete{&quot;keys&quot;:[&quot;super+backspace&quot;],&quot;command&quot;:&quot;run_macro_file&quot;,&quot;args&quot;:{&quot;file&quot;:&quot;Packages/Default/DeleteLine.sublime-macro&quot;}},//Reindentselectionwithcmd+alt+L{&quot;keys&quot;:[&quot;super+alt+l&quot;],&quot;command&quot;:&quot;reindent&quot;}]12345678910111213141516171819[    //Rebind&quot;gotofile&quot;tocmd+shift+O    {&quot;keys&quot;:[&quot;super+shift+o&quot;],&quot;command&quot;:&quot;show_overlay&quot;,&quot;args&quot;:{        &quot;overlay&quot;:&quot;goto&quot;,        &quot;show_files&quot;:true    }},     //Rebindswaplineup/downtocmd+shift+up/down    {&quot;keys&quot;:[&quot;super+shift+up&quot;],&quot;command&quot;:&quot;swap_line_up&quot;},    {&quot;keys&quot;:[&quot;super+shift+down&quot;],&quot;command&quot;:&quot;swap_line_down&quot;},     //Deletealinewithcmd+delete    {&quot;keys&quot;:[&quot;super+backspace&quot;],&quot;command&quot;:&quot;run_macro_file&quot;,&quot;args&quot;:{        &quot;file&quot;:&quot;Packages/Default/DeleteLine.sublime-macro&quot;    }},     //Reindentselectionwithcmd+alt+L    {&quot;keys&quot;:[&quot;super+alt+l&quot;],&quot;command&quot;:&quot;reindent&quot;}]命令行工具同TextMate的mate类似，SublimeText包含了一个命令行工具，允许你通过shell打开编辑器。工具名为sublis，默认不可用。要使之生效，在任一shell中运行下面：Pythonln-s/Applications/Sublime\\Text\\2.app/Contents/SharedSupport/bin/subl/usr/local/bin/subl1ln-s/Applications/Sublime\\Text\\2.app/Contents/SharedSupport/bin/subl/usr/local/bin/subl要将Sublime作为git互动命令的默认编辑器使用——举例，撰写提交信息——只需添加下面一行到你的~/.profile文件：PythonexportGIT_EDITOR=\"subl--wait--new-window\"1exportGIT_EDITOR=\"subl--wait--new-window\"更多灵感我希望这篇安装指南能够帮到你。如果你有任何建议或意见，敬请Twitter我一行或给我发邮件。另外，感谢下面的作者及其关于配置sublime的作品。它们启发了我很多：KennethReitz:SublimeText2LoveDrewBarontini:Sublime(2)FilippoPacifici:PythondevelopmentwithSublimeText2tipsandtricksopensourcehacker.org:SublimeText2tipsforPythonandwebdevelopers1赞1收藏评论"], "art_url": ["http://python.jobbole.com/40660/"]}
