[
{"art_content": ["原文出处：j_hao104   何为抽稀在处理矢量化数据时，记录中往往会有很多重复数据，对进一步数据处理带来诸多不便。多余的数据一方面浪费了较多的存储空间，另一方面造成所要表达的图形不光滑或不符合标准。因此要通过某种规则，在保证矢量曲线形状不变的情况下，最大限度地减少数据点个数，这个过程称为抽稀。通俗的讲就是对曲线进行采样简化，即在曲线上取有限个点，将其变为折线，并且能够在一定程度保持原有形状。比较常用的两种抽稀算法是：道格拉斯-普克(Douglas-Peuker)算法和垂距限值法。道格拉斯-普克(Douglas-Peuker)算法Douglas-Peuker算法(DP算法)过程如下:1、连接曲线首尾两点A、B；2、依次计算曲线上所有点到A、B两点所在曲线的距离；3、计算最大距离D，如果D小于阈值threshold,则去掉曲线上出A、B外的所有点；如果D大于阈值threshold,则把曲线以最大距离分割成两段；4、对所有曲线分段重复1-3步骤，知道所有D均小于阈值。即完成抽稀。这种算法的抽稀精度与阈值有很大关系，阈值越大，简化程度越大，点减少的越多；反之简化程度越低，点保留的越多，形状也越趋于原曲线。下面是Python代码实现:Python#-*-coding:utf-8-*-\"\"\"-------------------------------------------------FileName：DouglasPeukerDescription:道格拉斯-普克抽稀算法Author:J_haodate：2017/8/16-------------------------------------------------ChangeActivity:2017/8/16:道格拉斯-普克抽稀算法-------------------------------------------------\"\"\"from__future__importdivisionfrommathimportsqrt,pow__author__='J_hao'THRESHOLD=0.0001#阈值defpoint2LineDistance(point_a,point_b,point_c):\"\"\"计算点a到点bc所在直线的距离:parampoint_a::parampoint_b::parampoint_c::return:\"\"\"#首先计算bc所在直线的斜率和截距ifpoint_b[0]==point_c[0]:return9999999slope=(point_b[1]-point_c[1])/(point_b[0]-point_c[0])intercept=point_b[1]-slope*point_b[0]#计算点a到bc所在直线的距离distance=abs(slope*point_a[0]-point_a[1]+intercept)/sqrt(1+pow(slope,2))returndistanceclassDouglasPeuker(object):def__init__(self):self.threshold=THRESHOLDself.qualify_list=list()self.disqualify_list=list()defdiluting(self,point_list):\"\"\"抽稀:parampoint_list:二维点列表:return:\"\"\"iflen(point_list)<3:self.qualify_list.extend(point_list[::-1])else:#找到与收尾两点连线距离最大的点max_distance_index,max_distance=0,0forindex,pointinenumerate(point_list):ifindexin[0,len(point_list)-1]:continuedistance=point2LineDistance(point,point_list[0],point_list[-1])ifdistance>max_distance:max_distance_index=indexmax_distance=distance#若最大距离小于阈值，则去掉所有中间点。反之，则将曲线按最大距离点分割ifmax_distance<self.threshold:self.qualify_list.append(point_list[-1])self.qualify_list.append(point_list[0])else:#将曲线按最大距离的点分割成两段sequence_a=point_list[:max_distance_index]sequence_b=point_list[max_distance_index:]forsequencein[sequence_a,sequence_b]:iflen(sequence)<3andsequence==sequence_b:self.qualify_list.extend(sequence[::-1])else:self.disqualify_list.append(sequence)defmain(self,point_list):self.diluting(point_list)whilelen(self.disqualify_list)>0:self.diluting(self.disqualify_list.pop())printself.qualify_listprintlen(self.qualify_list)if__name__=='__main__':d=DouglasPeuker()d.main([[104.066228,30.644527],[104.066279,30.643528],[104.066296,30.642528],[104.066314,30.641529],[104.066332,30.640529],[104.066383,30.639530],[104.066400,30.638530],[104.066451,30.637531],[104.066468,30.636532],[104.066518,30.635533],[104.066535,30.634533],[104.066586,30.633534],[104.066636,30.632536],[104.066686,30.631537],[104.066735,30.630538],[104.066785,30.629539],[104.066802,30.628539],[104.066820,30.627540],[104.066871,30.626541],[104.066888,30.625541],[104.066906,30.624541],[104.066924,30.623541],[104.066942,30.622542],[104.066960,30.621542],[104.067011,30.620543],[104.066122,30.620086],[104.065124,30.620021],[104.064124,30.620022],[104.063124,30.619990],[104.062125,30.619958],[104.061125,30.619926],[104.060126,30.619894],[104.059126,30.619895],[104.058127,30.619928],[104.057518,30.620722],[104.057625,30.621716],[104.057735,30.622710],[104.057878,30.623700],[104.057984,30.624694],[104.058094,30.625688],[104.058204,30.626682],[104.058315,30.627676],[104.058425,30.628670],[104.058502,30.629667],[104.058518,30.630667],[104.058503,30.631667],[104.058521,30.632666],[104.057664,30.633182],[104.056664,30.633174],[104.055664,30.633166],[104.054672,30.633289],[104.053758,30.633694],[104.052852,30.634118],[104.052623,30.635091],[104.053145,30.635945],[104.053675,30.636793],[104.054200,30.637643],[104.054756,30.638475],[104.055295,30.639317],[104.055843,30.640153],[104.056387,30.640993],[104.056933,30.641830],[104.057478,30.642669],[104.058023,30.643507],[104.058595,30.644327],[104.059152,30.645158],[104.059663,30.646018],[104.060171,30.646879],[104.061170,30.646855],[104.062168,30.646781],[104.063167,30.646823],[104.064167,30.646814],[104.065163,30.646725],[104.066157,30.646618],[104.066231,30.645620],[104.066247,30.644621],])123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109#-*-coding:utf-8-*-\"\"\"-------------------------------------------------  FileName：    DouglasPeuker  Description:  道格拉斯-普克抽稀算法  Author:        J_hao  date：          2017/8/16-------------------------------------------------  ChangeActivity:                  2017/8/16:道格拉斯-普克抽稀算法-------------------------------------------------\"\"\"from__future__importdivision frommathimportsqrt,pow __author__='J_hao' THRESHOLD=0.0001  #阈值  defpoint2LineDistance(point_a,point_b,point_c):    \"\"\"    计算点a到点bc所在直线的距离    :parampoint_a:    :parampoint_b:    :parampoint_c:    :return:    \"\"\"    #首先计算bc所在直线的斜率和截距    ifpoint_b[0]==point_c[0]:        return9999999    slope=(point_b[1]-point_c[1])/(point_b[0]-point_c[0])    intercept=point_b[1]-slope*point_b[0]     #计算点a到bc所在直线的距离    distance=abs(slope*point_a[0]-point_a[1]+intercept)/sqrt(1+pow(slope,2))    returndistance  classDouglasPeuker(object):    def__init__(self):        self.threshold=THRESHOLD        self.qualify_list=list()        self.disqualify_list=list()     defdiluting(self,point_list):        \"\"\"        抽稀        :parampoint_list:二维点列表        :return:        \"\"\"        iflen(point_list)<3:            self.qualify_list.extend(point_list[::-1])        else:            #找到与收尾两点连线距离最大的点            max_distance_index,max_distance=0,0            forindex,pointinenumerate(point_list):                ifindexin[0,len(point_list)-1]:                    continue                distance=point2LineDistance(point,point_list[0],point_list[-1])                ifdistance>max_distance:                    max_distance_index=index                    max_distance=distance             #若最大距离小于阈值，则去掉所有中间点。反之，则将曲线按最大距离点分割            ifmax_distance<self.threshold:                self.qualify_list.append(point_list[-1])                self.qualify_list.append(point_list[0])            else:                #将曲线按最大距离的点分割成两段                sequence_a=point_list[:max_distance_index]                sequence_b=point_list[max_distance_index:]                 forsequencein[sequence_a,sequence_b]:                    iflen(sequence)<3andsequence==sequence_b:                        self.qualify_list.extend(sequence[::-1])                    else:                        self.disqualify_list.append(sequence)     defmain(self,point_list):        self.diluting(point_list)        whilelen(self.disqualify_list)>0:            self.diluting(self.disqualify_list.pop())        printself.qualify_list        printlen(self.qualify_list)  if__name__=='__main__':    d=DouglasPeuker()    d.main([[104.066228,30.644527],[104.066279,30.643528],[104.066296,30.642528],[104.066314,30.641529],            [104.066332,30.640529],[104.066383,30.639530],[104.066400,30.638530],[104.066451,30.637531],            [104.066468,30.636532],[104.066518,30.635533],[104.066535,30.634533],[104.066586,30.633534],            [104.066636,30.632536],[104.066686,30.631537],[104.066735,30.630538],[104.066785,30.629539],            [104.066802,30.628539],[104.066820,30.627540],[104.066871,30.626541],[104.066888,30.625541],            [104.066906,30.624541],[104.066924,30.623541],[104.066942,30.622542],[104.066960,30.621542],            [104.067011,30.620543],[104.066122,30.620086],[104.065124,30.620021],[104.064124,30.620022],            [104.063124,30.619990],[104.062125,30.619958],[104.061125,30.619926],[104.060126,30.619894],            [104.059126,30.619895],[104.058127,30.619928],[104.057518,30.620722],[104.057625,30.621716],            [104.057735,30.622710],[104.057878,30.623700],[104.057984,30.624694],[104.058094,30.625688],            [104.058204,30.626682],[104.058315,30.627676],[104.058425,30.628670],[104.058502,30.629667],            [104.058518,30.630667],[104.058503,30.631667],[104.058521,30.632666],[104.057664,30.633182],            [104.056664,30.633174],[104.055664,30.633166],[104.054672,30.633289],[104.053758,30.633694],            [104.052852,30.634118],[104.052623,30.635091],[104.053145,30.635945],[104.053675,30.636793],            [104.054200,30.637643],[104.054756,30.638475],[104.055295,30.639317],[104.055843,30.640153],            [104.056387,30.640993],[104.056933,30.641830],[104.057478,30.642669],[104.058023,30.643507],            [104.058595,30.644327],[104.059152,30.645158],[104.059663,30.646018],[104.060171,30.646879],            [104.061170,30.646855],[104.062168,30.646781],[104.063167,30.646823],[104.064167,30.646814],            [104.065163,30.646725],[104.066157,30.646618],[104.066231,30.645620],[104.066247,30.644621],])垂距限值法垂距限值法其实和DP算法原理一样，但是垂距限值不是从整体角度考虑，而是依次扫描每一个点，检查是否符合要求。算法过程如下:1、以第二个点开始，计算第二个点到前一个点和后一个点所在直线的距离d；2、如果d大于阈值，则保留第二个点，计算第三个点到第二个点和第四个点所在直线的距离d;若d小于阈值则舍弃第二个点，计算第三个点到第一个点和第四个点所在直线的距离d;3、依次类推，直线曲线上倒数第二个点。下面是Python代码实现：Python#-*-coding:utf-8-*-\"\"\"-------------------------------------------------FileName：LimitVerticalDistanceDescription:垂距限值抽稀算法Author:J_haodate：2017/8/17-------------------------------------------------ChangeActivity:2017/8/17:-------------------------------------------------\"\"\"from__future__importdivisionfrommathimportsqrt,pow__author__='J_hao'THRESHOLD=0.0001#阈值defpoint2LineDistance(point_a,point_b,point_c):\"\"\"计算点a到点bc所在直线的距离:parampoint_a::parampoint_b::parampoint_c::return:\"\"\"#首先计算bc所在直线的斜率和截距ifpoint_b[0]==point_c[0]:return9999999slope=(point_b[1]-point_c[1])/(point_b[0]-point_c[0])intercept=point_b[1]-slope*point_b[0]#计算点a到bc所在直线的距离distance=abs(slope*point_a[0]-point_a[1]+intercept)/sqrt(1+pow(slope,2))returndistanceclassLimitVerticalDistance(object):def__init__(self):self.threshold=THRESHOLDself.qualify_list=list()defdiluting(self,point_list):\"\"\"抽稀:parampoint_list:二维点列表:return:\"\"\"self.qualify_list.append(point_list[0])check_index=1whilecheck_index<len(point_list)-1:distance=point2LineDistance(point_list[check_index],self.qualify_list[-1],point_list[check_index+1])ifdistance<self.threshold:check_index+=1else:self.qualify_list.append(point_list[check_index])check_index+=1returnself.qualify_listif__name__=='__main__':l=LimitVerticalDistance()diluting=l.diluting([[104.066228,30.644527],[104.066279,30.643528],[104.066296,30.642528],[104.066314,30.641529],[104.066332,30.640529],[104.066383,30.639530],[104.066400,30.638530],[104.066451,30.637531],[104.066468,30.636532],[104.066518,30.635533],[104.066535,30.634533],[104.066586,30.633534],[104.066636,30.632536],[104.066686,30.631537],[104.066735,30.630538],[104.066785,30.629539],[104.066802,30.628539],[104.066820,30.627540],[104.066871,30.626541],[104.066888,30.625541],[104.066906,30.624541],[104.066924,30.623541],[104.066942,30.622542],[104.066960,30.621542],[104.067011,30.620543],[104.066122,30.620086],[104.065124,30.620021],[104.064124,30.620022],[104.063124,30.619990],[104.062125,30.619958],[104.061125,30.619926],[104.060126,30.619894],[104.059126,30.619895],[104.058127,30.619928],[104.057518,30.620722],[104.057625,30.621716],[104.057735,30.622710],[104.057878,30.623700],[104.057984,30.624694],[104.058094,30.625688],[104.058204,30.626682],[104.058315,30.627676],[104.058425,30.628670],[104.058502,30.629667],[104.058518,30.630667],[104.058503,30.631667],[104.058521,30.632666],[104.057664,30.633182],[104.056664,30.633174],[104.055664,30.633166],[104.054672,30.633289],[104.053758,30.633694],[104.052852,30.634118],[104.052623,30.635091],[104.053145,30.635945],[104.053675,30.636793],[104.054200,30.637643],[104.054756,30.638475],[104.055295,30.639317],[104.055843,30.640153],[104.056387,30.640993],[104.056933,30.641830],[104.057478,30.642669],[104.058023,30.643507],[104.058595,30.644327],[104.059152,30.645158],[104.059663,30.646018],[104.060171,30.646879],[104.061170,30.646855],[104.062168,30.646781],[104.063167,30.646823],[104.064167,30.646814],[104.065163,30.646725],[104.066157,30.646618],[104.066231,30.645620],[104.066247,30.644621],])printlen(diluting)print(diluting)1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889#-*-coding:utf-8-*-\"\"\"-------------------------------------------------  FileName：    LimitVerticalDistance  Description:  垂距限值抽稀算法  Author:        J_hao  date：          2017/8/17-------------------------------------------------  ChangeActivity:                  2017/8/17:-------------------------------------------------\"\"\"from__future__importdivision frommathimportsqrt,pow __author__='J_hao' THRESHOLD=0.0001  #阈值  defpoint2LineDistance(point_a,point_b,point_c):    \"\"\"    计算点a到点bc所在直线的距离    :parampoint_a:    :parampoint_b:    :parampoint_c:    :return:    \"\"\"    #首先计算bc所在直线的斜率和截距    ifpoint_b[0]==point_c[0]:        return9999999    slope=(point_b[1]-point_c[1])/(point_b[0]-point_c[0])    intercept=point_b[1]-slope*point_b[0]     #计算点a到bc所在直线的距离    distance=abs(slope*point_a[0]-point_a[1]+intercept)/sqrt(1+pow(slope,2))    returndistance  classLimitVerticalDistance(object):    def__init__(self):        self.threshold=THRESHOLD        self.qualify_list=list()     defdiluting(self,point_list):        \"\"\"        抽稀        :parampoint_list:二维点列表        :return:        \"\"\"        self.qualify_list.append(point_list[0])        check_index=1        whilecheck_index<len(point_list)-1:            distance=point2LineDistance(point_list[check_index],                                          self.qualify_list[-1],                                          point_list[check_index+1])             ifdistance<self.threshold:                check_index+=1            else:                self.qualify_list.append(point_list[check_index])                check_index+=1        returnself.qualify_list  if__name__=='__main__':    l=LimitVerticalDistance()    diluting=l.diluting([[104.066228,30.644527],[104.066279,30.643528],[104.066296,30.642528],[104.066314,30.641529],            [104.066332,30.640529],[104.066383,30.639530],[104.066400,30.638530],[104.066451,30.637531],            [104.066468,30.636532],[104.066518,30.635533],[104.066535,30.634533],[104.066586,30.633534],            [104.066636,30.632536],[104.066686,30.631537],[104.066735,30.630538],[104.066785,30.629539],            [104.066802,30.628539],[104.066820,30.627540],[104.066871,30.626541],[104.066888,30.625541],            [104.066906,30.624541],[104.066924,30.623541],[104.066942,30.622542],[104.066960,30.621542],            [104.067011,30.620543],[104.066122,30.620086],[104.065124,30.620021],[104.064124,30.620022],            [104.063124,30.619990],[104.062125,30.619958],[104.061125,30.619926],[104.060126,30.619894],            [104.059126,30.619895],[104.058127,30.619928],[104.057518,30.620722],[104.057625,30.621716],            [104.057735,30.622710],[104.057878,30.623700],[104.057984,30.624694],[104.058094,30.625688],            [104.058204,30.626682],[104.058315,30.627676],[104.058425,30.628670],[104.058502,30.629667],            [104.058518,30.630667],[104.058503,30.631667],[104.058521,30.632666],[104.057664,30.633182],            [104.056664,30.633174],[104.055664,30.633166],[104.054672,30.633289],[104.053758,30.633694],            [104.052852,30.634118],[104.052623,30.635091],[104.053145,30.635945],[104.053675,30.636793],            [104.054200,30.637643],[104.054756,30.638475],[104.055295,30.639317],[104.055843,30.640153],            [104.056387,30.640993],[104.056933,30.641830],[104.057478,30.642669],[104.058023,30.643507],            [104.058595,30.644327],[104.059152,30.645158],[104.059663,30.646018],[104.060171,30.646879],            [104.061170,30.646855],[104.062168,30.646781],[104.063167,30.646823],[104.064167,30.646814],            [104.065163,30.646725],[104.066157,30.646618],[104.066231,30.645620],[104.066247,30.644621],])    printlen(diluting)    print(diluting)最后其实DP算法和垂距限值法原理一样，DP算法是从整体上考虑一条完整的曲线，实现时较垂距限值法复杂，但垂距限值法可能会在某些情况下导致局部最优。另外在实际使用中发现采用点到另外两点所在直线距离的方法来判断偏离，在曲线弧度比较大的情况下比较准确。如果在曲线弧度比较小，弯曲程度不明显时，这种方法抽稀效果不是很理想，建议使用三点所围成的三角形面积作为判断标准。下面是抽稀效果:1赞3收藏评论"], "art_create_time": ["2017/11/19"], "art_title": ["曲线点抽稀算法- Python 实现"], "art_url": ["http://python.jobbole.com/88892/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2017/11/3be8f2c50649ea9862264793b74bdc66.png"]},
{"art_content": ["原文出处：imze5z   基于概率论的分类方法：朴素贝叶斯1.概述贝叶斯分类是一类分类算法的总称，这类算法均以贝叶斯定理为基础，故统称为贝叶斯分类。本章首先介绍贝叶斯分类算法的基础——贝叶斯定理。最后，我们通过实例来讨论贝叶斯分类的中最简单的一种:朴素贝叶斯分类。2.贝叶斯理论&条件概率2.1贝叶斯理论我们现在有一个数据集，它由两类数据组成，数据分布如下图所示：我们现在用p1(x,y)表示数据点(x,y)属于类别1（图中用圆点表示的类别）的概率，用p2(x,y)表示数据点(x,y)属于类别2（图中三角形表示的类别）的概率，那么对于一个新数据点(x,y)，可以用下面的规则来判断它的类别：Python如果p1(x,y)>p2(x,y)，那么类别为1如果p2(x,y)>p1(x,y)，那么类别为212如果p1(x,y)>p2(x,y)，那么类别为1如果p2(x,y)>p1(x,y)，那么类别为2也就是说，我们会选择高概率对应的类别。这就是贝叶斯决策理论的核心思想，即选择具有最高概率的决策。2.1.2条件概率如果你对p(x,y|c1)符号很熟悉，那么可以跳过本小节。有一个装了7块石头的罐子，其中3块是白色的，4块是黑色的。如果从罐子中随机取出一块石头，那么是白色石头的可能性是多少？由于取石头有7种可能，其中3种为白色，所以取出白色石头的概率为3/7。那么取到黑色石头的概率又是多少呢？很显然，是4/7。我们使用P(white)来表示取到白色石头的概率，其概率值可以通过白色石头数目除以总的石头数目来得到。如果这7块石头如下图所示，放在两个桶中，那么上述概率应该如何计算？计算P(white)或者P(black)，如果事先我们知道石头所在桶的信息是会改变结果的。这就是所谓的条件概率（conditionalprobablity）。假定计算的是从B桶取到白色石头的概率，这个概率可以记作P(white|bucketB)，我们称之为“在已知石头出自B桶的条件下，取出白色石头的概率”。很容易得到，P(white|bucketA)值为2/4，P(white|bucketB)的值为1/3。条件概率的计算公式如下：P(white|bucketB)=P(whiteandbucketB)/P(bucketB)首先，我们用B桶中白色石头的个数除以两个桶中总的石头数，得到P(whiteandbucketB)=1/7.其次，由于B桶中有3块石头，而总石头数为7，于是P(bucketB)就等于3/7。于是又P(white|bucketB)=P(whiteandbucketB)/P(bucketB)=(1/7)/(3/7)=1/3。另外一种有效计算条件概率的方法称为贝叶斯准则。贝叶斯准则告诉我们如何交换条件概率中的条件与结果，即如果已知P(x|c)，要求P(c|x)，那么可以使用下面的计算方法：使用条件概率来分类上面我们提到贝叶斯决策理论要求计算两个概率p1(x,y)和p2(x,y):Python如果p1(x,y)>p2(x,y),那么属于类别1;如果p2(x,y)>p1(X,y),那么属于类别2.12如果p1(x,y)>p2(x,y),那么属于类别1;如果p2(x,y)>p1(X,y),那么属于类别2.这并不是贝叶斯决策理论的所有内容。使用p1()和p2()只是为了尽可能简化描述，而真正需要计算和比较的是p(c1|x,y)和p(c2|x,y).这些符号所代表的具体意义是:给定某个由x、y表示的数据点，那么该数据点来自类别c1的概率是多少？数据点来自类别c2的概率又是多少？注意这些概率与概率p(x,y|c1)并不一样，不过可以使用贝叶斯准则来交换概率中条件与结果。具体地，应用贝叶斯准则得到:使用上面这些定义，可以定义贝叶斯分类准则为:Python如果P(c1|x,y)>P(c2|x,y),那么属于类别c1;如果P(c2|x,y)>P(c1|x,y),那么属于类别c2.12如果P(c1|x,y)>P(c2|x,y),那么属于类别c1;如果P(c2|x,y)>P(c1|x,y),那么属于类别c2.在文档分类中，整个文档（如一封电子邮件）是实例，而电子邮件中的某些元素则构成特征。我们可以观察文档中出现的词，并把每个词作为一个特征，而每个词的出现或者不出现作为该特征的值，这样得到的特征数目就会跟词汇表中的词的数目一样多。我们假设特征之间相互独立。所谓独立(independence)指的是统计意义上的独立，即一个特征或者单词出现的可能性与它和其他单词相邻没有关系，比如说，“我们”中的“我”和“们”出现的概率与这两个字相邻没有任何关系。这个假设正是朴素贝叶斯分类器中朴素(naive)一词的含义。朴素贝叶斯分类器中的另一个假设是，每个特征同等重要。Note:朴素贝叶斯分类器通常有两种实现方式:一种基于伯努利模型实现，一种基于多项式模型实现。这里采用前一种实现方式。该实现方式中并不考虑词在文档中出现的次数，只考虑出不出现，因此在这个意义上相当于假设词是等权重的。2.2朴素贝叶斯场景机器学习的一个重要应用就是文档的自动分类。在文档分类中，整个文档（如一封电子邮件）是实例，而电子邮件中的某些元素则构成特征。我们可以观察文档中出现的词，并把每个词作为一个特征，而每个词的出现或者不出现作为该特征的值，这样得到的特征数目就会跟词汇表中的词的数目一样多。朴素贝叶斯是上面介绍的贝叶斯分类器的一个扩展，是用于文档分类的常用算法。下面我们会进行一些朴素贝叶斯分类的实践项目。2.3朴素贝叶斯原理朴素贝叶斯工作原理提取所有文档中的词条并进行去重获取文档的所有类别计算每个类别中的文档数目对每篇训练文档:Python对每个类别:如果词条出现在文档中-->增加该词条的计数值（for循环或者矩阵相加）增加所有词条的计数值（此类别下词条总数）123对每个类别:    如果词条出现在文档中-->增加该词条的计数值（for循环或者矩阵相加）    增加所有词条的计数值（此类别下词条总数）对每个类别:Python对每个词条:将该词条的数目除以总词条数目得到的条件概率（P(词条|类别)）12对每个词条:    将该词条的数目除以总词条数目得到的条件概率（P(词条|类别)）返回该文档属于每个类别的条件概率（P(类别|文档的所有词条)）2.4朴素贝叶斯开发流程收集数据:可以使用任何方法。准备数据:需要数值型或者布尔型数据。分析数据:有大量特征时，绘制特征作用不大，此时使用直方图效果更好。训练算法:计算不同的独立特征的条件概率。测试算法:计算错误率。使用算法:一个常见的朴素贝叶斯应用是文档分类。可以在任意的分类场景中使用朴素贝叶斯分类器，不一定非要是文本。2.5朴素贝叶斯算法特点优点:在数据较少的情况下仍然有效，可以处理多类别问题。缺点:对于输入数据的准备方式较为敏感。适用数据类型:标称型数据。2.6朴素贝叶斯项目案例2.6.1项目案例1屏蔽社区留言板的侮辱性言论2.6.1.1项目概述构建一个快速过滤器来屏蔽在线社区留言板上的侮辱性言论。如果某条留言使用了负面或者侮辱性的语言，那么就将该留言标识为内容不当。对此问题建立两个类别:侮辱类和非侮辱类，使用1和0分别表示。2.6.1.2开发流程收集数据:可以使用任何方法准备数据:从文本中构建词向量分析数据:检查词条确保解析的正确性训练算法:从词向量计算概率测试算法:根据现实情况修改分类器使用算法:对社区留言板言论进行分类收集数据:可以使用任何方法2.6.1.3构造词表PythondefloadDataSet():\"\"\"创建数据集:return:单词列表postingList,所属类别classVec\"\"\"postingList=[['my','dog','has','flea','problems','help','please'],#[0,0,1,1,1......]['maybe','not','take','him','to','dog','park','stupid'],['my','dalmation','is','so','cute','I','love','him'],['stop','posting','stupid','worthless','garbage'],['mr','licks','ate','my','steak','how','to','stop','him'],['quit','buying','worthless','dog','food','stupid']]classVec=[0,1,0,1,0,1]#1isabusive,0notreturnpostingList,classVec12345678910111213defloadDataSet():    \"\"\"    创建数据集    :return:单词列表postingList,所属类别classVec    \"\"\"    postingList=[['my','dog','has','flea','problems','help','please'],#[0,0,1,1,1......]                  ['maybe','not','take','him','to','dog','park','stupid'],                  ['my','dalmation','is','so','cute','I','love','him'],                  ['stop','posting','stupid','worthless','garbage'],                  ['mr','licks','ate','my','steak','how','to','stop','him'],                  ['quit','buying','worthless','dog','food','stupid']]    classVec=[0,1,0,1,0,1]  #1isabusive,0not    returnpostingList,classVec2.6.1.4准备数据:从文本中构建词向量PythondefcreateVocabList(dataSet):\"\"\"获取所有单词的集合:paramdataSet:数据集:return:所有单词的集合(即不含重复元素的单词列表)\"\"\"vocabSet=set([])#createemptysetfordocumentindataSet:#操作符|用于求两个集合的并集vocabSet=vocabSet|set(document)#unionofthetwosetsreturnlist(vocabSet)defsetOfWords2Vec(vocabList,inputSet):\"\"\"遍历查看该单词是否出现，出现该单词则将该单词置1:paramvocabList:所有单词集合列表:paraminputSet:输入数据集:return:匹配列表[0,1,0,1...]，其中1与0表示词汇表中的单词是否出现在输入的数据集中\"\"\"#创建一个和词汇表等长的向量，并将其元素都设置为0returnVec=[0]*len(vocabList)#[0,0......]#遍历文档中的所有单词，如果出现了词汇表中的单词，则将输出的文档向量中的对应值设为1forwordininputSet:ifwordinvocabList:returnVec[vocabList.index(word)]=1else:print\"theword:%sisnotinmyVocabulary!\"%wordreturnreturnVec1234567891011121314151617181920212223242526272829defcreateVocabList(dataSet):    \"\"\"    获取所有单词的集合    :paramdataSet:数据集    :return:所有单词的集合(即不含重复元素的单词列表)    \"\"\"    vocabSet=set([])  #createemptyset    fordocumentindataSet:        #操作符|用于求两个集合的并集        vocabSet=vocabSet|set(document)  #unionofthetwosets    returnlist(vocabSet)  defsetOfWords2Vec(vocabList,inputSet):    \"\"\"    遍历查看该单词是否出现，出现该单词则将该单词置1    :paramvocabList:所有单词集合列表    :paraminputSet:输入数据集    :return:匹配列表[0,1,0,1...]，其中1与0表示词汇表中的单词是否出现在输入的数据集中    \"\"\"    #创建一个和词汇表等长的向量，并将其元素都设置为0    returnVec=[0]*len(vocabList)#[0,0......]    #遍历文档中的所有单词，如果出现了词汇表中的单词，则将输出的文档向量中的对应值设为1    forwordininputSet:        ifwordinvocabList:            returnVec[vocabList.index(word)]=1        else:            print\"theword:%sisnotinmyVocabulary!\"%word    returnreturnVec2.6.1.5分析数据:检查词条确保解析的正确性检查函数执行情况，检查词表，不出现重复单词，需要的话，可以对其进行排序。Python>>>listOPosts,listClasses=bayes.loadDataSet()>>>myVocabList=bayes.createVocabList(listOPosts)>>>myVocabList['cute','love','help','garbage','quit','I','problems','is','park','stop','flea','dalmation','licks','food','not','him','buying','posting','has','worthless','ate','to','maybe','please','dog','how','stupid','so','take','mr','steak','my']123456>>>listOPosts,listClasses=bayes.loadDataSet()>>>myVocabList=bayes.createVocabList(listOPosts)>>>myVocabList['cute','love','help','garbage','quit','I','problems','is','park','stop','flea','dalmation','licks','food','not','him','buying','posting','has','worthless','ate','to','maybe','please','dog','how','stupid','so','take','mr','steak','my']检查函数有效性。例如：myVocabList中索引为2的元素是什么单词？应该是是help。该单词在第一篇文档中出现了，现在检查一下看看它是否出现在第四篇文档中。Python>>>bayes.setOfWords2Vec(myVocabList,listOPosts[0])[0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0,0,0,0,0,1]>>>bayes.setOfWords2Vec(myVocabList,listOPosts[3])[0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,1,0,0,0,0,0]12345>>>bayes.setOfWords2Vec(myVocabList,listOPosts[0])[0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0,0,0,0,0,1] >>>bayes.setOfWords2Vec(myVocabList,listOPosts[3])[0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,1,0,0,0,0,0]2.6.1.6训练算法:从词向量计算概率现在已经知道了一个词是否出现在一篇文档中，也知道该文档所属的类别。接下来我们重写贝叶斯准则，将之前的x,y替换为w.粗体的w表示这是一个向量，即它由多个值组成。在这个例子中，数值个数与词汇表中的词个数相同。我们使用上述公式，对每个类计算该值，然后比较这两个概率值的大小。首先可以通过类别i(侮辱性留言或者非侮辱性留言)中的文档数除以总的文档数来计算概率p(ci)。接下来计算p(w|ci)，这里就要用到朴素贝叶斯假设。如果将w展开为一个个独立特征，那么就可以将上述概率写作p(w0,w1,w2…wn|ci)。这里假设所有词都互相独立，该假设也称作条件独立性假设（例如A和B两个人抛骰子，概率是互不影响的，也就是相互独立的，A抛2点的同时B抛3点的概率就是1/6*1/6），它意味着可以使用p(w0|ci)p(w1|ci)p(w2|ci)…p(wn|ci)来计算上述概率，这样就极大地简化了计算的过程。2.6.1.7朴素贝叶斯分类器训练函数Pythondef_trainNB0(trainMatrix,trainCategory):\"\"\"训练数据原版:paramtrainMatrix:文件单词矩阵[[1,0,1,1,1....],[],[]...]:paramtrainCategory:文件对应的类别[0,1,1,0....]，列表长度等于单词矩阵数，其中的1代表对应的文件是侮辱性文件，0代表不是侮辱性矩阵:return:\"\"\"#文件数numTrainDocs=len(trainMatrix)#单词数numWords=len(trainMatrix[0])#侮辱性文件的出现概率，即trainCategory中所有的1的个数，#代表的就是多少个侮辱性文件，与文件的总数相除就得到了侮辱性文件的出现概率pAbusive=sum(trainCategory)/float(numTrainDocs)#构造单词出现次数列表p0Num=zeros(numWords)#[0,0,0,.....]p1Num=zeros(numWords)#[0,0,0,.....]#整个数据集单词出现总数p0Denom=0.0p1Denom=0.0foriinrange(numTrainDocs):#是否是侮辱性文件iftrainCategory[i]==1:#如果是侮辱性文件，对侮辱性文件的向量进行加和p1Num+=trainMatrix[i]#[0,1,1,....]+[0,1,1,....]->[0,2,2,...]#对向量中的所有元素进行求和，也就是计算所有侮辱性文件中出现的单词总数p1Denom+=sum(trainMatrix[i])else:p0Num+=trainMatrix[i]p0Denom+=sum(trainMatrix[i])#类别1，即侮辱性文档的[P(F1|C1),P(F2|C1),P(F3|C1),P(F4|C1),P(F5|C1)....]列表#即在1类别下，每个单词出现的概率p1Vect=p1Num/p1Denom#[1,2,3,5]/90->[1/90,...]#类别0，即正常文档的[P(F1|C0),P(F2|C0),P(F3|C0),P(F4|C0),P(F5|C0)....]列表#即在0类别下，每个单词出现的概率p0Vect=p0Num/p0Denomreturnp0Vect,p1Vect,pAbusive1234567891011121314151617181920212223242526272829303132333435363738def_trainNB0(trainMatrix,trainCategory):    \"\"\"    训练数据原版    :paramtrainMatrix:文件单词矩阵[[1,0,1,1,1....],[],[]...]    :paramtrainCategory:文件对应的类别[0,1,1,0....]，列表长度等于单词矩阵数，其中的1代表对应的文件是侮辱性文件，0代表不是侮辱性矩阵    :return:    \"\"\"    #文件数    numTrainDocs=len(trainMatrix)    #单词数    numWords=len(trainMatrix[0])    #侮辱性文件的出现概率，即trainCategory中所有的1的个数，    #代表的就是多少个侮辱性文件，与文件的总数相除就得到了侮辱性文件的出现概率    pAbusive=sum(trainCategory)/float(numTrainDocs)    #构造单词出现次数列表    p0Num=zeros(numWords)#[0,0,0,.....]    p1Num=zeros(numWords)#[0,0,0,.....]     #整个数据集单词出现总数    p0Denom=0.0    p1Denom=0.0    foriinrange(numTrainDocs):        #是否是侮辱性文件        iftrainCategory[i]==1:            #如果是侮辱性文件，对侮辱性文件的向量进行加和            p1Num+=trainMatrix[i]#[0,1,1,....]+[0,1,1,....]->[0,2,2,...]            #对向量中的所有元素进行求和，也就是计算所有侮辱性文件中出现的单词总数            p1Denom+=sum(trainMatrix[i])        else:            p0Num+=trainMatrix[i]            p0Denom+=sum(trainMatrix[i])    #类别1，即侮辱性文档的[P(F1|C1),P(F2|C1),P(F3|C1),P(F4|C1),P(F5|C1)....]列表    #即在1类别下，每个单词出现的概率    p1Vect=p1Num/p1Denom#[1,2,3,5]/90->[1/90,...]    #类别0，即正常文档的[P(F1|C0),P(F2|C0),P(F3|C0),P(F4|C0),P(F5|C0)....]列表    #即在0类别下，每个单词出现的概率    p0Vect=p0Num/p0Denom    returnp0Vect,p1Vect,pAbusive2.6.1.8测试算法:根据现实情况修改分类器http://www.cnblogs.com/apache…1赞3收藏2评论"], "art_create_time": ["2017/10/20"], "art_title": ["基于概率论的分类方法：朴素贝叶斯"], "art_url": ["http://python.jobbole.com/88717/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2017/10/eba5f145122207e6dd3575ad30702bed.png"]},
{"art_content": ["来源：袁萌这个问题，即使你说出许多理由也无济于事，需要依靠实际统计数字来说话。4月6日，BlackDuckSoftware,Inc.发布一项内容十分周详的调查报告，题为”OpenSourceByTheNumbers“（报告人是RichSands），读后有感。该调查发现，当今最活跃的编程语言是C/C++，跟随其后的是Java，Python，JavaScript等编程语言，如下图所示：从上图可见，圆饼图左下方的淡蓝色扇形区域代表的就是Python编程语言。那么，我们为什么要孩子们学习Python，而不是学习C/C++和Java编程语言呢？该调查报告的最后结论是：”NewliveprojectstrendingtowardsPython,PHP,JavaScriptandawayfromC-familylanguages“，意思是说，新的活跃研究项目都倾向（trendingtowards）使用Python，PHP与JavaScript编程，而远离（awayfrom）C编程语言大家族。从某种意义上来说，Python模块化编程是近十年来软件开发的重大成就。英国”馅饼“小电脑首选Python编程是有根据的，我们应该给予足够的重视。实际上，Python的编辑环境”极妙“，很好使用并且容易”上瘾“（把人粘住）。孩子们喜欢在玩中学习，不能硬逼。……我很苦恼，至今找不到一位Python朋友。中国的家长，往往一听说Python（大蟒蛇）编程，就叫孩子赶忙远离这玩意儿。这是天大的误会。请大家帮忙解释一下。1赞收藏4评论"], "art_create_time": ["2012/04/12"], "art_title": ["孩子们为什么要学Python编程"], "art_url": ["http://python.jobbole.com/17295/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2012/04/languages-of-live-projects-1024x791.jpg"]},
{"art_content": ["原文出处：牧云云   Python是一门运用很广泛的语言，自动化脚本、爬虫，甚至在深度学习领域也都有Python的身影。作为一名前端开发者，也了解ES6中的很多特性借鉴自Python(比如默认参数、解构赋值、Decorator等)，同时本文会对Python的一些用法与JS进行类比。不管是提升自己的知识广度，还是更好地迎接AI时代，Python都是一门值得学习的语言。数据类型在Python中，最常用的能够直接处理的数据类型有以下几种：数字[整数(int)、浮点型(float)、长整型(long)、复数(complex)]字符串(str)布尔值(bool)空值(None)除此之外，Python还提供了列表（list）、字典（dict）等多种数据类型，这在下文中会介绍。类型转换与类型判断与JS十分类似，python也能实现不同数据类型间的强制与隐式转换，例子如下：强制类型转换:Pythonint('3')#3str(3.14)#'3.14'float('3.14')#3.14#区别于JS只有Number一种类型，Python中数字中的不同类型也能相互强制转换float(3)#3.0bool(3)#Truebool(0)#False1234567int('3')#3str(3.14)#'3.14'float('3.14')#3.14#区别于JS只有Number一种类型，Python中数字中的不同类型也能相互强制转换float(3)#3.0bool(3)#Truebool(0)#False隐式类型转换:Python1+1.0#2.01+False#11.0+True#2.0#区别于JS的String+Number=String,py中str+int会报错1+'1'#TypeError:cannotconcatenate'str'and'int'objects123451+1.0#2.01+False#11.0+True#2.0#区别于JS的String+Number=String,py中str+int会报错1+'1'#TypeError:cannotconcatenate'str'and'int'objects此外写代码的时候经常会需要判断值的类型，可以使用python提供的type()函数获取变量的类型，或者使用isinstance(x,type)来判断x是否属于相应的type类型。Pythontype(1.3)==float#Trueisinstance('a',str)#Trueisinstance(1.3,int)#Falseisinstance(True,bool)#Trueisinstance([],list)#Trueisinstance({},dict)#True123456type(1.3)==float#Trueisinstance('a',str)#Trueisinstance(1.3,int)#Falseisinstance(True,bool)#Trueisinstance([],list)#Trueisinstance({},dict)#True有序集合类型集合是指包含一组元素的数据结构，有序集合即集合里面的元素是是按照顺序排列的，Python中的有序集合大概有以下几类：list,tuple,str,unicode。list类型Python中List类型类似于JS中的Array,PythonL=[1,2,3]printL[-1]#'3'L.append(4)#末尾添加元素printL#[1,2,3,4]L.insert(0,'hi')#指定索引位置添加元素printL#['hi',1,2,3,4]L.pop()#末尾移除元素L.pop(2)??????2???printL#['hi',1,2,3]1234567891011L=[1,2,3]printL[-1]#'3' L.append(4)#末尾添加元素printL#[1,2,3,4] L.insert(0,'hi')#指定索引位置添加元素printL#['hi',1,2,3,4] L.pop()#末尾移除元素L.pop(2)??????2???printL#['hi',1,2,3]tuple类型tuple类型是另一种有序的列表，中文翻译为“元组”。tuple和list非常类似，但是，tuple一旦创建完毕，就不能修改了。Pythont=(1,2,3)printt[0]#1t[0]=11#TypeError:'tuple'objectdoesnotsupportitemassignmentt=(1)printt#1t的结果是整数1t=(1,)#为了避免出现如上有歧义的单元素tuple，所以Python规定，单元素tuple要多加一个逗号“,”printt#(1,)123456789t=(1,2,3)printt[0]#1t[0]=11#TypeError:'tuple'objectdoesnotsupportitemassignment t=(1)printt#1  t的结果是整数1 t=(1,)#为了避免出现如上有歧义的单元素tuple，所以Python规定，单元素tuple要多加一个逗号“,”printt#(1,)无序集合类型dict类型Python中的dict类型类似于JS中的{}(最大的不同是它是没有顺序的),它有如下特点:查找速度快(无论dict有10个元素还是10万个元素，查找速度都一样)占用内存大(与list类型相反)dict中的key不能重复dict中存储的key-value序对是没有顺序的Pythond={'a':1,'b':2,'c':3}printd#{'a':1,'c':3,'b':2}可以看出打印出的序对没有按正常的顺序打出#遍历dictforkey,valueind.items():print('%s:%s'%(key,value))#a:1#c:3#b:21234567891011121314d={    'a':1,    'b':2,    'c':3} printd#{'a':1,'c':3,'b':2}  可以看出打印出的序对没有按正常的顺序打出 #遍历dictforkey,valueind.items():    print('%s:%s'%(key,value))#a:1#c:3#b:2set类型有的时候，我们只想要dict的key，不关心key对应的value，而且要保证这个集合的元素不会重复，这时，set类型就派上用场了。set类型有如下特点：set存储的元素和dict的key类似，必须是不变对象set存储的元素也是没有顺序的Pythons=set(['A','B','C','C'])prints#set(['A','C','B'])s.add('D')prints#set(['A','C','B','D'])s.remove('D')prints#set(['A','C','B'])12345678s=set(['A','B','C','C'])prints#set(['A','C','B']) s.add('D')prints#set(['A','C','B','D']) s.remove('D')prints#set(['A','C','B'])Python中的迭代在介绍完Python中的有序集合和无序集合类型后，必然存在遍历集合的for循环。但是和其它语言的标准for循环不同，Python中的所有迭代是通过for…in来完成的。以下给出一些常用的迭代demos:索引迭代：PythonL=['apple','banana','orange']forindex,nameinenumerate(L):#enumerate()函数把['apple','banana','orange']变成了类似[(0,'apple),(1,'banana'),(2,'orange')]的形式printindex,'-',name#0-apple#1-banana#2-orange1234567L=['apple','banana','orange']forindex,nameinenumerate(L):  #enumerate()函数把['apple','banana','orange']变成了类似[(0,'apple),(1,'banana'),(2,'orange')]的形式    printindex,'-',name #0-apple#1-banana#2-orange迭代dict的value:Pythond={'apple':6,'banana':8,'orange':5}printd.values()#[6,8,5]forvind.values()printv#6#8#51234567d={'apple':6,'banana':8,'orange':5}printd.values()#[6,8,5]forvind.values()    printv#6#8#5迭代dict的key和value:Pythond={'apple':6,'banana':8,'orange':5}forkey,valueind.items()printkey,':',value#apple:6#banana:8#orange:5123456d={'apple':6,'banana':8,'orange':5}forkey,valueind.items()    printkey,':',value#apple:6#banana:8#orange:5切片操作符Python提供的切片操作符类似于JS提供的原生函数slice()。有了切片操作符，大大简化了一些原来得用循环的操作。PythonL=['apple','banana','orange','pear']L[0:2]#['apple','banana']取前2个元素L[:2]#['apple','banana']如果第一个索引是0，可以省略L[:]#['apple','banana','orange','pear']只用一个:，表示从头到尾L[::2]#['apple','orange']第三个参数表示每N个取一个，这里表示从头开始，每2个元素取出一个来12345L=['apple','banana','orange','pear']L[0:2]#['apple','banana']取前2个元素L[:2]#['apple','banana']如果第一个索引是0，可以省略L[:]#['apple','banana','orange','pear']只用一个:，表示从头到尾L[::2]#['apple','orange']第三个参数表示每N个取一个，这里表示从头开始，每2个元素取出一个来列表生成器如果要生成[1×1,2×2,3×3,…,10×10]怎么做？方法一是循环：PythonL=[]forxinrange(1,11):L.append(x*x)123L=[]forxinrange(1,11):    L.append(x*x)但是循环太繁琐，而列表生成式则可以用一行语句代替循环生成上面的list：Python#把要生成的元素x*x放到前面，后面跟for循环，就可以把list创建出来[x*xforxinrange(1,11)]#[1,4,9,16,25,36,49,64,81,100]123#把要生成的元素x*x放到前面，后面跟for循环，就可以把list创建出来[x*xforxinrange(1,11)]#[1,4,9,16,25,36,49,64,81,100]列表生成式的for循环后面还可以加上if判断(类似于JS中的filter()函数)，示例如下：Python[x*xforxinrange(1,11)ifx%2==0]#[4,16,36,64,100]12[x*xforxinrange(1,11)ifx%2==0]#[4,16,36,64,100]for循环可以嵌套，因此，在列表生成式中，也可以用多层for循环来生成列表。Python[m+nformin'ABC'fornin'123']#['A1','A2','A3','B1','B2','B3','C1','C2','C3']12[m+nformin'ABC'fornin'123']#['A1','A2','A3','B1','B2','B3','C1','C2','C3']Python函数默认参数JS中ES6的默认参数正是借鉴于Python，用法如下：Pythondefgreet(name='World'):print'Hello,'+name+'.'greet()#Hello,World.greet('Python')#Hello,Python.12345defgreet(name='World'):    print'Hello,'+name+'.' greet()#Hello,World.greet('Python')#Hello,Python.可变参数类似于JS函数中自动识别传入参数的个数，Python也提供了定义可变参数，即在可变参数的名字前面带上个*号。Pythondeffn(*args):printargsfn()#()fn('a')#('a',)fn('a','b')#('a','b')123456deffn(*args):    printargs fn()  #()fn('a')#('a',)fn('a','b')#('a','b')Python解释器会把传入的一组参数组装成一个tuple传递给可变参数，因此，在函数内部，直接把变量args看成一个tuple就好了。常用高阶函数Python中常用的函数(map、reduce、filter)的作用和JS中一致，只是用法稍微不同。map函数:接收一个函数f和一个list，并通过把函数f依次作用在list的每个元素上，得到一个新的list并返回。Pythondeff(x):returnx*xprintmap(f,[1,2,3,4,5,6,7,8,9])#[1,4,9,16,25,36,49,64,81]123deff(x):    returnx*xprintmap(f,[1,2,3,4,5,6,7,8,9])#[1,4,9,16,25,36,49,64,81]reduce函数:接收一个函数f和一个list(可以接受第三个值作为初始值)，reduce()对list的每个元素反复调用函数f，并返回最终结果值。Pythondeff(x,y):returnx*yreduce(f,[1,3,5])#151234deff(x,y):    returnx*y reduce(f,[1,3,5])#15filter函数:接收一个函数f和一个list，这个函数f的作用是对每个元素进行判断，返回True或False，filter()根据判断结果自动过滤掉不符合条件的元素，返回由符合条件元素组成的新list。Pythondefis_odd(x):returnx%2==1filter(is_odd,[1,4,6,7,9,12,17])#[1,7,9,17]1234defis_odd(x):    returnx%2==1 filter(is_odd,[1,4,6,7,9,12,17])#[1,7,9,17]匿名函数和JS的匿名函数不同的地方是，Python的匿名函数中只能有一个表达式，且不能写return。拿map()函数为例：Pythonmap(lambdax:x*x,[1,2,3,4,5,6,7,8,9])#[1,4,9,16,25,36,49,64,81]1map(lambdax:x*x,[1,2,3,4,5,6,7,8,9])#[1,4,9,16,25,36,49,64,81]关键词lambda表示匿名函数，冒号前面的x表示函数参数，可以看出匿名函数lambdax:x*x实际上就是:Pythondeff(x):returnx*x12deff(x):    returnx*x闭包之前写过一些关于JS闭包的文章，比如深入浅出JavaScript之闭包（Closure）、以及读书笔记-你不知道的JavaScript(上)，Python中闭包的定义和JS中的是一致的即：内层函数引用了外层函数的变量，然后返回内层函数。下面来看下Py中闭包之for循环经典问题：Python#希望一次返回3个函数，分别计算1x1,2x2,3x3:defcount():fs=[]foriinrange(1,4):deff():returni*ifs.append(f)returnfsf1,f2,f3=count()#这种写法相当于ES6中的解构赋值printf1(),f2(),f3()#9991234567891011#希望一次返回3个函数，分别计算1x1,2x2,3x3:defcount():    fs=[]    foriinrange(1,4):        deff():            returni*i        fs.append(f)    returnfs f1,f2,f3=count()#这种写法相当于ES6中的解构赋值printf1(),f2(),f3()#999老问题了，f1(),f2(),f3()结果不应该是1,4,9吗，实际结果为什么都是9呢？原因就是当count()函数返回了3个函数时，这3个函数所引用的变量i的值已经变成了3。由于f1、f2、f3并没有被调用，所以，此时他们并未计算i*i，当f1被调用时，i已经变为3了。要正确使用闭包，就要确保引用的局部变量在函数返回后不能变。代码修改如下:方法一:可以理解为创建了一个封闭的作用域，i的值传给j之后，就和i没任何关系了。每次循环形成的闭包都存进了内存中。Pythondefcount():fs=[]foriinrange(1,4):deff(j):defg():#方法一returnj*jreturngr=f(i)fs.append(r)returnfsf1,f2,f3=count()printf1(),f2(),f3()#14912345678910111213defcount():    fs=[]    foriinrange(1,4):        deff(j):            defg():#方法一                returnj*j            returng        r=f(i)        fs.append(r)    returnfs f1,f2,f3=count()printf1(),f2(),f3()#149方法二：思路比较巧妙，用到了默认参数j在函数定义时可以获取到i的值，虽然没有用到闭包，但是和方法一有异曲同工之处。Pythondefcount():fs=[]foriinrange(1,4):deff(j=i):#方法二returnj*jfs.append(f)returnfsf1,f2,f3=count()printf1(),f2(),f3()#14912345678910defcount():    fs=[]    foriinrange(1,4):        deff(j=i):#方法二            returnj*j        fs.append(f)    returnfs f1,f2,f3=count()printf1(),f2(),f3()#149decorator装饰器ES6的语法中的decorator正是借鉴了Python的decorator。decorator本质上就是一个高阶函数，它接收一个函数作为参数，然后返回一个新函数。那装饰器的作用在哪呢？先上一段日常项目中用ts写的网关代码：Python@Post('/rider/detail')//URL路由@log()//打印日志@ResponseBodypublicasyncgetRiderBasicInfo(@RequestBody('riderId')riderId:number,@RequestBody('cityId')cityId:number,){constresult=awaitthis.riderManager.findDetail(cityId,riderId)returnresult}12345678910@Post('/rider/detail')  //URL路由@log()                  //打印日志  @ResponseBody  publicasyncgetRiderBasicInfo(    @RequestBody('riderId')riderId:number,    @RequestBody('cityId')cityId:number,  ){    constresult=awaitthis.riderManager.findDetail(cityId,riderId)    returnresult  }可以看出使用装饰器可以极大地简化代码，避免每个函数(比如日志、路由、性能检测)编写重复性代码。回到Python上，Python提供的@语法来使用decorator，@等价于f=decorate(f)。下面来看看@log()在Python中的实现:Python#我们想把调用的函数名字给打印出来@log()deffactorial(n):returnreduce(lambdax,y:x*y,range(1,n+1))printfactorial(10)#来看看@log()的定义deflog():deflog_decorator(f):deffn(x):print'调用了函数'+f.__name__+'()'returnf(x)returnfnreturnlog_decorator#结果#调用了函数factorial()#3628800123456789101112131415161718#我们想把调用的函数名字给打印出来@log()deffactorial(n):    returnreduce(lambdax,y:x*y,range(1,n+1))printfactorial(10) #来看看@log()的定义deflog():    deflog_decorator(f):        deffn(x):            print'调用了函数'+f.__name__+'()'            returnf(x)        returnfn    returnlog_decorator #结果#调用了函数factorial()#3628800class面向对象编程面向对象编程是一种程序设计范式，基本思想是：用类定义抽象类型，然后根据类的定义创建出实例。在掌握其它语言的基础上，还是比较容易理解这块知识点的，比如从下面两种写法可以看出不同语言的语言特性间竟然有如此多的共性。es6:(附：本文的主题是python，所以只是初略展示下js中类的定义以及实例的创建，为了说明写法的相似性)PythonclassPerson{constructor(name,age){this.name=namethis.age=age}}constchild1=newPerson('XiaoMing',10)12345678classPerson{    constructor(name,age){        this.name=name        this.age=age    }} constchild1=newPerson('XiaoMing',10)Python:(核心要点写在注释中)Python#定义一个Person类：根据Person类就可以造成很多child实例classPerson(object):address='Earth'#类属性(实例公有)def__init__(self,name,age):#创建实例时，__init__()方法被自动调用self.name=nameself.age=agedefget_age(self):#定义实例方法，它的第一个参数永远是self，指向调用该方法的实例本身，其他参数和普通函数是一样的returnself.agechild1=Person('XiaoMing',10)child2=Person('XiaoHong',9)printchild1.name#'XiaoMing'printchild2.get_age()#9printchild1.address#'Earth'printchild2.address#'Earth'12345678910111213141516#定义一个Person类：根据Person类就可以造成很多child实例classPerson(object):    address='Earth'#类属性(实例公有)    def__init__(self,name,age):#创建实例时，__init__()方法被自动调用        self.name=name        self.age=age    defget_age(self):#定义实例方法，它的第一个参数永远是self，指向调用该方法的实例本身，其他参数和普通函数是一样的        returnself.age child1=Person('XiaoMing',10)child2=Person('XiaoHong',9) printchild1.name#'XiaoMing'printchild2.get_age()#9printchild1.address#'Earth'printchild2.address#'Earth'继承child属于Student类，Student类属于People类，这就引出了继承:即获得了父类的方法属性后又能添加自己的方法属性。PythonclassPerson(object):def__init__(self,name,age):self.name=nameself.age=ageclassStudent(Person):def__init__(self,name,age,grade):super(Student,self).__init__(name,age)#这里也能写出Person.__init__(self,name,age)self.grade=grades=Student('XiaoMing',10,90)prints.name#'XiaoMing'prints.grade#9012345678910111213classPerson(object):    def__init__(self,name,age):        self.name=name        self.age=age classStudent(Person):    def__init__(self,name,age,grade):        super(Student,self).__init__(name,age)#这里也能写出Person.__init__(self,name,age)        self.grade=grade s=Student('XiaoMing',10,90)prints.name#'XiaoMing'prints.grade#90可以看到子类在父类的基础上又增加了grade属性。我们可以再来看看s的类型。Pythonisinstance(s,Person)isinstance(s,Student)12isinstance(s,Person)isinstance(s,Student)可以看出，Python中在一条继承链上，一个实例可以看成它本身的类型，也可以看成它父类的类型。1赞1收藏评论"], "art_create_time": ["2017/11/11"], "art_title": ["走近 Python (类比 JS )"], "art_url": ["http://python.jobbole.com/88850/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2017/11/c60f2447af38c6a464404d473257881e.jpg"]},
{"art_content": ["原文出处：三次方根   对于以Python作为技术栈的数据科学工作者，Jupyter是不得不提的数据报告工具。可能对于R社区而言，鼎鼎大名的ggplot2是常见的可视化框架，而大家对于Python，以及Jupyter为核心的交互式报告的可个视化方案就并没有那么熟悉。本文试图比较几个常用的解决方案，方便大家选择。选择标准称述式还是命令式数据工作者使用的图的类别，常见的就三类：GIS可视化、网络可视化和统计图。因此，大多数场景下，我们并不想接触非常底层的基于点、线、面的命令，所以，选择一个好的封装的框架相当重要。当然，公认较好的封装是基于《TheGrammarofGraphics(StatisticsandComputing)》一书，R中的ggplot2基本上就是一个很好的实现。我们基本上可以像用「自然语言」（NaturalLanguage）一样使用这些绘图命令。我们姑且采用计算机科学领域的「陈述式」来表达这种绘图方式。相反，有时候，以下情形时，我们可能对于这种绘图命令可能并不在意：出图相当简单，要求绘制速度，一般大的框架较重（当然只是相对而言）；想要对细节做非常详尽的微调，一般大框架在微调方面会相对复杂或者退缩成一句句命令；是统计作图可视化的创新者，想要尝试做出新的可视化实践。这些情况下，显然，简单操作式并提供底层绘制命令的框架更让人愉快，与上面类似，我们借用「命令式」描述这类框架。是否交互与传统的交付静态图标不同，基于Web端的Jupter的一大好处就是可以绘制交互的图标（最近的RNotebook也有实现），因此，是否选择交互式，也是一个需要权衡的地方。交互图的优势：可以提供更多的数据维度和信息；用户端可以做更多诸如放大、选取、转存的操作；可以交付BI工程师相应的JavaScript代码用以工程化；效果上比较炫酷，考虑到报告接受者的特征可以选择。非交互图的优势：报告文件直接导出成静态文件时相对问题，不会因为转换而损失信息；图片可以与报告分离，必要时作为其他工作的成果；不需要在运行Notebook时花很多世界载入各类前端框架。是非内核交互Jupyter上大多数命令通过以下方式获取数据，而大多数绘图方式事实上只是通过Notebook内的代码在Notebook与内核交互后展示出输出结果。但ipywidgets框架则可以实现CodeCell中的代码与Notebook中的前端控件（比如按钮等）绑定来进行操作内核，提供不同的绘图结果，甚至某些绘图框架的每个元素都可以直接和内核进行交互。 用这些框架，可以搭建更复杂的Notebook的可视化应用，但缺点是因为基于内核，所以在呈递、展示报告时如果使用离线文件时，这些交互就会无效。框架罗列matplotlib最家喻户晓的绘图框架是matplotlib，它提供了几乎所有python内静态绘图框架的底层命令。如果按照上面对可视化框架的分法，matplotlib属于非交互式的的「命令式」作图框架。Python##matplotlib代码示例frompylabimport*X=np.linspace(-np.pi,np.pi,256,endpoint=True)C,S=np.cos(X),np.sin(X)plot(X,C)plot(X,S)show()12345678910##matplotlib代码示例frompylabimport* X=np.linspace(-np.pi,np.pi,256,endpoint=True)C,S=np.cos(X),np.sin(X) plot(X,C)plot(X,S) show()优点是相对较快，底层操作较多。缺点是语言繁琐，内置默认风格不够美观。matplotlib在jupyter中需要一些配置，可以展现更好的效果，详情参见这篇文章.ggplot和plotnine值得一说，对于R迁移过来的人来说，ggplot和plotnine简直是福音，基本克隆了ggplot2所有语法。横向比较的话，plotnine的效果更好。这两个绘图包的底层依旧是matplotlib，因此，在引用时别忘了使用%matplotlibinline语句。值得一说的是plotnine也移植了ggplot2中良好的配置语法和逻辑。Python##plotnine示例(ggplot(mtcars,aes('wt','mpg',color='factor(gear)'))+geom_point()+stat_smooth(method='lm')+facet_wrap('~gear'))12345##plotnine示例(ggplot(mtcars,aes('wt','mpg',color='factor(gear)'))+geom_point()+stat_smooth(method='lm')+facet_wrap('~gear'))Seabornseaborn准确上说属于matplotlib的扩展包，在其上做了许多非常有用的封装，基本上可以满足大部分统计作图的需求，以matplotlib+seaborn基本可以满足大部分业务场景，语法也更加「陈述式」。缺点是封装较高，基本上API不提供的图就完全不可绘制，对于各类图的拼合也不适合；此外配置语句语法又回归「命令式」，相对复杂且不一致。Python##seaborn示例importseabornassns;sns.set(color_codes=True)iris=sns.load_dataset(\"iris\")species=iris.pop(\"species\")g=sns.clustermap(iris)12345##seaborn示例importseabornassns;sns.set(color_codes=True)iris=sns.load_dataset(\"iris\")species=iris.pop(\"species\")g=sns.clustermap(iris)plotlyplotly是跨平台JavaScript交互式绘图包，由于开发者的核心是javascript，所以整个语法类似于写json配置，语法特质也介于「陈述式」和「命令式」之间，无服务版本是免费的。有点是学习成本不高，可以很快将语句移植到javascript版本；缺点是语言相对繁琐。Python##plotly示例importplotly.plotlyaspyimportplotly.graph_objsasgo#Adddatamonth=['January','February','March','April','May','June','July','August','September','October','November','December']high_2000=[32.5,37.6,49.9,53.0,69.1,75.4,76.5,76.6,70.7,60.6,45.1,29.3]low_2000=[13.8,22.3,32.5,37.2,49.9,56.1,57.7,58.3,51.2,42.8,31.6,15.9]high_2007=[36.5,26.6,43.6,52.3,71.5,81.4,80.5,82.2,76.0,67.3,46.1,35.0]low_2007=[23.6,14.0,27.0,36.8,47.6,57.7,58.9,61.2,53.3,48.5,31.0,23.6]high_2014=[28.8,28.5,37.0,56.8,69.7,79.7,78.5,77.8,74.1,62.6,45.3,39.9]low_2014=[12.7,14.3,18.6,35.5,49.9,58.0,60.0,58.6,51.7,45.2,32.2,29.1]#Createandstyletracestrace0=go.Scatter(x=month,y=high_2014,name='High2014',line=dict(color=('rgb(205,12,24)'),width=4))trace1=go.Scatter(x=month,y=low_2014,name='Low2014',line=dict(color=('rgb(22,96,167)'),width=4,))trace2=go.Scatter(x=month,y=high_2007,name='High2007',line=dict(color=('rgb(205,12,24)'),width=4,dash='dash')#dashoptionsinclude'dash','dot',and'dashdot')trace3=go.Scatter(x=month,y=low_2007,name='Low2007',line=dict(color=('rgb(22,96,167)'),width=4,dash='dash'))trace4=go.Scatter(x=month,y=high_2000,name='High2000',line=dict(color=('rgb(205,12,24)'),width=4,dash='dot'))trace5=go.Scatter(x=month,y=low_2000,name='Low2000',line=dict(color=('rgb(22,96,167)'),width=4,dash='dot'))data=[trace0,trace1,trace2,trace3,trace4,trace5]#Editthelayoutlayout=dict(title='AverageHighandLowTemperaturesinNewYork',xaxis=dict(title='Month'),yaxis=dict(title='Temperature(degreesF)'),)fig=dict(data=data,layout=layout)py.iplot(fig,filename='styled-line')1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677##plotly示例importplotly.plotlyaspyimportplotly.graph_objsasgo #Adddatamonth=['January','February','March','April','May','June','July',        'August','September','October','November','December']high_2000=[32.5,37.6,49.9,53.0,69.1,75.4,76.5,76.6,70.7,60.6,45.1,29.3]low_2000=[13.8,22.3,32.5,37.2,49.9,56.1,57.7,58.3,51.2,42.8,31.6,15.9]high_2007=[36.5,26.6,43.6,52.3,71.5,81.4,80.5,82.2,76.0,67.3,46.1,35.0]low_2007=[23.6,14.0,27.0,36.8,47.6,57.7,58.9,61.2,53.3,48.5,31.0,23.6]high_2014=[28.8,28.5,37.0,56.8,69.7,79.7,78.5,77.8,74.1,62.6,45.3,39.9]low_2014=[12.7,14.3,18.6,35.5,49.9,58.0,60.0,58.6,51.7,45.2,32.2,29.1] #Createandstyletracestrace0=go.Scatter(    x=month,    y=high_2014,    name='High2014',    line=dict(        color=('rgb(205,12,24)'),        width=4))trace1=go.Scatter(    x=month,    y=low_2014,    name='Low2014',    line=dict(        color=('rgb(22,96,167)'),        width=4,))trace2=go.Scatter(    x=month,    y=high_2007,    name='High2007',    line=dict(        color=('rgb(205,12,24)'),        width=4,        dash='dash')#dashoptionsinclude'dash','dot',and'dashdot')trace3=go.Scatter(    x=month,    y=low_2007,    name='Low2007',    line=dict(        color=('rgb(22,96,167)'),        width=4,        dash='dash'))trace4=go.Scatter(    x=month,    y=high_2000,    name='High2000',    line=dict(        color=('rgb(205,12,24)'),        width=4,        dash='dot'))trace5=go.Scatter(    x=month,    y=low_2000,    name='Low2000',    line=dict(        color=('rgb(22,96,167)'),        width=4,        dash='dot'))data=[trace0,trace1,trace2,trace3,trace4,trace5] #Editthelayoutlayout=dict(title='AverageHighandLowTemperaturesinNewYork',              xaxis=dict(title='Month'),              yaxis=dict(title='Temperature(degreesF)'),              ) fig=dict(data=data,layout=layout)py.iplot(fig,filename='styled-line')注意：此框架在jupyter中使用需要使用init_notebook_mode()加载JavaScript框架。bokehbokeh是pydata维护的比较具有潜力的开源交互可视化框架。值得一说的是，该框架同时提供底层语句和「陈述式」绘图命令。相对来说语法也比较清楚，但其配置语句依旧有很多可视化框架的问题，就是与「陈述式」命令不符，没有合理的结构。此外，一些常见的交互效果都是以底层命令的方式使用的，因此如果要快速实现Dashboard或者作图时就显得较为不便了。Python##Bokeh示例importnumpyasnpimportscipy.specialfrombokeh.layoutsimportgridplotfrombokeh.plottingimportfigure,show,output_filep1=figure(title=\"NormalDistribution(μ=0,σ=0.5)\",tools=\"save\",background_fill_color=\"#E8DDCB\")mu,sigma=0,0.5measured=np.random.normal(mu,sigma,1000)hist,edges=np.histogram(measured,density=True,bins=50)x=np.linspace(-2,2,1000)pdf=1/(sigma*np.sqrt(2*np.pi))*np.exp(-(x-mu)**2/(2*sigma**2))cdf=(1+scipy.special.erf((x-mu)/np.sqrt(2*sigma**2)))/2p1.quad(top=hist,bottom=0,left=edges[:-1],right=edges[1:],fill_color=\"#036564\",line_color=\"#033649\")p1.line(x,pdf,line_color=\"#D95B43\",line_width=8,alpha=0.7,legend=\"PDF\")p1.line(x,cdf,line_color=\"white\",line_width=2,alpha=0.7,legend=\"CDF\")p1.legend.location=\"center_right\"p1.legend.background_fill_color=\"darkgrey\"p1.xaxis.axis_label='x'p1.yaxis.axis_label='Pr(x)'p2=figure(title=\"LogNormalDistribution(μ=0,σ=0.5)\",tools=\"save\",background_fill_color=\"#E8DDCB\")mu,sigma=0,0.5measured=np.random.lognormal(mu,sigma,1000)hist,edges=np.histogram(measured,density=True,bins=50)x=np.linspace(0.0001,8.0,1000)pdf=1/(x*sigma*np.sqrt(2*np.pi))*np.exp(-(np.log(x)-mu)**2/(2*sigma**2))cdf=(1+scipy.special.erf((np.log(x)-mu)/(np.sqrt(2)*sigma)))/2p2.quad(top=hist,bottom=0,left=edges[:-1],right=edges[1:],fill_color=\"#036564\",line_color=\"#033649\")p2.line(x,pdf,line_color=\"#D95B43\",line_width=8,alpha=0.7,legend=\"PDF\")p2.line(x,cdf,line_color=\"white\",line_width=2,alpha=0.7,legend=\"CDF\")p2.legend.location=\"center_right\"p2.legend.background_fill_color=\"darkgrey\"p2.xaxis.axis_label='x'p2.yaxis.axis_label='Pr(x)'p3=figure(title=\"GammaDistribution(k=1,θ=2)\",tools=\"save\",background_fill_color=\"#E8DDCB\")k,theta=1.0,2.0measured=np.random.gamma(k,theta,1000)hist,edges=np.histogram(measured,density=True,bins=50)x=np.linspace(0.0001,20.0,1000)pdf=x**(k-1)*np.exp(-x/theta)/(theta**k*scipy.special.gamma(k))cdf=scipy.special.gammainc(k,x/theta)/scipy.special.gamma(k)p3.quad(top=hist,bottom=0,left=edges[:-1],right=edges[1:],fill_color=\"#036564\",line_color=\"#033649\")p3.line(x,pdf,line_color=\"#D95B43\",line_width=8,alpha=0.7,legend=\"PDF\")p3.line(x,cdf,line_color=\"white\",line_width=2,alpha=0.7,legend=\"CDF\")p3.legend.location=\"center_right\"p3.legend.background_fill_color=\"darkgrey\"p3.xaxis.axis_label='x'p3.yaxis.axis_label='Pr(x)'p4=figure(title=\"WeibullDistribution(λ=1,k=1.25)\",tools=\"save\",background_fill_color=\"#E8DDCB\")lam,k=1,1.25measured=lam*(-np.log(np.random.uniform(0,1,1000)))**(1/k)hist,edges=np.histogram(measured,density=True,bins=50)x=np.linspace(0.0001,8,1000)pdf=(k/lam)*(x/lam)**(k-1)*np.exp(-(x/lam)**k)cdf=1-np.exp(-(x/lam)**k)p4.quad(top=hist,bottom=0,left=edges[:-1],right=edges[1:],fill_color=\"#036564\",line_color=\"#033649\")p4.line(x,pdf,line_color=\"#D95B43\",line_width=8,alpha=0.7,legend=\"PDF\")p4.line(x,cdf,line_color=\"white\",line_width=2,alpha=0.7,legend=\"CDF\")p4.legend.location=\"center_right\"p4.legend.background_fill_color=\"darkgrey\"p4.xaxis.axis_label='x'p4.yaxis.axis_label='Pr(x)'output_file('histogram.html',title=\"histogram.pyexample\")show(gridplot(p1,p2,p3,p4,ncols=2,plot_width=400,plot_height=400,toolbar_location=None))123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106##Bokeh示例importnumpyasnpimportscipy.special frombokeh.layoutsimportgridplotfrombokeh.plottingimportfigure,show,output_file p1=figure(title=\"NormalDistribution(μ=0,σ=0.5)\",tools=\"save\",            background_fill_color=\"#E8DDCB\") mu,sigma=0,0.5 measured=np.random.normal(mu,sigma,1000)hist,edges=np.histogram(measured,density=True,bins=50) x=np.linspace(-2,2,1000)pdf=1/(sigma*np.sqrt(2*np.pi))*np.exp(-(x-mu)**2/(2*sigma**2))cdf=(1+scipy.special.erf((x-mu)/np.sqrt(2*sigma**2)))/2 p1.quad(top=hist,bottom=0,left=edges[:-1],right=edges[1:],        fill_color=\"#036564\",line_color=\"#033649\")p1.line(x,pdf,line_color=\"#D95B43\",line_width=8,alpha=0.7,legend=\"PDF\")p1.line(x,cdf,line_color=\"white\",line_width=2,alpha=0.7,legend=\"CDF\") p1.legend.location=\"center_right\"p1.legend.background_fill_color=\"darkgrey\"p1.xaxis.axis_label='x'p1.yaxis.axis_label='Pr(x)'   p2=figure(title=\"LogNormalDistribution(μ=0,σ=0.5)\",tools=\"save\",            background_fill_color=\"#E8DDCB\") mu,sigma=0,0.5 measured=np.random.lognormal(mu,sigma,1000)hist,edges=np.histogram(measured,density=True,bins=50) x=np.linspace(0.0001,8.0,1000)pdf=1/(x*sigma*np.sqrt(2*np.pi))*np.exp(-(np.log(x)-mu)**2/(2*sigma**2))cdf=(1+scipy.special.erf((np.log(x)-mu)/(np.sqrt(2)*sigma)))/2 p2.quad(top=hist,bottom=0,left=edges[:-1],right=edges[1:],        fill_color=\"#036564\",line_color=\"#033649\")p2.line(x,pdf,line_color=\"#D95B43\",line_width=8,alpha=0.7,legend=\"PDF\")p2.line(x,cdf,line_color=\"white\",line_width=2,alpha=0.7,legend=\"CDF\") p2.legend.location=\"center_right\"p2.legend.background_fill_color=\"darkgrey\"p2.xaxis.axis_label='x'p2.yaxis.axis_label='Pr(x)'   p3=figure(title=\"GammaDistribution(k=1,θ=2)\",tools=\"save\",            background_fill_color=\"#E8DDCB\") k,theta=1.0,2.0 measured=np.random.gamma(k,theta,1000)hist,edges=np.histogram(measured,density=True,bins=50) x=np.linspace(0.0001,20.0,1000)pdf=x**(k-1)*np.exp(-x/theta)/(theta**k*scipy.special.gamma(k))cdf=scipy.special.gammainc(k,x/theta)/scipy.special.gamma(k) p3.quad(top=hist,bottom=0,left=edges[:-1],right=edges[1:],        fill_color=\"#036564\",line_color=\"#033649\")p3.line(x,pdf,line_color=\"#D95B43\",line_width=8,alpha=0.7,legend=\"PDF\")p3.line(x,cdf,line_color=\"white\",line_width=2,alpha=0.7,legend=\"CDF\") p3.legend.location=\"center_right\"p3.legend.background_fill_color=\"darkgrey\"p3.xaxis.axis_label='x'p3.yaxis.axis_label='Pr(x)'   p4=figure(title=\"WeibullDistribution(λ=1,k=1.25)\",tools=\"save\",            background_fill_color=\"#E8DDCB\") lam,k=1,1.25 measured=lam*(-np.log(np.random.uniform(0,1,1000)))**(1/k)hist,edges=np.histogram(measured,density=True,bins=50) x=np.linspace(0.0001,8,1000)pdf=(k/lam)*(x/lam)**(k-1)*np.exp(-(x/lam)**k)cdf=1-np.exp(-(x/lam)**k) p4.quad(top=hist,bottom=0,left=edges[:-1],right=edges[1:],      fill_color=\"#036564\",line_color=\"#033649\")p4.line(x,pdf,line_color=\"#D95B43\",line_width=8,alpha=0.7,legend=\"PDF\")p4.line(x,cdf,line_color=\"white\",line_width=2,alpha=0.7,legend=\"CDF\") p4.legend.location=\"center_right\"p4.legend.background_fill_color=\"darkgrey\"p4.xaxis.axis_label='x'p4.yaxis.axis_label='Pr(x)'   output_file('histogram.html',title=\"histogram.pyexample\") show(gridplot(p1,p2,p3,p4,ncols=2,plot_width=400,plot_height=400,toolbar_location=None))bqplotbqplot是基于ipywidgets和d3.js组合发展的内核交互式的可视化框架。语法上采用了和matplotlib大致一致的语法已经相对封装较高的「陈述式语法」。优点是直接和内核交互，可以使用大量控件来实现更多的图像处理，缺点也是直接的，离线文档则不会显示任何图案、控件也都失效。Python##bqplot示例importnumpyasnpfromIPython.displayimportdisplayfrombqplotimport(OrdinalScale,LinearScale,Bars,Lines,Axis,Figure)size=20np.random.seed(0)x_data=np.arange(size)x_ord=OrdinalScale()y_sc=LinearScale()bar=Bars(x=x_data,y=np.random.randn(2,size),scales={'x':x_ord,'y':y_sc},type='stacked')line=Lines(x=x_data,y=np.random.randn(size),scales={'x':x_ord,'y':y_sc},stroke_width=3,colors=['red'],display_legend=True,labels=['Linechart'])ax_x=Axis(scale=x_ord,grid_lines='solid',label='X')ax_y=Axis(scale=y_sc,orientation='vertical',tick_format='0.2f',grid_lines='solid',label='Y')Figure(marks=[bar,line],axes=[ax_x,ax_y],title='APIExample',legend_location='bottom-right')1234567891011121314151617181920212223242526##bqplot示例importnumpyasnpfromIPython.displayimportdisplayfrombqplotimport(    OrdinalScale,LinearScale,Bars,Lines,Axis,Figure) size=20np.random.seed(0) x_data=np.arange(size) x_ord=OrdinalScale()y_sc=LinearScale() bar=Bars(x=x_data,y=np.random.randn(2,size),scales={'x':x_ord,'y':y_sc},type='stacked')line=Lines(x=x_data,y=np.random.randn(size),scales={'x':x_ord,'y':y_sc},            stroke_width=3,colors=['red'],display_legend=True,labels=['Linechart']) ax_x=Axis(scale=x_ord,grid_lines='solid',label='X')ax_y=Axis(scale=y_sc,orientation='vertical',tick_format='0.2f',            grid_lines='solid',label='Y') Figure(marks=[bar,line],axes=[ax_x,ax_y],title='APIExample',      legend_location='bottom-right')其他特殊需求的作图除了统计作图，网络可视化和GIS可视化也是很常用的，在此只做一个简单的罗列：GIS类：gmap：交互，使用googlemaps接口ipyleaflet：交互，使用leaflet接口网络类：networkx：底层为matplotlibplotly总结底层实现交互方式语法语言结构备注推荐程度matplotlib–无命令式底层语言可以实现复杂底层操作★★★gglotmatplotlib无陈述式类ggplot2建议选择plotnine★★plotninematplotlib无陈述式类ggplot2完全移植ggplot2★★★★★seabornmatplotlib无陈述式高级语言有很多有用的统计图类的封装；但不适合做图拼装★★★★★plotlyplotly.js前端交互介于命令式和陈述式之间类似JavaScript语法类似于json配置★★★★bokeh–前端交互命令、陈述式同时有底层语言和高级语言社区具有潜力★★★bqplotd3.js内核交互命令、陈述式有类似matplotlib底层语言，已经封装好的高级语言内核交互★★★★1赞5收藏2评论"], "art_create_time": ["2017/11/15"], "art_title": ["Jupyter 常见可视化框架选择"], "art_url": ["http://python.jobbole.com/88862/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2017/11/cf07224a24a4f2e382a74152a82bbce6.png"]},
{"art_content": ["本文作者：伯乐在线-翱翔的翱。未经作者许可，禁止转载！欢迎加入伯乐在线专栏作者。一、概述不知怎么回事，提到决策树我就想起”道生一，一生二，二生三，三生万物“这句话，大概是因为决策树从一个根节点慢慢“长”成一棵树，也要经历“一生二，二生三”的过程。决策树本质上就是一种二叉树，根据特定的标准不停的分成左右两个子树，直到符合某种条件停止。树算法解释性强、简单直观以及接近人的决策方式使它成为流行的机器学习算法之一。当决策树与装袋法(Bag)、提升法(Boosting)结合之后，可以成为更强大的算法。决策树按响应值的类型大致分为分类树和回归树，实现决策树的方法也很多，比如CART、ID3、C4.5等等，本文将对CART这种算法进行介绍。二、算法一棵树要长成要解决两方面的问题，一是如何分，二是何时停。这两点对于分类和回归略有区别，先说如何分，对于定量变量一般是将小于某个值的数据划分为左子树，大于等于某个值的划分为右子树；对于定性变量一般是将等于某个值的划分为左子树，不等于某个值的划分为右子树。那么什么才是一个好的划分呢？分类树大致分为两种，一种是按纯度(Purity)，纯度是通过基尼系数(GiniIndex)进行定义的，基尼系数越小，纯度越大，那么划分效果越好。基尼系数的计算方法如下式所示： G=∑k=1Kp^k(1−p^k)p^k代表第k类所占比例，当p^k接近0或1时，基尼系数会很小。另一种标准是互熵(Cross-entropy)，互熵的定义如下： D=−∑k=1Kp^klogp^k由定义可以看到，和基尼系数类似，当p^k接近0或1时，互熵也很小，划分的效果也越好。回归树则根据残差平方和RSS： RSS=1N∑i=1N(yi−y¯)2y¯代表平均响应值，可以看到这实际上就是方差，我们都知道方差是衡量数据变异性的量，因此RSS越小表示回归模型效果越好。注意上面的纯度、互熵以及残差平方和均是树的一个分枝上的值，总的值要对左右分枝进行加权平均，例如基尼系数的最终值应该这样计算， Gtotal=NleftNGleft+NrightNGrightN表示总的样本数，Nleft，Nright分别代表左分枝和右分枝的样本数，互熵和残差平方和的计算方式类似。说了如何分，那什么时候停呢？一般的惯例是子树中的预测变量或响应值都一样了就可以停止分裂了。有时候这个条件可能有些苛刻，这时候可以设置一个NodeSize值，表示叶子节点包含的最小的样本数。分裂过程中如果一个子树的样本数小于等于这个值就停止分裂，分类数取数目最多的那个类，回归树取响应的均值。说了这么多，下面举个例子，来演示下决策树算法，比如这里有一份城市和农村儿童身高数据，注意这里的数据都是我杜撰的，只是为了演示决策树的算法。如果已知一个儿童身高和性别，如何判断所处的区域？身高性别地区100男城市90女城市90男农村80女农村下面尝试根据基尼系数来构造一个分类树，第一次分裂：身高身高身高性别=男：2/4x(1/2x1/2+1/2x1/2)+2/4x(1/2x1/2+1/2x1/2)=1/2性别=女：2/4x(1/2x1/2+1/2x1/2)+2/4x(1/2x1/2+1/2x1/2)=1/2可以看到前面两个都是1/3，选择哪一个都行，这里我选择第一个最小值：“身高左子树身高性别地区90女城市90男农村80女农村右子树身高性别地区100男城市第二次分裂：由于右面的子树只有一条数据，因此只需计算左边子树的基尼系数，身高身高性别=女：2/3x(1/2x1/2+1/2x1/2)+1/3x(1x0)=1/3性别=男：1/3x(1x0)+2/3x(1/2x1/2+1/2x1/2)=1/3同上选择第一个最低值“身高左子树身高性别地区80女农村右子树身高性别地区90女城市90男农村第三次分裂：同理，左边子树只有一条数据，只需计算右子树身高性别=女：1/2x(1x0)+1/2x(1x0)=0性别=男：1/2x(1x0)+1/2x(1x0)=0选择“性别=女”这个条件，至此所有的子树的响应值都是唯一的，停止分裂。最终这个分类树的样子大概如下，三、树的剪枝其实树的剪枝就是正则化，剪枝一般分为两种：一种称为预剪枝，通过设置NodeSize的大小来达到控制树的分枝个数的目的，这种方式简单易用，但有短视的风险；另一种称为后剪枝，原理是让树充分“生长”，然后尝试合并树的分枝，通过对比合并前后错误率是否降低来决定是否真得合并，这种方式效果较前一种好，但是实现稍微复杂一些。四、说了就练俗话说，光说不练假把式，下面我用R语言实现一个决策树，并尝试分析两个实际的数据集。1、鸢尾花(iris)数据集，这个数据集包括五个变量：花萼长度(Sepal.Length)，花萼宽度(Sepal.Width)，花瓣长度(Petal.Length)，花瓣宽度(Petal.Width)，种类(Species)，下面尝试使用花萼长度(Sepal.Length)和花萼宽度(Sepal.Width)这两个变量来预测鸢尾花的种类(Species)。为了简便，我采用的是预剪枝的方式。那么选择多大的NodeSize合适呢？关于这个问题通常的方法就是交叉验证(Cross-validation)。下图是采用10折交叉验证(k-foldcross-validation)得到的错误率,可以看到，当NodeSize为40的时候测试集的错误率Eout最低，从另一个方面也可以看到如果不进行剪枝，Eout约为0.4，比剪枝后的错误率高了将近0.2。从下面的第一张图也可以直观的看到当NodeSize从小到大增加时，分类边界(DecisionBoundary)从过拟合(Overfit)到欠拟合(Underfit)的变化趋势。第二张图是根据交叉验证得到的最佳分类边界，它和NodeSize为30的分类边界非常相似。最终的错误率约为0.2，从上面第二张图可以看到versicolor和virginica这两类的鸢尾花有些数据在二维空间完全重合在了一起，仅仅依靠花萼长度(Sepal.Length)，花萼宽度(Sepal.Width)这两个变量是无法把它们分开的，这个时候单纯的增加样本数无法进一步提高模型的质量，这个时候最好去寻找新的变量，事实上，当加上花瓣长度(Petal.Length)，花瓣宽度(Petal.Width)这两个变量时，预测的错误率可以降低到0.06左右。树的样子如下，[L]和[R]分别代表左右分枝。2、上面是个分类问题，那么再看一个回归问题。北京二手房这个数据集有13个特征，下面使用决策树根据房子的区域(area)、是否学区(school)、是否有地铁(subway)、总价(num)这四个变量来预测房价(price)。同样，祭出我们的法宝交叉验证得到一个合适的NodeSize，如下所示，对于回归,我采用了决策系数R2作为衡量模型效果的标准，由于R2是越大越好，且0得到的决策系数R2约为0.7，也就是区域、是否学区、是否有地铁、总价这四个变量解释了70%房价变异。由这个相对误差图可以看出大部分的数据都落在了0附近，实际上有20275条数据落在[-0.2,0.2]，28379条数据落在[-0.5,0.5]。那么，那些误差比较大的都是些什么数据呢？Python下面的数据为相对误差大于3的arearegionzonemetersdirectionconflooryearschoolsubwaytaxnumprice海淀东小营甲1号5室2厅350南西北旺二手房低楼层1998无学区无地铁非免税2808000朝阳北苑家园望春园1室0厅36南北北苑二手房地下室2008无学区无地铁非免税205556昌平香堂文化新村二期5室4厅460南北昌平其它二手房低楼层2010无学区无地铁非免税2204783昌平东亚上北中心1室0厅738北回龙观二手房地下室2007无学区无地铁非免税3705012123456下面的数据为相对误差大于3的arearegionzonemetersdirectionconflooryearschoolsubwaytaxnumprice海淀东小营甲1号5室2厅350南西北旺二手房低楼层1998无学区无地铁非免税280  8000朝阳北苑家园望春园1室0厅36南北北苑二手房地下室2008无学区无地铁非免税  20  5556昌平香堂文化新村二期5室4厅460南北昌平其它二手房低楼层2010无学区无地铁非免税2204783昌平东亚上北中心1室0厅738北回龙观二手房地下室2007无学区无地铁非免税370  5012感觉这些数据好像异常数据，北京还有低于1万的房价？！五、总结当一个小小的种子慢慢成长为一颗参天大树，独霸森林一方，常常让人感受生命的强大，而决策树算法同样让人惊叹，易于实现又足够灵活，既能用于分类又能用于回归，也在机器学习领域赢得了一席之地。本文简单介绍了决策树的算法和剪枝，在此基础上用R实现了一个决策树，并在两个数据集上进行了测验，证实了决策树的能力。打赏支持我写出更多好文章，谢谢！打赏作者打赏支持我写出更多好文章，谢谢！任选一种支付方式1赞3收藏2评论关于作者：翱翔的翱Javaisagoodboy!个人主页·我的文章·6·"], "art_create_time": ["2017/11/16"], "art_title": ["三生万物：决策树"], "art_url": ["http://python.jobbole.com/88858/"], "art_img": ["http://img.blog.csdn.net/20171111215334992"]},
{"art_content": ["本文由伯乐在线-伯乐在线读者翻译。未经许可，禁止转载！英文出处：preshing。欢迎加入翻译组。这里是显示彭罗斯点阵的Python的脚本。是的，这是可以运行的有效Phython代码。译注：彭罗斯点阵，物理学术语。上世纪70年代英国数学家彭罗斯第一次提出了这个概念，称为彭罗斯点阵(Pen-rosetiles)。Python_=\\\"\"\"if!1:\"e,V=1000,(0j-1)**-.2;v,S=.5/V.real,[(0,0,4*e,4*e*V)];w=1-v\"def!E(T,A,B,C):P,Q,R=B*w+A*v,B*w+C*v,A*w+B*v;return[(1,Q,C,A),(1,P,Q,B),(0,Q,P,A)]*T+[(0,C,R,B),(1,R,C,A)]*(1-T)\"for!i!in!_[:11]:S=sum([E(*x)for!x!in!S],[])\"import!cairo!as!O;s=O.ImageSurface(1,e,e);c=O.Context(s);M,L,G=c.move_to,c.line_to,c.set_source_rgba\"def!z(f,a):f(-a.imag,a.real-e-e)\"for!T,A,B,C!in[i!for!i!in!S!if!i[\"\"\";exec(reduce(lambdax,i:x.replace(chr(i),\"\\n\"[34-i:]),range(35),_+\"\"\"0]]:z(M,A);z(L,B);z(L,C);c.close_path()\"G(.4,.3,1);c.paint();G(.7,.7,1);c.fill()\"for!i!in!range(9):\"!g=1-i/8;d=i/4*g;G(d,d,d,1-g*.8)\"!def!y(f,a):z(f,a+(1+2j)*(1j**(i/2.))*g)\"!for!T,A,B,C!in!S:y(M,C);y(L,A);y(M,A);y(L,B)\"!c.stroke()\"s.write_to_png('penrose.png')\"\"\"))123456789101112131415161718192021222324252627282930_                                =\\                                \"\"\"if!                              1:\"e,V=100                            0,(0j-1)**-.2;                          v,S=.5/  V.real,                        [(0,0,4      *e,4*e*                      V)];w=1          -v\"def!                      E(T,A,              B,C):P                  ,Q,R=B*w+                A*v,B*w+C            *v,A*w+B*v;retur              n[(1,Q,C,A),(1,P    ,Q,B),(0,Q,P,A)]*T+[(0,C            ,R,B),(1,R,C,A)]*(1-T)\"for!i!in!_[:11]:S      =sum([E          (*x)for      !x!in!S],[])\"imp  ort!cair              o!as!O;      s=O.Ima              geSurfac  e(1,e,e)              ;c=O.Con  text(s);              M,L,G=c.    move_to                ,c.line_to,c.s                et_sour      ce_rgb                a\"def!z(f,a)                :f(-a.        imag,a.      real-e-e)\"for!T,A,B,C!in[i      !for!i!          in!S!if!i[\"\"\";exec(reduce(lambdax,i:x.replace(chr          (i),\"\\n\"[34-i:]),  range(  35),_+\"\"\"0]]:z(M,A            );z(L,B);z        (L,C);        c.close_pa            th()\"G            (.4,.3            ,1);c.            paint(            );G(.7            ,.7,1)            ;c.fil            l()\"fo            r!i!in            !range            (9):\"!            g=1-i/            8;d=i/          4*g;G(d,d,d,          1-g*.8            )\"!def    !y(f,a):z(f,a+(1+2j)*(    1j**(i            /2.))*g)\"!for!T,A,B,C!in!S:y(M,C);y(L,A);y(M            ,A);y(L,B)\"!c.st            roke()\"s.write_t            o_png('pen                        rose.png')            \"\"\"                                      ))当这个程序运行时，它输出了一个1000×1000的图像文件，包含大约2212个由3D立体效应渲染的彭罗斯点阵。这里是该图像的一部分（点击放大）。运行该脚本需要Pycairo。它只在Python它是标准的Python脚本，但我努力想把它变得更简洁，于是我又从中删减了一些。编注：Pycairo是一组Python版本的Cario图形库。彭罗斯点阵很酷，因为它们非周期性地覆盖了整个平面——图片的转换副本与原型从来不会一致。它们是由RogerPenrose先生通过将五边形的平面平铺在一起的一系列尝试而发明的。与C或Perl相比，Python并不是让人迷惑的编程语言。这种比较似乎也从未发生，而且在网上也没有多少让人费解的Python的例子：你可以在官方的Python常见问题中或各种网页如这里和这里找到一些例子。在2011年的PyCon对此还有专题讨论。我相信输出一个高分辨率的图像是第一个让人费解的Python程序。如果你知道其它的例子，可以在评论中告诉我。 翻译：伯乐在线 –张秀君1赞收藏评论关于作者：伯乐在线读者①本账号用于发布那些在伯乐在线无账号的读者的投稿，包括译文和原创文章。②欢迎加入伯乐在线专栏作者：http://blog.jobbole.com/99322/个人主页·我的文章·33"], "art_create_time": ["2011/09/13"], "art_title": ["谁说不能用Python写出让人迷惑的代码？"], "art_url": ["http://python.jobbole.com/1414/"], "art_img": ["http://ww1.sinaimg.cn/mw690/005N3SJDgw1eknq3byyroj30er0crgrb.jpg"]},
{"art_content": ["英文出处：MattDeboard，译文出处：36KR我的故事在海军陆战队服役超过10年后，我于去年7月份退役了。随后在8月份找到了一份州彩票（statelottery）做公关的工作，到今年2月中旬的时候又被辞退了。到5月中旬的时候我在DE协会找到了一份临时的“初级用户体验工程师”工作，而到了8月底我则成了正式的“用户体验工程师”。当我丢掉州彩票的那份工作时，我就在想公关这行可能真的不适合我。我想做一名程序员。于是我开始节衣缩食学习编程。家人对我的情况非常担心。从2月份到5月份的那段时间，我几乎只要是没睡着就是在学习编程，学习Linux以及计算机科学。我自学Python，自学Django。我学了一些函数式编程和命令式编程。对Linux命令行有了一个不错的了解。我没有做的有人问我：“你如何在11周内学会了Django？”事实上，我并没有针对Django本身去学。而是在为了使得用Python编写应用更加容易的情况下学到的。也就是说完全出于偶然。因此我不想被称作一名“Django开发者”。换句话说，如果我过去是在花大量的时间去专门学Django本身，而不是去学如何使用Django工作，我可能远没有现在的编程能力。以下是好奇的朋友们给我的一些问题以及我的回答你是以网络资源开始的还是以书本资源开始的？我都有用。这包括Djangoproject，StackOverflow和MIT关于计算机科学的开放课。你的每天或者是每周时间是怎么安排的？有时每天8个小时，有时12个小时，有时16个小时，总之每天都要花很多时间。因为没工作，又是单身父亲，所以会特别有意志特别有激情。这样的一个境况使得我能十倍努力的工作，不玩游戏，不看电视，甚至整天不睡觉，完全沉浸在代码，编程里。你有指导老师吗？是的我有。他是一个非常聪明而且成功的人，几乎在我成为程序员路上的每一个方面都给了我指导。这其中包括很多非常具体的编程知识（比如Python&Django），还有职业建议等。之前有过什么特别的经历对于你现在自学成为程序员有什么帮助的吗？没有很多特殊的经历。我很早之前对电脑有过狂热的爱好，学过一点QBasic&VisualBasic，后来又断断续续的弄过一点Python，但大部分时间都没怎么弄。除此之外没有其他的了。你怎样选择学习Django的？这个很简单。因为我想要模仿的一个人就是通过Django取得了很大的成功。可以分享一下你的学习过程吗？我想重申一下我并不是一个自学天才也没有什么很特殊的天分。我只是很努力的学习罢了，因为我穷困潦倒而且没有其他选择。我几乎消耗了所以可以帮助我达到现在这个地步的资源——一个既可以让我赚钱又喜欢的工作。这就是我的学习过程。想了解一下你具体学习Django的过程，或者给我一些建议或推荐一些学习资源（HTML/CSS，JavaScript）？事实上，我唯一的建议就是动手去做。我真的花了很多时间学习，而且我也享受学习的过程。正如我在上面已经说过，我没有刻意去学Django，RubyonRails或者Noir。我想帮助我成功的一个重要因素是学习语言以及其背后的工作思想，然后再通过一个网络框架去更好的学习那门语言。因此对网络框架的学习都是我在学习编程语言中偶然所得的。我建议想学的朋友去看看irc.freenode.net，去读读Django文档并不懂就问。我就是这样做的，而且效果也很不错。不过我并不是完全坐着读文档，大部分的时间我都会自己做一些东西以更好的理解背后的工作思想。我个人是一个动手学习者，有些人可能不是，但是动手帮助我获得了成功。而你可以选择更适合你自己的方式。你是如何向公司展示你自己的技能的？是给他们看你的项目了吗？Github，Github还是Github。我觉得强调的再多有不过分，做一些东西，放到Github上去，让人们知道你很富有激情也很聪明好学。另外还有网络。参加一些行业活动，发微博，写博客，和你周围的圈子进行互动。在我看到我现在这份工作的前一周，我就曾在一个论坛上发布过一个简短的演讲，这也使得有些人注意到了我。结论如果要我概括我的整个学习过程的话，我想以下几点值得一说：1.问问题，有好奇心，富于热情2.学习一门语言，而不是一个网络开发框架3.努力学习4.构建一个网络，参加行业活动，写博客等，告诉人们你是一个值得共事的人5.（选择性的）将你自己放到一个没有退路的地方，破釜沉舟最后我想说的就是我觉得自己非常幸运。我现在还算不上一个很棒的开发者，而且我的职业也才刚刚开始。但是我很高兴通过自己的努力改变了我的生活轨迹。我也希望我的经历可以帮到你们。后记：有人可能会对文中提到的神秘老师感兴趣，作者Matt并未在文中说明他是如何遇到这位老师的，不过在读者评论下面他给出了回复。Matt是通过经常逛这位前辈的网络论坛认识他的。后来随着逐渐的熟悉便开始寻求他的帮助指导，最后也才有了Matt今天的程序员之路。1赞1收藏1评论"], "art_create_time": ["2011/11/25"], "art_title": ["我是如何在12周内成为一名程序员的"], "art_url": ["http://python.jobbole.com/8464/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2011/11/How-I-Became-a-Programmer.jpeg"]},
{"art_content": ["如果你是个学生，你应该会C，C++和Java。还会一些VB，或C#/.NET。多少你还可能开发过一些Web网页，你知道一些HTML，CSS和JavaScript知识。总体上说，我们很难发现会有学生显露出掌握超出这几种语言范围外的语言的才能。这真让人遗憾，因为还有很多种编程语言，它们能让你成为一个更好的程序员。在这篇文章里，我将会告诉你，为什么你一定要学习Python或Ruby语言。跟C/C++/Java相比—Python/Ruby能让你用少的多的多的代码写出相同的程序。有人计算过，Python或Ruby写出的程序的代码行数只相当于相对应的Java代码的行数的五分之一。如果没有绝对的必要，为什么要花这么多时间写出这么多的代码呢？而且有人说，一个优秀的程序员能维护的代码量最多是2万行。这不区分用的语言究竟是汇编，C还是Python/Ruby/PHP/Lisp。所以，如果你用Python/Ruby写，你一个人干的，不管是干什么，如果换用Java/C/C++，那都需要一个5人的小团队来干。跟VB/PHP比较—跟PHP/VB相比，Python/Ruby的是一种从设计上讲比它们好的不知多少倍的语言。PHP和VB分别是在开发网站和桌面应用程序上非常流行的语言。它们流行的原因是非常的易学。不懂计算机的人也很容易的上手。如果你用这些语言开发过大型的项目，你就会发现这些语言的设计是如此的糟糕。是朋友，他就不会劝你使用PHP/VB。跟Lisp/Scala/Haskell/Closure/Erlang相比—Python/Ruby跟它们比起来显得相当的“主流”。确实，这些语言每种都有其很酷的特征，对于高级编程人员，了解这些语言能给他们对编程的思考带来实际的提升。但这些应该在你以后的职业生涯中才去决定学哪一两种。对于现在，Python/Ruby是在语言功能和实际运用之间平衡后的更好的选择。跟Perl相比—Python和Ruby都受恩于Perl，在这两种语言异军突起前，Perl是最好、最大的一种动态语言。但现在，Perl已是昨日黄花，越来越多的人转向Ruby/Python。我感觉Perl的面向对象机制有点做作，很不好用。通常认为，Perl一种比较难学的语言，因为它提供你了太多不同的方法去完成同一个任务，它的语法有点像密码，非常不直观—除非你对它掌握的非常好。总之，我感觉Perl是一种对于学生来说不是很合适的语言—除非你有特殊的理由去学它(例如，你有很多正则表达式要处理，这是Perl的闪光点)。跟sh/sed/awk/bash相比—如果你使用Linux/Unix，你可能需要做一些shell编程，甚至会编写一些不小的程序。但是，对于这些语言，一旦程序达到一定的行数，事情就会开始变得让你痛苦不堪，你最好是用Python去做这些事情。当然，做这种事情，Perl是最好的选择，Python排第二。(Ruby对于系统shell脚本不是很合适)。你可以在Google上搜一下“为什么X比Y好”—其中把X换成Python或Ruby，把Y换成另外一种语言—你就会发现，有无数的文章来说明它们为什么这么好。如果你有选择你的毕业设计使用的编程语言的自由，你应该选择Python或Ruby，它们能让你在开发项目的过程中节省一半的时间(除非你要开发的是移动应用，这样你必须要使用Java或Objective-C)。下面是xkcd上的一幅漫画，告诉你掌握Python后你会变得多么的强大：如何去学它们呢？很多很多的网站上都提供了学习Python和Ruby的教材和课程。下面的是我从中选出的一些：谷歌的Python课程，学习Python的好资源。RubyLearning，学习Ruby的一个好网站。有疑问吗？请在评论了写出来，我会尽量回答你们。尾注：1：我的这篇文章可能会让很多Perl爱好者很郁闷，现在回味一下，我认识到对这种语言的要求过于苛刻了。因此，我把关于Perl的一节改写了一下。Python和Ruby都受恩于Perl，在这两种语言出现之前，Perl是最大、最好的动态语言。但Perl现在太老了。它的面向对象性不完整。它很久没有升级更新了，它的市场份额正在丢失。对于一些新的、很火的事物(例如Web编程框架，WebAPI)，它不如Python&Ruby那样能跟上时代的步伐。基本上，Python/Ruby在兴起，Perl在衰退。2：本文中的所有语言的比较都是用来给印度计算机科学专业的学生选编程语言时做参考的。像“X比Y好”这样的句子准确的讲是毫无意义的，因为所有的语言都是经过时间的考验而存活下来的，有些语言会在某些领域比另外一种要强，这也是它们存活下来的原因。换句话说，总有一些情况下，PHP/Java/C/C++/Perl看起来会比Ruby/Python等其它语言显的更适合。译文：外刊IT评论　　原文：reliscore1赞收藏3评论"], "art_create_time": ["2011/07/25"], "art_title": ["每个程序员都应该学习使用Python或Ruby"], "art_url": ["http://python.jobbole.com/1141/"], "art_img": ["/wp-content/uploads/vb/1141-thumb_python1.png"]},
{"art_content": ["原文出处：j_hao104   本文简要介绍Python自然语言处理(NLP)，使用Python的NLTK库。NLTK是Python的自然语言处理工具包，在NLP领域中，最常使用的一个Python库。什么是NLP？简单来说，自然语言处理(NLP)就是开发能够理解人类语言的应用程序或服务。这里讨论一些自然语言处理(NLP)的实际应用例子，如语音识别、语音翻译、理解完整的句子、理解匹配词的同义词，以及生成语法正确完整句子和段落。这并不是NLP能做的所有事情。NLP实现搜索引擎:比如谷歌，Yahoo等。谷歌搜索引擎知道你是一个技术人员，所以它显示与技术相关的结果；社交网站推送:比如FacebookNewsFeed。如果NewsFeed算法知道你的兴趣是自然语言处理，就会显示相关的广告和帖子。语音引擎:比如Apple的Siri。垃圾邮件过滤:如谷歌垃圾邮件过滤器。和普通垃圾邮件过滤不同，它通过了解邮件内容里面的的深层意义，来判断是不是垃圾邮件。NLP库下面是一些开源的自然语言处理库(NLP)：Naturallanguagetoolkit(NLTK);ApacheOpenNLP;StanfordNLPsuite;GateNLPlibrary其中自然语言工具包(NLTK)是最受欢迎的自然语言处理库(NLP)，它是用Python编写的，而且背后有非常强大的社区支持。NLTK也很容易上手，实际上，它是最简单的自然语言处理(NLP)库。在这个NLP教程中，我们将使用PythonNLTK库。安装NLTK如果您使用的是Windows/Linux/Mac，您可以使用pip安装NLTK:Pythonpipinstallnltk1pipinstallnltk打开python终端导入NLTK检查NLTK是否正确安装：Pythonimportnltk1importnltk如果一切顺利，这意味着您已经成功地安装了NLTK库。首次安装了NLTK，需要通过运行以下代码来安装NLTK扩展包:Pythonimportnltknltk.download()123importnltk nltk.download()这将弹出NLTK下载窗口来选择需要安装哪些包:您可以安装所有的包，因为它们的大小都很小，所以没有什么问题。使用PythonTokenize文本首先，我们将抓取一个web页面内容，然后分析文本了解页面的内容。我们将使用urllib模块来抓取web页面:Pythonimporturllib.requestresponse=urllib.request.urlopen('http://php.net/')html=response.read()print(html)12345importurllib.request response=urllib.request.urlopen('http://php.net/')html=response.read()print(html)从打印结果中可以看到，结果包含许多需要清理的HTML标签。然后BeautifulSoup模块来清洗这样的文字:Pythonfrombs4importBeautifulSoupimporturllib.requestresponse=urllib.request.urlopen('http://php.net/')html=response.read()soup=BeautifulSoup(html,\"html5lib\")#这需要安装html5lib模块text=soup.get_text(strip=True)print(text)123456789frombs4importBeautifulSoup importurllib.requestresponse=urllib.request.urlopen('http://php.net/')html=response.read()soup=BeautifulSoup(html,\"html5lib\")#这需要安装html5lib模块text=soup.get_text(strip=True)print(text)现在我们从抓取的网页中得到了一个干净的文本。下一步，将文本转换为tokens,像这样:Pythonfrombs4importBeautifulSoupimporturllib.requestresponse=urllib.request.urlopen('http://php.net/')html=response.read()soup=BeautifulSoup(html,\"html5lib\")text=soup.get_text(strip=True)tokens=text.split()print(tokens)123456789frombs4importBeautifulSoupimporturllib.request response=urllib.request.urlopen('http://php.net/')html=response.read()soup=BeautifulSoup(html,\"html5lib\")text=soup.get_text(strip=True)tokens=text.split()print(tokens)统计词频text已经处理完毕了，现在使用PythonNLTK统计token的频率分布。可以通过调用NLTK中的FreqDist()方法实现:Pythonfrombs4importBeautifulSoupimporturllib.requestimportnltkresponse=urllib.request.urlopen('http://php.net/')html=response.read()soup=BeautifulSoup(html,\"html5lib\")text=soup.get_text(strip=True)tokens=text.split()freq=nltk.FreqDist(tokens)forkey,valinfreq.items():print(str(key)+':'+str(val))123456789101112frombs4importBeautifulSoupimporturllib.requestimportnltk response=urllib.request.urlopen('http://php.net/')html=response.read()soup=BeautifulSoup(html,\"html5lib\")text=soup.get_text(strip=True)tokens=text.split()freq=nltk.FreqDist(tokens)forkey,valinfreq.items():    print(str(key)+':'+str(val))如果搜索输出结果，可以发现最常见的token是PHP。您可以调用plot函数做出频率分布图:Pythonfreq.plot(20,cumulative=False)#需要安装matplotlib库12freq.plot(20,cumulative=False)#需要安装matplotlib库这上面这些单词。比如of,a,an等等，这些词都属于停用词。一般来说，停用词应该删除，防止它们影响分析结果。处理停用词NLTK自带了许多种语言的停用词列表，如果你获取英文停用词:Pythonfromnltk.corpusimportstopwordsstopwords.words('english')123fromnltk.corpusimportstopwords stopwords.words('english')现在，修改下代码,在绘图之前清除一些无效的token:Pythonclean_tokens=list()sr=stopwords.words('english')fortokenintokens:iftokennotinsr:clean_tokens.append(token)12345clean_tokens=list()sr=stopwords.words('english')fortokenintokens:    iftokennotinsr:        clean_tokens.append(token)最终的代码应该是这样的:Pythonfrombs4importBeautifulSoupimporturllib.requestimportnltkfromnltk.corpusimportstopwordsresponse=urllib.request.urlopen('http://php.net/')html=response.read()soup=BeautifulSoup(html,\"html5lib\")text=soup.get_text(strip=True)tokens=text.split()clean_tokens=list()sr=stopwords.words('english')fortokenintokens:ifnottokeninsr:clean_tokens.append(token)freq=nltk.FreqDist(clean_tokens)forkey,valinfreq.items():print(str(key)+':'+str(val))123456789101112131415161718frombs4importBeautifulSoupimporturllib.requestimportnltkfromnltk.corpusimportstopwords response=urllib.request.urlopen('http://php.net/')html=response.read()soup=BeautifulSoup(html,\"html5lib\")text=soup.get_text(strip=True)tokens=text.split()clean_tokens=list()sr=stopwords.words('english')fortokenintokens:    ifnottokeninsr:        clean_tokens.append(token)freq=nltk.FreqDist(clean_tokens)forkey,valinfreq.items():    print(str(key)+':'+str(val))现在再做一次词频统计图，效果会比之前好些，因为剔除了停用词:Pythonfreq.plot(20,cumulative=False)1freq.plot(20,cumulative=False)使用NLTKTokenize文本在之前我们用split方法将文本分割成tokens，现在我们使用NLTK来Tokenize文本。文本没有Tokenize之前是无法处理的，所以对文本进行Tokenize非常重要的。token化过程意味着将大的部件分割为小部件。你可以将段落tokenize成句子，将句子tokenize成单个词，NLTK分别提供了句子tokenizer和单词tokenizer。假如有这样这段文本:PythonHelloAdam,howareyou?Ihopeeverythingisgoingwell.Todayisagoodday,seeyoudude.1HelloAdam,howareyou?Ihopeeverythingisgoingwell.Todayisagoodday,seeyoudude.使用句子tokenizer将文本tokenize成句子:Pythonfromnltk.tokenizeimportsent_tokenizemytext=\"HelloAdam,howareyou?Ihopeeverythingisgoingwell.Todayisagoodday,seeyoudude.\"print(sent_tokenize(mytext))1234fromnltk.tokenizeimportsent_tokenize mytext=\"HelloAdam,howareyou?Ihopeeverythingisgoingwell.Todayisagoodday,seeyoudude.\"print(sent_tokenize(mytext))输出如下:Python['HelloAdam,howareyou?','Ihopeeverythingisgoingwell.','Todayisagoodday,seeyoudude.']1['HelloAdam,howareyou?','Ihopeeverythingisgoingwell.','Todayisagoodday,seeyoudude.']这是你可能会想，这也太简单了，不需要使用NLTK的tokenizer都可以，直接使用正则表达式来拆分句子就行，因为每个句子都有标点和空格。那么再来看下面的文本:PythonHelloMr.Adam,howareyou?Ihopeeverythingisgoingwell.Todayisagoodday,seeyoudude.1HelloMr.Adam,howareyou?Ihopeeverythingisgoingwell.Todayisagoodday,seeyoudude.这样如果使用标点符号拆分,HelloMr将会被认为是一个句子，如果使用NLTK:Pythonfromnltk.tokenizeimportsent_tokenizemytext=\"HelloMr.Adam,howareyou?Ihopeeverythingisgoingwell.Todayisagoodday,seeyoudude.\"print(sent_tokenize(mytext))1234fromnltk.tokenizeimportsent_tokenize mytext=\"HelloMr.Adam,howareyou?Ihopeeverythingisgoingwell.Todayisagoodday,seeyoudude.\"print(sent_tokenize(mytext))输出如下:Python['HelloMr.Adam,howareyou?','Ihopeeverythingisgoingwell.','Todayisagoodday,seeyoudude.']1['HelloMr.Adam,howareyou?','Ihopeeverythingisgoingwell.','Todayisagoodday,seeyoudude.']这才是正确的拆分。接下来试试单词tokenizer:Pythonfromnltk.tokenizeimportword_tokenizemytext=\"HelloMr.Adam,howareyou?Ihopeeverythingisgoingwell.Todayisagoodday,seeyoudude.\"print(word_tokenize(mytext))1234fromnltk.tokenizeimportword_tokenize mytext=\"HelloMr.Adam,howareyou?Ihopeeverythingisgoingwell.Todayisagoodday,seeyoudude.\"print(word_tokenize(mytext))输出如下:Python['Hello','Mr.','Adam',',','how','are','you','?','I','hope','everything','is','going','well','.','Today','is','a','good','day',',','see','you','dude','.']1['Hello','Mr.','Adam',',','how','are','you','?','I','hope','everything','is','going','well','.','Today','is','a','good','day',',','see','you','dude','.']Mr.这个词也没有被分开。NLTK使用的是punkt模块的PunktSentenceTokenizer，它是NLTK.tokenize的一部分。而且这个tokenizer经过训练，可以适用于多种语言。非英文TokenizeTokenize时可以指定语言:Pythonfromnltk.tokenizeimportsent_tokenizemytext=\"BonjourM.Adam,commentallez-vous?J'espèrequetoutvabien.Aujourd'huiestunbonjour.\"print(sent_tokenize(mytext,\"french\"))1234fromnltk.tokenizeimportsent_tokenize mytext=\"BonjourM.Adam,commentallez-vous?J'espèrequetoutvabien.Aujourd'huiestunbonjour.\"print(sent_tokenize(mytext,\"french\"))输出结果如下:Python['BonjourM.Adam,commentallez-vous?',\"J'espèrequetoutvabien.\",\"Aujourd'huiestunbonjour.\"]1['BonjourM.Adam,commentallez-vous?',\"J'espèrequetoutvabien.\",\"Aujourd'huiestunbonjour.\"]同义词处理使用nltk.download()安装界面，其中一个包是WordNet。WordNet是一个为自然语言处理而建立的数据库。它包括一些同义词组和一些简短的定义。您可以这样获取某个给定单词的定义和示例:Pythonfromnltk.corpusimportwordnetsyn=wordnet.synsets(\"pain\")print(syn[0].definition())print(syn[0].examples())12345fromnltk.corpusimportwordnet syn=wordnet.synsets(\"pain\")print(syn[0].definition())print(syn[0].examples())输出结果是:Pythonasymptomofsomephysicalhurtordisorder['thepatientdevelopedseverepainanddistension']12asymptomofsomephysicalhurtordisorder['thepatientdevelopedseverepainanddistension']WordNet包含了很多定义：Pythonfromnltk.corpusimportwordnetsyn=wordnet.synsets(\"NLP\")print(syn[0].definition())syn=wordnet.synsets(\"Python\")print(syn[0].definition())123456fromnltk.corpusimportwordnet syn=wordnet.synsets(\"NLP\")print(syn[0].definition())syn=wordnet.synsets(\"Python\")print(syn[0].definition())结果如下:PythonthebranchofinformationsciencethatdealswithnaturallanguageinformationlargeOldWorldboas12thebranchofinformationsciencethatdealswithnaturallanguageinformationlargeOldWorldboas可以像这样使用WordNet来获取同义词:Pythonfromnltk.corpusimportwordnetsynonyms=[]forsyninwordnet.synsets('Computer'):forlemmainsyn.lemmas():synonyms.append(lemma.name())print(synonyms)1234567fromnltk.corpusimportwordnet synonyms=[]forsyninwordnet.synsets('Computer'):    forlemmainsyn.lemmas():        synonyms.append(lemma.name())print(synonyms)输出:Python['computer','computing_machine','computing_device','data_processor','electronic_computer','information_processing_system','calculator','reckoner','figurer','estimator','computer']1['computer','computing_machine','computing_device','data_processor','electronic_computer','information_processing_system','calculator','reckoner','figurer','estimator','computer']反义词处理也可以用同样的方法得到反义词：Pythonfromnltk.corpusimportwordnetantonyms=[]forsyninwordnet.synsets(\"small\"):forlinsyn.lemmas():ifl.antonyms():antonyms.append(l.antonyms()[0].name())print(antonyms)12345678fromnltk.corpusimportwordnet antonyms=[]forsyninwordnet.synsets(\"small\"):    forlinsyn.lemmas():        ifl.antonyms():            antonyms.append(l.antonyms()[0].name())print(antonyms)输出:Python['large','big','big']1['large','big','big']词干提取语言形态学和信息检索里，词干提取是去除词缀得到词根的过程，例如working的词干为work。搜索引擎在索引页面时就会使用这种技术，所以很多人为相同的单词写出不同的版本。有很多种算法可以避免这种情况，最常见的是波特词干算法。NLTK有一个名为PorterStemmer的类，就是这个算法的实现:Pythonfromnltk.stemimportPorterStemmerstemmer=PorterStemmer()print(stemmer.stem('working'))print(stemmer.stem('worked'))12345fromnltk.stemimportPorterStemmer stemmer=PorterStemmer()print(stemmer.stem('working'))print(stemmer.stem('worked'))输出结果是:Pythonworkwork12workwork还有其他的一些词干提取算法，比如Lancaster词干算法。非英文词干提取除了英文之外，SnowballStemmer还支持13种语言。支持的语言:Pythonfromnltk.stemimportSnowballStemmerprint(SnowballStemmer.languages)123fromnltk.stemimportSnowballStemmer print(SnowballStemmer.languages)Python'danish','dutch','english','finnish','french','german','hungarian','italian','norwegian','porter','portuguese','romanian','russian','spanish','swedish'1'danish','dutch','english','finnish','french','german','hungarian','italian','norwegian','porter','portuguese','romanian','russian','spanish','swedish'你可以使用SnowballStemmer类的stem函数来提取像这样的非英文单词：Pythonfromnltk.stemimportSnowballStemmerfrench_stemmer=SnowballStemmer('french')print(french_stemmer.stem(\"Frenchword\"))12345fromnltk.stemimportSnowballStemmer french_stemmer=SnowballStemmer('french') print(french_stemmer.stem(\"Frenchword\"))单词变体还原单词变体还原类似于词干，但不同的是，变体还原的结果是一个真实的单词。不同于词干，当你试图提取某些词时，它会产生类似的词:Pythonfromnltk.stemimportPorterStemmerstemmer=PorterStemmer()print(stemmer.stem('increases'))12345fromnltk.stemimportPorterStemmer stemmer=PorterStemmer() print(stemmer.stem('increases'))结果:Pythonincreas1increas现在，如果用NLTK的WordNet来对同一个单词进行变体还原，才是正确的结果:Pythonfromnltk.stemimportWordNetLemmatizerlemmatizer=WordNetLemmatizer()print(lemmatizer.lemmatize('increases'))12345fromnltk.stemimportWordNetLemmatizer lemmatizer=WordNetLemmatizer() print(lemmatizer.lemmatize('increases'))结果:Pythonincrease1increase结果可能会是一个同义词或同一个意思的不同单词。有时候将一个单词做变体还原时，总是得到相同的词。这是因为语言的默认部分是名词。要得到动词，可以这样指定：Pythonfromnltk.stemimportWordNetLemmatizerlemmatizer=WordNetLemmatizer()print(lemmatizer.lemmatize('playing',pos=\"v\"))12345fromnltk.stemimportWordNetLemmatizer lemmatizer=WordNetLemmatizer() print(lemmatizer.lemmatize('playing',pos=\"v\"))结果:Pythonplay1play实际上，这也是一种很好的文本压缩方式，最终得到文本只有原先的50%到60%。结果还可以是动词(v)、名词(n)、形容词(a)或副词(r)：Pythonfromnltk.stemimportWordNetLemmatizerlemmatizer=WordNetLemmatizer()print(lemmatizer.lemmatize('playing',pos=\"v\"))print(lemmatizer.lemmatize('playing',pos=\"n\"))print(lemmatizer.lemmatize('playing',pos=\"a\"))print(lemmatizer.lemmatize('playing',pos=\"r\"))1234567fromnltk.stemimportWordNetLemmatizer lemmatizer=WordNetLemmatizer()print(lemmatizer.lemmatize('playing',pos=\"v\"))print(lemmatizer.lemmatize('playing',pos=\"n\"))print(lemmatizer.lemmatize('playing',pos=\"a\"))print(lemmatizer.lemmatize('playing',pos=\"r\"))输出:Pythonplayplayingplayingplaying1234playplayingplayingplaying词干和变体的区别通过下面例子来观察:Pythonfromnltk.stemimportWordNetLemmatizerfromnltk.stemimportPorterStemmerstemmer=PorterStemmer()lemmatizer=WordNetLemmatizer()print(stemmer.stem('stones'))print(stemmer.stem('speaking'))print(stemmer.stem('bedroom'))print(stemmer.stem('jokes'))print(stemmer.stem('lisa'))print(stemmer.stem('purple'))print('----------------------')print(lemmatizer.lemmatize('stones'))print(lemmatizer.lemmatize('speaking'))print(lemmatizer.lemmatize('bedroom'))print(lemmatizer.lemmatize('jokes'))print(lemmatizer.lemmatize('lisa'))print(lemmatizer.lemmatize('purple'))123456789101112131415161718fromnltk.stemimportWordNetLemmatizerfromnltk.stemimportPorterStemmer stemmer=PorterStemmer()lemmatizer=WordNetLemmatizer()print(stemmer.stem('stones'))print(stemmer.stem('speaking'))print(stemmer.stem('bedroom'))print(stemmer.stem('jokes'))print(stemmer.stem('lisa'))print(stemmer.stem('purple'))print('----------------------')print(lemmatizer.lemmatize('stones'))print(lemmatizer.lemmatize('speaking'))print(lemmatizer.lemmatize('bedroom'))print(lemmatizer.lemmatize('jokes'))print(lemmatizer.lemmatize('lisa'))print(lemmatizer.lemmatize('purple'))输出:Pythonstonespeakbedroomjokelisapurpl---------------------stonespeakingbedroomjokelisapurple12345678910111213stonespeakbedroomjokelisapurpl---------------------stonespeakingbedroomjokelisapurple词干提取不会考虑语境，这也是为什么词干提取比变体还原快且准确度低的原因。个人认为，变体还原比词干提取更好。单词变体还原返回一个真实的单词，即使它不是同一个单词，也是同义词，但至少它是一个真实存在的单词。如果你只关心速度，不在意准确度，这时你可以选用词干提取。在此NLP教程中讨论的所有步骤都只是文本预处理。在以后的文章中，将会使用PythonNLTK来实现文本分析。我已经尽量使文章通俗易懂。希望能对你有所帮助。1赞6收藏6评论"], "art_create_time": ["2017/11/19"], "art_title": ["Python NLP入门教程"], "art_url": ["http://python.jobbole.com/88874/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2017/11/eefc6b90a73fee96a70b63f207f9a302.png"]},
{"art_content": ["原文出处：弄浪的鱼   Python真火来学习一下，先来看一个库NumPy。NumPy是Python语言的一个扩充程序库。支持高级大量的维度数组与矩阵运算，此外也针对数组运算提供大量的数学函数库。1.读取文件numpy.genfromtxt()用于读取txt文件，其中传入的参数依次为：需要读取的txt文件位置，此处文件与程序位于同一目录下分割的标记转换类型，如果文件中既有文本类型也有数字类型，就先转成文本类型help(numpy.genfromtxt)用于查看帮助文档：如果不想看API可以启动一个程序用help查看指令的详细用法Pythonimportnumpyworld_alcohol=numpy.genfromtxt(\"world_alcohol.txt\",delimiter=\",\",dtype=str)print(type(world_alcohol))print(world_alcohol)print(help(numpy.genfromtxt))123456importnumpy world_alcohol=numpy.genfromtxt(\"world_alcohol.txt\",delimiter=\",\",dtype=str)print(type(world_alcohol))print(world_alcohol)print(help(numpy.genfromtxt))2.构造ndarraynumpy.array()构造ndarraynumpy.array()中传入数组参数，可以是一维的也可以是二维三维的。numpy会将其转变成ndarray的结构。Pythonvector=numpy.array([1,2,3,4])matrix=numpy.array([[1,2,3],[4,5,6]])12vector=numpy.array([1,2,3,4])matrix=numpy.array([[1,2,3],[4,5,6]])传入的参数必须是同一结构,不是同一结构将发生转换。Pythonvector=numpy.array([1,2,3,4])array([1,2,3,4])123vector=numpy.array([1,2,3,4]) array([1,2,3,4])均为int类型Pythonvector=numpy.array([1,2,3,4.0])array([1.,2.,3.,4.])123vector=numpy.array([1,2,3,4.0]) array([1.,  2.,  3.,  4.])转为浮点数类型Pythonvector=numpy.array([1,2,'3',4])array(['1','2','3','4'],dtype='<U21')123vector=numpy.array([1,2,'3',4]) array(['1','2','3','4'],dtype='<U21')转为字符类型利用.shape查看结构能够了解array的结构，debug时通过查看结构能够更好地了解程序运行的过程。Pythonprint(vector.shape)print(matrix.shape)(4,)(2,3)1234print(vector.shape)print(matrix.shape)(4,)(2,3)利用dtype查看类型Pythonvector=numpy.array([1,2,3,4])vector.dtypedtype('int64')1234vector=numpy.array([1,2,3,4])vector.dtype dtype('int64')ndim查看维度一维Pythonvector=numpy.array([1,2,3,4])vector.ndim11234vector=numpy.array([1,2,3,4])vector.ndim 1二维Pythonmatrix=numpy.array([[1,2,3],[4,5,6],[7,8,9]])matrix.ndim2123456matrix=numpy.array([[1,2,3],                      [4,5,6],                    [7,8,9]])matrix.ndim 2size查看元素数量Pythonmatrix.size912matrix.size93.获取与计算numpy能使用切片获取数据Pythonmatrix=numpy.array([[1,2,3],[4,5,6],[7,8,9]])123matrix=numpy.array([[1,2,3],                      [4,5,6],                    [7,8,9]])根据条件获取numpy能够依次比较vector和元素之间是否相同Pythonvector=numpy.array([5,10,15,20])vector==10array([False,True,False,False],dtype=bool)1234vector=numpy.array([5,10,15,20])vector==10 array([False,  True,False,False],dtype=bool)根据返回值获取元素Pythonvector=numpy.array([5,10,15,20])equal_to_ten=(vector==10)print(equal_to_ten)print(vector[equal_to_ten])[FalseTrueFalseFalse][10]1234567vector=numpy.array([5,10,15,20])equal_to_ten=(vector==10)print(equal_to_ten)print(vector[equal_to_ten]) [False  TrueFalseFalse][10]进行运算之后获取Pythonvector=numpy.array([5,10,15,20])equal_to_ten_and_five=(vector==10)&(vector==5)12vector=numpy.array([5,10,15,20])equal_to_ten_and_five=(vector==10)&(vector==5)Pythonvector=numpy.array([5,10,15,20])equal_to_ten_or_five=(vector==10)|(vector==5)12vector=numpy.array([5,10,15,20])equal_to_ten_or_five=(vector==10)|(vector==5)类型转换将整体类型进行转换Pythonvector=numpy.array([5,10,15,20])print(vector.dtype)vector=vector.astype(str)print(vector.dtype)int64<U211234567vector=numpy.array([5,10,15,20])print(vector.dtype)vector=vector.astype(str)print(vector.dtype) int64<U21求和sum()能够对ndarray进行各种求和操作，比如分别按行按列进行求和Pythonmatrix=numpy.array([[1,2,3],[4,5,6],[7,8,9]])print(matrix.sum())print(matrix.sum(1))print(matrix.sum(0))45[61524][121518]12345678910matrix=numpy.array([[1,2,3],                      [4,5,6],                    [7,8,9]])print(matrix.sum())print(matrix.sum(1))print(matrix.sum(0)) 45[61524][121518]sum(1)是sum(axis=1))的缩写，1表示按照x轴方向求和，0表示按照y轴方向求和4.常用函数reshape生成从0-14的15个数字，使用reshape(3,5)将其构造成一个三行五列的array。Pythonimportnumpyasnparr=np.arange(15).reshape(3,5)arrarray([[0,1,2,3,4],[5,6,7,8,9],[10,11,12,13,14]])1234567importnumpyasnparr=np.arange(15).reshape(3,5)arr array([[0,  1,  2,  3,  4],      [5,  6,  7,  8,  9],      [10,11,12,13,14]])zeros生成指定结构的默认为0.的arrayPythonnp.zeros((3,4))array([[0.,0.,0.,0.],[0.,0.,0.,0.],[0.,0.,0.,0.]])12345np.zeros((3,4)) array([[0.,  0.,  0.,  0.],      [0.,  0.,  0.,  0.],      [0.,  0.,  0.,  0.]])ones生成一个三维的array,通过dtype指定类型Pythonnp.ones((2,3,4),dtype=np.int32)array([[[1,1,1,1],[1,1,1,1],[1,1,1,1]],[[1,1,1,1],[1,1,1,1],[1,1,1,1]]])123456789np.ones((2,3,4),dtype=np.int32) array([[[1,1,1,1],        [1,1,1,1],        [1,1,1,1]],       [[1,1,1,1],        [1,1,1,1],        [1,1,1,1]]])range指定范围和数值间的间隔生成array，注意范围包左不包右Pythonnp.arange(0,10,2)array([0,2,4,6,8])123np.arange(0,10,2) array([0,2,4,6,8])random随机数生成指定结构的随机数，可以用于生成随机权重Pythonnp.random.random((2,3))array([[0.86166627,0.37756207,0.94265883],[0.9768257,0.96915312,0.33495431]])1234np.random.random((2,3)) array([[0.86166627,  0.37756207,  0.94265883],      [0.9768257,  0.96915312,  0.33495431]])5.ndarray运算元素之间依次相减相减Pythona=np.array([10,20,30,40])b=np.array(4)a-barray([6,16,26,36])12345a=np.array([10,20,30,40])b=np.array(4) a-barray([6,16,26,36])乘方Pythona**2array([100,400,900,1600])12a**2array([100,  400,  900,1600])开根号Pythonnp.sqrt(B)array([[1.41421356,0.],[1.73205081,2.]])1234np.sqrt(B) array([[1.41421356,  0.        ],      [1.73205081,  2.        ]])e求方Pythonnp.exp(B)array([[7.3890561,1.],[20.08553692,54.59815003]])1234np.exp(B) array([[  7.3890561,  1.        ],      [20.08553692,  54.59815003]])向下取整Pythona=np.floor(10*np.random.random((2,2)))aarray([[0.,0.],[3.,6.]])12345a=np.floor(10*np.random.random((2,2)))a array([[0.,  0.],      [3.,  6.]])行列变换Pythona.Tarray([[0.,3.],[0.,6.]])1234a.T array([[0.,  3.],      [0.,  6.]])变换结构Pythona.resize(1,4)aarray([[0.,0.,3.,6.]])1234a.resize(1,4)a array([[0.,  0.,  3.,  6.]])6.矩阵运算矩阵之间的运算PythonA=np.array([[1,1],[0,1]])B=np.array([[2,0],[3,4]])1234A=np.array([[1,1],              [0,1]])B=np.array([[2,0],              [3,4]])对应位置一次相乘PythonA*Barray([[2,0],[0,4]])1234A*B array([[2,0],      [0,4]])矩阵乘法Pythonprint(A.dot(B))print(np.dot(A,B))[[54][34]]12345print(A.dot(B))print(np.dot(A,B)) [[54][34]]横向相加Pythona=np.floor(10*np.random.random((2,2)))b=np.floor(10*np.random.random((2,2)))print(a)print(b)print(np.hstack((a,b)))[[2.3.][9.3.]][[8.1.][0.0.]][[2.3.8.1.][9.3.0.0.]]12345678910111213a=np.floor(10*np.random.random((2,2)))b=np.floor(10*np.random.random((2,2))) print(a)print(b)print(np.hstack((a,b))) [[2.  3.][9.  3.]][[8.  1.][0.  0.]][[2.  3.  8.  1.][9.  3.  0.  0.]]纵向相加Pythonprint(np.vstack((a,b)))[[2.3.][9.3.][8.1.][0.0.]]123456print(np.vstack((a,b))) [[2.  3.][9.  3.][8.  1.][0.  0.]]矩阵分割Python#横向分割print(np.hsplit(a,3))#纵向风格print(np.vsplit(a,3))1234#横向分割print(np.hsplit(a,3))#纵向风格print(np.vsplit(a,3))7.复制的区别地址复制通过b=a复制a的值，b与a指向同一地址，改变b同时也改变a。Pythona=np.arange(12)b=aprint(aisb)print(a.shape)print(b.shape)b.shape=(3,4)print(a.shape)print(b.shape)True(12,)(12,)(3,4)(3,4)123456789101112131415a=np.arange(12)b=aprint(aisb) print(a.shape)print(b.shape)b.shape=(3,4)print(a.shape)print(b.shape) True(12,)(12,)(3,4)(3,4)复制值通过a.view()仅复制值，当对c值进行改变会改变a的对应的值，而改变c的shape不改变a的shapePythona=np.arange(12)c=a.view()print(cisa)c.shape=2,6c[0,0]=9999print(a)print(c)False[99991234567891011][[999912345][67891011]]1234567891011121314a=np.arange(12)c=a.view()print(cisa) c.shape=2,6c[0,0]=9999 print(a)print(c) False[9999    1    2    3    4    5    6    7    8    9  10  11][[9999    1    2    3    4    5][  6    7    8    9  10  11]]完整拷贝a.copy()进行的完整的拷贝，产生一份完全相同的独立的复制Pythona=np.arange(12)c=a.copy()print(cisa)c.shape=2,6c[0,0]=9999print(a)print(c)False[01234567891011][[999912345][67891011]]1234567891011121314a=np.arange(12)c=a.copy()print(cisa) c.shape=2,6c[0,0]=9999 print(a)print(c) False[0  1  2  3  4  5  6  7  8  91011][[9999    1    2    3    4    5][  6    7    8    9  10  11]]1赞4收藏评论"], "art_create_time": ["2017/11/15"], "art_title": ["Numpy 小结"], "art_url": ["http://python.jobbole.com/88860/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2014/12/6da94dec8f6f96417f14c8291e6345801.png"]},
{"art_content": ["原文出处：南山younger   一、深拷贝与浅拷贝深拷贝：赋值时值完全复制，完全的copy，对其中一个作出改变，不会影响另一个浅拷贝：赋值时，引用赋值，相当于取了一个别名。对其中一个修改，会影响另一个对于PHP而言，=赋值时，普通对象是深拷贝，但对对象来说，是浅拷贝，即引用赋值。当对象作为参数传递时，无论参数前是否有&引用符号，都将被看做是赋值引用。对于python而言，情况可能会有点小复杂，因为python一切皆为对象，所以python的普通赋值、深拷贝和浅拷贝之间都是有细微区别的。二、php下的他们在php5中，对象的=赋值和传递都是引用。要想实现拷贝副本，php提供了clone函数实现。clone完全copy了一份副本。但是clone时，我们可能不希望copy源对象的所有内容，那我们可以利用__clone来操作。请看如下代码段：Python<?php//普通对象赋值，深拷贝，完全值复制$m=1;$n=$m;$n=2;echo$m;//值复制，对新对象的改变不会对m作出改变，输出1.深拷贝echoPHP_EOL;/*==================*///对象赋值，浅拷贝，引用赋值classTest{public$a=1;}$m=newTest();$n=$m;//引用赋值$m->a=2;//修改m，n也随之改变echo$n->a;//输出2，浅拷贝echoPHP_EOL;?>12345678910111213141516171819<?php//普通对象赋值，深拷贝，完全值复制$m=1;$n=$m;$n=2;echo$m;//值复制，对新对象的改变不会对m作出改变，输出1.深拷贝echoPHP_EOL;/*==================*///对象赋值，浅拷贝，引用赋值classTest{    public$a=1;}$m=newTest();$n=$m;//引用赋值$m->a=2;//修改m，n也随之改变echo$n->a;//输出2，浅拷贝echoPHP_EOL;?>由于对象的赋值时引用，要想实现值复制，php提供了clone函数来实现复制对象。但是clone函数存在这么一个问题，克隆对象时，原对象的普通属性能值复制，但是源对象的对象属性赋值时还是引用赋值，浅拷贝。Python<?phpclassTest{public$a=1;}classTestOne{public$b=1;public$obj;//包含了一个对象属性，clone时，它会是浅拷贝publicfunction__construct(){$this->obj=newTest();}}$m=newTestOne();$n=$m;//这是完全的浅拷贝，无论普通属性还是对象属性$p=clone$m;//普通属性实现了深拷贝，改变普通属性b，不会对源对象有影响$p->b=2;echo$m->b;//输出原来的1echoPHP_EOL;//对象属性是浅拷贝，改变对象属性中的a，源对象m中的对象属性中a也改变$p->obj->a=3;echo$m->obj->a;//输出3，随新对象改变?>12345678910111213141516171819202122232425262728<?phpclassTest{    public$a=1;}classTestOne{    public$b=1;    public$obj;    //包含了一个对象属性，clone时，它会是浅拷贝    publicfunction__construct(){        $this->obj=newTest();    }}$m=newTestOne();$n=$m;//这是完全的浅拷贝，无论普通属性还是对象属性$p=clone$m;//普通属性实现了深拷贝，改变普通属性b，不会对源对象有影响$p->b=2;echo$m->b;//输出原来的1echoPHP_EOL;//对象属性是浅拷贝，改变对象属性中的a，源对象m中的对象属性中a也改变$p->obj->a=3;echo$m->obj->a;//输出3，随新对象改变?>要想实现对象真正的深拷贝，有以下两种方法：1、利用序列化反序列化实现Python<?phpclassTest{public$a=1;}classTestOne{public$b=1;public$obj;//包含了一个对象属性，clone时，它会是浅拷贝publicfunction__construct(){$this->obj=newTest();}}$m=newTestOne();//方法二，序列化反序列化实现对象深拷贝$n=serialize($m);$n=unserialize($n);$n->b=2;echo$m->b;//输出原来的1echoPHP_EOL;//可以看到，普通属性实现了深拷贝，改变普通属性b，不会对源对象有影响$n->obj->a=3;echo$m->obj->a;//输出1，不随新对象改变，还是保持了原来的属性,可以看到，序列化和反序列化可以实现对象的深拷贝?>123456789101112131415161718192021222324252627282930<?phpclassTest{    public$a=1;}classTestOne{    public$b=1;    public$obj;    //包含了一个对象属性，clone时，它会是浅拷贝    publicfunction__construct(){        $this->obj=newTest();    }    }$m=newTestOne();//方法二，序列化反序列化实现对象深拷贝$n=serialize($m);$n=unserialize($n);$n->b=2;echo$m->b;//输出原来的1echoPHP_EOL;//可以看到，普通属性实现了深拷贝，改变普通属性b，不会对源对象有影响$n->obj->a=3;echo$m->obj->a;//输出1，不随新对象改变，还是保持了原来的属性,可以看到，序列化和反序列化可以实现对象的深拷贝?>2、写clone函数Python<?phpclassTest{public$a=1;}classTestOne{public$b=1;public$obj;//包含了一个对象属性，clone时，它会是浅拷贝publicfunction__construct(){$this->obj=newTest();}//方法一：重写clone函数publicfunction__clone(){$this->obj=clone$this->obj;}}$m=newTestOne();$n=clone$m;$n->b=2;echo$m->b;//输出原来的1echoPHP_EOL;//可以看到，普通属性实现了深拷贝，改变普通属性b，不会对源对象有影响//由于改写了clone函数，现在对象属性也实现了真正的深拷贝，对新对象的改变，不会影响源对象$n->obj->a=3;echo$m->obj->a;//输出1，不随新对象改变，还是保持了原来的属性?>1234567891011121314151617181920212223242526272829303132<?phpclassTest{    public$a=1;}classTestOne{    public$b=1;    public$obj;    //包含了一个对象属性，clone时，它会是浅拷贝    publicfunction__construct(){        $this->obj=newTest();    }        //方法一：重写clone函数    publicfunction__clone(){        $this->obj=clone$this->obj;    }}$m=newTestOne();$n=clone$m;$n->b=2;echo$m->b;//输出原来的1echoPHP_EOL;//可以看到，普通属性实现了深拷贝，改变普通属性b，不会对源对象有影响//由于改写了clone函数，现在对象属性也实现了真正的深拷贝，对新对象的改变，不会影响源对象$n->obj->a=3;echo$m->obj->a;//输出1，不随新对象改变，还是保持了原来的属性?>三、python下的他们“对一个对象进行浅拷贝其实是新创建了一个类型和原来对象一样，但是内容是原来对象元素的引用。换句话说，这个拷贝的对象本身是新的，但是它的内容不是”，摘自《Python核心编程》。这是我个人对python下浅拷贝和深拷贝的理解：赋值：简单地拷贝对象的引用，两个对象的id相同。浅拷贝：创建一个新的组合对象，这个新对象与原对象共享内存中的子对象。深拷贝：创建一个新的组合对象，同时递归地拷贝所有子对象，新的组合对象与原对象没有任何关联。虽然实际上会共享不可变的子对象，但不影响它们的相互独立性。浅拷贝和深拷贝的不同仅仅是对组合对象来说，所谓的组合对象就是包含了其它对象的可变对象，如列表，类实例。而对于数字、字符串以及其它“原子”类型，没有拷贝一说，产生的都是原对象的引用。下面的代码希望能对你有进一步的帮助；Python#!/usr/bin/python#-*-coding:UTF-8-*-importcopy#浅拷贝a=[1,\"a\",3,[4,5,6],[[7,8,9]]]b=ac=list(a)d=copy.deepcopy(a)print\"原地址&&&\"printid(a)print\"赋值地址&&&\"printid(b)print\"浅拷贝地址&&&\"printid(c)print\"深拷贝地址&&&\"printid(d)print\"赋值地址###\"fori,jinzip(a,b):printid(i),id(j)print\"浅拷贝地址###\"fori,jinzip(a,c):printid(i),id(j)print\"深拷贝地址###\"fori,jinzip(a,d):printid(i),id(j)print\"######\"a[0]=2a[3][0]=14print\"原值变化为%d,%d\"%(a[0],a[3][0])print\"*******\"print\"赋值变化\"printb[0],b[3][0]print\"浅拷贝变化\"printc[0],c[3][0]print\"深拷贝变化\"printd[0],d[3][0]print\"**##12345678910111213141516171819202122232425262728293031323334353637383940414243444546#!/usr/bin/python#-*-coding:UTF-8-*-?importcopy?#????a=[1,\"a\",3,[4,5,6],[[7,8,9]]]?b=a?c=list(a)?d=copy.deepcopy(a)print\"???&&&\"printid(a)print\"????&&&\"printid(b)print\"?????&&&\"printid(c)print\"?????&&&\"printid(d)?print\"????###\"fori,jinzip(a,b):????printid(i),id(j)print\"?????###\"fori,jinzip(a,c):????printid(i),id(j)print\"?????###\"fori,jinzip(a,d):????printid(i),id(j)print\"######\"?a[0]=2a[3][0]=14print\"?????%d,%d\"%(a[0],a[3][0])print\"*******\"print\"????\"printb[0],b[3][0]print\"?????\"printc[0],c[3][0]print\"?????\"printd[0],d[3][0]print\"**##\"/>\"printa</textarea></div><divclass=\"crayon-main\"style=\"\"><tableclass=\"crayon-table\"><trclass=\"crayon-row\"><tdclass=\"crayon-nums\"data-settings=\"show\"><divclass=\"crayon-nums-content\"style=\"font-size:13px!important;line-height:15px!important;\"><divclass=\"crayon-num\"data-line=\"crayon-5a9de2766148f471354834-1\">1</div><divclass=\"crayon-numcrayon-striped-num\"data-line=\"crayon-5a9de2766148f471354834-2\">2</div><divclass=\"crayon-num\"data-line=\"crayon-5a9de2766148f471354834-3\">3</div><divclass=\"crayon-numcrayon-striped-num\"data-line=\"crayon-5a9de2766148f471354834-4\">4</div><divclass=\"crayon-num\"data-line=\"crayon-5a9de2766148f471354834-5\">5</div><divclass=\"crayon-numcrayon-striped-num\"data-line=\"crayon-5a9de2766148f471354834-6\">6</div><divclass=\"crayon-num\"data-line=\"crayon-5a9de2766148f471354834-7\">7</div><divclass=\"crayon-numcrayon-striped-num\"data-line=\"crayon-5a9de2766148f471354834-8\">8</div><divclass=\"crayon-num\"data-line=\"crayon-5a9de2766148f471354834-9\">9</div><divclass=\"crayon-numcrayon-striped-num\"data-line=\"crayon-5a9de2766148f471354834-10\">10</div><divclass=\"crayon-num\"data-line=\"crayon-5a9de2766148f471354834-11\">11</div><divclass=\"crayon-numcrayon-striped-num\"data-line=\"crayon-5a9de2766148f471354834-12\">12</div><divclass=\"crayon-num\"data-line=\"crayon-5a9de2766148f471354834-13\">13</div><divclass=\"crayon-numcrayon-striped-num\"data-line=\"crayon-5a9de2766148f471354834-14\">14</div><divclass=\"crayon-num\"data-line=\"crayon-5a9de2766148f471354834-15\">15</div><divclass=\"crayon-numcrayon-striped-num\"data-line=\"crayon-5a9de2766148f471354834-16\">16</div><divclass=\"crayon-num\"data-line=\"crayon-5a9de2766148f471354834-17\">17</div><divclass=\"crayon-numcrayon-striped-num\"data-line=\"crayon-5a9de2766148f471354834-18\">18</div><divclass=\"crayon-num\"data-line=\"crayon-5a9de2766148f471354834-19\">19</div><divclass=\"crayon-numcrayon-striped-num\"data-line=\"crayon-5a9de2766148f471354834-20\">20</div><divclass=\"crayon-num\"data-line=\"crayon-5a9de2766148f471354834-21\">21</div><divclass=\"crayon-numcrayon-striped-num\"data-line=\"crayon-5a9de2766148f471354834-22\">22</div><divclass=\"crayon-num\"data-line=\"crayon-5a9de2766148f471354834-23\">23</div><divclass=\"crayon-numcrayon-striped-num\"data-line=\"crayon-5a9de2766148f471354834-24\">24</div><divclass=\"crayon-num\"data-line=\"crayon-5a9de2766148f471354834-25\">25</div><divclass=\"crayon-numcrayon-striped-num\"data-line=\"crayon-5a9de2766148f471354834-26\">26</div><divclass=\"crayon-num\"data-line=\"crayon-5a9de2766148f471354834-27\">27</div><divclass=\"crayon-numcrayon-striped-num\"data-line=\"crayon-5a9de2766148f471354834-28\">28</div><divclass=\"crayon-num\"data-line=\"crayon-5a9de2766148f471354834-29\">29</div><divclass=\"crayon-numcrayon-striped-num\"data-line=\"crayon-5a9de2766148f471354834-30\">30</div><divclass=\"crayon-num\"data-line=\"crayon-5a9de2766148f471354834-31\">31</div><divclass=\"crayon-numcrayon-striped-num\"data-line=\"crayon-5a9de2766148f471354834-32\">32</div><divclass=\"crayon-num\"data-line=\"crayon-5a9de2766148f471354834-33\">33</div><divclass=\"crayon-numcrayon-striped-num\"data-line=\"crayon-5a9de2766148f471354834-34\">34</div><divclass=\"crayon-num\"data-line=\"crayon-5a9de2766148f471354834-35\">35</div><divclass=\"crayon-numcrayon-striped-num\"data-line=\"crayon-5a9de2766148f471354834-36\">36</div><divclass=\"crayon-num\"data-line=\"crayon-5a9de2766148f471354834-37\">37</div><divclass=\"crayon-numcrayon-striped-num\"data-line=\"crayon-5a9de2766148f471354834-38\">38</div><divclass=\"crayon-num\"data-line=\"crayon-5a9de2766148f471354834-39\">39</div><divclass=\"crayon-numcrayon-striped-num\"data-line=\"crayon-5a9de2766148f471354834-40\">40</div><divclass=\"crayon-num\"data-line=\"crayon-5a9de2766148f471354834-41\">41</div><divclass=\"crayon-numcrayon-striped-num\"data-line=\"crayon-5a9de2766148f471354834-42\">42</div><divclass=\"crayon-num\"data-line=\"crayon-5a9de2766148f471354834-43\">43</div><divclass=\"crayon-numcrayon-striped-num\"data-line=\"crayon-5a9de2766148f471354834-44\">44</div><divclass=\"crayon-num\"data-line=\"crayon-5a9de2766148f471354834-45\">45</div><divclass=\"crayon-numcrayon-striped-num\"data-line=\"crayon-5a9de2766148f471354834-46\">46</div></div></td><tdclass=\"crayon-code\"><divclass=\"crayon-pre\"style=\"font-size:13px!important;line-height:15px!important;-moz-tab-size:4;-o-tab-size:4;-webkit-tab-size:4;tab-size:4;\"><divclass=\"crayon-line\"id=\"crayon-5a9de2766148f471354834-1\"><spanclass=\"crayon-c\">#!/usr/bin/python</span></div><divclass=\"crayon-linecrayon-striped-line\"id=\"crayon-5a9de2766148f471354834-2\"><spanclass=\"crayon-c\">#-*-coding:UTF-8-*-</span></div><divclass=\"crayon-line\"id=\"crayon-5a9de2766148f471354834-3\">?</div><divclass=\"crayon-linecrayon-striped-line\"id=\"crayon-5a9de2766148f471354834-4\"><spanclass=\"crayon-r\">import</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-k\">copy</span></div><divclass=\"crayon-line\"id=\"crayon-5a9de2766148f471354834-5\">?</div><divclass=\"crayon-linecrayon-striped-line\"id=\"crayon-5a9de2766148f471354834-6\"><spanclass=\"crayon-c\">#???</span></div><divclass=\"crayon-line\"id=\"crayon-5a9de2766148f471354834-7\">?</div><divclass=\"crayon-linecrayon-striped-line\"id=\"crayon-5a9de2766148f471354834-8\"><spanclass=\"crayon-v\">a</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-o\">=</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-sy\">[</span><spanclass=\"crayon-cn\">1</span><spanclass=\"crayon-sy\">,</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-s\">\"a\"</span><spanclass=\"crayon-sy\">,</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-cn\">3</span><spanclass=\"crayon-sy\">,</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-sy\">[</span><spanclass=\"crayon-cn\">4</span><spanclass=\"crayon-sy\">,</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-cn\">5</span><spanclass=\"crayon-sy\">,</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-cn\">6</span><spanclass=\"crayon-sy\">]</span><spanclass=\"crayon-sy\">,</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-sy\">[</span><spanclass=\"crayon-sy\">[</span><spanclass=\"crayon-cn\">7</span><spanclass=\"crayon-sy\">,</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-cn\">8</span><spanclass=\"crayon-sy\">,</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-cn\">9</span><spanclass=\"crayon-sy\">]</span><spanclass=\"crayon-sy\">]</span><spanclass=\"crayon-sy\">]</span></div><divclass=\"crayon-line\"id=\"crayon-5a9de2766148f471354834-9\">?</div><divclass=\"crayon-linecrayon-striped-line\"id=\"crayon-5a9de2766148f471354834-10\"><spanclass=\"crayon-v\">b</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-o\">=</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-i\">a</span></div><divclass=\"crayon-line\"id=\"crayon-5a9de2766148f471354834-11\">?</div><divclass=\"crayon-linecrayon-striped-line\"id=\"crayon-5a9de2766148f471354834-12\"><spanclass=\"crayon-v\">c</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-o\">=</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-k\">list</span><spanclass=\"crayon-sy\">(</span><spanclass=\"crayon-v\">a</span><spanclass=\"crayon-sy\">)</span></div><divclass=\"crayon-line\"id=\"crayon-5a9de2766148f471354834-13\">?</div><divclass=\"crayon-linecrayon-striped-line\"id=\"crayon-5a9de2766148f471354834-14\"><spanclass=\"crayon-v\">d</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-o\">=</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-k\">copy</span><spanclass=\"crayon-sy\">.</span><spanclass=\"crayon-e\">deepcopy</span><spanclass=\"crayon-sy\">(</span><spanclass=\"crayon-v\">a</span><spanclass=\"crayon-sy\">)</span></div><divclass=\"crayon-line\"id=\"crayon-5a9de2766148f471354834-15\"><spanclass=\"crayon-k\">print</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-s\">\"???&&&\"</span></div><divclass=\"crayon-linecrayon-striped-line\"id=\"crayon-5a9de2766148f471354834-16\"><spanclass=\"crayon-k\">print</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-k\">id</span><spanclass=\"crayon-sy\">(</span><spanclass=\"crayon-v\">a</span><spanclass=\"crayon-sy\">)</span></div><divclass=\"crayon-line\"id=\"crayon-5a9de2766148f471354834-17\"><spanclass=\"crayon-k\">print</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-s\">\"????&&&\"</span></div><divclass=\"crayon-linecrayon-striped-line\"id=\"crayon-5a9de2766148f471354834-18\"><spanclass=\"crayon-k\">print</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-k\">id</span><spanclass=\"crayon-sy\">(</span><spanclass=\"crayon-v\">b</span><spanclass=\"crayon-sy\">)</span></div><divclass=\"crayon-line\"id=\"crayon-5a9de2766148f471354834-19\"><spanclass=\"crayon-k\">print</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-s\">\"?????&&&\"</span></div><divclass=\"crayon-linecrayon-striped-line\"id=\"crayon-5a9de2766148f471354834-20\"><spanclass=\"crayon-k\">print</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-k\">id</span><spanclass=\"crayon-sy\">(</span><spanclass=\"crayon-v\">c</span><spanclass=\"crayon-sy\">)</span></div><divclass=\"crayon-line\"id=\"crayon-5a9de2766148f471354834-21\"><spanclass=\"crayon-k\">print</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-s\">\"?????&&&\"</span></div><divclass=\"crayon-linecrayon-striped-line\"id=\"crayon-5a9de2766148f471354834-22\"><spanclass=\"crayon-k\">print</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-k\">id</span><spanclass=\"crayon-sy\">(</span><spanclass=\"crayon-v\">d</span><spanclass=\"crayon-sy\">)</span></div><divclass=\"crayon-line\"id=\"crayon-5a9de2766148f471354834-23\">?</div><divclass=\"crayon-linecrayon-striped-line\"id=\"crayon-5a9de2766148f471354834-24\"><spanclass=\"crayon-k\">print</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-s\">\"????###\"</span></div><divclass=\"crayon-line\"id=\"crayon-5a9de2766148f471354834-25\"><spanclass=\"crayon-st\">for</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-v\">i</span><spanclass=\"crayon-sy\">,</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-i\">j</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-st\">in</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-k\">zip</span><spanclass=\"crayon-sy\">(</span><spanclass=\"crayon-v\">a</span><spanclass=\"crayon-sy\">,</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-v\">b</span><spanclass=\"crayon-sy\">)</span><spanclass=\"crayon-o\">:</span></div><divclass=\"crayon-linecrayon-striped-line\"id=\"crayon-5a9de2766148f471354834-26\"><spanclass=\"crayon-h\">????</span><spanclass=\"crayon-k\">print</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-k\">id</span><spanclass=\"crayon-sy\">(</span><spanclass=\"crayon-v\">i</span><spanclass=\"crayon-sy\">)</span><spanclass=\"crayon-sy\">,</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-k\">id</span><spanclass=\"crayon-sy\">(</span><spanclass=\"crayon-v\">j</span><spanclass=\"crayon-sy\">)</span></div><divclass=\"crayon-line\"id=\"crayon-5a9de2766148f471354834-27\"><spanclass=\"crayon-k\">print</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-s\">\"?????###\"</span></div><divclass=\"crayon-linecrayon-striped-line\"id=\"crayon-5a9de2766148f471354834-28\"><spanclass=\"crayon-st\">for</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-v\">i</span><spanclass=\"crayon-sy\">,</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-i\">j</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-st\">in</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-k\">zip</span><spanclass=\"crayon-sy\">(</span><spanclass=\"crayon-v\">a</span><spanclass=\"crayon-sy\">,</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-v\">c</span><spanclass=\"crayon-sy\">)</span><spanclass=\"crayon-o\">:</span></div><divclass=\"crayon-line\"id=\"crayon-5a9de2766148f471354834-29\"><spanclass=\"crayon-h\">????</span><spanclass=\"crayon-k\">print</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-k\">id</span><spanclass=\"crayon-sy\">(</span><spanclass=\"crayon-v\">i</span><spanclass=\"crayon-sy\">)</span><spanclass=\"crayon-sy\">,</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-k\">id</span><spanclass=\"crayon-sy\">(</span><spanclass=\"crayon-v\">j</span><spanclass=\"crayon-sy\">)</span></div><divclass=\"crayon-linecrayon-striped-line\"id=\"crayon-5a9de2766148f471354834-30\"><spanclass=\"crayon-k\">print</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-s\">\"?????###\"</span></div><divclass=\"crayon-line\"id=\"crayon-5a9de2766148f471354834-31\"><spanclass=\"crayon-st\">for</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-v\">i</span><spanclass=\"crayon-sy\">,</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-i\">j</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-st\">in</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-k\">zip</span><spanclass=\"crayon-sy\">(</span><spanclass=\"crayon-v\">a</span><spanclass=\"crayon-sy\">,</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-v\">d</span><spanclass=\"crayon-sy\">)</span><spanclass=\"crayon-o\">:</span></div><divclass=\"crayon-linecrayon-striped-line\"id=\"crayon-5a9de2766148f471354834-32\"><spanclass=\"crayon-h\">????</span><spanclass=\"crayon-k\">print</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-k\">id</span><spanclass=\"crayon-sy\">(</span><spanclass=\"crayon-v\">i</span><spanclass=\"crayon-sy\">)</span><spanclass=\"crayon-sy\">,</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-k\">id</span><spanclass=\"crayon-sy\">(</span><spanclass=\"crayon-v\">j</span><spanclass=\"crayon-sy\">)</span></div><divclass=\"crayon-line\"id=\"crayon-5a9de2766148f471354834-33\"><spanclass=\"crayon-k\">print</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-s\">\"######\"</span></div><divclass=\"crayon-linecrayon-striped-line\"id=\"crayon-5a9de2766148f471354834-34\">?</div><divclass=\"crayon-line\"id=\"crayon-5a9de2766148f471354834-35\"><spanclass=\"crayon-v\">a</span><spanclass=\"crayon-sy\">[</span><spanclass=\"crayon-cn\">0</span><spanclass=\"crayon-sy\">]</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-o\">=</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-cn\">2</span></div><divclass=\"crayon-linecrayon-striped-line\"id=\"crayon-5a9de2766148f471354834-36\"><spanclass=\"crayon-v\">a</span><spanclass=\"crayon-sy\">[</span><spanclass=\"crayon-cn\">3</span><spanclass=\"crayon-sy\">]</span><spanclass=\"crayon-sy\">[</span><spanclass=\"crayon-cn\">0</span><spanclass=\"crayon-sy\">]</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-o\">=</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-cn\">14</span></div><divclass=\"crayon-line\"id=\"crayon-5a9de2766148f471354834-37\"><spanclass=\"crayon-k\">print</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-s\">\"?????%d,%d\"</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-o\">%</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-sy\">(</span><spanclass=\"crayon-v\">a</span><spanclass=\"crayon-sy\">[</span><spanclass=\"crayon-cn\">0</span><spanclass=\"crayon-sy\">]</span><spanclass=\"crayon-sy\">,</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-v\">a</span><spanclass=\"crayon-sy\">[</span><spanclass=\"crayon-cn\">3</span><spanclass=\"crayon-sy\">]</span><spanclass=\"crayon-sy\">[</span><spanclass=\"crayon-cn\">0</span><spanclass=\"crayon-sy\">]</span><spanclass=\"crayon-sy\">)</span></div><divclass=\"crayon-linecrayon-striped-line\"id=\"crayon-5a9de2766148f471354834-38\"><spanclass=\"crayon-k\">print</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-s\">\"*******\"</span></div><divclass=\"crayon-line\"id=\"crayon-5a9de2766148f471354834-39\"><spanclass=\"crayon-k\">print</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-s\">\"????\"</span></div><divclass=\"crayon-linecrayon-striped-line\"id=\"crayon-5a9de2766148f471354834-40\"><spanclass=\"crayon-k\">print</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-v\">b</span><spanclass=\"crayon-sy\">[</span><spanclass=\"crayon-cn\">0</span><spanclass=\"crayon-sy\">]</span><spanclass=\"crayon-sy\">,</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-v\">b</span><spanclass=\"crayon-sy\">[</span><spanclass=\"crayon-cn\">3</span><spanclass=\"crayon-sy\">]</span><spanclass=\"crayon-sy\">[</span><spanclass=\"crayon-cn\">0</span><spanclass=\"crayon-sy\">]</span></div><divclass=\"crayon-line\"id=\"crayon-5a9de2766148f471354834-41\"><spanclass=\"crayon-k\">print</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-s\">\"?????\"</span></div><divclass=\"crayon-linecrayon-striped-line\"id=\"crayon-5a9de2766148f471354834-42\"><spanclass=\"crayon-k\">print</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-v\">c</span><spanclass=\"crayon-sy\">[</span><spanclass=\"crayon-cn\">0</span><spanclass=\"crayon-sy\">]</span><spanclass=\"crayon-sy\">,</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-v\">c</span><spanclass=\"crayon-sy\">[</span><spanclass=\"crayon-cn\">3</span><spanclass=\"crayon-sy\">]</span><spanclass=\"crayon-sy\">[</span><spanclass=\"crayon-cn\">0</span><spanclass=\"crayon-sy\">]</span></div><divclass=\"crayon-line\"id=\"crayon-5a9de2766148f471354834-43\"><spanclass=\"crayon-k\">print</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-s\">\"?????\"</span></div><divclass=\"crayon-linecrayon-striped-line\"id=\"crayon-5a9de2766148f471354834-44\"><spanclass=\"crayon-k\">print</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-v\">d</span><spanclass=\"crayon-sy\">[</span><spanclass=\"crayon-cn\">0</span><spanclass=\"crayon-sy\">]</span><spanclass=\"crayon-sy\">,</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-v\">d</span><spanclass=\"crayon-sy\">[</span><spanclass=\"crayon-cn\">3</span><spanclass=\"crayon-sy\">]</span><spanclass=\"crayon-sy\">[</span><spanclass=\"crayon-cn\">0</span><spanclass=\"crayon-sy\">]</span></div><divclass=\"crayon-line\"id=\"crayon-5a9de2766148f471354834-45\"><spanclass=\"crayon-k\">print</span><spanclass=\"crayon-h\"></span><spanclass=\"crayon-s\">\"**##\"printa输出如下： 参考博文http://www.cnblogs.com/taijun…http://blog.csdn.net/u0115085…http://www.cnblogs.com/zxlove…1赞2收藏1评论"], "art_create_time": ["2017/11/19"], "art_title": ["探索 Php 和 Python 下对象的深拷贝和浅拷贝"], "art_url": ["http://python.jobbole.com/88889/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2017/11/ac652d1b0feb1192f18b75d650e1c73b.png"]},
{"art_content": ["英文原文：CreatesuccessfulPythonprojects，编译：Elaine.Ye创建一个成功的开源Python项目所涉及的并不仅仅是编写有用的代码，与其相关的还有社区的参与、越来越多的合作机会、技艺以及支持等。探索最佳的做法有助于你创建出自己的成功项目。开源Python项目的生态系统丰富多样，这使得您能够站在巨人的肩膀上来开发下一个开源项目。此外，这意味着存在一系列的社区规范和最佳做法，通过遵守这些约定并把这些做法应用到项目中，你可以为自己的软件赢得更广范围的采用。本文涵盖了一些在构建大型和小型的项目时都运作得很好的实践做法，这些项目都已经赢得了广泛的用户群体。这里给出的这些建议的都是合理的、有其意义的，不过，因为结果可能会有所不同，所以不必把它们当成严格的教条来遵守。首先我们来讨论一下，解耦的过程如何能够带来一个更强健的社区，在编写、维护和支持开源软件等各方面都带来更大的生产能力。合作（collaboration）和互助（cooperation）的对比在DjangoCon2011大会期间，DavidEaves作了基调发言，雄辩地表达了这样的想法，即尽管合作（collaboration）和互助（cooperation）有着类似的定义，但还是有着细微的差别：“我认为，和作（collaboration）不同于互助（cooperation），其需要项目涉及到的各方通力来解决问题。”Eaves接着又给出了一整篇文章，专门说明GitHub如何成为革新开源的运作方式——特别是在社区管理方面的运作方式的推动力。在“HowGitHubSavedOpenSource”这篇文章（参见参考资料）中，Eaves说到：“我相信，当捐献者能够以低交易成本的互助方式来参与，并且高交易成本的合作尽可能的少时，开源项目的运作能达到最好。开源的高明之处就在于其不需要一个工作组来共同讨论每个问题以及解决问题，而是恰恰相反。”他接着谈到了分支（forking）的价值所在，以及其如何通过在参与者之间启用低成本的互助来降低合作的高成本，这些参与者能够在无需批准的情况下推进项目。这种分支把需要的合作搁置起来，直到解决方案已经做好了合并的准备为止，如此来支持更快速的动态的实验。你可以以类似的方式来打造自己的项目，目标是相同的：在编写、管理和支持项目的整个期间，增加低成本的互助，同时尽可能减少代价高昂的合作。编写从一张白纸开始，创建一些新鲜的东西，制造一些有创意的东西——或仅仅是一些与现有的略不同的东西，没有什么事情比得上启动一个新的项目并和全世界分享你的努力成果让人感觉更好的了。与维护不同，在编写代码时，你是在创造新的东西而不是在修改或是修正已有的东西。编写和构思一个项目除了是一门科学之外还是一种艺术形式，其他人会看到实现的情况并会对代码的质量作出判断，而你的名字将会永远和它连在一起。因此。了解工匠的心态以及据此来编写软件的方法是很重要的。编写新的项目不仅仅是意味着生成代码：项目的创建和构思包括了编写有着精美风格的让人乐于阅读的代码、在适当的时候为验证项目中的功能创建测试代码，以及制作详尽的有帮助的文档。技艺工艺（craft）一般是指艺术行业或是职业需要特殊的技能来手工制作一些东西，通常是小规模生产的物理器件。就软件工匠关注的更多的是质量而非数量这一意义而言，你可以延伸这一定义，把它应用在软件上。对于工匠来说，产品具有吸引力而非只是功用是很重要的。具体来说，在软件中，工匠要努力确保代码的干净和美观、应用编程接口（API）的悦目，以及文档和测试用例能够给用户带来是在使用坚实的产品进行工作的这种感受。在这种心态下工作，对于心灵来说是一种奖赏，也是在制作开源软件时能感受到诸多享受的原因：你不再受困于回应最后期限、客户以及其他的外部需求，而是按照自己的时间来，享受制作一些美好事物的乐趣。代码风格和规范检查Python的增强建议（PythonEnhancementProposal，PEP）8（参见参考资料）是一个详细的Python风格指南，你应该基于该指南来建立自己的Python项目（或至少是基于你的项目的风格指南）。不是非要教条地采用PEP8，不过你的工作成果越接近PEP8规范，其他的Python开发者就越容易提交以标准的Python社区风格实现的整洁的补丁包。除了风格的一致性之外，在捕捉诸如缺失导入和未定义变量一类的错误方面，代码规范（linting）的概念也是很有作用的。除了风格检查器会帮助你进行检查，找出违背了默认规则或是自定义规则的代码之外，现还有一些规范器（linter）或是一些工具，最常用到的一些实用程序是：1.pyflakes2.pylint3.pep8请参阅参考资料获得到这些工具的链接。无论你选择遵从的是哪一种约定，如果这些约定偏离了PEP8的话，我建议文档化它们，以便让那些想要为你的项目做贡献的人了解你所采用的编码风格，显式的说明要好于隐含不语。pyflakes是一个特别有用的规范器，它很好地平衡了有用的功能、捕捉和标出错误这两方面，不会过度地揪住微小的古怪做法不放。下面是一个在某个Python项目上使用pyflakes的示例会话：Python$pyflakeskaleokaleo/forms.py:1:'form'importedbutunusedkaleo/forms.py:4:undefinedname'forms'kaleo/forms.py:6:undefinedname'forms'1234$pyflakeskaleokaleo/forms.py:1:'form'importedbutunusedkaleo/forms.py:4:undefinedname'forms'kaleo/forms.py:6:undefinedname'forms'立刻，该工具告诉了我有一个import的输入错误，查看文件kaleo/forms.py，我发现：Python;html-script:false]1:fromdjangoimportform2:3:classInviteForm(forms.Form):4:email_address=forms.EmailField()1234;html-script:false]1:fromdjangoimportform2:3:classInviteForm(forms.Form):4:email_address=forms.EmailField()从内容中可看出来，要把第1行改为fromdjangoimportforms。测试在项目中提供验证代码有效性的测试始终是一件好事，以此来防止回归被忽视，以及在某些情况下作为一种文档形式，通过阅读其中的测试代码可以让其他人知道你的库API是如何工作的。话虽如此，但我不会根据项目是否包括测试用例或是完成这些测试的方式来判断项目的完整性或可行性。测试用例的存在并不能保证代码的质量，这可能是一个有争议的观点，但我相信，完全没有测试比去测一些错误的东西要来得好一些。在编写测试代码时，考虑为每个测试单元给出各种输入是一件很重要的事情。文档不过，与测试不同的是，你可以根据项目文档的质量和广博性来判断项目的质量和技艺水平。用与创作和维护代码相同的方式来创作和维护稳定，编写良好的并且是有深度的文档会鼓励捐献者效仿你的做法，使你的项目变得更易于为用户接受。使用诸如Sphinx和ReadtheDocs一类的工具（参见参考资料），你可以发布及时更新的、外观极为不错的文档。使用这些工具是一件简单的事情，也就是写一些文字内容并并推送提交。习惯于尽可能地使用commit来提交文档的变更是很适当的一种做法。维护在PythonPackageIndex（PyPI）上发布了第一个版本，并通过各种Tweet消息和博客文章公布该版本的消息，开始有了一些使用者之后，你就需要在任何后续的创作活动中加入维护方面的考虑了。用户会报告错误、要求添加功能、提一些文档中没有明显涉及到问题，诸如此类等等。有些事情你会选择不去处理，给出一些权变措施；但其他的一些问题，你会打算或是修正文档或是修正代码。使用诸如git一类的分布式版本控制系统（distributedversioncontrolsystem，DVCS）并常常发布开发者包，这种做法可以大大简化维护工作，使之变成一件不再是烦人的事情。源控制有许多可用的DVCS，其中就包括了git和mercurial（参见参考资料），无论你选择的是哪一个控制系统，请确保它提供了源控制功能，这种功能赋予你这样的能力，可以让用户分支你的项目，然后自己来解决其中的错误。进行变更的速率取决于许多因素，一个关键的因素是目标受众（例如，其他开发者、非技术型的最终用户）。如果你的项目是针对开发者来编写的，那么鼓励通过拉请求（pullrequest）来报告错误或是请求功能之类的做法可以真正地做到降低维护者的负担。这种做法还提升了社区的归属感，因为大家都把他们的捐献合并到了将来的版本中。开发构建你会希望尽早地以及经常性地发布开发版本，在每次有一组附加的补丁包出来之后都会发布版本，如此多次。这会让其他在工作中使用你的项目的开发者能够更容易地针对项目中的最新更改来运行。越多的人在不同的情况下使用这些代码，那么一旦到发布一个新的稳定版本的时候，该版本就会有越高的质量。支持支持是和维护相随的，参与并构建一个由用户和捐献者组成的社区至关重要。赋予其他人通过支持来帮助你的权利，你就是在增强项目的全面合作因素，在项目的规模方面提供更好的伸缩性，以及自然而然地增加了解决用户问题的做法。为了达到该目的，请确保提供多种渠道来增加接触的机会，让用户更加容易地与你接洽以及参与到项目中。可选的沟通渠道包括IRC、邮件列表以及诸如Twitter一类的社交媒体汇聚点。IRC在诸如freenode一类的IRC平台上设置一个沟通频道是一个好主意，我就为自己的项目设置了一个：nashvegas；除了我之外只有一个用户，虽然这种情况很少有，但我的IRC客户端还是悄无声息地运行在后台。当偶尔有用户提问时，我能够只花很少的交易成本就以一种比通过邮件要动态得多的方式来做出响应。邮件列表对于大多数的开源项目来说，有一个用于支持的邮件列表并在捐献者之间讨论开发进程是一种标准的做法。我的建议是，把支持放在一个邮件列表中，只有在内容已经变得太多，彼此影响到了各小组的讨论的时候，才把它分成“用户”列表和“开发”列表。Twitter为项目开设一个Twitter帐户，大家可以在这里与你快速地讨论工作。Twitter帐户还是一个可以作为发布项目消息的好地方。结束语给Python社区中的开源软件编写并捐献代码是一种有趣且有益的体验。在增加低成本互助机会的同时侧重于减少高成本的合作，这种做法有助于项目与活跃的捐献者一起成长。在开源领域，就你的项目来说，你有大把的自由来成为一个能工巧匠，充分利用这一点并享受它。把关注的重点放在一致的代码风格、坚实的测试和编写良好的文档上，以此来提高项目被用户和其他开发者采用的几率。此外，要利用DVCS，关注拉请求，经常性地发布开发版本。最后还有一点就是，你可以提供多种支持渠道，以及允许社区协助你提供这种支持，通过这些做法来进一步提升项目的采用率并促进项目的成长。 参考资料1.阅读MarkPilgrim的DiveintoPython，获取关于该语言的一个介绍。2.欲了解更多关于打包Python项目方面的信息，可以读一下 AguidetoPythonpackaging（PatrickAltman，developerWorks，2011年10月）这篇文章。3.阅读更多DavidEaves的博客文章： WikisandOpenSource:CollaborativeorCooperative? 和HowGitHubSavedOpenSource.4.在潜心进行下一个Python项目之前，请确保已了解PEP8，Python代码的这一“官方”风格指南。5.浏览一下我的项目nashvegas的GitHub页面，以此来做为一个使用DVCS的Python项目的例子。6.看一看PyPI。7.了解更多关于分布式Python模块方面的内容。8. developerWorks开源专区提供了丰富的关于开源工具和使用开源技术方面的信息。9.在Twitter上关注developerWorks。10.在EasyandbeautifuldocumentationwithSphinx (AlfredoDeza，developerWorks，2011年11月)一文中了解更多关于Sphinx的内容。1赞1收藏评论"], "art_create_time": ["2012/02/02"], "art_title": ["创建成功的Python项目"], "art_url": ["http://python.jobbole.com/12649/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2012/02/python-logo.png"]},
{"art_content": ["FredrikHaard最近发表了一篇“为什么Python对你如此重要”的文章，引起了开发者的热烈讨论。iteye 对其简要编译。我相信Python对软件开发人员很重要。现今已经诞生了不少的编程语言，它们都有各自不同的特性：强大者如Lisp，快速如C，运用广泛如Java，论古怪则如Haskell。与这些语言不同，Python是一门比较中庸的编程语言，它将语言的很多特性进行了融合，迄今我还未有其他语言如Python这般协调。 Python知道开发人员阅读代码比编写它花的时间要多得多，因此将精力集中于引导开发者编写易读的代码。当然，Python也能写出令人晦涩（obfuscated）的代码，但是写代码最舒服的方式还是（假如你了解Python）保持适度精炼，即：代码总能明确反映你的意图。这一点很重要。使用Python开发可谓轻而易举。甚至包括许多库，也能用Python完美编写，保证其易读性（你可以比较一下其他语言的框架实现，比如用Java编写的Spring）。 同样Python也意识到对开发人员而言开发速度的重要性。易读而精炼的代码只是一部分，另一部分取决于强大的构造函数，可避免许多繁琐重复的代码。此外，可维护性也是很重要的——代码行数（LineofCode，LoC）在很多度量结果都会中出现，或许没什么用，不过它至少说明了你需要审查多少代码，需要理解多少代码并从中发现问题。此外，FredrikHaard还提到了Python的另一个优势——Toolmaking。快速的软件开发速度、简练的技巧（其他语言开发人员也能轻松掌握Python基本技巧）、庞大的标准库维系了这一优势。任何项目都会遭遇任务自动化任务情况，在我的经验中，用Python写的自动化任务比其他主流语言要快一个数量级——事实上，这也是我学习Python的原因。…… 能够轻松开发customtool其实还包含了另一层意思，即开发和维护customsoftware也会很容易。这也是为什么，在庞大的Django成为最著名的PythonWeb框架之后，还是有大量成功的小巧甚至微型框架存在的原因。当使用一门强大的编程语言，拥有大量标准及第三方库的时候，你并不经常需要考虑妥协（trade-off），而这在使用许多现成的（off-the-shelf）大型框架时是必然会遭遇的。根据Fredrik的观点，编写能够很好契合客户模型而不是一个框架的软件这点很重要。而许多开发人员将时间都耗在了框架配置以及掩盖它们的缺点上，而不是真正的开发。你是如何看的呢？1赞收藏评论"], "art_create_time": ["2012/02/14"], "art_title": ["为什么Python对程序员重要？"], "art_url": ["http://python.jobbole.com/13153/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2012/02/python-logo.png"]},
{"art_content": ["来源： InfoQ最近，Google在其GoogleAppEngine（以下简称GAE）官方博客上宣布正式支持Python2.7，对开发者来说又多了一种选择。GAE在几个月之前实验性的引入了Python2.7环境，此后一直在修改缺陷和进行优化。随着这次Python2.7的正式支持，相信对开发人员更有帮助：首先，它支持程序利用并发请求，帮助开发者构建更加稳定、高效的应用。如果你的应用没有完全利用CPU，那么你现在有机会通过并发请求来降低实例的数量。同时，我们还增加了开发者迫切需要的工具库，包括PIL、NumPy和lxml等，这些库已经被开发者提了将近两千次了。通过使用GAE，用户可以在Google基础架构上运行网络应用程序，目前GAE支持使用几种编程语言编写的应用程序，包括Java、Python（2.5和2.7）、Go等语言。相比2.5版，Python2.7提供了更多的特性和工具库，开发者可以更加得心应手:1、多线程——2.5不支持此特性，在2.7中，开发者可以利用多线程库。单个请求中的所有线程必须在请求时限（在线请求为60秒，离线请求为10分钟）之前完成。2、并发请求——2.5不支持此特性，现在开发者可以在GAE中修改配置来支持并发请求。3、字节码修改——在2.5版本中由于运行时的限制无法修改字节码，Python2.7不再包括此限制。生成和处理字节码的工具库可以在运行时正常工作。4、字节码上传——2.5版不支持，Python2.7可以上传.pyc文件，但是不能和.py一起，但是可以上传包含.py和.pyc文件的.zip文件。除此之外，两版之间的区别还包括对数据存储的支持、Django、JSON等工具库的版本更新等等。GAE的Python2.7运行时环境支持一下第三方工具库：1、lxml(2.3)——lxml工具库支持开发者更加方便的处理XML和HTML文档，对于GAE上Web服务的开发人员来说，是个必不可少的工具。2、jinja2(2.6)——Jinja2是基于python的多功能的被广泛使用的模板引擎，功能比较类似于于PHP的Smarty、J2EE的Freemarker和Velocity。它支持Unicode，并具有集成的沙箱执行环境，Jinja2模板引擎弥补了Django自带模板系统许多不足。Python2.7不再推荐使用WebApp模板，jinja2成为了替代者。3、MarkupSafe(0.15)——MarkupSafe为XML、HTML和XHTML提供了安全转义的字符串。4、NumPy(1.6.1)——Numpy提供了若干数据处理的工具。5、PIL(1.1.7)——全称为PythonImagingLibrary，提供了许多用于处理图片的函数。6、PyCrypto(2.3)——全称为PythonCryptographyToolkit，提供了许多密码学算法函数，比如随机数生成等。7、setuptools(0.6c11)——帮助开发者下载、构建、安装、升级和卸载Python包。8、WebOb(1.1.1)——对HTTP请求和响应做了面向对象的封装，GAE将其作为webapp框架的一部分。9、YAML(3.10)——支持可读性高的消息序列化数据格式，GAESDK使用YAML作为其配置文件的格式。除了Python语言之外，GAE还支持Java和Go语言（实验阶段）：GoogleAppEngine支持Java5和Java6。在AppEngine上运行Java应用程序时，将使用Java6虚拟机(JVM)和标准库运行该应用程序。理想情况下，开发者应使用Java6编译和测试应用程序，以确保本地服务器的工作方式与AppEngine类似。AppEngineSDK与Java5兼容，这非常适用于不能方便获取Java6的开发人员（如使用MacOSX的开发人员）。开发者可以将使用Java5编译的类和JAR上传到AppEngine。1赞收藏评论"], "art_create_time": ["2012/03/02"], "art_title": ["Google App Engine正式支持Python 2.7"], "art_url": ["http://python.jobbole.com/14174/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2012/02/python-logo.png"]},
{"art_content": ["本文由伯乐在线-黄利民翻译。未经许可，禁止转载！英文出处：fmeyer。欢迎加入翻译组。导读：下面代码来自 fmeyer ，列举了各种程序员所写的阶乘算法代码，甚至包括网页设计师的。:)Python#新手程序员deffactorial(x):ifx==0:return1else:returnx*factorial(x-1)printfactorial(6)#有一年Pascal经验的程序员deffactorial(x):result=1i=2whilei&lt;=x:result=result*ii=i+1returnresultprintfactorial(6)#有一年C经验的程序员deffact(x):#{result=i=1;while(i&lt;=x):#{result*=i;i+=1;#}returnresult;#}print(fact(6))#有一年SICP经验的程序员@tailcalldeffact(x,acc=1):if(x&gt;1):return(fact((x-1),(acc*x)))else:returnaccprint(fact(6))#有一年Python经验的程序员defFactorial(x):res=1foriinxrange(2,x+1):res*=ireturnresprintFactorial(6)#懒惰的Python程序员deffact(x):returnx&gt;1andx*fact(x-1)or1printfact(6)#更懒惰的Python程序员f=lambdax:xandx*f(x-1)or1printf(6)#专家级Python程序员importoperatorasopimportfunctionalasffact=lambdax:f.foldl(op.mul,1,xrange(2,x+1))printfact(6)#Python黑客importsys@tailcalldeffact(x,acc=1):ifx:returnfact(x.__sub__(1),acc.__mul__(x))returnaccsys.stdout.write(str(fact(6))+'\\n')#专家级程序员importc_mathfact=c_math.factprintfact(6)#英国专家级程序员（译注：在英式英语中，“数学”的简写，多用“maths”，不是“math\"。）importc_mathsfact=c_maths.factprintfact(6)#网页设计师deffactorial(x):#-------------------------------------------------#---这段代码是从MathVault那弄过来滴---#---计算阶乘（C）亚瑟·史密斯1999年---#-------------------------------------------------result=str(1)i=1#谢谢亚当whilei&lt;=x:#result=result*i#It'sfastertouse*=#result=str(result*result+i)#result=int(result*=i)#??????resultstr(int(result)*i)#result=int(str(result)*i)i=i+1returnresultprintfactorial(6)#Unix程序员importosdeffact(x):os.system('factorial'+str(x))fact(6)#Windows程序员NULL=NonedefCalculateAndPrintFactorialEx(dwNumber,hOutputDevice,lpLparam,lpWparam,lpsscSecurity,*dwReserved):iflpsscSecurity!=NULL:returnNULL#NotimplementeddwResult=dwCounter=1whiledwCounter&lt;=dwNumber:dwResult*=dwCounterdwCounter+=1hOutputDevice.write(str(dwResult))hOutputDevice.write('\\n')return1importsysCalculateAndPrintFactorialEx(6,sys.stdout,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL)#企业程序员defnew(cls,*args,**kwargs):returncls(*args,**kwargs)classNumber(object):passclassIntegralNumber(int,Number):deftoInt(self):returnnew(int,self)classInternalBase(object):def__init__(self,base):self.base=base.toInt()defgetBase(self):returnnew(IntegralNumber,self.base)classMathematicsSystem(object):def__init__(self,ibase):Abstract@classmethoddefgetInstance(cls,ibase):try:cls.__instanceexceptAttributeError:cls.__instance=new(cls,ibase)returncls.__instanceclassStandardMathematicsSystem(MathematicsSystem):def__init__(self,ibase):ifibase.getBase()!=new(IntegralNumber,2):raiseNotImplementedErrorself.base=ibase.getBase()defcalculateFactorial(self,target):result=new(IntegralNumber,1)i=new(IntegralNumber,2)whilei&lt;=target:result=result*ii=i+new(IntegralNumber,1)returnresultprintStandardMathematicsSystem.getInstance(new(InternalBase,new(IntegralNumber,2))).calculateFactorial(new(IntegralNumber,6))123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166#新手程序员deffactorial(x):    ifx==0:        return1    else:        returnx*factorial(x-1)printfactorial(6) #有一年Pascal经验的程序员deffactorial(x):    result=1    i=2    whilei&lt;=x:        result=result*i        i=i+1    returnresultprintfactorial(6) #有一年C经验的程序员deffact(x):#{    result=i=1;    while(i&lt;=x):#{        result*=i;        i+=1;    #}    returnresult;#}print(fact(6)) #有一年SICP经验的程序员@tailcalldeffact(x,acc=1):    if(x&gt;1):return(fact((x-1),(acc*x)))    else:returnaccprint(fact(6)) #有一年Python经验的程序员defFactorial(x):    res=1    foriinxrange(2,x+1):        res*=i    returnresprintFactorial(6) #懒惰的Python程序员deffact(x):    returnx&gt;1andx*fact(x-1)or1printfact(6) #更懒惰的Python程序员f=lambdax:xandx*f(x-1)or1printf(6) #专家级Python程序员importoperatorasopimportfunctionalasffact=lambdax:f.foldl(op.mul,1,xrange(2,x+1))printfact(6) #Python黑客importsys@tailcalldeffact(x,acc=1):    ifx:returnfact(x.__sub__(1),acc.__mul__(x))    returnaccsys.stdout.write(str(fact(6))+'\\n') #专家级程序员importc_mathfact=c_math.factprintfact(6) #英国专家级程序员（译注：在英式英语中，“数学”的简写，多用“maths”，不是“math\"。）importc_mathsfact=c_maths.factprintfact(6) #网页设计师deffactorial(x):    #-------------------------------------------------    #---这段代码是从MathVault那弄过来滴---    #---计算阶乘（C）亚瑟·史密斯1999年---    #-------------------------------------------------    result=str(1)    i=1#谢谢亚当    whilei&lt;=x:        #result=result*i#It'sfastertouse*=        #result=str(result*result+i)          #result=int(result*=i)#??????        resultstr(int(result)*i)        #result=int(str(result)*i)        i=i+1    returnresultprintfactorial(6) #Unix程序员importosdeffact(x):    os.system('factorial'+str(x))fact(6) #Windows程序员NULL=NonedefCalculateAndPrintFactorialEx(dwNumber,                                hOutputDevice,                                lpLparam,                                lpWparam,                                lpsscSecurity,                                *dwReserved):    iflpsscSecurity!=NULL:        returnNULL#Notimplemented    dwResult=dwCounter=1    whiledwCounter&lt;=dwNumber:        dwResult*=dwCounter        dwCounter+=1    hOutputDevice.write(str(dwResult))    hOutputDevice.write('\\n')    return1importsysCalculateAndPrintFactorialEx(6,sys.stdout,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL) #企业程序员defnew(cls,*args,**kwargs):    returncls(*args,**kwargs) classNumber(object):    pass classIntegralNumber(int,Number):    deftoInt(self):        returnnew(int,self) classInternalBase(object):    def__init__(self,base):        self.base=base.toInt()     defgetBase(self):        returnnew(IntegralNumber,self.base) classMathematicsSystem(object):    def__init__(self,ibase):        Abstract     @classmethod    defgetInstance(cls,ibase):        try:            cls.__instance        exceptAttributeError:            cls.__instance=new(cls,ibase)        returncls.__instance classStandardMathematicsSystem(MathematicsSystem):    def__init__(self,ibase):        ifibase.getBase()!=new(IntegralNumber,2):            raiseNotImplementedError        self.base=ibase.getBase()     defcalculateFactorial(self,target):        result=new(IntegralNumber,1)        i=new(IntegralNumber,2)        whilei&lt;=target:            result=result*i            i=i+new(IntegralNumber,1)        returnresult printStandardMathematicsSystem.getInstance(new(InternalBase,new(IntegralNumber,2))).calculateFactorial(new(IntegralNumber,6))下面代码是kohashi给出的，他说是在邮局看到的。Python#VBA程序员deffactorial(x):ifx==0:return1ifx==1:returnxifx==2:returnx*(x-1)ifx==3:returnx*(x-1)*(x-2)ifx==4:returnx*(x-1)*(x-2)*(x-3)ifx==5:returnx*(x-1)*(x-2)*(x-3)*(x-4)ifx==6:returnx*(x-1)*(x-2)*(x-3)*(x-4)*(x-5)printfactorial(6)1234567891011121314151617#VBA程序员deffactorial(x):    ifx==0:        return1    ifx==1:        returnx    ifx==2:        returnx*(x-1)    ifx==3:        returnx*(x-1)*(x-2)    ifx==4:        returnx*(x-1)*(x-2)*(x-3)    ifx==5:        returnx*(x-1)*(x-2)*(x-3)*(x-4)    ifx==6:        returnx*(x-1)*(x-2)*(x-3)*(x-4)*(x-5)printfactorial(6)打赏支持我翻译更多好文章，谢谢！打赏译者打赏支持我翻译更多好文章，谢谢！任选一种支付方式1赞2收藏9评论关于作者：黄利民伯乐在线联合发起人，关注IT和互联网。个人主页·我的文章·99·"], "art_create_time": ["2012/03/11"], "art_title": ["趣文：Python程序员的进化史"], "art_url": ["http://python.jobbole.com/15005/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2012/02/python-logo.png"]},
{"art_content": ["英文原文：Yourfavouriteprogramminglanguageisnotgoodenough，翻译：外刊IT评论我的《为什么Python对程序员重要？》这篇博客引来如此多的评论，让我颇为意外。大多数的评论是正面的，但同时多少也都带点不太积极的色彩。我发现，这些评论反映出的一个很突出的问题是，程序员如何看待编程语言：他们去追求完美的语言，热爱它，保护它。为什么如此多的程序员会对他们最喜欢的编程语言如此感性化？试想，其实世上没有哪种语言能够完美的把你的脑子里的思想转变成机器代码，所有的语言都有一定的局限性——它们都会限制你的思想的发挥或不能让机器的充分的施展能力。我相信，程序员热烈的去为某种语言编程辩护的行为，其主要的一个原因就是：懒惰。当然，优秀的程序员通常是很懒惰的(他们喜欢让所有的工作都能自动化完成)，但我说的这种懒惰是特指的，是非常不好的懒惰——懒得去学习。如果我最爱的语言是最好的，或只要不比其他语言差，我就不需要花时间和精力学习新的语言了。这种现象主要的问题是，不仅你不可能找到一种完美的语言，而且当你仅习惯于一两种语言，仅满足于一两种语言后，你解决问的思路也就会仅限于这一两种语言能够实现的方式——如果你会的这一两种语言很相似的话，属于同一种语系，问题就更严重了。当你解决一个问题需要选择一种语言时，尽一切可能，选择一种你感觉能最胜任的语言——最强大的，效率最高的，最适宜的，拥有最多程序库的语言。如果你是一个很认真的程序员，而不是那种随便搞搞的程序员，你就需要去学习新的语言，你需要抛弃这种认为这种语言比其它语言都好的思想。所有的编程语言都各有利弊，没有一个是完美的。我们可以说，某种强于其它语言，但没有一种语言会擅长做任何事情，没有一种语言会样样都好。Python有其自身的问题(我指的不是它的动态类型)，各种Lisp方言也是这样(我指的不是它们的太多的括弧)，Haskell语言也是(人们公认它有很多奇异之处*)。学习新的语言。学会不要去盲目崇拜某种语言，学会不要去为“你的”语言辩护、反驳所有的批评。如果你觉得还不到火候，读一读《计算机程序的构造和解释(StructureandInterpretationofComputerPrograms)》，学习一种Lisp方言——它会让你看清楚，让你感受到其它语言的局限性，这种痛苦会让你成为一名更好的程序员——不论你是什么语言的程序员。*我是在开玩笑。事实上，Haskell将是我下一种要学习的语言。1赞收藏评论"], "art_create_time": ["2012/03/27"], "art_title": ["你最喜爱的编程语言不够好"], "art_url": ["http://python.jobbole.com/16231/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2012/01/programming-languages.jpg"]},
{"art_content": ["本文作者：伯乐在线-iPytLab。未经作者许可，禁止转载！欢迎加入伯乐在线专栏作者。前言本文对遗传算法中的几种选择策略进行了总结,其中包括:ProportionateRouletteWheelSelectionLinearRankingSelectionExponentialRankingSelectionTournamentSelection对于每种选择策略我都使用Python进行了相应的实现并以内置插件的形式整合进了本人所写的遗传算法框架GAFT中。对需要使用遗传算法优化问题以及学习遗传算法的童鞋可以作为参考.项目链接:GitHub: https://github.com/PytLab/gaftPyPI: https://pypi.python.org/pypi/gaft遗传算法中的选择算子遗传算法(geneticalgorithms,GAs)是一种自适应的启发式搜索算法,它模仿达尔文进化论中的“适者生存”的原则,最终获取优化目标的最优解。下图描述了一个简单的遗传算法流程:对于种群中需要进行杂交的物种选择方法有很多，而且选择一种合适的选择策略对于遗传算法整体性能的影响将是很大的。如果一个选择算法选择多样性降低，便会导致种群过早的收敛到局部最优点而不是我们想要的全局最优点，也就是所谓的”早熟”。而选择策略过于发散则会导致算法难以收敛到最优点。因此在这两点中我们需要进行平衡才能使遗传算法以一种高效的方式收敛到全局最优点。GAFT框架中的算子插件GAFT是我根据自己需求开发的一个遗传算法框架，相关介绍的博客可以参见《GAFT-一个使用Python实现的遗传算法框架》,《使用MPI并行化遗传算法框架GAFT》。该框架提供了插件接口，用户可以通过自定义算子以及on-the-fly分析插件来放到gaft框架中运行遗传算法流程对目标问题进行优化。本部分我稍微介绍下gaft关于遗传算子相关接口规范，以及编写能用于gaft的算子的编写方法。在gaft中遗传算子的编写都是需要继承框架内置的基类，然后根据基类提供的接口，实现自己的算子。其中基类的定义都在/gaft/plugin_interfaces/operators/目录下，下面我以选择算子为例，介绍下接口。gaft中选择算子的基类为GASelection，其中在遗传算法流程中会调用该类实例的select方法，进而根据算子中的相关选择策略完成从种群中选取一对物种作为父亲和母亲产生子代。基类的定义为：PythonclassGASelection(metaclass=SelectionMeta):'''Classforprovidinganinterfacetoeasilyextendthebehaviorofselectionoperation.'''defselect(self,population,fitness):'''Calledwhenweneedtoselectparentsfromapopulationtolaterbreeding.:parampopulation:Thecurrentpopulation.:typepopulation:GAPopulation:returnparents:Twoselectedindividualsforcrossover.:typeparents:TupleoftowGAIndividualobjects.'''raiseNotImplementedError1234567891011121314classGASelection(metaclass=SelectionMeta):    '''    Classforprovidinganinterfacetoeasilyextendthebehaviorofselection    operation.    '''    defselect(self,population,fitness):        '''        Calledwhenweneedtoselectparentsfromapopulationtolaterbreeding.        :parampopulation:Thecurrentpopulation.        :typepopulation:GAPopulation        :returnparents:Twoselectedindividualsforcrossover.        :typeparents:TupleoftowGAIndividualobjects.        '''        raiseNotImplementedErrorselect的方法的参数为当前种群population以及相应的适应度函数fitness，其中population需要是GAPopulation对象，fitness也必须是callable的对象。当然，这些在Python这种动态类型语言中貌似看起来有些鸡肋，但是为了能够更加规范使用者，我利用Python的元类在实例化类对象的时候对接口的实现以及接口的参数类型加以限制。具体的实现都在/gaft/plugin_interfaces/metaclasses.py中，有兴趣的童鞋可以看看实现方法。具体自定义算子的编写方法我将在下一部分同选择策略一起贴出来。不同的选择策略本部分我主要对四种不同的选择策略进行总结并加以gaft插件形式的Python实现。选择算子决定了哪些个体将会从种群中被选择出来用于繁衍下一代种群中的新个体。其主要的原则就是:thebetterisanindividual;thehigherisitschanceofbeingaparent选择算子在遗传算法迭代中将适应度函数引入进来，因为适应度函数式标定一个个体是否足够“好”的重要标准。但是选择过程又不能仅仅完全依赖于适应度函数，因为一个种群中的最优物种并不一定是在全局最优点附近。因此我们也应该给相对来说并那么“好”的个体一点机会让他们繁衍后代,避免“早熟”。ProportionateRouletteWheelSelection此轮盘赌选择策略，是最基本的选择策略之一，种群中的个体被选中的概率与个体相应的适应度函数的值成正比。我们需要将种群中所有个体的适应度值进行累加然后归一化，最终通过随机数对随机数落在的区域对应的个体进行选取，类似赌场里面的旋转的轮盘。每个个体ai被选中的概率为:好了，下面可以将此算法写成一个可以gaft中执行的算子。Pythonfromrandomimportrandomfrombisectimportbisect_rightfromitertoolsimportaccumulatefrom...plugin_interfaces.operators.selectionimportGASelectionclassRouletteWheelSelection(GASelection):def__init__(self):'''Selectionoperatorwithfitnessproportionateselection(FPS)orso-calledroulette-wheelselectionimplementation.'''passdefselect(self,population,fitness):'''SelectapairofparentusingFPSalgorithm.'''#Normalizefitnessvaluesforallindividuals.fit=[fitness(indv)forindvinpopulation.individuals]min_fit=min(fit)fit=[(i-min_fit)foriinfit]#Createroulettewheel.sum_fit=sum(fit)wheel=list(accumulate([i/sum_fitforiinfit]))#Selectafatherandamother.father_idx=bisect_right(wheel,random())father=population[father_idx]mother_idx=(father_idx+1)%len(wheel)mother=population[mother_idx]returnfather,mother12345678910111213141516171819202122232425262728293031323334fromrandomimportrandomfrombisectimportbisect_rightfromitertoolsimportaccumulate from...plugin_interfaces.operators.selectionimportGASelection classRouletteWheelSelection(GASelection):    def__init__(self):        '''        Selectionoperatorwithfitnessproportionateselection(FPS)or        so-calledroulette-wheelselectionimplementation.        '''        pass     defselect(self,population,fitness):        '''        SelectapairofparentusingFPSalgorithm.        '''        #Normalizefitnessvaluesforallindividuals.        fit=[fitness(indv)forindvinpopulation.individuals]        min_fit=min(fit)        fit=[(i-min_fit)foriinfit]         #Createroulettewheel.        sum_fit=sum(fit)        wheel=list(accumulate([i/sum_fitforiinfit]))         #Selectafatherandamother.        father_idx=bisect_right(wheel,random())        father=population[father_idx]        mother_idx=(father_idx+1)%len(wheel)        mother=population[mother_idx]         returnfather,mother过程主要分为下面几个:继承GASelection类实现select方法select的参数为GAPopulation实例和适应度函数根据算法选择出两个需要繁衍的物种并返回即可TournamentSelection由于算法执行的效率以及易实现的的特点，锦标赛选择算法是遗传算法中最流行的选择策略。在本人的实际应用中的确此策略比基本的轮盘赌效果要好些。他的策略也很直观，就是我们再整个种群中抽取n个个体，让他们进行竞争(锦标赛)，抽取其中的最优的个体。参加锦标赛的个体个数成为tournamentsize。通常当n=2便是最常使用的大小，也称作BinaryTournamentSelection.TournamentSelection的优势:更小的复杂度O(n)易并行化处理不易陷入局部最优点不需要对所有的适应度值进行排序处理下图显示了n=3的TournamentSelection的过程:可以开始写成自定义算子在gaft运行了:Pythonfromrandomimportsamplefrom...plugin_interfaces.operators.selectionimportGASelectionclassTournamentSelection(GASelection):def__init__(self,tournament_size=2):'''SelectionoperatorusingTournamentStrategywithtournamentsizeequalstotwobydefault.'''self.tournament_size=tournament_sizedefselect(self,population,fitness):'''SelectapairofparentusingTournamentstrategy.'''#Competitionfunction.complete=lambdacompetitors:max(competitors,key=fitness)#Checkvalidityoftournamentsize.ifself.tournament_size>=len(population):msg='Tournamentsize({})islargerthanpopulationsize({})'raiseValueError(msg.format(self.tournament_size,len(population)))#Pickwinnersoftwogroupsasparent.competitors_1=sample(population.individuals,self.tournament_size)competitors_2=sample(population.individuals,self.tournament_size)father,mother=complete(competitors_1),complete(competitors_2)returnfather,mother123456789101112131415161718192021222324252627282930fromrandomimportsample from...plugin_interfaces.operators.selectionimportGASelection classTournamentSelection(GASelection):    def__init__(self,tournament_size=2):        '''        SelectionoperatorusingTournamentStrategywithtournamentsizeequals        totwobydefault.        '''        self.tournament_size=tournament_size     defselect(self,population,fitness):        '''        SelectapairofparentusingTournamentstrategy.        '''        #Competitionfunction.        complete=lambdacompetitors:max(competitors,key=fitness)         #Checkvalidityoftournamentsize.        ifself.tournament_size>=len(population):            msg='Tournamentsize({})islargerthanpopulationsize({})'            raiseValueError(msg.format(self.tournament_size,len(population)))         #Pickwinnersoftwogroupsasparent.        competitors_1=sample(population.individuals,self.tournament_size)        competitors_2=sample(population.individuals,self.tournament_size)        father,mother=complete(competitors_1),complete(competitors_2)         returnfather,motherLinearRankingSelection下面两个介绍的选择策略都是基于排序的选择策略，上面提到的第一种基本轮盘赌选择算法，有一个缺点，就是如果一个个体的适应度值为0的话，则被选中的概率将会是0,这个个体将不能产生后代。于是我们需要一种基于排序的算法，来给每个个体安排相应的选中概率。在LinearRankingSelection中，种群中的个体首先根据适应度的值进行排序，然后给所有个体赋予一个序号，最好的个体为N,被选中的概率为Pmax,最差的个体序号为1,被选中的概率为Pmin，于是其他的在他们中间的个体的概率便可以根据如下公式得到:实现代码:Pythonfromrandomimportrandomfromitertoolsimportaccumulatefrombisectimportbisect_rightfrom...plugin_interfaces.operators.selectionimportGASelectionclassLinearRankingSelection(GASelection):def__init__(self,pmin=0.1,pmax=0.9):'''SelectionoperatorusingLinearRankingselectionmethod.Reference:BakerJE.Adaptiveselectionmethodsforgeneticalgorithms[C]//ProceedingsofanInternationalConferenceonGeneticAlgorithmsandtheirapplications.1985:101-111.'''#Selectionprobabilitiesfortheworstandbestindividuals.self.pmin,self.pmax=pmin,pmaxdefselect(self,population,fitness):'''Selectapairofparentindividualsusinglinearrankingmethod.'''#Individualnumber.NP=len(population)#Addranktoallindividualsinpopulation.sorted_indvs=sorted(population.individuals,key=fitness,reverse=True)#Assignselectionprobabilitieslinearly.#NOTE:Heretherankibelongsto{1,...,N}p=lambdai:(self.pmin+(self.pmax-self.pmin)*(i-1)/(NP-1))probabilities=[self.pmin]+[p(i)foriinrange(2,NP)]+[self.pmax]#Normalizeprobabilities.psum=sum(probabilities)wheel=list(accumulate([p/psumforpinprobabilities]))#Selectparents.father_idx=bisect_right(wheel,random())father=population[father_idx]mother_idx=(father_idx+1)%len(wheel)mother=population[mother_idx]returnfather,mother123456789101112131415161718192021222324252627282930313233343536373839404142fromrandomimportrandomfromitertoolsimportaccumulatefrombisectimportbisect_right from...plugin_interfaces.operators.selectionimportGASelection classLinearRankingSelection(GASelection):    def__init__(self,pmin=0.1,pmax=0.9):        '''        SelectionoperatorusingLinearRankingselectionmethod.        Reference:BakerJE.Adaptiveselectionmethodsforgenetic        algorithms[C]//ProceedingsofanInternationalConferenceonGenetic        Algorithmsandtheirapplications.1985:101-111.        '''        #Selectionprobabilitiesfortheworstandbestindividuals.        self.pmin,self.pmax=pmin,pmax     defselect(self,population,fitness):        '''        Selectapairofparentindividualsusinglinearrankingmethod.        '''        #Individualnumber.        NP=len(population)        #Addranktoallindividualsinpopulation.        sorted_indvs=sorted(population.individuals,key=fitness,reverse=True)         #Assignselectionprobabilitieslinearly.        #NOTE:Heretherankibelongsto{1,...,N}        p=lambdai:(self.pmin+(self.pmax-self.pmin)*(i-1)/(NP-1))        probabilities=[self.pmin]+[p(i)foriinrange(2,NP)]+[self.pmax]         #Normalizeprobabilities.        psum=sum(probabilities)        wheel=list(accumulate([p/psumforpinprobabilities]))         #Selectparents.        father_idx=bisect_right(wheel,random())        father=population[father_idx]        mother_idx=(father_idx+1)%len(wheel)        mother=population[mother_idx]         returnfather,motherExponentialRankingSelection类似上面的LinearRanking选择策略，这种指数排序便是在确定每个个体的选择概率的时候使用了指数形式的表达式,其中c为底数，满足0<c<1:实现代码:Pythonfromrandomimportrandomfromitertoolsimportaccumulatefrombisectimportbisect_rightfrom...plugin_interfaces.operators.selectionimportGASelectionclassExponentialRankingSelection(GASelection):def__init__(self,base=0.5):'''SelectionoperatorusingExponentialRankingselectionmethod.:parambase:Thebaseofexponent:typebase:floatinrange(0.0,1.0)'''ifnot(0.0<base<1.0):raiseValueError('Thebaseofexponentcmustinrange(0.0,1.0)')self.base=basedefselect(self,population,fitness):'''Selectapairofparentindividualsusingexponentialrankingmethod.'''#Individualnumber.NP=len(population)#NOTE:Heretherankibelongsto{1,...,N}p=lambdai:self.base**(NP-i)probabilities=[p(i)foriinrange(1,NP+1)]#Normalizeprobabilities.psum=sum(probabilities)wheel=list(accumulate([p/psumforpinprobabilities]))#Selectparents.father_idx=bisect_right(wheel,random())father=population[father_idx]mother_idx=(father_idx+1)%len(wheel)mother=population[mother_idx]returnfather,mother1234567891011121314151617181920212223242526272829303132333435fromrandomimportrandomfromitertoolsimportaccumulatefrombisectimportbisect_right from...plugin_interfaces.operators.selectionimportGASelection classExponentialRankingSelection(GASelection):    def__init__(self,base=0.5):        '''        SelectionoperatorusingExponentialRankingselectionmethod.        :parambase:Thebaseofexponent        :typebase:floatinrange(0.0,1.0)        '''        ifnot(0.0<base<1.0):            raiseValueError('Thebaseofexponentcmustinrange(0.0,1.0)')        self.base=base     defselect(self,population,fitness):        '''        Selectapairofparentindividualsusingexponentialrankingmethod.        '''        #Individualnumber.        NP=len(population)        #NOTE:Heretherankibelongsto{1,...,N}        p=lambdai:self.base**(NP-i)        probabilities=[p(i)foriinrange(1,NP+1)]        #Normalizeprobabilities.        psum=sum(probabilities)        wheel=list(accumulate([p/psumforpinprobabilities]))        #Selectparents.        father_idx=bisect_right(wheel,random())        father=population[father_idx]        mother_idx=(father_idx+1)%len(wheel)        mother=population[mother_idx]        returnfather,mother总结本文对于遗传算法中四种不同的选择策略进行了介绍和总结，同时对于本文所写的遗传算法框架的自定义算子接口进行了简要的介绍，针对本文中的选择策略分别根据接口的要求实现了相应的算子，这些算子也作为GAFT框架的内置算子放入到GAFT中，对于使用GAFT的童鞋可以直接拿来使用。参考Shukla,Anupriya,HariMohanPandey,andDeeptiMehrotra.“Comparativereviewofselectiontechniquesingeneticalgorithm.”FuturisticTrendsonComputationalAnalysisandKnowledgeManagement(ABLAZE),2015InternationalConferenceon.IEEE,2015.打赏支持我写出更多好文章，谢谢！打赏作者打赏支持我写出更多好文章，谢谢！1赞2收藏评论关于作者：iPytLab喜欢写程序的计算化学狗，Python/C/C++/Fortran,个人博客http://pytlab.org个人主页·我的文章·22·"], "art_create_time": ["2017/09/19"], "art_title": ["遗传算法中几种不同选择算子及Python实现"], "art_url": ["http://python.jobbole.com/88608/"], "art_img": ["http://pytlab.org/assets/images/blog_img/2017-09-19-%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95%E4%B8%AD%E5%87%A0%E7%A7%8D%E4%B8%8D%E5%90%8C%E9%80%89%E6%8B%A9%E7%AE%97%E5%AD%90%E7%9A%84%E6%AF%94%E8%BE%83/flowchart.png"]},
{"art_content": ["原文出处：LearnPython   在文章：一个Flask应用运行过程剖析中，在一个上下文环境中可以处理请求。如果不考虑在处理请求前后做的一些操作，Flask源码中真正处理请求的是dispatch_request()方法。其源码如下：Pythondefdispatch_request(self):\"\"\"Doestherequestdispatching.MatchestheURLandreturnsthereturnvalueofthevieworerrorhandler.Thisdoesnothavetobearesponseobject.Inordertoconvertthereturnvaluetoaproperresponseobject,call:func:`make_response`.\"\"\"try:endpoint,values=self.match_request()returnself.view_functions[endpoint](**values)exceptHTTPException,e:handler=self.error_handlers.get(e.code)ifhandlerisNone:returnereturnhandler(e)exceptException,e:handler=self.error_handlers.get(500)ifself.debugorhandlerisNone:raisereturnhandler(e)12345678910111213141516171819defdispatch_request(self):    \"\"\"Doestherequestdispatching.  MatchestheURLandreturnsthe    returnvalueofthevieworerrorhandler.  Thisdoesnothaveto    bearesponseobject.  Inordertoconvertthereturnvaluetoa    properresponseobject,call:func:`make_response`.    \"\"\"    try:        endpoint,values=self.match_request()        returnself.view_functions[endpoint](**values)    exceptHTTPException,e:        handler=self.error_handlers.get(e.code)        ifhandlerisNone:            returne        returnhandler(e)    exceptException,e:        handler=self.error_handlers.get(500)        ifself.debugorhandlerisNone:            raise        returnhandler(e)从上面的源码中可以看到，dispatch_request()方法做了如下的工作：对请求的URL进行匹配；如果URL可以匹配，则返回相对应视图函数的结果；如果不可以匹配，则进行错误处理。对于错误的处理，本文暂不做介绍。本文主要对Flask应用的URL模式以及请求处理过程中的URL匹配进行剖析。Flask应用的url_mapFlask应用实例化的时候，会为应用增添一个url_map属性。这个属性是一个Map类，这个类在werkzeug.routing模块中定义，其主要的功能是为了给应用增加一些URL规则，这些URL规则形成一个Map实例的过程中会生成对应的正则表达式，可以进行URL匹配。相关的概念和内容可以参考：Werkzeug库——routing模块。在Flask源码中，它通过两个方法可以很方便地定制应用的URL。这两个方法是：route装饰器和add_url_rule方法。1.add_url_rulePythondefadd_url_rule(self,rule,endpoint,**options):options['endpoint']=endpointoptions.setdefault('methods',('GET',))self.url_map.add(Rule(rule,**options))1234defadd_url_rule(self,rule,endpoint,**options):    options['endpoint']=endpoint    options.setdefault('methods',('GET',))    self.url_map.add(Rule(rule,**options))add_url_rule方法很简单，只要向其传递一条URL规则rule和一个endpoint即可。endpoint一般为和这条URL相关的视图函数的名字，这样处理就可以将URL和视图函数关联起来。除此之外，还可以传递一些关键字参数。调用该方法后，会调用Map实例的add方法，它会将URL规则添加进Map实例中。2.route装饰器为了更加方便、优雅地写应用的URL，Flask实现了一个route装饰器。Pythondefroute(self,rule,**options):defdecorator(f):self.add_url_rule(rule,f.__name__,**options)self.view_functions[f.__name__]=freturnfreturndecorator123456defroute(self,rule,**options):    defdecorator(f):        self.add_url_rule(rule,f.__name__,**options)        self.view_functions[f.__name__]=f        returnf    returndecoratorroute装饰器会装饰一个视图函数。经route装饰的视图函数首先会调用add_url_rule方法，将装饰器中的URL规则添加进Map实例中，视图函数的名字会作为endpoint进行传递。然后在该应用的view_functions中增加endpoint和视图函数的对应关系。这种对应关系可以在请求成功时方便地调用对应的视图函数。3.一个简单的例子我们用一个简单的例子来说明以上过程的实现：Python>>>fromflaskimportFlask>>>app=Flask(__name__)>>>@app.route('/')defindex():return\"Hello,World!\">>>@app.route('/<username>')defuser(username):return\"Hello,%s\"%username>>>@app.route('/page/<int:id>')defpage(id):return\"Thisispage%d\"%id1234567891011>>>fromflaskimportFlask>>>app=Flask(__name__)>>>@app.route('/')    defindex():        return\"Hello,World!\">>>@app.route('/<username>')    defuser(username):        return\"Hello,%s\"%username>>>@app.route('/page/<int:id>')    defpage(id):        return\"Thisispage%d\"%id以上代码，我们创建了一个Flask应用app，并且通过route装饰器的形式为app增加了3条URL规则。首先：我们看一下Flask应用的url_map长啥样：Python>>>url_map=app.url_map>>>url_mapMap([<Rule'/'(HEAD,GET)->index>,<Rule'/static/<filename>'->static>,<Rule'/page/<id>'(HEAD,GET)->page>,<Rule'/<username>'(HEAD,GET)->user>])1234567>>>url_map=app.url_map>>>url_mapMap([<Rule'/'(HEAD,GET)->index>,    <Rule'/static/<filename>'->static>,    <Rule'/page/<id>'(HEAD,GET)->page>,    <Rule'/<username>'(HEAD,GET)->user>    ])可以看到，url_map是一个Map实例，这个实例中包含4个Rule实例，分别对应4条URL规则，其中/static/<filename>在Flask应用实例化时会自动添加，其余3条是用户创建的。整个Map类便构成了Flask应用app的URL“地图”，可以用作URL匹配的依据。接下来：我们看一下url_map中的一个属性：_rules_by_endpoint：Python>>>rules_by_endpoint=url_map._rules_by_endpoint>>>rules_by_endpoint{'index':[<Rule'/'(HEAD,GET)->index>],'page':[<Rule'/page/<id>'(HEAD,GET)->page>],'static':[<Rule'/static/<filename>'->static>],'user':[<Rule'/<username>'(HEAD,GET)->user>]}1234567>>>rules_by_endpoint=url_map._rules_by_endpoint>>>rules_by_endpoint{'index':[<Rule'/'(HEAD,GET)->index>],'page':[<Rule'/page/<id>'(HEAD,GET)->page>],'static':[<Rule'/static/<filename>'->static>],'user':[<Rule'/<username>'(HEAD,GET)->user>]}可以看出，_rules_by_endpoint属性是一个字典，反映了endpoint和URL规则的对应关系。由于用route装饰器创建URL规则时，会将视图函数的名字作为endpoint进行传递，所以以上字典的内容也反映了视图函数和URL规则的对应关系。再接下来：我们看一下Flask应用的view_functions：Python>>>view_functions=app.view_functions>>>view_functions{'index':<function__main__.index>,'page':<function__main__.page>,'user':<function__main__.user>}123456>>>view_functions=app.view_functions>>>view_functions{'index':<function__main__.index>,'page':<function__main__.page>,'user':<function__main__.user>}在用route装饰器创建URL规则时，它还会做一件事情：self.view_functions[f.__name__]=f。这样做是将函数名和视图函数的对应关系放在Flask应用的view_functions。由于Map实例中存储了函数名和URL规则的对应关系，这样只要在匹配URL规则时，如果匹配成功，只要返回一个函数名，那么便可以在view_functions中运行对应的视图函数。最后：我们看一下URL如何和Map实例中的URL规则进行匹配。我们以/page/<int:id>这条规则为例：Python>>>rule=url_map._rules[2]>>>rule<Rule'/page/<id>'(HEAD,GET)->page>>>>rule._regexre.compile(ur'^\\|\\/page\\/(?P<id>\\d+)$',re.UNICODE)>>>rule._regex.patternu'^\\\\|\\\\/page\\\\/(?P<id>\\\\d+)$'1234567>>>rule=url_map._rules[2]>>>rule<Rule'/page/<id>'(HEAD,GET)->page>>>>rule._regexre.compile(ur'^\\|\\/page\\/(?P<id>\\d+)$',re.UNICODE)>>>rule._regex.patternu'^\\\\|\\\\/page\\\\/(?P<id>\\\\d+)$'可以看到，在将一条URL规则的实例Rule添加进Map实例的时候，会为这个Rule生成一个正则表达式的属性_regex。这样当这个Flask应用处理请求时，实际上会将请求中的url和Flask应用中每一条URL规则的正则表达式进行匹配。如果匹配成功，则会返回endpoint和一些参数，返回的endpoint可以用来在view_functions找到对应的视图函数，返回的参数可以传递给视图函数。具体的过程就是：Pythontry:#match_request()可以进行URL匹配endpoint,values=self.match_request()returnself.view_functions[endpoint](**values)...12345try:    #match_request()可以进行URL匹配    endpoint,values=self.match_request()    returnself.view_functions[endpoint](**values)    ...1赞2收藏评论"], "art_create_time": ["2017/09/20"], "art_title": ["Flask 应用中的 URL 处理"], "art_url": ["http://python.jobbole.com/88618/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2014/12/6da94dec8f6f96417f14c8291e6345801.png"]},
{"art_content": ["本文由伯乐在线-逆旅翻译，黄利民校稿。未经许可，禁止转载！英文出处：iluxonchik。欢迎加入翻译组。引言第一次接触Python是在一节编程入门课上。其实，在此之前了解过它，所以在上课之前我对它的语法已经很熟悉了，但在上课之前我没有用它做过真正的项目。尽管对它没有太大兴趣，但我认为把它介绍给人们去学习编程还是很好的。我对它不是不喜欢，而是一种“无所谓”的态度。原因很简单：它里面有太多“魔法”。C和Java这些语言，对底层的行为描述的很清晰，Python则完全相反。另外，Python结构松散：写大型复杂程序时，遇到规则严谨的程序结构体（比如每个文件一个公共类），比其他语言（比如Java）要费些力气。但是，在这些方面Python给了你很大的自由。另一件事是严格的编码风格和调试：因为Python是解释型语言，查找问题不太容易：如果C语言有语法错误，编译器会直接停止编译，但在解释型语言中，直到执行到问题行，问题才会被发现。试着在需要整数的时候传一个字符串？cc会马上提醒你，Python解释器却对此一点都不介意（虽然有工具可以发现这个问题，比如mypy，但我讨论的是通用的Python）。我提到的这些问题是解释型语言的通病，并非Python独有，但这些是我不喜欢它的主要原因。还有一个烦人的问题是强制缩进。我们老师（很优秀）认为这是好事情，因为“它强制我们形成简洁的代码风格”。确实如此，但还是有点烦，当代码没有按预期执行时，你分析代码想要找出bug，它却无影无踪，过了很长时间之后你发现if语句那一行有一个多余的空格。我曾经和同事聊过Python，告诉他为什么我之前对这个语言不感冒，他笑着问我“问什么不喜欢Python呢？因为它读起来很像英语？”。是的。因为这个语言做了很多底层的工作，有时候会不清楚发生了什么。举个读文件的例子，假设你想一行一行读取文件内容并打印出来。C会这么做：C#include<stdio>intmain(void){FILE*fp;charbuff[256];//assumingalinewon'tcontainmorethan256charsfp=fopen(\"hello.txt\",\"r\");while(fgets(buff,256,fp)){printf(\"%s\",buff);}fclose(fp);return0;}1234567891011121314#include<stdio> intmain(void){    FILE*fp;    charbuff[256];//assumingalinewon'tcontainmorethan256chars    fp=fopen(\"hello.txt\",\"r\");     while(fgets(buff,256,fp)){        printf(\"%s\",buff);    }     fclose(fp);    return0;}python这么做：Pythonwithopen('hello.txt')asf:forlineinf:print(line)123withopen('hello.txt')asf:    forlineinf:        print(line)现在，很多人会认为这是python的优势，然而，第一个例子中，干了什么一目了然：获取一个文件指针从文件读取每一行数据到缓存中，打印缓存中的内容关闭文件流python的例子中看不到这些，它是一种“魔法般的”过程。现在，有人认为这是好事，因为将程序员与底层实现细节隔离（我同意这个说法），但我想知道到底发生了什么。有趣的是，我以上提到的缺点，我现在认为都是优点。为了公平起见，我强调，Python里边没有魔法，如果你多了解一点，你会发现真的没有，有的只是语言解释代码的方式，从这点来看，我发现它挺有意思的。如果你也这么觉得，我建议你深入了解它的工作机制，如果有东西像魔法，就找出来到底发生了什么，事情就会变得清晰，魔法就变成了便利。我的认识发生很大的变化，尤其是我决定使用Python后，事实上我现在是Python的死忠！现在你也许会想我将会在哪里说服你学Python是个好主意，不要担心，马上就到。作为引言的结尾，我想说明，这只是我对这个语言的个人感受，只是个人偏好。我没有试图以“如果你用Python，你就不是真正的程序员（实际上，我不这么认为）”的理由劝说人们学C。当有人问我他们的入门语言应该选哪个，我通常建议他们选Python，基于我上边提到的“缺点”的原因。我的感觉来源于我的兴趣，我曾经在做一些很底层的东西，你能想到，Python并不适用。Python语言精粹在借用了JavaScript畅销书《JavaScript语言精粹》作为本节标题后，我们开始讨论本文的主题：为什么你(没错，就是你！)应该学Python。1、通用脚本语言这是我使用Python的主要原因。我曾经和很多人做过很多项目，不同的人用不同的系统。就我而言，我经常在windows系统和linux系统之间切换。举一个实际的例子，有一个项目，我写了项目的自动测试脚本，结果发现只有我能用，因为是用PowerShell写的，而我是项目中唯一使用Windows的。当时同事们自然认为bash是最好的，我还向他们解释PowerShell遵循一种不同的模式并且有它的强项（例如，它提供了.NET框架接口），它是面向对象的脚本语言，和bash完全不一样。现在我不想讨论哪个更好，因为这不是本文的重点。那么这个问题怎么解决呢？嗯…现在，是否有一种脚本语言可以在所有主流平台上运行呢？你猜对了，它就是Python。除了可以在主流平台上运行，它还是开箱即用的脚本语言。标准库包含不少实用程序，提供了独立于系统的常用接口。举一个简洁明了的例子，假设你想获取文件夹下所有文件的文件名，然后对其进行处理，在UNIX下，你要这么做：forfin*;doecho\"Processing$ffile...\";done1forfin*;doecho\"Processing$ffile...\";done用PowerShell做类似的事情：Get-ChildItem\".\"|Foreach-Object{$name=$_.NameWrite-Output\"Processing$($name)file...\"}12345Get-ChildItem\".\"|Foreach-Object{    $name=$_.Name    Write-Output\"Processing$($name)file...\"}AnequivalentfunctionalityinPythoncanbeachievedwith:python这么做：fromosimportlistdirforfinlistdir('.'):print('Processing{}file...'.format(f))1234fromosimportlistdir forfinlistdir('.'):    print('Processing{}file...'.format(f))现在我认为，Python除了可以跑在Linux，MacOSX和Windows上，它也很易读。上边例子中的脚本很简单，在复杂的例子中不同语言的易读性差异会更明显。就像我之前提到的，Python自带了许多强大的库用来取代shell脚本，你会发现，最有用的是：os–提供系统无关功能，比如文件目录和文件读写。subprocess–产生新进程、与输入输出流和返回代码交互。可以用它来启动系统已安装的程序，但请记住如果你担心脚本的可移植性，这不是最好的选择。shutil–提供对文件和文件集合的高级操作。argparse–解析命令行参数，构建命令行接口。好了，假设你get到了重点，跨平台和易读性听起来挺不错的，但是你真的喜欢类UNIXshell类似的语法怎么办？告诉你个好消息，鱼和熊掌可以兼得！看看Plumbum，它是一个Python模块，它的座右铭是“再也不写shell脚本”。它模仿了shell语法，同时保持了跨平台。不要完全抛弃shell脚本即使Python可以完全取代shell脚本，但也不是必须这么做，因为Python脚本天生适合Unix命令行理念，你要做的就是让它们从sys.stdin(标准输入)读数据，向sys.stdout(标准输出)写数据。举个例子，假设你有一个文件，每行有一个单词，你想知道每个单词在文中出现的次数。这种情况就没必要全部是用Python，我们可以使用cat命令和我们的脚本，称它为namecount.py一起来完成这个任务。假设有一个文件，名为names.txt，内容如下：catdogmousebirdcatcatdog1234567catdogmousebirdcatcatdog现在使用我们的脚本：$>catnames.txt|namecount.py1$>catnames.txt|namecount.pyPowershell:$>Get-Contentnames.txt|pythonnamecount.py1$>Get-Contentnames.txt|pythonnamecount.py期望的输出如下（顺序可能会变化）：bird1mouse1cat3dog21234bird1mouse1cat3dog2namecount.py源码:#!/usr/bin/envpython3importsysdefcount_names():names={}fornameinsys.stdin.readlines():name=name.strip()ifnameinnames:names[name]+=1else:names[name]=1forname,countinnames.items():sys.stdout.write(\"{0}\\t{1}\\n\".format(name,count))if__name__==\"__main__\":count_names()123456789101112131415161718#!/usr/bin/envpython3importsys defcount_names():    names={}    fornameinsys.stdin.readlines():        name=name.strip()         ifnameinnames:            names[name]+=1        else:            names[name]=1                forname,countinnames.items():        sys.stdout.write(\"{0}\\t{1}\\n\".format(name,count)) if__name__==\"__main__\":    count_names()无序的信息可读性差，你可能想按单词出现的次数对其排序，让我们试试。我们要用管道输出文件内容供内建命令处理。按数字降序排序，我们要做的就是$>catnames.txt|namecount.py|sort-rn。如果使用PowerShell应该这样：$>Get-Contentnames.txt|pythonnamecount.py|Sort-Object{[int]$_.split()[-1]}-Descending（你可能听到了Unixer的吐槽声了，PowerShell怎么这么繁琐）。这回我们的输出是确定的，如下所示：cat3dog2bird1mouse11234cat3dog2bird1mouse1（旁注：如果你用PowerShell，cat是Get-Content的别名，sort是Sort_object的别名，所以以上命令可以写成：$>catnames.txt|pythonnamecount.py和$>catnames.txt|pythonnamecount.py|sort{[int]$_.split()[-1]}-Descending）但愿我成功说服你python是你某些脚本的替代品，你不必完全抛弃shell脚本，因为你可以将Python融合到你现有的工作流和工具箱中，还可以从它跨平台，更好的可读性，还有丰富的库中获益（后面会讲）。2、大量优秀的库Python有非常丰富的库。我的意思是，几乎任何事都有库（有趣的是：如果你在你的Python解释器中输入importantigravity，在浏览器中打开xkdc漫画的页面，是不是很酷？）。我不是很推崇堆叠模块式的编程，但你不必这样。因为有太多的库，不表示你都要使用。我也不喜欢堆叠模块（它有点像CBSE），我在了解它们之后才使用。例如，我决定研究马尔科夫链，我想了一个项目：抓取一个艺术家的所有歌词，建立一个马尔科夫链，然后从其中生成歌曲。这个项目的目的是生成的歌曲应该能反映出艺术家的风格。所以我到处找相关的东西，搞出了lyricst项目（这只是个样品，还不成熟，只是一个测试项目，如我所言，我只是随便搞了一下，没想深入。如果你想玩的话，它包含有命令行界面和示例的说明文档）。我认为，最好的找歌词的地方是RAPGenius，因为它很活跃，经常更新。为了获取艺术家所有的歌词，我必须从网站上爬，然后处理HTML。幸运的是，Python很适合做网络爬虫，它有强大的库像BeautifulSoup可以处理HTML。所以我是这么做的，先使用BeautifulSoup从网页中抽取我需要的信息（就是歌词）然后用这些信息构建马尔科夫链。当然我曾经想用正则表达式构建自己的HTML解析器，但是这个库的存在让我更关注项目的最终目的：把玩马尔科夫链，让它更有趣，比方说，从文件中读取些内容出来。3、用来做渗透测试很强大如果你在作渗透测试或仅仅是喜欢玩玩，Python是你的好帮手！由于Python在所有LInux和MACOS机器上都有安装，还有丰富的库，完善的语法，还是一门脚本语言，让它很适合干这个。另一个我为什么决定使用Python的原因（除了我之前提到的）是我对安全很感兴趣，Python是用来做渗透测试的完美选择。我在第一次进入领域是通过Scapy（或Scapy3k，python3），我印象很深。Scapy能够创建、监听、解析数据包。它的API很简单，文档也很完善。你可以很容易的创建不同层的数据（我指的是OSI模型）或者捕获它们对其进行分析或修改。你甚至可以导出pcap文件用Wireshark打开。虽然除了抓包还能做很多事情，还有很多其他的库也可以，但我在这里不会涉及，因为这不是本文的重点而且要展开讲的话需要一篇文章。有人可能会说，“哦，太棒了，但我感兴趣的是Windows设备，里边不会自带Python”。别当心，你可以用py2exe把你的脚本编译成.exe文件。文件可能会有点大（取决于你是用的库的数量），但这不是重点。如果你很好奇，请参考listofPythonpentestingtools。文末我还推荐了几本书。4、黑客的语言Python是可塑性很强的语言。你可以用各种方法改造它。可参见《alteringthewayimportswork》和《messingwithclassesbeforetheyarecreated》这两篇文章。这只是一些例子。也让它成为强大的脚本语言（在第一节有说）适合做渗透测试（第三节），因为它给了你很大的自由。我不想讲太多，但我会讲述它让我惊讶的地方。当时，我在做一个网络爬虫(Python很适合干这个！)，我用的其中一个工具是BeautifulSoup。这是我用来学习Python的项目之一。Beautifulsoup处理HTML的语法清晰直观，原因是在自定义行为方面，Python给了你很大的自由。了解一番API后，发现有“魔法”。和这种情况类似：Pythonfrombs4importBeautifulSoupsoup=BeautifulSoup('<pclass=\"someclass\">Hello</p>','html.parser')soup.p1234frombs4importBeautifulSoup soup=BeautifulSoup('<pclass=\"someclass\">Hello</p>','html.parser')soup.p上面的代码利用第一个字符串参数创建了一个BeautifulsSoup实例，第二个参数表示我想使用Python自带的HTML解析器（BeautifulSoup可以搭配多种解析器）。soup.p返回一个Tag(bs4.element.Tag)对象，表示将作为第一个参数。以上代码的输出是：XHTML<pclass=\"someclass\">Hello</p>1<pclass=\"someclass\">Hello</p>现在你可能会想，你说的魔法在哪？马上就来。魔法在于上面的代码可以被修改为任何标签，甚至可以是自定义的。它意味着下面的代码也可以正常运行：Pythonfrombs4importBeautifulSoupsoup=BeautifulSoup('<foobarfooclass=\"someclass\">Hello</foobarfoo>','html.parser')soup.foobarfoo1234frombs4importBeautifulSoupsoup=BeautifulSoup('<foobarfooclass=\"someclass\">Hello</foobarfoo>','html.parser')soup.foobarfooTheoutputisthefollowing:输出如下：<foobarfooclass=\"someclass\">Hello</foobarfoo>1<foobarfooclass=\"someclass\">Hello</foobarfoo>当我发现这样也能运行，我的反应是“怎么回事？”。因为，第一个例子很容易实现，我的意思是最直接的方法是为每一个HTML标签定义一个属性（实例变量），在解析过程中如果找到了，就赋值给它们。但是这对第二种情况不适用，不可能对所有的字符串定义属性。我想知道它是怎么实现的，所以我打开BeautifulSoups源代码开始寻找。我没有发现任何命名为p的属性，这一点也不奇怪，解析函数没有对其赋值。谷歌一番后，我找到了答案：魔法方法。什么是魔法方法，为什么要叫这个名字？事实上，魔法方法是给你的类赋予魔法的方法。这种方法通常前后有两条下划线（例如__init__()）,在Python文档的DataModelmodelsection有对它的说明。真正让BeautifulSoup拥有这个功能的魔法方法是__getattr__(self,name)（self在python中指向实例，和Java中的this类似）。如果去查看文档，你会发现第一段如下：如果在属性常见地方找不到属性时，比如既不是实例属性，又没有在self类树中找到，则调用该方法（object.__getattr__(self,name)）。参数name就是属性名这个方法应当返回（计算过的）属性值或抛出AttributeError异常。当你尝试访问一个不存在的属性，对象的__getattr__(self,name)方法会被调用，将返回一个以name作为名字的属性的字符串。举个例子。假设你有一个Person类，拥有first_name属性。我们给使用者访问和name相同属性的内容的能力。下面是代码：classPerson(object):def__init__(self,first_name):self.first_name=first_namedef__getattr__(self,name):if(name=='name'):returnself.first_nameraiseAttributeError('Personobjecthasnoattribute\\'{}\\''.format(name))12345678classPerson(object):    def__init__(self,first_name):        self.first_name=first_name     def__getattr__(self,name):        if(name=='name'):            returnself.first_name        raiseAttributeError('Personobjecthasnoattribute\\'{}\\''.format(name))我们在终端运行代码：person=Person('Jason')>>>person.first_name'Jason'>>>person.name'Jason'>>>person.abcTraceback(mostrecentcalllast):File\"<stdin>\",line1,in<module>File\"<stdin>\",line7,in__getattr__AttributeError:Personobjecthasnoattribute'abc'123456789101112person=Person('Jason')>>>person.first_name'Jason' >>>person.name'Jason' >>>person.abcTraceback(mostrecentcalllast):  File\"<stdin>\",line1,in<module>  File\"<stdin>\",line7,in__getattr__AttributeError:Personobjecthasnoattribute'abc'这意味着我们能凭空构造实例属性，是不是很棒？所以你可以偷偷的让你的Dog除了汪汪叫之外，还会喵喵叫：classDog(object):defbark(self):print('Ruff,ruff!')def__getattr__(self,name):if(name=='meow'):returnlambda:print('Meeeeeeow')raiseAttributeError('Idon\\'tknowwhatyou\\'retalkingabout...')12345678classDog(object):    defbark(self):        print('Ruff,ruff!')        def__getattr__(self,name):        if(name=='meow'):          returnlambda:  print('Meeeeeeow')        raiseAttributeError('Idon\\'tknowwhatyou\\'retalkingabout...')>>>snoop=Dog()>>>snoop.bark()Ruff,ruff!>>>snoop.meow()Meeeeeeow1234567>>>snoop=Dog() >>>snoop.bark()Ruff,ruff! >>>snoop.meow()Meeeeeeow你可以在没有reflection的情况下，随意添加新属性。object.__dict__是（字典）[https://docs.python.org/3.5/library/stdtypes.html#typesmapping]包含object的属性和它们的值（注意我说的是object.dict,object是一个实例，还有一个class.dict,是类的属性的字典)。意思是：classDog(object):def__init__(self):self.name='DoggyDogg'1234classDog(object):        def__init__(self):        self.name='DoggyDogg'等价于：classDog(object):def__init__(self):self.__dict__['name']='DoggyDogg'1234classDog(object):        def__init__(self):        self.__dict__['name']='DoggyDogg'两者输出是一样的：snoop=Dog()>>>snoop.name'DoggyDogg'1234snoop=Dog() >>>snoop.name'DoggyDogg'到这里你会想，是挺好的，但是有什么用呢？答案很简单：magicalAPIs。你有没有用过一些Python库让你感觉像魔法？这是让它们变的有”魔法”的一种情况。虽然一旦你懂了底层发生的事情,就会发现没有魔法。如果你还想了解更多，可以查看文档中的DescriptionProtocol。Python的面向对象Python的面向对象有点奇怪。例如，类中没有私有变量和方法。所以你想在类中创建一个实例变量或私有方法，你必须遵守规则：一个下划线 (_)表示私有变量和方法。两个下划线(__) 表示的变量和方法，它们的名字会被修改。举个例子，假设你有如下类：classFoo(object):def__init__(self):self.public='public'self._private='public'self.__secret='secret'12345classFoo(object):    def__init__(self):        self.public='public'        self._private='public'        self.__secret='secret'转到解释器：>>>foo=Foo()>>>foo.public'public'>>>foo._private'public'>>>foo.__secretTraceback(mostrecentcalllast):File\"<stdin>\",line1,in<module>AttributeError:'Foo'objecthasnoattribute'__secret'123456789>>>foo=Foo()>>>foo.public'public'>>>foo._private'public'>>>foo.__secretTraceback(mostrecentcalllast):  File\"<stdin>\",line1,in<module>AttributeError:'Foo'objecthasnoattribute'__secret'如你所见，你可以访问_private变量，但是最后一个例子发生了什么，它是否意味着有两个下划线的变量是真正的私有变量？答案是NO，它们的名字被改变了，实际上，它被Python替换成了_Foo_secret。如果你想访问的话，你仍然可以访问：>>>foo._Foo__secret'secret'12>>>foo._Foo__secret'secret'然而，PEP8建议只在父类中使用双下划线来避免属性名冲突。“PEP”，表示“PythonEnhancementProposal”，它用来描述Python特性或作用。如果你想要添加一个新特性，你可以创建一个PEP，这样可以让整个社区可以看到并讨论。你可以在这里了解更多的PEPs。可见，Python很信任程序员。我不会再深入讲OO了，因为它需要单独一篇文章（甚至是一系列）来讲解。我确实想给你提个醒，Python的OO可不像Java语言那么自然，你需要慢慢适应，但你知道吗，它只是做事的方法不同而已。举个例子，它没有抽象类，你必须使用装饰器来实现这个行为。结语希望这篇文章，能够给你一个学习Python的理由。这篇文章来自一个为过去说了Python的坏话而愧疚，如今在到处宣传Python的人。我先申明一点，这只是个人喜好问题，当有人问我先学哪门语言时，我通常推荐Python。如果你还没决定，那就给它一次机会！用上一两个小时，多读些关于它的东西。如果你喜欢从书上学习，我也会帮你，看看《FluentPython》，下节还有更多。书籍推荐我兑现了诺言，这一节推荐书籍。我会尽量保持简短一些，只包含一些我读过的书籍。《FluentPython》——一本讲Python3的好书。无论你是新手、熟手还是高手都值得一读。包含了Python的来龙去脉。《WebScrapingWithPython》——标题已经说明了一切，讲如何用Python来做网络爬虫。你会探索如何爬网上的内容，解析HTML等。我觉得这本书对爬虫领域的新手和熟手很有帮助。即使你之前从没用过Python，你也可以看懂。它没有涉及任何高级主题。《BlackHatPython》——这个有趣！你可以创建反弹SSHshell，木马等等！如果你想知道Python如何做渗透测试，请一定要读它。注意它使用的是Python2，我有一个仓库，用的是Python3。《ViolentPython:ACookbookforHackers,ForensicAnalysts,PenetrationTestersandSecurityEngineers》——比上面的主题要多，你会学到如何写一个常见的用于实战的渗透测试，取证分析和安全脚本。打赏支持我翻译更多好文章，谢谢！打赏译者打赏支持我翻译更多好文章，谢谢！2赞11收藏5评论关于作者：逆旅aprogrammer,Doone'slevelbestandleavetheresttoGod'swill.Ilovethissaying.个人主页·我的文章·17"], "art_create_time": ["2017/09/20"], "art_title": ["为什么你应该学 Python ？"], "art_url": ["http://python.jobbole.com/88622/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2015/02/86a1298ce44ca9807871520b81767e6f.gif"]},
{"art_content": ["原文出处：LearnPython   SocketServer是Python标准库中的一个模块，其作用是创建网络服务器。SocketServer模块定义了一些类来处理诸如TCP、UDP、UNIX流和UNIX数据报之上的同步网络请求。SocketServer模块处理网络请求的功能，可以通过两个主要的类来实现：一个是服务器类，一个是请求处理类。服务器类处理通信问题，如监听一个套接字并接收连接等；请求处理类处理“协议”问题，如解释到来的数据、处理数据并把数据发回给客户端等。这种实现将服务器的实现过程和请求处理的实现过程解耦，这意味着我们可以将不同的服务器实现和请求处理实现结合起来来处理一些定制的协议，例如一个TCP服务器类和一个流请求处理类结合，处理基于TCP的网络请求。同时，也可以基于SocketServer模块中的服务器类和请求处理类，实现网络层之上应用层的服务器和请求处理实现，例如基于TCP服务器类实现HTTP服务器，基于流处理请求类实现HTTP请求处理类等。服务器类SocketServer模块中定义了五种服务器类。BaseServer(服务器的基类，定义了API)TCPServer(使用TCP/IP套接字)UDPServer(使用数据报套接字)UnixStreamServer(使用UNIX域套接字，只适用UNIX平台)UnixDatagramServer(使用UNIX域套接字，只适用UNIX平台)1.构造服务器对象要构建一个服务器对象，需要向它传递一个地址server_address（服务器将在这个地址上监听请求），以及一个请求处理类RequestHandlerClass（不是请求处理实例）。服务器类基类的构造函数如下：PythonclassBaseServer:def__init__(self,server_address,RequestHandlerClass):\"\"\"Constructor.Maybeextended,donotoverride.\"\"\"self.server_address=server_addressself.RequestHandlerClass=RequestHandlerClassself.__is_shut_down=threading.Event()self.__shutdown_request=False1234567classBaseServer:    def__init__(self,server_address,RequestHandlerClass):        \"\"\"Constructor.  Maybeextended,donotoverride.\"\"\"        self.server_address=server_address        self.RequestHandlerClass=RequestHandlerClass        self.__is_shut_down=threading.Event()        self.__shutdown_request=False之后，可以构造TCPServer、UDPServer、UnixStreamServer、UnixDatagramServer。其中，TCPServer继承自BaseServer，UDPServer和UnixStreamServer继承自TCPServer，UnixDatagramServer继承自UDPServer。各个服务器类型可以根据自己的特点对基类进行扩展，例如创建监听套接字、绑定监听地址和端口、进行监听等。一旦实例化服务器对象，便可以使用服务器的方法来监听和处理请求。2.实现服务器由于SocketServer模块中定义的五种服务器类中，除了基类BaseServer和TCPServer外，其余的三个类都是直接或间接地继承自TCPServer。因此，以下以TCPServer的实现过程为例进行说明。构造TCPServer。构造TCPServer时，构造函数创建了一个套接字（这个套接字可以通过更改地址簇和类型用于其他服务器）用于监听请求。并且调用server_bind()绑定监听的地址和端口，调用server_activate()开始监听。启动服务器。服务器实例化后，可以使用serve_forever()或者handle_request()来监听和处理请求，实现服务器功能。这两个方法的具体实现依赖于_handle_request_noblock()方法。这个方法是BaseServer类中定义的。具体实现如下：Pythondef_handle_request_noblock(self):\"\"\"Handleonerequest,withoutblocking.Iassumethatselect.selecthasreturnedthatthesocketisreadablebeforethisfunctionwascalled,sothereshouldbenoriskofblockinginget_request().\"\"\"try:request,client_address=self.get_request()exceptsocket.error:returnifself.verify_request(request,client_address):try:self.process_request(request,client_address)except:self.handle_error(request,client_address)self.shutdown_request(request)else:self.shutdown_request(request)123456789101112131415161718def_handle_request_noblock(self):    \"\"\"Handleonerequest,withoutblocking.    Iassumethatselect.selecthasreturnedthatthesocketis    readablebeforethisfunctionwascalled,sothereshouldbe    noriskofblockinginget_request().    \"\"\"    try:        request,client_address=self.get_request()    exceptsocket.error:        return    ifself.verify_request(request,client_address):        try:            self.process_request(request,client_address)        except:            self.handle_error(request,client_address)            self.shutdown_request(request)    else:        self.shutdown_request(request)处理请求。根据上一步骤启动服务器后，服务器便开始监听请求。如果接收到请求信息，便开始处理请求。由_handle_request_noblock()可以看出有几个函数比较重要。get_request()——这个函数可以在子类中重写。在TCPServer中，该函数调用监听套接字的accept()方法，返回请求request和客户端地址client_address。verify_request(request,client_address)——这个函数可以在子类中重写。该函数返回True表示处理请求，返回False表示忽略请求。process_request(request,client_address)——这个函数可以在子类中重写。该函数将调用finish_request()具体完成请求的处理过程，并且在处理完请求后关闭请求。finish_request(request,client_address)——该函数将构造一个请求处理类的实例。请求处理类被实例化后将调用其handle()方法处理请求。3.进程/线程支持SocketServer模块中还提供了一些”mix-in”类：ForkingMixIn和ThreadingMixIn。这些类可以和服务器类混合使用，很容易改变服务器，为每个请求使用一个单独的进程或线程。具体的服务器类有：classForkingUDPServer(ForkingMixIn,UDPServer)classForkingTCPServer(ForkingMixIn,TCPServer)classThreadingUDPServer(ThreadingMixIn,UDPServer)classThreadingTCPServer(ThreadingMixIn,TCPServer)classThreadingUnixStreamServer(ThreadingMixIn,UnixStreamServer)classThreadingUnixDatagramServer(ThreadingMixIn,UnixDatagramServer)请求处理类要接收到来的请求以及确定采取什么行动，其中大部分的工作都是由请求处理类完成的。请求处理类负责在套接字层之上实现协议。具体过程为：读取请求、处理请求、写回响应。请求处理类基类中定义了3个方法，子类中需要重写。setup()——为请求准备请求处理器handle()——对请求完成具体的工作。诸如解析到来的请求，处理数据，并发回响应等。finish()——清理setup()期间创建的所有数据1赞5收藏评论"], "art_create_time": ["2017/09/20"], "art_title": ["SocketServer ——网络通信服务器"], "art_url": ["http://python.jobbole.com/88626/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2015/02/2c224095b3bd5c6fee7c908a481292f7.jpg"]},
{"art_content": ["原文出处：LearnPython   在SocketServer——网络通信服务器中我们介绍了Python标准库中的SocketServer模块，了解了要实现网络通信服务，就要构建一个服务器类和请求处理类。同时，该模块还为我们创建了不同的服务器类和请求处理类。1.服务器类BaseServerTCPServer(BaseServer)UDPServer(TCPServer)UnixStreamServerUnixDatagramServer2.请求处理类BaseRequestHandlerStreamRequestHandler(BaseRequestHandler)DatagramRequestHandler(BaseRequestHandler)通过服务器类和请求处理类的搭配，我们可以创建不同类型的服务器，实现不同的协议类型。本文介绍的BaseHTTPServer模块便是继承TCPServer和StreamRequestHandler，实现了Web服务器的通信。HTTP服务器HTTP服务器继承自SocketServer模块中的TCPServer类。它的定义非常简单，只是重写了其中的一个方法。PythonclassHTTPServer(SocketServer.TCPServer):allow_reuse_address=1#Seemstomakesenseintestingenvironmentdefserver_bind(self):\"\"\"Overrideserver_bindtostoretheservername.\"\"\"SocketServer.TCPServer.server_bind(self)host,port=self.socket.getsockname()[:2]self.server_name=socket.getfqdn(host)self.server_port=port12345678classHTTPServer(SocketServer.TCPServer):    allow_reuse_address=1    #Seemstomakesenseintestingenvironment    defserver_bind(self):        \"\"\"Overrideserver_bindtostoretheservername.\"\"\"        SocketServer.TCPServer.server_bind(self)        host,port=self.socket.getsockname()[:2]        self.server_name=socket.getfqdn(host)        self.server_port=port重写的server_bind()方法主要是为了获取服务器名和端口。其余方法以及服务器的实现过程详见SocketServer——网络通信服务器。此外，还可以从SocketServer模块中引入’mix-in’类，基于HTTPServer创建支持进程或线程的服务器。HTTP请求处理基类为了处理HTTP请求，BaseHTTPServer模块构造了HTTP请求处理基类BaseHTTPRequestHandler，它继承自SocketServer模块中的StreamRequestHandler类。HTTP请求处理基类中有一些重要的方法：1.handle()——这个方法是请求处理类真正处理请求具体工作的方法，例如解析到来的请求，处理数据，并发回响应等。在BaseHTTPRequestHandler中它是一个入口文件，将调用其他的方法完成请求处理。2.handle_one_request()——由handle()调用，用于处理请求。其主要工作包括：调用parse_request()方法，解析请求，获取请求报文中的信息，包括请求的方法、请求URL、请求的HTTP版本号、请求首部等。如果解析失败，则调用send_error()方法发回一个错误响应。调用do_SPAM()方法。这个方法中的SPAM指代GET、POST、HEAD等请求方法，需要在请求处理类中构建具体的请求处理方法，例如do_GET处理GET请求，do_POST处理POST请求。do_SPAM()方法可以调用send_response()、send_header()、end_headers()等方法创建响应首行和响应首部等内容。3.parse_request()——解析请求。4.send_error()——发回错误响应。5.send_response()——创建响应首行和响应首部等内容。6.send_header()——设置响应首部内容。7.end_headers()——调用此方法可以在首部后增加一个空行，表示首部内容结束（不适用于HTTP/0.9）8.还包括其他的一些辅助函数。需要注意的是：BaseHTTPRequestHandler是HTTP请求处理的基类，并不包含诸如do_GET、do_POST等方法，其他继承该类的请求处理类需要自己实现这些方法，已完成对具体请求的处理。对此，可以参考SimpleHTTPServer模块，也可查看文章SimpleHTTPServer——一个简单的HTTP服务器。1赞5收藏评论"], "art_create_time": ["2017/09/20"], "art_title": ["BaseHTTPServer ——实现 Web 服务器"], "art_url": ["http://python.jobbole.com/88629/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2015/10/bfa0d07e7eb2fac2eb80cd5df9567931.jpg"]},
{"art_content": ["原文出处：LearnPython   Python标准库中的BaseHTTPServer模块实现了一个基础的HTTP服务器基类和HTTP请求处理类。这在文章BaseHTTPServer——实现Web服务器中进行了相关的介绍。然而，BaseHTTPServer模块中并没有定义相关的请求方法，诸如GET、HEAD、POST等。在BaseHTTPServer模块的基础上，Python标准库中的SimpleHTTPServer模块实现了简单的GET、HEAD请求。在该模块中，它沿用了BaseHTTPServer模块中实现的HTTPServer服务器，这里就不再赘述。而请求处理类则是继承了BaseHTTPServer模块中的BaseHTTPRequestHandler类。SimpleHTTPServer模块实现了具有GET、HEAD请求方法的HTTP通信服务。根据文章BaseHTTPServer——实现Web服务器中的介绍，只需要在请求处理类中定义do_GET()和do_HEAD()方法即可。do_GET()do_GET()方法的源码如下：Pythondefdo_GET(self):\"\"\"ServeaGETrequest.\"\"\"f=self.send_head()iff:try:self.copyfile(f,self.wfile)finally:f.close()12345678defdo_GET(self):    \"\"\"ServeaGETrequest.\"\"\"    f=self.send_head()    iff:        try:            self.copyfile(f,self.wfile)        finally:            f.close()在这个方法中，它调用了send_head()方法来返回一个响应。send_head()方法会调用send_response()、send_header()、send_error()方法等设置响应报文等。do_HEAD()do_HEAD()方法的源码如下：Pythondefdo_HEAD(self):\"\"\"ServeaHEADrequest.\"\"\"f=self.send_head()iff:f.close()12345defdo_HEAD(self):    \"\"\"ServeaHEADrequest.\"\"\"    f=self.send_head()    iff:        f.close()do_HEAD()方法和do_GET()方法的实现类似。测试例子SimpleHTTPServer模块还提供了一个测试函数。只需要在命令行中运行如下代码：PythonpythonSimpleHTTPServer.py#SimpleHTTPServer.py指代Python标准库中的SimpleHTTPServer模块，注意文件位置。1pythonSimpleHTTPServer.py  #SimpleHTTPServer.py指代Python标准库中的SimpleHTTPServer模块，注意文件位置。如果在本地环境中运行以上代码，将会调用请求处理类的translate_path和list_directory方法展示一个文件目录。然后在浏览器中访问127.0.0.1:8000即可查看SimpleHTTPServer.py文件所在目录下的所有文件。1赞5收藏评论"], "art_create_time": ["2017/09/20"], "art_title": ["SimpleHTTPServer ——一个简单的HTTP服务器"], "art_url": ["http://python.jobbole.com/88632/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2015/02/3cccc8c943fe706b4be2fc3d02ba4a1e.jpg"]},
{"art_content": ["原文出处：LearnPython   Flask是当下流行的Web框架，它是用Python实现的。Flask显著的特点是：它是一个“微”框架。”微”意味着Flask旨在保持核心的简单，但同时又易于扩展。默认情况下，Flask不包含数据库抽象层、表单验证，或是其它任何已有多种库可以胜任的功能。然而，Flask支持用扩展来给应用添加这些功能。众多的扩展提供了数据库集成、表单验证、上传处理、各种各样的开放认证技术等功能。Flask的这些特性，使得它在Web开发方面变得非常流行。文章目录本板块将对Flask的运行机制进行分析和探讨，同时还将包括使用Flask过程中的心得体会、疑问等等。一个Flask应用运行过程剖析Flask应用中的URL处理Flask中模块化应用的实现Flask中的蓝图管理Flask中的请求上下文和应用上下文后续将继续更新······1赞2收藏评论"], "art_create_time": ["2017/09/23"], "art_title": ["Flask 框架简介"], "art_url": ["http://python.jobbole.com/88621/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2015/02/edecceebedd0d04aa17bccba430ddcaf.jpg"]},
{"art_content": ["原文出处：LearnPython   引子在前几篇文章：SocketServer——网络通信服务器、BaseHTTPServer——实现Web服务器、SimpleHTTPServer——一个简单的HTTP服务器，我们介绍了Python标准库对于网络通信的支持，并且还介绍了标准库中的一些模块，例如TCPServer、UDPServer、BaseHTTPServer等。这些模块能够实现基础的网路通信服务，例如TCP/UDP层的通信、HTTP应用层的通信。上述模块对于网络通信的实现，基本的流程是：创建一个服务器。例如TCP服务器、UDP服务器、HTTP服务器。服务器可以监听和接收请求；创建请求处理程序。请求处理程序可以解析到达的请求，并发回一个响应。以上流程基本上反映了Python标准库中关于网络通信的基本过程。服务器类和请求处理类的解耦，意味着很多应用可以使用某个现有的服务器类，而不需要其他任何的修改，只需要提供一个可以处理这些应用的请求处理类即可。但随着近些年网络编程越来越复杂，对于服务器网络通信提出了较大的挑战。以Web编程为例，挑战主要在于：服务器不再仅仅提供简单的、静态的HTML页面，更多的是要与丰富的Web应用进行相互通信。如果将请求处理程序的构建放在服务器端实现，那对于每个Web应用构建一个请求处理程序显然不现实；如果将请求处理程序的构建放在开发Web应用的过程中，那无疑增加了Web应用程序开发的难度和复杂度，也是不太理想的。为了解决这些问题，常用的做法是提供一个中间层，通常称为网关接口。网关接口在服务器和应用中间承担一个“翻译官”的角色。只要应用程序符合网关接口的标准，那么服务器就只要做好服务器的角色，应用程序只要做好应用程序的作用，服务器和应用程序之间的通信全靠网关接口来协调。常用的网关接口有CGI、WSGI，本文就以WSGI网关接口来对此进行说明。WSGI网关接口WSGI(PythonWebServerGatewayInterface,PythonWeb服务器网关接口)是一个Web服务器和Web应用程序之间的标准化接口，用于增进应用程序在不同的Web服务器和框架之间的可移植性。关于该标准的官方说明可以参考PEP333。WSGI的主要作用是在Web服务器和Web应用程序承担“翻译官”的角色。对于这一角色可以这样理解：Web服务器的责任在于监听和接收请求。在处理请求的时候调用WSGI提供的标准化接口，将请求的信息转给WSGI；WSGI的责任在于“中转”请求和响应信息。WSGI接收到Web服务器提供的请求信息后可以做一些处理，之后通过标准化接口调用Web应用，并将请求信息传递给Web应用。同时，WSGI还将会处理Web应用返回的响应信息，并通过服务器返回给客户端；Web应用的责任在于接收请求信息，并且生成响应。根据以上分析，要实现符合WSGI标准的Web服务，服务器和应用程序的设计就要符合WSGI规范。WSGI规范WSGI规范如下：服务器的请求处理程序中要调用符合WSGI规范的网关接口；网关接口调用应用程序，并且要定义start_response(status,headers)函数，用于返回响应；应用程序中实现一个函数或者一个可调用对象webapp(environ,start_response)。其中environ是环境设置的字典，由服务器和WSGI网关接口设置，start_response是由网关接口定义的函数。在Python标准库中，wsgiref包就是符合WSGI标准的Web服务实现。后面简单对wsgiref包进行介绍，以此来对符合WSGI标准的Web服务的实现过程进行梳理。wsgiref包wsgiref包为实现WSGI标准提供了一个参考，它可以作为独立的服务器测试和调试应用程序。在实际的生产环境中尽量不要使用。wsgiref包含有以下模块：simple_server模块——simple_server模块实现了可以运行单个WSGI应用的简单的HTTP服务器。headers模块——管理响应首部的模块。handlers模块——符合WSGI标准的Web服务网关接口实现。该模块包含了一些处理程序对象，用来设置WSGI执行环境，以便应用程序能够在其他的Web服务器中运行。validate模块——“验证包装”模块，确保应用程序和服务器都能够按照WSGI标准进行操作。util模块——一些有用的工具集。以上模块暂时不做详细的介绍。本文剩余内容将simple_server模块单独拿出来，以其中的测试例子简单说明符合WSGI标准的Web服务器的实现过程。simple_server——一个简单的符合WSGI规范的服务器wsgiref包的simple_server模块实现了一个符合WSGI规范的服务器。测试代码如下：Pythonif__name__=='__main__':httpd=make_server('',8000,demo_app)sa=httpd.socket.getsockname()print\"ServingHTTPon\",sa[0],\"port\",sa[1],\"...\"importwebbrowserwebbrowser.open('http://localhost:8000/xyz?abc')httpd.handle_request()#serveonerequest,thenexithttpd.server_close()12345678if__name__=='__main__':    httpd=make_server('',8000,demo_app)    sa=httpd.socket.getsockname()    print\"ServingHTTPon\",sa[0],\"port\",sa[1],\"...\"    importwebbrowser    webbrowser.open('http://localhost:8000/xyz?abc')    httpd.handle_request()  #serveonerequest,thenexit    httpd.server_close()1.创建HTTP服务器上述测试代码中httpd=make_server(”,8000,demo_app)创建了一个HTTP服务器。其中make_server函数用来创建服务器：Pythondefmake_server(host,port,app,server_class=WSGIServer,handler_class=WSGIRequestHandler):\"\"\"CreateanewWSGIserverlisteningon`host`and`port`for`app`\"\"\"server=server_class((host,port),handler_class)server.set_app(app)returnserver1234567defmake_server(    host,port,app,server_class=WSGIServer,handler_class=WSGIRequestHandler):    \"\"\"CreateanewWSGIserverlisteningon`host`and`port`for`app`\"\"\"    server=server_class((host,port),handler_class)    server.set_app(app)    returnservermake_server函数使用WSGIServer类构建符合WSGI规范的HTTP服务器，使用WSGIRequestHandler类作为处理请求的类，使用demo_app作为一个Web应用。该函数返回一个服务器实例，并开始监听请求。可以通过httpd.socket.getsockname()获取服务器地址和端口号。2.使用webbrowser模块创建请求紧接着，测试例子导入webbrowser模块，使用函数创建了一个请求。Pythonwebbrowser.open('http://localhost:8000/xyz?abc')1webbrowser.open('http://localhost:8000/xyz?abc')3.服务器处理请求服务器通过handle_request()方法处理请求。关于处理请求的过程简单介绍如下：handle_request()方法通过调用get_request、verify_request、process_request、finish_request等方法创建一个请求处理实例（该过程可以参考TCPServer、HTTPServer的实现过程）；请求处理实例调用handle()方法处理请求。handle()在WSGIRequestHandler类中进行了重写。代码如下：Pythondefhandle(self):\"\"\"HandleasingleHTTPrequest\"\"\"self.raw_requestline=self.rfile.readline(65537)iflen(self.raw_requestline)>65536:self.requestline=''self.request_version=''self.command=''self.send_error(414)returnifnotself.parse_request():#Anerrorcodehasbeensent,justexitreturnhandler=ServerHandler(self.rfile,self.wfile,self.get_stderr(),self.get_environ())handler.request_handler=self#backpointerforlogginghandler.run(self.server.get_app())12345678910111213141516defhandle(self):    \"\"\"HandleasingleHTTPrequest\"\"\"    self.raw_requestline=self.rfile.readline(65537)    iflen(self.raw_requestline)>65536:        self.requestline=''        self.request_version=''        self.command=''        self.send_error(414)        return    ifnotself.parse_request():#Anerrorcodehasbeensent,justexit        return    handler=ServerHandler(        self.rfile,self.wfile,self.get_stderr(),self.get_environ()    )    handler.request_handler=self      #backpointerforlogging    handler.run(self.server.get_app())上面handle()函数先解析了请求，之后创建了一个WSGI网关类实例handler，这个实例可以作为服务器和应用程序之间的接口存在。4.WSGI网关的请求处理过程WSGI网关的定义在handlers模块。上一步骤中通过调用WSGI网关类实例handler的run方法，WSGI网关开始处理请求。run方法的代码如下：Pythondefrun(self,application):\"\"\"Invoketheapplication\"\"\"#Notetoself:don'tmovetheclose()!Asynchronousserversshouldn't#callclose()fromfinish_response(),soifyouclose()anywherebut#thedouble-errorbranchhere,you'llbreakasynchronousserversby#prematurelyclosing.Asyncserversmustreturnfrom'run()'without#closingiftheremightstillbeoutputtoiterateover.try:self.setup_environ()self.result=application(self.environ,self.start_response)self.finish_response()except:try:self.handle_error()except:#Ifwegetanerrorhandlinganerror,justgiveupalready!self.close()raise#...andlettheactualserverfigureitout.123456789101112131415161718defrun(self,application):    \"\"\"Invoketheapplication\"\"\"    #Notetoself:don'tmovetheclose()!  Asynchronousserversshouldn't    #callclose()fromfinish_response(),soifyouclose()anywherebut    #thedouble-errorbranchhere,you'llbreakasynchronousserversby    #prematurelyclosing.  Asyncserversmustreturnfrom'run()'without    #closingiftheremightstillbeoutputtoiterateover.    try:        self.setup_environ()        self.result=application(self.environ,self.start_response)        self.finish_response()    except:        try:            self.handle_error()        except:            #Ifwegetanerrorhandlinganerror,justgiveupalready!            self.close()            raise  #...andlettheactualserverfigureitout.run方法的主要功能有：通过setup_environ()方法创建WSGI相关的环境；调用WSGI应用的函数或者WSGI应用的可调用对象。本测试例子中的WSGI应用是一个简单的函数，其作用是将请求的environ信息打印出来。调用finish_response()方法将WSGI应用返回的数据作为响应发回。5.关闭服务器请求结束后，服务器会调用一系列函数关闭请求连接。之后测试代码调用server_close()方法关闭服务器。1赞2收藏评论"], "art_create_time": ["2017/09/23"], "art_title": ["Wsgiref 包——符合 WSGI 标准的 Web 服务实现（一）"], "art_url": ["http://python.jobbole.com/88637/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2015/10/bfa0d07e7eb2fac2eb80cd5df9567931.jpg"]},
{"art_content": ["原文出处：PythonScientists   我想大部分Python开发者最先接触到的方向是WEB方向（因为总是有开发者希望马上给自己做个博客出来，例如我），既然是WEB，免不了接触到一些WEB框架，例如Django,Flask,Torando等等，在开发过程中，看过一些文档总会介绍生产环境和开发环境服务器的配置问题，服务器又设计web服务器和应用服务器，总而言之，我们碰到最多的，必定是这个词—WSGI。接下来的文章，会分为以下几个部分：1.WSGI介绍1.1什么是WSGI1.2怎么实现WSGI2.由Django框架分析WSGI3.实际环境使用的wsgi服务器4.WSGI服务器比较开始1WSGI介绍1.1什么是WSGI首先介绍几个关于WSGI相关的概念WSGI：全称是WebServerGatewayInterface，WSGI不是服务器，python模块，框架，API或者任何软件，只是一种规范，描述webserver如何与webapplication通信的规范。server和application的规范在PEP3333中有具体描述。要实现WSGI协议，必须同时实现webserver和webapplication，当前运行在WSGI协议之上的web框架有Torando,Flask,Djangouwsgi：与WSGI一样是一种通信协议，是uWSGI服务器的独占协议，用于定义传输信息的类型(typeofinformation)，每一个uwsgipacket前4byte为传输信息类型的描述，与WSGI协议是两种东西，据说该协议是fcgi协议的10倍快。uWSGI：是一个web服务器，实现了WSGI协议、uwsgi协议、http协议等。WSGI协议主要包括server和application两部分：PythonWSGIserver负责从客户端接收请求，将request转发给application，将application返回的response返回给客户端；WSGIapplication接收由server转发的request，处理请求，并将处理结果返回给server。application中可以包括多个栈式的中间件(middlewares)，这些中间件需要同时实现server与application，因此可以在WSGI服务器与WSGI应用之间起调节作用：对服务器来说，中间件扮演应用程序，对应用程序来说，中间件扮演服务器。12WSGIserver负责从客户端接收请求，将request转发给application，将application返回的response返回给客户端；WSGIapplication接收由server转发的request，处理请求，并将处理结果返回给server。application中可以包括多个栈式的中间件(middlewares)，这些中间件需要同时实现server与application，因此可以在WSGI服务器与WSGI应用之间起调节作用：对服务器来说，中间件扮演应用程序，对应用程序来说，中间件扮演服务器。WSGI协议其实是定义了一种server与application解耦的规范，即可以有多个实现WSGIserver的服务器，也可以有多个实现WSGIapplication的框架，那么就可以选择任意的server和application组合实现自己的web应用。例如uWSGI和Gunicorn都是实现了WSGIserver协议的服务器，Django，Flask是实现了WSGIapplication协议的web框架，可以根据项目实际情况搭配使用。以上介绍了相关的常识，接下来我们来看看如何简单实现WSGI协议。1.2怎么实现WSGI上文说过，实现WSGI协议必须要有wsgiserver和application，因此，我们就来实现这两个东西。我们来看看官方WSGI使用WSGI的wsgiref模块实现的小demo有关于wsgiref的快速入门可以看看这篇博客Pythondefdemo_app(environ,start_response):fromStringIOimportStringIOstdout=StringIO()print>>stdout,\"Helloworld!\"print>>stdouth=environ.items();h.sort()fork,vinh:print>>stdout,k,'=',repr(v)start_response(\"200OK\",[('Content-Type','text/plain')])return[stdout.getvalue()]httpd=make_server('localhost',8002,demo_app)httpd.serve_forever()#使用select12345678910111213defdemo_app(environ,start_response):      fromStringIOimportStringIO      stdout=StringIO()      print>>stdout,\"Helloworld!\"      print>>stdout      h=environ.items();h.sort()      fork,vinh:          print>>stdout,k,'=',repr(v)      start_response(\"200OK\",[('Content-Type','text/plain')])      return[stdout.getvalue()]    httpd=make_server('localhost',8002,  demo_app)  httpd.serve_forever()  #使用select实现了一个application，来获取客户端的环境和回调函数两个参数，以及httpd服务端的实现，我们来看看make_server的源代码Pythondefmake_server(host,port,app,server_class=WSGIServer,handler_class=WSGIRequestHandler):\"\"\"CreateanewWSGIserverlisteningon`host`and`port`for`app`\"\"\"server=server_class((host,port),handler_class)server.set_app(app)returnserver1234567defmake_server(      host,port,app,server_class=WSGIServer,handler_class=WSGIRequestHandler  ):    \"\"\"CreateanewWSGIserverlisteningon`host`and`port`for`app`\"\"\"    server=server_class((host,port),handler_class)    server.set_app(app)    returnserver接受一系列函数，返回一个server对象,实现还是比较简单，下面我们来看看在django中如何实现其自身的wsgi服务器的。下面我们自己来实现一遍：WSGI规定每个python程序（Application）必须是一个可调用的对象（实现了__call__函数的方法或者类），接受两个参数environ（WSGI的环境信息）和start_response（开始响应请求的函数），并且返回iterable。几点说明：Pythonenviron和start_response由httpserver提供并实现environ变量是包含了环境信息的字典Application内部在返回前调用start_responsestart_response也是一个callable，接受两个必须的参数，status（HTTP状态）和response_headers（响应消息的头）可调用对象要返回一个值，这个值是可迭代的。12345environ和start_response由httpserver提供并实现environ变量是包含了环境信息的字典Application内部在返回前调用start_responsestart_response也是一个callable，接受两个必须的参数，status（HTTP状态）和response_headers（响应消息的头）可调用对象要返回一个值，这个值是可迭代的。Python#1.可调用对象是一个函数defapplication(environ,start_response):response_body='Therequestmethodwas%s'%environ['REQUEST_METHOD']#HTTPresponsecodeandmessagestatus='200OK'#应答的头部是一个列表，每对键值都必须是一个tuple。response_headers=[('Content-Type','text/plain'),('Content-Length',str(len(response_body)))]#调用服务器程序提供的start_response，填入两个参数start_response(status,response_headers)#返回必须是iterablereturn[response_body]#2.可调用对象是一个类classAppClass:\"\"\"这里的可调用对象就是AppClass这个类，调用它就能生成可以迭代的结果。使用方法类似于：forresultinAppClass(env,start_response):do_somthing(result)\"\"\"def__init__(self,environ,start_response):self.environ=environself.start=start_responsedef__iter__(self):status='200OK'response_headers=[('Content-type','text/plain')]self.start(status,response_headers)yield\"Helloworld!\\n\"#3.可调用对象是一个实例classAppClass:\"\"\"这里的可调用对象就是AppClass的实例，使用方法类似于：app=AppClass()forresultinapp(environ,start_response):do_somthing(result)\"\"\"def__init__(self):passdef__call__(self,environ,start_response):status='200OK'response_headers=[('Content-type','text/plain')]self.start(status,response_headers)yield\"Helloworld!\\n\"12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#1.可调用对象是一个函数defapplication(environ,start_response):  response_body='Therequestmethodwas%s'%environ['REQUEST_METHOD']  #HTTPresponsecodeandmessage  status='200OK'  #应答的头部是一个列表，每对键值都必须是一个tuple。  response_headers=[('Content-Type','text/plain'),                      ('Content-Length',str(len(response_body)))]  #调用服务器程序提供的start_response，填入两个参数  start_response(status,response_headers)  #返回必须是iterable  return[response_body]      #2.可调用对象是一个类classAppClass:    \"\"\"这里的可调用对象就是AppClass这个类，调用它就能生成可以迭代的结果。        使用方法类似于：        forresultinAppClass(env,start_response):            do_somthing(result)    \"\"\"    def__init__(self,environ,start_response):        self.environ=environ        self.start=start_response    def__iter__(self):        status='200OK'        response_headers=[('Content-type','text/plain')]        self.start(status,response_headers)        yield\"Helloworld!\\n\"#3.可调用对象是一个实例classAppClass:    \"\"\"这里的可调用对象就是AppClass的实例，使用方法类似于：        app=AppClass()        forresultinapp(environ,start_response):            do_somthing(result)    \"\"\"    def__init__(self):        pass    def__call__(self,environ,start_response):        status='200OK'        response_headers=[('Content-type','text/plain')]        self.start(status,response_headers)        yield\"Helloworld!\\n\"服务器程序端上面已经说过，标准要能够确切地实行，必须要求程序端和服务器端共同遵守。上面提到，envrion和start_response都是服务器端提供的。下面就看看，服务器端要履行的义务。Python准备environ参数定义start_response函数调用程序端的可调用对象123准备environ参数定义start_response函数调用程序端的可调用对象Pythonimportos,sysdefrun_with_cgi(application):#application是程序端的可调用对象#准备environ参数，这是一个字典，里面的内容是一次HTTP请求的环境变量environ=dict(os.environ.items())environ['wsgi.input']=sys.stdinenviron['wsgi.errors']=sys.stderrenviron['wsgi.version']=(1,0)environ['wsgi.multithread']=Falseenviron['wsgi.multiprocess']=Trueenviron['wsgi.run_once']=Trueenviron['wsgi.url_scheme']='http'headers_set=[]headers_sent=[]#把应答的结果输出到终端defwrite(data):sys.stdout.write(data)sys.stdout.flush()#实现start_response函数，根据程序端传过来的status和response_headers参数，#设置状态和头部defstart_response(status,response_headers,exc_info=None):headers_set[:]=[status,response_headers]returnwrite#调用客户端的可调用对象，把准备好的参数传递过去result=application(environ,start_response)#处理得到的结果，这里简单地把结果输出到标准输出。try:fordatainresult:ifdata:#don'tsendheadersuntilbodyappearswrite(data)finally:ifhasattr(result,'close'):result.close()1234567891011121314151617181920212223242526272829303132333435363738importos,sysdefrun_with_cgi(application):    #application是程序端的可调用对象    #准备environ参数，这是一个字典，里面的内容是一次HTTP请求的环境变量    environ=dict(os.environ.items())    environ['wsgi.input']        =sys.stdin    environ['wsgi.errors']      =sys.stderr    environ['wsgi.version']      =(1,0)    environ['wsgi.multithread']  =False    environ['wsgi.multiprocess']=True    environ['wsgi.run_once']    =True                environ['wsgi.url_scheme']='http'    headers_set=[]    headers_sent=[]    #把应答的结果输出到终端    defwrite(data):        sys.stdout.write(data)        sys.stdout.flush()    #实现start_response函数，根据程序端传过来的status和response_headers参数，    #设置状态和头部    defstart_response(status,response_headers,exc_info=None):        headers_set[:]=[status,response_headers]          returnwrite    #调用客户端的可调用对象，把准备好的参数传递过去    result=application(environ,start_response)        #处理得到的结果，这里简单地把结果输出到标准输出。    try:        fordatainresult:            ifdata:    #don'tsendheadersuntilbodyappears                write(data)    finally:        ifhasattr(result,'close'):            result.close()2由Django框架分析WSGI下面我们以django为例，分析一下wsgi的整个流程djangoWSGIapplicationWSGIapplication应该实现为一个可调用iter对象，例如函数、方法、类(包含**call**方法)。需要接收两个参数：一个字典，该字典可以包含了客户端请求的信息以及其他信息，可以认为是请求上下文，一般叫做environment（编码中多简写为environ、env），一个用于发送HTTP响应状态（HTTPstatus）、响应头（HTTPheaders）的回调函数,也就是start_response()。通过回调函数将响应状态和响应头返回给server，同时返回响应正文(responsebody)，响应正文是可迭代的、并包含了多个字符串。下面是Django中application的具体实现部分：PythonclassWSGIHandler(base.BaseHandler):initLock=Lock()request_class=WSGIRequestdef__call__(self,environ,start_response):#加载中间件ifself._request_middlewareisNone:withself.initLock:try:#Checkthatmiddlewareisstilluninitialized.ifself._request_middlewareisNone:self.load_middleware()except:#Unloadwhatevermiddlewarewegotself._request_middleware=Noneraiseset_script_prefix(get_script_name(environ))#请求处理之前发送信号signals.request_started.send(sender=self.__class__,environ=environ)try:request=self.request_class(environ)exceptUnicodeDecodeError:logger.warning('BadRequest(UnicodeDecodeError)',exc_info=sys.exc_info(),extra={'status_code':400,}response=http.HttpResponseBadRequest()else:response=self.get_response(request)response._handler_class=self.__class__status='%s%s'%(response.status_code,response.reason_phrase)response_headers=[(str(k),str(v))fork,vinresponse.items()]forcinresponse.cookies.values():response_headers.append((str('Set-Cookie'),str(c.output(header=''))))#server提供的回调方法，将响应的header和status返回给serverstart_response(force_str(status),response_headers)ifgetattr(response,'file_to_stream',None)isnotNoneandenviron.get('wsgi.file_wrapper'):response=environ['wsgi.file_wrapper'](response.file_to_stream)returnresponse12345678910111213141516171819202122232425262728classWSGIHandler(base.BaseHandler):  initLock=Lock()  request_class=WSGIRequest  def__call__(self,environ,start_response):  #加载中间件    ifself._request_middlewareisNone:        withself.initLock:            try:#Checkthatmiddlewareisstilluninitialized.                ifself._request_middlewareisNone:                    self.load_middleware()            except:#Unloadwhatevermiddlewarewegot                    self._request_middleware=Noneraise              set_script_prefix(get_script_name(environ))#请求处理之前发送信号      signals.request_started.send(sender=self.__class__,environ=environ)    try:          request=self.request_class(environ)      exceptUnicodeDecodeError:          logger.warning('BadRequest(UnicodeDecodeError)',exc_info=sys.exc_info(),extra={'status_code':400,}          response=http.HttpResponseBadRequest()    else:          response=self.get_response(request)    response._handler_class=self.__class__status='%s%s'%(response.status_code,response.reason_phrase)    response_headers=[(str(k),str(v))fork,vinresponse.items()]forcinresponse.cookies.values():response_headers.append((str('Set-Cookie'),str(c.output(header=''))))    #server提供的回调方法，将响应的header和status返回给server        start_response(force_str(status),response_headers)    ifgetattr(response,'file_to_stream',None)isnotNoneandenviron.get('wsgi.file_wrapper'):          response=environ['wsgi.file_wrapper'](response.file_to_stream)    returnresponse可以看出application的流程包括:加载所有中间件，以及执行框架相关的操作，设置当前线程脚本前缀，发送请求开始信号；处理请求，调用get_response()方法处理当前请求，该方法的的主要逻辑是通过urlconf找到对应的view和callback，按顺序执行各种middleware和callback。调用由server传入的start_response()方法将响应header与status返回给server。返回响应正文djangoWSGIServer负责获取http请求，将请求传递给WSGIapplication，由application处理请求后返回response。以Django内建server为例看一下具体实现。通过runserver运行django项目，在启动时都会调用下面的run方法，创建一个WSGIServer的实例，之后再调用其serve_forever()方法启动服务。Pythondefrun(addr,port,wsgi_handler,ipv6=False,threading=False):server_address=(addr,port)ifthreading:httpd_cls=type(str('WSGIServer'),(socketserver.ThreadingMixIn,WSGIServer),{})else:httpd_cls=WSGIServer#这里的wsgi_handler就是WSGIApplicationhttpd=httpd_cls(server_address,WSGIRequestHandler,ipv6=ipv6)ifthreading:httpd.daemon_threads=Truehttpd.set_app(wsgi_handler)httpd.serve_forever()12345678910defrun(addr,port,wsgi_handler,ipv6=False,threading=False):  server_address=(addr,port)  ifthreading:        httpd_cls=type(str('WSGIServer'),(socketserver.ThreadingMixIn,WSGIServer),{})  else:        httpd_cls=WSGIServer#这里的wsgi_handler就是WSGIApplication  httpd=httpd_cls(server_address,WSGIRequestHandler,ipv6=ipv6)    ifthreading:        httpd.daemon_threads=Truehttpd.set_app(wsgi_handler)        httpd.serve_forever()下面表示WSGIserver服务器处理流程中关键的类和方法。WSGIServerrun()方法会创建WSGIServer实例，主要作用是接收客户端请求，将请求传递给application，然后将application返回的response返回给客户端。创建实例时会指定HTTP请求的handler：WSGIRequestHandler类，通过set_app和get_app方法设置和获取WSGIApplication实例wsgi_handler。处理http请求时，调用handler_request方法，会创建WSGIRequestHandler，实例处理http请求。WSGIServer中get_request方法通过socket接受请求数据。WSGIRequestHandler由WSGIServer在调用handle_request时创建实例，传入request、cient_address、WSGIServer三个参数，__init__方法在实例化同时还会调用自身的handle方法handle方法会创建ServerHandler实例，然后调用其run方法处理请求ServerHandlerWSGIRequestHandler在其handle方法中调用run方法，传入self.server.get_app()参数，获取WSGIApplication，然后调用实例(__call__)，获取response，其中会传入start_response回调，用来处理返回的header和status。通过application获取response以后，通过finish_response返回responseWSGIHandlerWSGI协议中的application，接收两个参数，environ字典包含了客户端请求的信息以及其他信息，可以认为是请求上下文，start_response用于发送返回status和header的回调函数虽然上面一个WSGIserver涉及到多个类实现以及相互引用，但其实原理还是调用WSGIHandler，传入请求参数以及回调方法start_response()，并将响应返回给客户端。3实际环境使用的wsgi服务器因为每个web框架都不是专注于实现服务器方面的，因此，在生产环境部署的时候使用的服务器也不会简单的使用web框架自带的服务器，这里，我们来讨论一下用于生产环境的服务器有哪些？1.gunicornGunicorn（从Ruby下面的Unicorn得到的启发）应运而生：依赖Nginx的代理行为，同Nginx进行功能上的分离。由于不需要直接处理用户来的请求（都被Nginx先处理），Gunicorn不需要完成相关的功能，其内部逻辑非常简单：接受从Nginx来的动态请求，处理完之后返回给Nginx，由后者返回给用户。由于功能定位很明确，Gunicorn得以用纯Python开发：大大缩短了开发时间的同时，性能上也不会很掉链子。同时，它也可以配合Nginx的代理之外的别的Proxy模块工作，其配置也相应比较简单。配置上的简单，大概是它流行的最大的原因。2.uwsgi因为使用C语言开发，会和底层接触的更好，配置也是比较方便，目前和gunicorn两个算是部署时的唯二之选。以下是通常的配置文件Python[uwsgi]http=$(HOSTNAME):9033http-keepalive=1pythonpath=../module=servicemaster=1processes=8daemonize=logs/uwsgi.logdisable-logging=1buffer-size=16384harakiri=5pidfile=uwsgi.pidstats=$(HOSTNAME):1733运行：uwsgi--iniconf.ini12345678910111213141516[uwsgi]http=$(HOSTNAME):9033http-keepalive=1pythonpath=../module=servicemaster=1processes=8daemonize=logs/uwsgi.logdisable-logging=1buffer-size=16384harakiri=5pidfile=uwsgi.pidstats=$(HOSTNAME):1733  运行：uwsgi--ini  conf.ini3.fcgi不多数，估计使用的人也是比较少，这里只是提一下4.bjoernPythonWSGI界最牛逼性能的Server其中一个是bjoern，纯C，小于1000行代码，就是看不惯uWSGI的冗余自写的。4WSGI服务器比较综合广大Python开发者的实际经历，我们可以得出，使用最广的当属uWSGI以及gunicorn，我们这里来比较比较两者与其他服务器的区别。1.gunicorn本身是个多进程管理器，需要指定相关的不同类型的worker去工作，使用gevent作为worker时单机大概是3000RPSHelloWorld，胜过torando自带的服务器大概是2000左右，uWSGI则会更高一点。2.相比于tornado对于现有代码需要大规模重构才能用上高级特性，Gevent只需要一个monkey，容易对代码进行快速加工。3.gunicorn可以做prehookandposthook.下面来对比以下uWSGI和gunicorn的速度差比可以看到，如果单纯追求性能，那uWSGI会更好一点，而gunicorn则会更易安装和结合gevent。结合这篇文章,我们也可以得出相同结论，在阻塞响应较多的情况下，gunicorn的gevent模式无疑性能会更加强大。功能实现方面，无疑uWSGI会更多一些，配置也会更加复杂一些，可以看看uWSGI的配置和gunicorn的配置。至于怎么去选择，就看大家的项目结构怎么样了。最后，宣传一下我们的开源组织，PSC开源组，希望以开源项目的方式让每个人都能更有融入性的去学习，公开化你的学习。github地址：https://github.com/PythonScie…官方论坛：http://www.pythonscientists.com1赞9收藏评论"], "art_create_time": ["2017/09/30"], "art_title": ["Python Web开发最难懂的WSGI协议，到底包含哪些内容？"], "art_url": ["http://python.jobbole.com/88653/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2017/09/c817a776a88a62b252eaaef8c27f8271.png"]},
{"art_content": ["本文作者：伯乐在线-iPytLab。未经作者许可，禁止转载！欢迎加入伯乐在线专栏作者。前言本文尝试对遗传算法中不同适值函数的标定(Scaling)方法进行下总结，并针对常用的线性标定和动态线性标定进行了Python实现，以装饰器的形式添加到遗传算法框架GAFT中，这样在使用GAFT运行遗传算法迭代的时候可以更加Pythonic的给自定义的适值函数进行标定。最后针对能够防止早熟情况的大变异算法进行了相应的实现。目前(动态)线性标定装饰器以及大变异算子均已添加到GAFT中，gaft项目链接:GitHub: https://github.com/PytLab/gaftPyPI: https://pypi.python.org/pypi/gaft适值函数的标定选择压力Thetendencytoselectthebestmemberofthecurrentgenerationisknownasselectivepressure.选择压力也就是种群中最好个体与最坏个体被选中概率的差值，这个差距越大，选中好个体的趋势就越大，则成为选择压力大。适值函数的标定一般情况下，直接拿目标函数作为适值函数十分的方便，但是很多情况下却不能这么做，例如对于求最小值问题，我们必须将目标函数取反才能作为适值函数(这是最简单的情况)。当我们遗传算法中不同个体适值函数的值相对差别很小的时候，我们根据适应度值的大小进行个体选择的选择压力(Selectivepressure)就会变小，选优的能力弱化，这个时候我们需要对原始的适值函数进行标定(Scaling)是的他们相对差别增大，进而增大选择压力，增强算法的选优能力。例如: 局部搜索、广域搜索与选择压力的关系在遗传算法中，局部搜索同广域搜索其实相互矛盾的，注重局部搜索则会陷入局部最优，但是注重广域搜索会导致算法精确开发能力不强。因此需要综合两者考虑，我们可以在搜索刚刚开始的时候使用较小的选择压力来广域搜索，随着迭代的进行可以动态的增大选择压力来使算法偏向于局部搜索。几种不同的适值函数标定方法对目标函数的标定方法一般有:线性标定、动态线性标定、幂律标定、对数标定等线性标定线性标定的形式:其中f′为标定后的适值函数，ff为原始的目标函数。求最大值对于求目标函数的最大值的时候,即 argmaxf(x)我们取a=1,b=−fmin+ξ,其中ξ是一个较小的数，目的是使得种群中最差个体也有被选中的机会，不然自身减掉f−fmin=0, ξ的存在可以增加种群的多样性。最终的适值函数表达式:求最小值当我们需要求目标函数最小值的时候，argminf(x)，我们需要对目标函数进行取反操作,即a=−1,b=fmax−f(x)+ξ最终的适值函数表达式:GAFT中添加对于目标函数的标定由于适值函数标定并不针对某个目标函数，我便想通过装饰器的方式来方便给任何自定义的fitness函数进行标定。对于基本的线性标定，我在GAEngine中添加了个带参数的装饰器:Pythondeflinear_scaling(self,target='max',ksi=0.5):'''Adecoratorconstructorforfitnessfunctionlinearscaling.:paramtarget:Theoptimizationtarget,maximizationorminimization.:typetarget:str,'max'or'min':paramksi:Selectivepressureadjustmentvalue.:typeksi:floatLinearScaling:1.argmaxf(x),thenf'=f-min{f(x)}+ksi;2.argminf(x),thenf'=max{f(x)}-f(x)+ksi;'''def_linear_scaling(fn):#Fororiginalfitnesscalculation.self.ori_fitness=fn@wraps(fn)def_fn_with_linear_scaling(indv):#Originalfitnessvalue.f=fn(indv)#Determinethevalueofaandb.iftarget=='max':f_prime=f-self.ori_fmin+ksieliftarget=='min':f_prime=self.ori_fmax-f+ksielse:raiseValueError('Invalidtargettype({})'.format(target))returnf_primereturn_fn_with_linear_scalingreturn_linear_scaling12345678910111213141516171819202122232425262728deflinear_scaling(self,target='max',ksi=0.5):    '''    Adecoratorconstructorforfitnessfunctionlinearscaling.    :paramtarget:Theoptimizationtarget,maximizationorminimization.    :typetarget:str,'max'or'min'    :paramksi:Selectivepressureadjustmentvalue.    :typeksi:float    LinearScaling:        1.argmaxf(x),thenf'=f-min{f(x)}+ksi;        2.argminf(x),thenf'=max{f(x)}-f(x)+ksi;    '''    def_linear_scaling(fn):        #Fororiginalfitnesscalculation.        self.ori_fitness=fn        @wraps(fn)        def_fn_with_linear_scaling(indv):            #Originalfitnessvalue.            f=fn(indv)            #Determinethevalueofaandb.            iftarget=='max':                f_prime=f-self.ori_fmin+ksi            eliftarget=='min':                f_prime=self.ori_fmax-f+ksi            else:                raiseValueError('Invalidtargettype({})'.format(target))            returnf_prime        return_fn_with_linear_scaling    return_linear_scaling这个时候如果我们在定义了一个自己的目标函数以后，想对其进行线性标定便可以使用engine的这个装饰器对函数进行修饰即可,像下面这样:Python#CreateaGAengine...#先标定，后注册到引擎中@engine.fitness_register@engine.linear_scaling(target='min',ksi=0.5)deffitness(indv):x,=indv.variantsreturnx+10*sin(5*x)+7*cos(4*x)1234567#CreateaGAengine...#先标定，后注册到引擎中@engine.fitness_register@engine.linear_scaling(target='min',ksi=0.5)deffitness(indv):    x,=indv.variants    returnx+10*sin(5*x)+7*cos(4*x)其中装饰器中的参数分别为:target:优化目标函数到最小值还是最大值，值可以是:'max'或者'min'ksi:即公式中ξξ动态线性标定动态线性标定是遗传算法中最常用的标定方法，他是基于上面提到的线性标定，在线性标定中的ξξ在动态线性标定中并不是一成不变的，而是随着迭代次数的增加而变化。动态线性标定的函数表达式:其中，k为迭代指标，表示ξ会随着迭代数而不同。求最大值当我们的优化目标是目标函数的最大值，这是我们取ak=1,bk=−fmin+ξk,这是的函数表达为:求最小值求最小值的时候需要取反操作，这时取ak=−1,bk=fmax+ξk,最终函数表达式:关于ξk动态线性标定中的ξk作用同线性标定中的ξ为选择压力调节值,它的存在使得种群中最坏的个体仍有被选中的机会，但是动态标定中的ξkξk的值会随着kk增大而减小。ξkξk的取值: ξ0=M,ξk=ξk−1⋅r,r∈[0.9,0.999],我们通过调节M和r来调节ξk通过可以动态变化的ξk，我们可以使广域搜索范围宽保持种群的多样性，局部搜索保持收敛性，即，开始时希望选择小，迭代到后面希望选择压力逐渐变大.GAFT中添加给目标函数添加动态线性标定与上面线性标定的方法相同，GAFT中同样使用了标定装饰器来装饰用户自定义的目标函数，实现代码:Pythondefdynamic_linear_scaling(self,target='max',ksi0=2,r=0.9):'''Adecoratorconstructorforfitnessdynamiclinearscaling.:paramtarget:Theoptimizationtarget,maximizationorminimization.:typetarget:str,'max'or'min':paramksi0:Initialselectivepressureadjustmentvalue,defaultvalueis2:typeksi0:float:paramr:Thereductionfactorforselectivepressureadjustmentvalue,ksi^(k-1)*ristheadjustmentvalueforgenerationk,defaultvalueis0.9:typer:floatinrange[0.9,0.999]DynamicLinearScaling:Formaximizaiton,f'=f(x)-min{f(x)}+ksi^k,kisgenerationnumber.'''def_dynamic_linear_scaling(fn):#Fororiginalfitnesscalculation.self.ori_fitness=fn@wraps(fn)def_fn_with_dynamic_linear_scaling(indv):f=fn(indv)k=self.current_generation+1iftarget=='max':f_prime=f-self.ori_fmin+ksi0*(r**k)eliftarget=='min':f_prime=self.ori_fmax-f+ksi0*(r**k)else:raiseValueError('Invalidtargettype({})'.format(target))returnf_primereturn_fn_with_dynamic_linear_scalingreturn_dynamic_linear_scaling12345678910111213141516171819202122232425262728293031defdynamic_linear_scaling(self,target='max',ksi0=2,r=0.9):    '''    Adecoratorconstructorforfitnessdynamiclinearscaling.    :paramtarget:Theoptimizationtarget,maximizationorminimization.    :typetarget:str,'max'or'min'    :paramksi0:Initialselectivepressureadjustmentvalue,defaultvalue                is2    :typeksi0:float    :paramr:Thereductionfactorforselectivepressureadjustmentvalue,              ksi^(k-1)*ristheadjustmentvalueforgenerationk,default              valueis0.9    :typer:floatinrange[0.9,0.999]    DynamicLinearScaling:        Formaximizaiton,f'=f(x)-min{f(x)}+ksi^k,kisgenerationnumber.    '''    def_dynamic_linear_scaling(fn):        #Fororiginalfitnesscalculation.        self.ori_fitness=fn        @wraps(fn)        def_fn_with_dynamic_linear_scaling(indv):            f=fn(indv)            k=self.current_generation+1            iftarget=='max':                f_prime=f-self.ori_fmin+ksi0*(r**k)            eliftarget=='min':                f_prime=self.ori_fmax-f+ksi0*(r**k)            else:                raiseValueError('Invalidtargettype({})'.format(target))            returnf_prime        return_fn_with_dynamic_linear_scaling    return_dynamic_linear_scaling这里充分的利用Python的闭包，在engine中获取当前种群最大值与最小值的相关数据。在脚本中修饰目标函数便可以这样:Python@engine.fitness_register@engine.dynamic_linear_scaling(target='max',ksi0=2,r=0.9)deffitness(indv):x,=indv.variantsreturnx+10*sin(5*x)+7*cos(4*x)12345@engine.fitness_register@engine.dynamic_linear_scaling(target='max',ksi0=2,r=0.9)deffitness(indv):    x,=indv.variants    returnx+10*sin(5*x)+7*cos(4*x)其他标定方法这里简要的介绍下其他标定方法。幂律标定函数表达式: f′=fαα的取值, α>1增大选择压力, α<1减小选择压力对数标定函数表达式: f′=aLnf+b作用:缩小目标函数之间的差别指数标定函数表达式: f′=aebf+c作用:扩大目标函数间的差别窗口技术函数表达式: f′=af−fwfw为前W代中的目标函数最小值，他考虑了各代fmin的波动，这样fw具有记忆性大变异算法众所周知，简单的遗传算法存在“早熟”的问题，也就是算法过早的收敛到一个非全局最优点，出现此问题的主要原因是一种被称为“顶端优势”的现象存在，即当算法进行到某一代时，在种群中某个个体的适应度远远大于任何一个个体的适应度，导致选择算法总是会选到此个体生成子代个体，极限情况下就是所有个体都来自统一祖先，即”早熟”。除了对目标函数进行标定，我们可以通过大变异算法来避免早熟。大致思路:当某代中所有个体集中在一起时，我们以一个远大于通常变异概率的概率执行一次变异操作，具有大变异概率的变异操作能够随机、独立的产生许多新的个体，从而是整个种群脱了“早熟”。如何判断种群个体的集中程度通常采取比较种群中所有个体的适应度值的平均值favg与最大值fmax的接近程度来判断，如果最大值与平均值越接近说明个体就越集中。具体过程当某一代的最大适应度fmax与平均适应度值favg满足:其中，0.5<α<1,被称为密集因子，表征个体集中程度。随后，我们以一个大变异概率进行一次变异操作(通常大5倍以上),即“打散”。大变异操作的两个参数密集因子α:决定大变异操作在整个过程中所占的比重，其数值约接近0.5，大变异操作越频繁大变异概率:概率越大，大变异算法的稳定性就越好，但是收敛速度可能会降低，当大变异概率的数值为0.5的时候，大变异操作就近似退化为随机搜索GAFT中的大变异算子大变异操作与具体的变异算子实现无关，这里我还是依据内置的FlipBitMutation算子为基础,具体的代码实现参见https://github.com/PytLab/gaft/blob/master/gaft/operators/mutation/flip_bit_mutation.pyPythonclassFlipBitBigMutation(FlipBitMutation):def__init__(self,pm,pbm,alpha):'''MutationoperatorusingFlipBitmutationimplementationwithadaptivebigmutationratetoovercomeprematureorlocal-bestsolution.:parampm:Theprobabilityofmutation(usuallybetween0.001~0.1):typepm:floatin(0.0,1.0]:parampbm:Theprobabilityofbigmutation,usuallymorethan5timesbiggerthanpm.:typepbm:float:paramalpha:intensivefactor:typealpha:float,inrange(0.5,1)'''super(self.__class__,self).__init__(pm)ifnot(0.0<pbm<1.0):raiseValueError('Invalidbigmutationprobability')ifpbm<5*pm:self.logger.warning('Relativelowprobabilityforbigmutation')self.pbm=pbm#Intensivefactor.ifnot(0.5<alpha<1.0):raiseValueError('Invalidintensivefactor,shouldbein(0.5,1.0)')self.alpha=alphadefmutate(self,individual,engine):'''Mutatetheindividualwithadaptivebigmutationrate.'''pm=self.pmifengine.fmax*self.alpha<engine.fmean:self.pm=self.pbmself.logger.info('Bigmutationprobabilty:{}->{}'.format(pm,self.pm))#Mutatewithbigprobability.individual=super(self.__class__,self).mutate(individual,engine)#Recoverprobability.self.pm=pmreturnindividual123456789101112131415161718192021222324252627282930313233343536classFlipBitBigMutation(FlipBitMutation):    def__init__(self,pm,pbm,alpha):        '''        MutationoperatorusingFlipBitmutationimplementationwithadaptive        bigmutationratetoovercomeprematureorlocal-bestsolution.        :parampm:Theprobabilityofmutation(usuallybetween0.001~0.1)        :typepm:floatin(0.0,1.0]        :parampbm:Theprobabilityofbigmutation,usuallymorethan5times                    biggerthanpm.        :typepbm:float        :paramalpha:intensivefactor        :typealpha:float,inrange(0.5,1)        '''        super(self.__class__,self).__init__(pm)        ifnot(0.0<pbm<1.0):            raiseValueError('Invalidbigmutationprobability')        ifpbm<5*pm:            self.logger.warning('Relativelowprobabilityforbigmutation')        self.pbm=pbm        #Intensivefactor.        ifnot(0.5<alpha<1.0):            raiseValueError('Invalidintensivefactor,shouldbein(0.5,1.0)')        self.alpha=alpha    defmutate(self,individual,engine):        '''        Mutatetheindividualwithadaptivebigmutationrate.        '''        pm=self.pm        ifengine.fmax*self.alpha<engine.fmean:            self.pm=self.pbm            self.logger.info('Bigmutationprobabilty:{}->{}'.format(pm,self.pm))        #Mutatewithbigprobability.        individual=super(self.__class__,self).mutate(individual,engine)        #Recoverprobability.        self.pm=pm        returnindividual总结本文尝试对遗传算法中不同适值函数的标定(Scaling)方法进行下总结，并针对常用的线性标定和动态线性标定进行了Python实现，以装饰器的形式添加到遗传算法框架GAFT中，这样在使用GAFT运行遗传算法迭代的时候可以更加Pythonic的给自定义的适值函数进行标定。最后针对能够防止早熟情况的大变异算法进行了相应的实现。参考《MATLAB最优化计算(第三版)》马钧水,刘贵忠,贾玉兰.改进遗传算法搜索性能的大变异操作[J].控制理论与应用,1998(3):404-408.打赏支持我写出更多好文章，谢谢！打赏作者打赏支持我写出更多好文章，谢谢！2赞2收藏评论关于作者：iPytLab喜欢写程序的计算化学狗，Python/C/C++/Fortran,个人博客http://pytlab.org个人主页·我的文章·22·"], "art_create_time": ["2017/09/24"], "art_title": ["遗传算法中适值函数的标定与大变异算法"], "art_url": ["http://python.jobbole.com/88641/"], "art_img": ["http://pytlab.org/assets/images/blog_img/2017-09-23-遗传算法中适值函数的标定与大变异算法/feature.png"]},
{"art_content": ["原文出处：MichaelDiBernardo   译文出处：你逗比   这篇文章是对500LinesorLess一书中高效爬虫一章的部分翻译，原文在此->HowPythonGeneratorsWork。建议结合《流畅的Python》食用。在掌握Python生成器之前，你必须了解常规Python函数的工作原理。通常，当一个Python函数调用子程序（subroutine）时，这个子程序将一直持有控制权，只有当子程序结束（返回或者抛出异常）后，控制权才还给调用者：Python>>>deffoo():...bar()...>>>defbar():...pass12345>>>deffoo():...    bar()...>>>defbar():...    pass标准的Python解释器是用C写的。解释器用一个叫做PyEval_EvalFrameEx的C函数来执行Python函数。它接受一个Python的堆栈帧（stackframe）对象，并在这个堆栈帧的上下文中执行Python字节码。这是foo的字节码：Python>>>importdis>>>dis.dis(foo)20LOAD_GLOBAL0(bar)3CALL_FUNCTION0(0positional,0keywordpair)6POP_TOP7LOAD_CONST0(None)10RETURN_VALUE1234567>>>importdis>>>dis.dis(foo)  2          0LOAD_GLOBAL              0(bar)              3CALL_FUNCTION            0(0positional,0keywordpair)              6POP_TOP              7LOAD_CONST              0(None)            10RETURN_VALUEfoo函数将bar加载到堆栈中并调用它，然后从堆栈中弹出返回值，最后加载并返回None。当PyEval_EvalFrameEx遇到CALL_FUNCTION字节码的时候，它会创建一个新的Python堆栈帧，然后用这个新的帧作为参数递归调用PyEval_EvalFrameEx来执行bar。Python的堆栈帧是分配在堆内存中的，理解这一点非常重要！Python解释器是个普通的C程序，所以它的堆栈帧就是普通的堆栈。但是它操作的Python堆栈帧是在堆上的。除了其他惊喜之外，这意味着Python的堆栈帧可以在它的调用之外存活。(FIXME:可以在它调用结束后存活)。要以交互方式查看，请从bar内保存当前帧：Python>>>importinspect>>>frame=None>>>deffoo():...bar()...>>>defbar():...globalframe...frame=inspect.currentframe()...>>>foo()>>>#Theframewasexecutingthecodefor'bar'.>>>frame.f_code.co_name'bar'>>>#Itsbackpointerreferstot>>>defbar():...globalframe...frame=inspect.currentframe()heframefor'foo'.>>>caller_frame=frame.f_back>>>caller_frame.f_code.co_name'foo'1234567891011121314151617181920>>>importinspect>>>frame=None>>>deffoo():...    bar()...>>>defbar():...    globalframe...    frame=inspect.currentframe()...>>>foo()>>>#Theframewasexecutingthecodefor'bar'.>>>frame.f_code.co_name'bar'>>>#Itsbackpointerreferstot>>>defbar():...    globalframe...    frame=inspect.currentframe()heframefor'foo'.>>>caller_frame=frame.f_back>>>caller_frame.f_code.co_name'foo'现在这项技术被用到了Python生成器（generator）上——使用代码对象和堆栈帧这些相同的组件来产生奇妙的效果。这是一个生成器函数（generatorfunction）：Python>>>defgen_fn():...result=yield1...print('resultofyield:{}'.format(result))...result2=yield2...print('resultof2ndyield:{}'.format(result2))...return'done'...1234567>>>defgen_fn():...    result=yield1...    print('resultofyield:{}'.format(result))...    result2=yield2...    print('resultof2ndyield:{}'.format(result2))...    return'done'...当Python将gen_fn编译为字节码时，它会看到yield语句，然后知道gen_fn是个生成器函数，而不是普通函数。它会设置一个标志来记住这个事实：Python>>>#Thegeneratorflagisbitposition5.>>>generator_bit=1<<5>>>bool(gen_fn.__code__.co_flags&generator_bit)True1234>>>#Thegeneratorflagisbitposition5.>>>generator_bit=1<<5>>>bool(gen_fn.__code__.co_flags&generator_bit)True当你调用一个生成器函数时，Python会看到生成器标志，实际上并不运行该函数，而是创建一个生成器（generator）：Python>>>gen=gen_fn()>>>type(gen)<class'generator'>123>>>gen=gen_fn()>>>type(gen)<class'generator'>Python生成器封装了一个堆栈帧和一个对生成器函数代码的引用，在这里就是对gen_fn函数体的引用：Python>>>gen.gi_code.co_name'gen_fn'12>>>gen.gi_code.co_name'gen_fn'调用gen_fn产生的所有生成器都指向同一个代码对象，但是每个都有自己的堆栈帧。这个堆栈帧并不存在于实际的堆栈上，它在堆内存上等待着被使用堆栈帧有个“lastinstruction”(FIXME:translatethisornot?)指针，指向最近执行的那条指令。刚开始的时候lastinstruction指针是-1，意味着生成器尚未开始：Python>>>gen.gi_frame.f_lasti-112>>>gen.gi_frame.f_lasti-1当我们调用send时，生成器达到第一个yield处然后暂停执行。send的返回值是1，这是因为gen把1传给了yield表达式：Python>>>gen.send(None)112>>>gen.send(None)1现在生成器的指令指针（instructionpointer）向前移动了3个字节码，这些是编译好的56字节的Python代码的一部分：Python>>>gen.gi_frame.f_lasti3>>>len(gen.gi_code.co_code)561234>>>gen.gi_frame.f_lasti3>>>len(gen.gi_code.co_code)56生成器可以在任何时候被任何函数恢复执行，因为它的堆栈帧实际上不在堆栈上——它在堆（内存）上。生成器在调用调用层次结构中的位置不是固定的，它不需要遵循常规函数执行时遵循的先进后出顺序。生成器被是被解放了的，它像云一样浮动。我们可以将“hello”发送到这个生成器中，它会成为yield表达式的值，然后生成器会继续执行，直到产出（yield）了2：Python>>>gen.send('hello')resultofyield:hello2123>>>gen.send('hello')resultofyield:hello2现在这个生成器的堆栈帧包含局部变量result：Python>>>gen.gi_frame.f_locals{'result':'hello'}12>>>gen.gi_frame.f_locals{'result':'hello'}从gen_fn创建的其他生成器将具有自己的堆栈帧和局部变量。当我们再次调用send时，生成器将从它第二个yield处继续执行，然后以产生特殊异常StopIteration结束：Python>>>gen.send('goodbye')resultof2ndyield:goodbyeTraceback(mostrecentcalllast):File\"<input>\",line1,in<module>StopIteration:done12345>>>gen.send('goodbye')resultof2ndyield:goodbyeTraceback(mostrecentcalllast):    File\"<input>\",line1,in<module>StopIteration:done异常有一个值，它是那个生成器的返回值：字符串“done”。1赞4收藏评论"], "art_create_time": ["2017/10/12"], "art_title": ["Python 生成器原理详解"], "art_url": ["http://python.jobbole.com/88677/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2017/10/ec54f06aac31a0fd6546af075530a578.png"]},
{"art_content": ["本文作者：伯乐在线-iPytLab。未经作者许可，禁止转载！欢迎加入伯乐在线专栏作者。前言前段时间一直在用自己写的遗传算法框架测试算法在优化力场参数的效果，但是跑起来效率很慢，因为适应度函数需要调用多次力场程序计算能量，但是还是比我预想中的慢我也没有及时对程序进行profiling和优化。直到放假前在github有个使用gaft做SVM参数优化的童鞋开了个issue中说道在gaft优化的过程中会大量调用适应度函数，这才使我在国庆放假期间对gaft进行了profiling找到程序瓶颈并针对性的优化。本文就记录下自己gaft做profiling并优化的过程以及优化的效果。正文对GAFT进行性能分析(Profiling)关于如何对Python程序进行性能分析生成分析报告并可视化分析报告，我在之前的一篇博客里《Python优化第一步:性能分析实践》进行了详细的介绍，这里我就直接分析了。为了能针对gaft中不同的函数进行分析，借助Python内置的cProfile和pstats模块我写了个装饰器方便分析并生成不同的分析统计文件。Pythondefdo_profile(filename,sortby='tottime'):'''Constructorforfunctionprofilingdecorator.'''def_do_profile(func):'''Functionprofilingdecorator.'''@wraps(func)defprofiled_func(*args,**kwargs):'''Decoratedfunction.'''#Flagfordoingprofilingornot.DO_PROF=os.getenv('PROFILING')ifDO_PROF:profile=cProfile.Profile()profile.enable()result=func(*args,**kwargs)profile.disable()ps=pstats.Stats(profile).sort_stats(sortby)ps.dump_stats(filename)else:result=func(*args,**kwargs)returnresultreturnprofiled_funcreturn_do_profile123456789101112131415161718192021222324252627defdo_profile(filename,sortby='tottime'):    '''    Constructorforfunctionprofilingdecorator.    '''    def_do_profile(func):        '''        Functionprofilingdecorator.        '''        @wraps(func)        defprofiled_func(*args,**kwargs):            '''            Decoratedfunction.            '''            #Flagfordoingprofilingornot.            DO_PROF=os.getenv('PROFILING')            ifDO_PROF:                profile=cProfile.Profile()                profile.enable()                result=func(*args,**kwargs)                profile.disable()                ps=pstats.Stats(profile).sort_stats(sortby)                ps.dump_stats(filename)            else:                result=func(*args,**kwargs)            returnresult        returnprofiled_func    return_do_profile对上面的带参数的装饰器我在这里稍微解释下，装饰器构造器do_profile的两个参数filename和sortby分别指定分析结果报告的文件名以及统计结果的排序方式。它会对需要进行性能分析的函数进行装饰，然后在函数运行完后在当前目录生成结果报告。例如我需要对gaft中遗传算法迭代主循环进行分析，则需要:Python@do_profile(filename='gaft_run.prof')defrun(self,ng=100):...123@do_profile(filename='gaft_run.prof')defrun(self,ng=100):    ...同时为了方便，我还需要一个环境变量PROFILING来启动分析:ShellexportPROFILING=y1exportPROFILING=y分析结果这里为了方便查看函数的相互调用关系，我是用了pyprof2calltree然后使用Mac上的QCacheGrind来可视化分析结果:Shellpyprof2calltree-igaft_run.prof-k1pyprof2calltree-igaft_run.prof-k将Python的profiling文件转换并直接调用QCacheGrind便可以方便的查看分析相关信息。通过调用关系图可以看到，gaft的初始版本的min,max,mean等函数多次调用best_indv和worst_indv会多次调用适应度函数来相互比较，而通常情况下用户自定义的适应度函数都是需要额外去调用外部程序的，一般都比较费时。所以必须要通过优化best_indv和worst_indv对fitness的调用次数才能提升gaft的效率。优化GAFT函数返回值缓存从之前我写的best_indv中可以看到，我将fitness作为key用于获取最大值，Python内置的max函数会内部调用fitness进行相互比较来获取最大值，这个时候便对fitness进行了多余的调用，因为在遗传算法中，每一代的population中的个体是不会发生变化的我们只需要在每一次迭代的一开始调用fitnessn次就好了(n为种群大小)，每一代中再次需要用到适应度值的地方直接获取。这样需要我们对种群中的个体进行惰性求值，也就是对所有的fitness的值进行缓存。这种操作我在优化自己的催化动力学程序的时候也使用过，叫做函数返回值缓存.但是在gaft中这种缓存有稍微麻烦一点，因为缓存并不是缓存一次就可以一直用了，它会随着条件的变化需要重新计算种群中所有个体的适应度然后重新缓存。重新计算适应度值需要同时满足的条件种群中的所有个体没有发生任何变化(如果变化了那肯定要重新计算适应度值了)。已有缓存的适应度值(如果是第一次那肯定需要计算一次所有个体的适应度值)。计算适应度值的适应度函数与之前比较没有发生变化(如果计算适应度函数都改变了，那当然需要重新估计适应度值了)。函数返回值缓存描述符为此我写了个装饰器来缓存函数的返回值:PythonclassMemoized(object):'''Descriptorforpopulationstatisticalvariblescaching.'''def__init__(self,func):self.func=funcself.result=Noneself.fitness=Nonedef__get__(self,instance,cls):self.instance=instancereturnselfdef__call__(self,fitness):if((notself.instance._updated)#populationnotchangedand(self.resultisnotNone)#resultalreadycachedand(fitness==self.fitness)):#fitnessnotchanged#Returncachedresultdirectly.returnself.resultelse:#Updatefitnessfunction.self.fitness=fitness#Updateandmemoizeresult.self.result=self.func(self.instance,fitness)#Recoverflag.self.instance._updated=Falsereturnself.result12345678910111213141516171819202122232425classMemoized(object):    '''    Descriptorforpopulationstatisticalvariblescaching.    '''    def__init__(self,func):        self.func=func        self.result=None        self.fitness=None    def__get__(self,instance,cls):        self.instance=instance        returnself    def__call__(self,fitness):        if((notself.instance._updated)        #populationnotchanged                and(self.resultisnotNone)    #resultalreadycached                and(fitness==self.fitness)):  #fitnessnotchanged            #Returncachedresultdirectly.            returnself.result        else:            #Updatefitnessfunction.            self.fitness=fitness            #Updateandmemoizeresult.            self.result=self.func(self.instance,fitness)            #Recoverflag.            self.instance._updated=False            returnself.result动态监视种群的变化好了上面我们可以通过描述符来缓存函数返回值，但是一旦种群不满足上述的三个条件就需要重新计算适应度值，那我们如何监控种群的变化呢？我在GAPopulation中添加了一个标记_updated用于标记种群是否已经发生了变化,然后我们的任务就是在其他能够影响到种群的地方试图去更新这个flag。如何能更Pythonic的更新这个标记呢？所谓的种群发生变化，也是就种群中的个体列表发生了变化，种群中的个体我都放在了一个列表中，我需要监控这个列表是否发生变化以便更新flag，具体又是那些变化呢？列表整体是否发生了变化(赋值操作)列表中的元素是否发生变化(对列表中的元素赋值操作，列表的append, extend操作等)好了我们要具体怎么实现呢？1）对于第一种，由于Python中无法进行赋值运算符重载，但是我们可以通过描述符的__set__来处理:PythonclassGAIndividuals(object):'''Descriptorforallindividualsinpopulation.'''def__init__(self,name):self.name='_{}'.format(name)def__get__(self,instance,owner):returninstance.__dict__[self.name]def__set__(self,instance,value):instance.__dict__[self.name]=value#Updateflag.instance._updated=True123456789101112classGAIndividuals(object):    '''    Descriptorforallindividualsinpopulation.    '''    def__init__(self,name):        self.name='_{}'.format(name)    def__get__(self,instance,owner):        returninstance.__dict__[self.name]    def__set__(self,instance,value):        instance.__dict__[self.name]=value        #Updateflag.        instance._updated=True2）对于第二种情况，我们需要对Python的List类型的相应方法进行override但是嘞，即使重写了list的接口，又如何更新population中的变量呢？这个时候就需要用闭包了。在GAPopulation的构造函数__init__中定义list的派生类，并立即实例化，这时候派生类的便可以获取population对象了，于是GAPopulation的构造函数可以这些写:Pythondef__init__(self,indv_template,size=100):#...#Flagformonitoringchangesofpopulation.self._updated=False#Containerforallindividuals.classIndvList(list):'''Aproxyclassinheritedfrombuilt-inlisttocontainallindividualswhichcanupdatethepopulation._updatedflagautomaticallywhenitscontentischanged.'''#NOTE:Use'this'heretoavoidnameconflict.def__init__(this,*args):super(this.__class__,this).__init__(*args)def__setitem__(this,key,value):'''Override__setitem__inbuilt-inlisttype.'''old_value=this[key]ifold_value==value:returnsuper(this.__class__,self).__setitem__(key,value)#Updatepopulationflag.self._updated=Truedefappend(this,item):'''Overrideappendmethodofbuilt-inlisttype.'''super(this.__class__,this).append(item)#Updatepopulationflag.self._updated=Truedefextend(this,iterable_item):ifnotiterable_item:returnsuper(this.__class__,this).extend(iterable_item)#Updatepopulationflag.self._updated=Trueself._individuals=IndvList()1234567891011121314151617181920212223242526272829303132333435363738def__init__(self,indv_template,size=100):    #...    #Flagformonitoringchangesofpopulation.    self._updated=False    #Containerforallindividuals.    classIndvList(list):        '''        Aproxyclassinheritedfrombuilt-inlisttocontainall        individualswhichcanupdatethepopulation._updatedflag        automaticallywhenitscontentischanged.        '''        #NOTE:Use'this'heretoavoidnameconflict.        def__init__(this,*args):            super(this.__class__,this).__init__(*args)        def__setitem__(this,key,value):            '''            Override__setitem__inbuilt-inlisttype.            '''            old_value=this[key]            ifold_value==value:                return            super(this.__class__,self).__setitem__(key,value)            #Updatepopulationflag.            self._updated=True        defappend(this,item):            '''            Overrideappendmethodofbuilt-inlisttype.            '''            super(this.__class__,this).append(item)            #Updatepopulationflag.            self._updated=True        defextend(this,iterable_item):            ifnotiterable_item:                return            super(this.__class__,this).extend(iterable_item)            #Updatepopulationflag.            self._updated=True    self._individuals=IndvList()优化效果通过上面对代码的优化，我们看看我们优化的效果如何，使用分析描述符来分析GAEngine.run跑一代种群的情况，其中种群大小为10。如下图为cProfile生成的分析报告对比:可以看到优化后的跑一代种群的时间缩短为将近原来的1/7!优化效果还是很明显的。然后看一看调用关系图:energy_fitness的调用次数从3807降到了621次！总结本文记录了遗传算法框架GAFT的一次profiling和优化过程，通过缓存值的方式极大的减少了适值函数的调用次数，在时间上，跑一代种群的效率提升了7倍左右。打赏支持我写出更多好文章，谢谢！打赏作者打赏支持我写出更多好文章，谢谢！1赞4收藏评论关于作者：iPytLab喜欢写程序的计算化学狗，Python/C/C++/Fortran,个人博客http://pytlab.org个人主页·我的文章·22·"], "art_create_time": ["2017/10/09"], "art_title": ["遗传算法框架GAFT优化小记"], "art_url": ["http://python.jobbole.com/88666/"], "art_img": ["http://pytlab.org/assets/images/blog_img/2017-10-08-%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95%E6%A1%86%E6%9E%B6GAFT%E4%BC%98%E5%8C%96%E5%B0%8F%E8%AE%B0/optimized.png"]},
{"art_content": ["本文由伯乐在线-PJing翻译，艾凌风校稿。未经许可，禁止转载！英文出处：www.eversql.com。欢迎加入翻译组。什么是ORM？在介绍Python的ORM框架（Django和SQLAlchemy）不同之前，我们先要确保完全理解ORM框架的用途。ORM代表对象关系映射（ObjectRelationalMapping）。让我们依次看看这三个单词，它们正好解释了ORM在真实环境中的用处:●对象–这部分表示使用框架的对象和编程语言，例如Python。● 关系–这部分表示正在使用的RDBMS(关系数据库管理系统)数据库。其中包括许多流行的关系数据库，而你可能正在使用以下数据库—MSSQL、MySQL、Oracle数据库、PostgreSQL、MariaDB、PerconaDB、TokuDB。大多数关系数据库之间的共同点是它们的关系结构(表、列，键、约束等)。●映射–最后这部分表示前两部分对象和数据表之间的桥梁和连接。因此可以得出的结论是ORM是为了将编程语言与数据库之间相连，以便简化创建依赖于数据的应用程序过程。Django和SQLAlchemy之间比较活动记录vs数据映射DjangoORM采用活动记录实现—大多数ORM中能看到这种实现。基本上也可以说是数据库中每一行都直接映射到代码中的对象，反之亦然。ORM框架（如Django）不需要为了在代码中使用属性而预先定义架构，只需要使用它们，因为框架可以通过查看数据库架构“理解”结构。此外，也可以只保存记录到数据库，因为它也映射到表中的特定行。SQLAlchemy采用数据映射实现—当使用这种方式实现时，数据库结构和对象结构之间存在间隙（它们不像活动记录的实现是1:1）。大多数情况下，必须使用另外的持久层来保持与数据库的交互（例如保存对象）。因此当采用活动记录实现的时候不能只调用save()方法（反对观点），但另一方面，代码不需要知道数据库中整个关系结构的运行，因为代码和数据库之间没有直接关系。那么它们之间谁获胜了呢？都没有。这取决于你要实现什么。我相信如果你的应用程序大多是CRUD(创建、读取、更新、删除)程序，而在不同数据实体之间没有使用困难且复杂规则，那么应该采用活动记录实现(Django)。它将帮助你轻松快速地为产品设置MVP，而不会有任何困难。如果有许多“业务规则”和限制条件，最好采用数据映射模型，因为它不会捆绑并强迫严格遵照活动记录来考量。使用复杂查询在某些情况下，Django和SQLAlchemy可以同时使用。现实环境中我多次见到主用例是Django用于所有常规CRUD操作，而SQLAlchemy用于更复杂的查询，通常是只读查询。有关这方面更多的信息和实例，可以看看BetterWorks工程博客(我们没有任何联系，但不管怎样，我们喜欢他们的博客)。主键自动生成两个框架之间的另一个不同是Django能为表自动创建主键,SQLAlchemy却做不到。必须手动为每张表创建主键。权衡利弊—你认为哪种框架最清楚符合表的主键？根据团队的知识和经验，可以自行决定。自动提交默认情况下，Django会自动提交，SQLAlchemy却不行。自动提交会影响使用框架的方式（事务、回滚等）。支持的数据库Django和SQLAlchemy都能用于MySQL、PostgreSQL、Oracle和SQLite。如果你正在使用MSSQL，则应该使用SQLAlchemy，因为它完全支持MSSQL，并且也可以找到更多相关的信息和文档。学习曲线在网上有一个普遍的观点，认为Django更容易学习。这是显而易见的，由于它通常都用在没有特别复杂的用例上。因此，应该考虑愿意投入多少精力来学习框架，与SQLAlchemy交叉学习以便获得更多的灵活性（假使你真的需要它）。社区规模毫无疑问，在PythonORM框架中SQLAlchemy拥有最大的社区。如果社区对你至关重要（我认为它应该是），SQLAlchemy该是你的选择。这并不说明对于其它框架，你不能找到任何帮助，例如Django。你也可以获得bug修复，从StackOverflow得到问题的答案和其它需要的帮助，但概率仅仅比SQLAlchemy高。性能我认为只在这里写（X比Y快）是不负责任的。由于ORM具有如此多特征和功能，并且它们在每个框架中也不同，这将很难得出结论。根据我的经验，使用框架特性的方式，会对应用程序中数据层的整体性能产生极大影响。因此我建议不要通过性能来选择框架，而是应该学习如何合理利用框架。假如在ORM框架中使用原始的SQL查询、使用Jooq或者只是部分查询不使用ORM，可以了解EverSQL查询优化器，这可能是最简单优化任何查询的方法。总结任何比较中，我认为最好把决策权交还给读者。每个用例之间是不同的，不同的技术也可以更适用。看看上面指出的差异，让我们知道你做出了什么决定。1赞4收藏1评论关于作者：PJing简介真还没来得及写:）:(个人主页·我的文章·13"], "art_create_time": ["2017/10/13"], "art_title": ["Django vs SQLAlchemy：哪个 Python ORM 更好"], "art_url": ["http://python.jobbole.com/88672/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2015/10/bfa0d07e7eb2fac2eb80cd5df9567931.jpg"]},
{"art_content": ["原文出处：Rookie   最近在阅读Python微型Web框架Bottle的源码，发现了Bottle中有一个既是装饰器类又是描述符的有趣实现。刚好这两个点是Python比较的难理解，又混合在一起，让代码有些晦涩难懂。但理解代码之后不由得为Python语言的简洁优美赞叹。所以把相关知识和想法稍微整理，以供分享。正文Bottle是Python的一个微型Web框架，所有代码都在一个bottle.py文件中，只依赖标准库实现，兼容Python2和Python3，而且最新的稳定版0.12代码也只有3700行左右。虽然小，但它实现了Web框架基本功能。这里就不以过多的笔墨去展示Bottle框架，需要的请访问其网站了解更多。这里着重介绍与本文相关的重要对象request。在Bottle里，request对象代表了当前线程处理的请求，客户端发送的请求数据如表单数据，请求网站和cookie都可以从request对象中获得。下面是官方文档中的两个例子frombottleimportrequest,route,response,templatePython#获取客户端cookie以实现登陆时问候用户功能@route('/hello')defhello():name=request.cookie.usernameor'Guest'returntemplate('Hello{{name}}',name=name)#获取形如/forum?id=1&page=5的查询字符串中id和page变量的值route('/forum')defdisplay_forum():forum_id=request.query.idpage=request.query.pageor'1'returntemplate('ForumID:{{id}}(page{{page}})',id=forum_id,page=page)123456789101112#获取客户端cookie以实现登陆时问候用户功能@route('/hello')defhello():    name=request.cookie.usernameor'Guest'    returntemplate('Hello{{name}}',name=name) #获取形如/forum?id=1&page=5的查询字符串中id和page变量的值route('/forum')defdisplay_forum():    forum_id=request.query.id    page=request.query.pageor'1'    returntemplate('ForumID:{{id}}(page{{page}})',id=forum_id,page=page)那么Bottle是如何实现的呢？根据WSGI接口规定，所有的HTTP请求信息都包含在一个名为envrion的dict对象中。所以Bottle要做的就是把HTTP请求信息从environ解析出来。在深入Request类如何实现之前先要了解下Bottle的FormsDict。FormsDict与字典类相似，但扩展了一些功能，比如支持属性访问、一对多的键值对、WTForms支持等。它在Bottle中被广泛应用，如上面的示例中cookie和query数据都以FormsDict存储，所以我们可以用request.query.page的方式获取相应属性值。下面是0.12版Bottle中Request类的部分代码，0.12版中Request类继承了BaseRequest，为了方便阅读我把代码合并在一起，同时还有重要的DictProperty的代码。需要说明的是Request类__init__传入的environ参数就是WSGI协议中包含HTTP请求信息的envrion，而query方法中的_parse_qsl函数可以接受形如/forum?id=1&page=5原始查询字符串然后以[(key1,value1),(ke2,value2),…]的list返回。PythonclassDictProperty(object):\"\"\"Propertythatmapstoakeyinalocaldict-likeattribute.\"\"\"def__init__(self,attr,key=None,read_only=False):self.attr,self.key,self.read_only=attr,key,read_onlydef__call__(self,func):functools.update_wrapper(self,func,updated=[])self.getter,self.key=func,self.keyorfunc.__name__returnselfdef__get__(self,obj,cls):ifobjisNone:returnselfkey,storage=self.key,getattr(obj,self.attr)ifkeynotinstorage:storage[key]=self.getter(obj)returnstorage[key]def__set__(self,obj,value):ifself.read_only:raiseAttributeError(\"Read-Onlyproperty.\")getattr(obj,self.attr)[self.key]=valuedef__delete__(self,obj):ifself.read_only:raiseAttributeError(\"Read-Onlyproperty.\")delgetattr(obj,self.attr)[self.key]classRequest:def__init__(self,environ=None):self.environ{}ifenvironisNoneelseenvrionself.envrion['bottle.request']=self@DictProperty('environ','bottle.request.query',read_only=True)defquery(self):get=self.environ['bottle.get']=FormsDict()pairs=_parse_qsl(self.environ.get('QUERY_STRING',''))forkey,valueinpairs:get[key]=valuereturnget12345678910111213141516171819202122232425262728293031323334353637classDictProperty(object):    \"\"\"Propertythatmapstoakeyinalocaldict-likeattribute.\"\"\"     def__init__(self,attr,key=None,read_only=False):        self.attr,self.key,self.read_only=attr,key,read_only     def__call__(self,func):        functools.update_wrapper(self,func,updated=[])        self.getter,self.key=func,self.keyorfunc.__name__        returnself     def__get__(self,obj,cls):        ifobjisNone:returnself        key,storage=self.key,getattr(obj,self.attr)        ifkeynotinstorage:storage[key]=self.getter(obj)        returnstorage[key]     def__set__(self,obj,value):        ifself.read_only:raiseAttributeError(\"Read-Onlyproperty.\")        getattr(obj,self.attr)[self.key]=value     def__delete__(self,obj):        ifself.read_only:raiseAttributeError(\"Read-Onlyproperty.\")        delgetattr(obj,self.attr)[self.key] classRequest:    def__init__(self,environ=None):        self.environ{}ifenvironisNoneelseenvrion        self.envrion['bottle.request']=self        @DictProperty('environ','bottle.request.query',read_only=True)    defquery(self):        get=self.environ['bottle.get']=FormsDict()        pairs=_parse_qsl(self.environ.get('QUERY_STRING',''))        forkey,valueinpairs:            get[key]=value        returngetquery方法的逻辑和代码都比较简单，就是从environ中获取’QUERY_STRING’，并用把原始查询字符串解析为一个FormsDict，将这个FormsDict赋值给environ[‘bottle.request.query’]并返回。但这个函数的装饰器的作用就有些难以理解，装饰器的实现方式都是”dunder”特殊方法，有些晦涩难懂。如果上来就看这些源码可能难以理解代码实现的功能。那不如这些放一边，假设自己要实现这些方法，你会写出什么代码。一开始你可能写出这样的代码。Python#version1classRequest:\"\"\"somecodeshere\"\"\"defquery(self):get=self.environ['bottle.get']=FormsDict()pairs=_parse_qsl(self.environ.get('QUERY_STRING',''))forkey,valueinpairs:get[key]=valuereturnget1234567891011#version1classRequest:    \"\"\"    somecodeshere    \"\"\"    defquery(self):        get=self.environ['bottle.get']=FormsDict()        pairs=_parse_qsl(self.environ.get('QUERY_STRING',''))        forkey,valueinpairs:            get[key]=value        returnget这样确实实现了解析查询字符串的功能，但每次在调用这个方法时都需要对原始查询字符串解析一次，实际上在处理某特请求时，查询字符串是不会改变的，所以我们只需要解析一次并把它保存起来，下次使用时直接返回就好了。另外此时的query方法还是一个普通方法，必须使用这样的方法来调用它Python#获取idrequest.query().id#获取pagerequest.query().page1234#获取idrequest.query().id#获取pagerequest.query().pagequery后面的小括号让语句显得不那么协调，其实就是我觉得它丑。要是也能和官方文档中的示例实现以属性访问的方式获取相应的数据就好了。所以代码还得改改。Python#querymethodversion2classRequest:\"\"\"somecodeshere\"\"\"@propertydefquery(self):if'bootle.get.query'notinself.environ:get=self.environ['bottle.get']=FormsDict()pairs=_parse_qsl(self.environ.get('QUERY_STRING',''))forkey,valueinpairs:get[key]=valuereturnself.environ['bottle.get.query']12345678910111213#querymethodversion2classRequest:    \"\"\"    somecodeshere    \"\"\"    @property    defquery(self):        if'bootle.get.query'notinself.environ:            get=self.environ['bottle.get']=FormsDict()            pairs=_parse_qsl(self.environ.get('QUERY_STRING',''))            forkey,valueinpairs:                get[key]=value        returnself.environ['bottle.get.query']第二版改变的代码就两处，一个是使用property装饰器，实现了request.query的访问方式；另一个就是在query函数体中增加了判断’bottle.get.query’是否在environ中的判断语句，实现了只解析一次的要求。第二版几乎满足了所有要求，它表现得就像Bottle中真正的query方法一样。但它还是有些缺陷。首先，Request类并不只有query一个方法，如果要编写完整的Request类就会发现，有很多方法的代码与query相似，都是从environ中解析出需要的数据，而且都只需要解析一次，保存起来，第二次或以后访问时返回保存的数据就好了。所以可以考虑将属性管理的代码从方法体内抽象出来，正好Python中的描述符可以实现这样的功能。另外如果使用Bottle的开发者在写代码时不小心尝试进行request.query=some_data的赋值时，将会抛出如下错误。Python>>>AttributeError:can'tsetattribute1>>>AttributeError:can'tsetattribute我们确实希望属性是只读的，在对其赋值时应该抛出错误，但这样的报错信息并没有提供太多有用的信息，导致调bug时一头雾水，找不到方向。我们更希望抛出如Python>>>AttributeError:Read-onlyproperty1>>>AttributeError:Read-onlyproperty这样明确的错误信息。所以第三版的代码可以这样写Python#querymethodversion3classDescriptor:def__init__(self,attr,key,getter,read_only=False):self.attr=attrself.key=keyself.getter=getterself.read_only=read_onlydef__set__(self,obj,value):ifself.read_only:raiseAttributeError('Readonlyproperty.')getattr(obj,self.attr)[self.key]=valuedef__get__(self,obj,cls):ifobjisNone:returnselfkey,storage=self.key,getattr(obj,self.attr)ifkeynotinstorage:storage[key]=self.getter(obj)returnstorage[key]def__delete__(self,obj):ifself.read_only:raiseAttributeError('Readonlyproperty.')delgetattr(obj,self.attr)[self.key]classReqeust:\"\"\"somecodes\"\"\"defquery(self):get=self.environ['bottle.get']=FormsDict()pairs=_parse_qsl(self.environ.get('QUERY_STRING',''))forkey,valueinpairs:get[key]=valuereturngetquery=Descriptor('environ','bottle.get.query',query,read_only=True)12345678910111213141516171819202122232425262728293031323334353637#querymethodversion3classDescriptor:    def__init__(self,attr,key,getter,read_only=False):        self.attr=attr        self.key=key        self.getter=getter        self.read_only=read_only        def__set__(self,obj,value):        ifself.read_only:                raiseAttributeError('Readonlyproperty.')        getattr(obj,self.attr)[self.key]=value        def__get__(self,obj,cls):        ifobjisNone:            returnself        key,storage=self.key,getattr(obj,self.attr)        ifkeynotinstorage:            storage[key]=self.getter(obj)        returnstorage[key]        def__delete__(self,obj):        ifself.read_only:            raiseAttributeError('Readonlyproperty.')        delgetattr(obj,self.attr)[self.key] classReqeust:    \"\"\"    somecodes    \"\"\"    defquery(self):        get=self.environ['bottle.get']=FormsDict()        pairs=_parse_qsl(self.environ.get('QUERY_STRING',''))        forkey,valueinpairs:            get[key]=value        returnget      query=Descriptor('environ','bottle.get.query',query,read_only=True)第三版的代码没有使用property装饰器，而是使用了描述符这个技巧。如果你之前没有见到过描述符，在这里限于篇幅只能做个简单的介绍，但描述符涉及知识点众多，如果有不清楚之处可以看看《流畅的Python》第20章属性描述符，里面有非常详细的介绍。简单来说，描述符是对多个属性运用相同存取逻辑的一种方式，如Bottle框架里我们需要对很多属性都进行判断某个键是否在environ中，如果在则返回，如果不在，需要解析一次这样的存取逻辑。而描述符需要实现特定协议，包括__set__,__get__,__delete___方法，分别对应设置，读取和删除属性的方法。他么的参数也比较特殊，如__get__方法的三个参数self,obj,cls分别对应描述符实例的引用，对第三版的代码来说就是Descriptor(‘environ’,‘bottle.get.query’,query,read_only=True)创建的实例的引用；obj则对应将某个属性托管给描述的实例对象的引用，对应的应该为request对象；而cls则为Request类的引用。在调用request.query时编译器会自动传入这些参数。如果以Request.query的方式调用，那么obj参数的传入值为None，这时候通常的处理是返回描述符实例。在Descriptor中__get__方法的代码最多，也比较难理解，但如果记住其参数的意义也没那么难。下面以query的实现为例，我添加一些注释来帮助理解Pythonkey,storage=self.key,getattr(obj,self.attr)#key='bottle.get.query'#storage=environ即包含HTTP请求的信息的environ#判断envrion中是否包含key来决定是否需要解析ifkeynotinstorage:storage[key]=self.getter(obj)#self.getter(obj)就是调用了原来的query方法，不过要传入一个Request实例，也就是objreturnstorage[key]123456789key,storage=self.key,getattr(obj,self.attr)#key='bottle.get.query'#storage=environ即包含HTTP请求的信息的environ #判断envrion中是否包含key来决定是否需要解析ifkeynotinstorage:    storage[key]=self.getter(obj)    #self.getter(obj)就是调用了原来的query方法，不过要传入一个Request实例，也就是objreturnstorage[key]而__set__,__delete__代码比较简单，在这里我们把只读属性在赋值和删除时抛出的错误定制为AttributeError(‘Readonlyproperty.’)，方便调试。通过使用描述符这个有些难懂的方法，我们可以在Request的方法中专心于编写如何解析的代码，不用担心属性的存取逻辑。和在每个方法中都使用if判断相比高到不知道哪里去。但美中不足的是，这样让我们的方法代码后面拖着一个“小尾巴”，即Pythonquery=Descriptor('envrion','bottle.get.query',query,read_only=True)1query=Descriptor('envrion','bottle.get.query',query,read_only=True)怎么去掉这个这个“小尾巴“呢？回顾之前的代码几乎都是对query之类的方法进行修饰，所以可以尝试使用装饰器，毕竟装饰器就是对某个函数进行修饰的，而且我们应该使用参数化的装饰器，这样才能将envrion等参数传递给装饰器。如果要实现参数化装饰器就需要一个装饰器工厂函数，也就是说装饰器的代码里需要嵌套至少3个函数体，写起来有写绕，代码可阅读性也有差。更大的问题来自如何将描述符与装饰器结合起来，因为Descriptor是一个类而不是方法。解决办法其实挺简单的。如果知道Python中函数也是对象，实现了__call__方法的对象可以表现得像函数一样。所以我们可以修改Descirptor的代码，实现__call__方法，让它的实例成为callable对象就可以把它用作装饰器；而要传入的参数可以以实例属性存储起来，通过self.attribute的形式访问，而不是像使用工厂函数实现参数化装饰器时通过闭包来实现参数的访问获取。这时候再来看看Bottle里的DictProperty代码PythonclassDictProperty(object):\"\"\"Propertythatmapstoakeyinalocaldict-likeattribute.\"\"\"def__init__(self,attr,key=None,read_only=False):self.attr,self.key,self.read_only=attr,key,read_onlydef__call__(self,func):functools.update_wrapper(self,func,updated=[])self.getter,self.key=func,self.keyorfunc.__name__returnselfdef__get__(self,obj,cls):ifobjisNone:returnselfkey,storage=self.key,getattr(obj,self.attr)ifkeynotinstorage:storage[key]=self.getter(obj)returnstorage[key]def__set__(self,obj,value):ifself.read_only:raiseAttributeError(\"Read-Onlyproperty.\")getattr(obj,self.attr)[self.key]=valuedef__delete__(self,obj):ifself.read_only:raiseAttributeError(\"Read-Onlyproperty.\")delgetattr(obj,self.attr)[self.key]123456789101112131415161718192021222324classDictProperty(object):    \"\"\"Propertythatmapstoakeyinalocaldict-likeattribute.\"\"\"     def__init__(self,attr,key=None,read_only=False):        self.attr,self.key,self.read_only=attr,key,read_only     def__call__(self,func):        functools.update_wrapper(self,func,updated=[])        self.getter,self.key=func,self.keyorfunc.__name__        returnself     def__get__(self,obj,cls):        ifobjisNone:returnself        key,storage=self.key,getattr(obj,self.attr)        ifkeynotinstorage:storage[key]=self.getter(obj)        returnstorage[key]     def__set__(self,obj,value):        ifself.read_only:raiseAttributeError(\"Read-Onlyproperty.\")        getattr(obj,self.attr)[self.key]=value     def__delete__(self,obj):        ifself.read_only:raiseAttributeError(\"Read-Onlyproperty.\")        delgetattr(obj,self.attr)[self.key]其实就是一个有描述符作用的装饰器类，它的使用方法很简单：Python@DictProperty('environ','bottle.get.query',read_only=True)defquery(self):\"\"\"somecodes\"\"\"123@DictProperty('environ','bottle.get.query',read_only=True)defquery(self):    \"\"\"somecodes\"\"\"拆开会更好理解点：Pythonproperty=DictProperty('environ','bottle.get.query',read_only=True)@propertydefquery(self):\"\"\"somecodes\"\"\"1234property=DictProperty('environ','bottle.get.query',read_only=True)@propertydefquery(self):    \"\"\"somecodes\"\"\"再把@实现的语法糖拆开：Pythondefquery(self):\"\"\"somecodes\"\"\"property=DictProperty('environ','bottle.get.query',read_only=True)query=property(query)#@实现的语法糖12345defquery(self):    \"\"\"somecodes\"\"\" property=DictProperty('environ','bottle.get.query',read_only=True)query=property(query)#@实现的语法糖再修改以下代码形式：Pythondefquery(self):\"\"\"somecodes\"\"\"query=DictProperty('environ','bottle.get.query',read_only=True)(query)1234defquery(self):    \"\"\"somecodes\"\"\" query=DictProperty('environ','bottle.get.query',read_only=True)(query)是不是和第三版的实现方式非常相似。Pythondefquery(self):\"\"\"somecodes\"\"\"query=Descriptor('environ','bottle.get.query',query,read_only=True)1234defquery(self):    \"\"\"somecodes\"\"\" query=Descriptor('environ','bottle.get.query',query,read_only=True)但我们可以使用装饰器把方法体后面那个不和谐的赋值语句”小尾巴“去掉，将属性存取管理抽象出来，而且只需要使用一行非常简便的装饰器把这个功能添加到某个方法上。这也许就是Python的美之一吧。写在后面DictProperty涉及知识远不止文中涉及的那么简单，如果你还是不清楚DictProperty的实现功能，建议阅读《流畅的Python》第7章和第22章，对装饰器和描述符有详细的描述，另外《PythonCookbook》第三版第9章元编程有关于参数化装饰器和装饰器类的叙述和示例。如果你对Bottle为什么要实现这样的功能感到困惑，建议阅读Bottle的文档和WSGI相关的文章。其实前一阵再阅读Bottle源码时就想写一篇文章，但奈何许久不写东西文笔生疏加上医院实习期间又比较忙，一直推到现在才终于磕磕绊绊地把我阅读的Bottle源码的一些感悟写出来，希望对喜欢Python的各位有些帮助把。1赞2收藏评论"], "art_create_time": ["2017/10/15"], "art_title": ["Bottle 框架中的装饰器类和描述符应用"], "art_url": ["http://python.jobbole.com/88701/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2015/02/edecceebedd0d04aa17bccba430ddcaf.jpg"]},
{"art_content": ["本文由伯乐在线-Coureur翻译，艾凌风校稿。未经许可，禁止转载！英文出处：JeffKnupp。欢迎加入翻译组。Python中由于使用了全局解释锁（GIL）的原因，代码并不能同时在多核上并发的运行，也就是说，Python的多线程不能并发，很多人会发现使用多线程来改进自己的Python代码后，程序的运行效率却下降了。这篇文章对Python中的全局解释锁（GIL）进行了介绍。作者认为这是Python中最令人头疼的问题。十年多年来，Python的全局解释器锁（GIL）给新手和专家们带来了巨大的挫折感和好奇心。悬而未决的问题每个领域都会有这么一个问题：它难度大、耗时多，仅仅是尝试解决这个问题都会让人震惊。整个社区在很久以前就放弃了这个问题，现在只有少数人在努力试图解决它。对于初学者来说，解决这样高难度的问题，会给他带来足够的声誉。计算机科学领域中的P=NP就是这样的问题。如果能用多项式时间复杂度解决这个问题，那简直就可以改变世界了。Python中最困难的问题比P=NP要容易一些，不过迄今仍然没有一个满意的答案，解决这个问题和解决P=NP问题一样具有革命性。正因为如此，Python社区会有如此多的人关注于这个的问题:“对于全局解释器锁（GIL）能做什么?”Python的底层要理解GIL的含义，我们需要从Python的基础说起。像C++这样的语言属于编译型语言，顾名思义，该类型语言的代码输入到编译器，由编译器根据语言的语法进行解析，生成与语言无关的中间表示，最后链接成由高度优化的机器码组成的可执行程序。因为编译器可以获取全部代码（或者是一大段相对独立的代码），所以编译器可以对代码进行深度优化。这使得它可以对不同的语言结构之间的交互进行推理，从而做出更有效的优化。相反，Python是解释型语言。代码被输入到解释器来运行。解释器在执行之前对代码一无所知；它只知道Python的规则，以及如何在执行过程中动态地应用这些规则。它也有一些优化，但是和编译型语言的优化完全不同。由于解释器不能很好地对代码进行推导，Python的大部分优化其实是解释器本身的优化。更快的解释器自然意味着更快的程序运行速度，而这种优化对开发者来说是免费的。也就是说，解释器优化后，开发者不用修改Python代码就可以坐享优化带来的好处。这是非常重要的一点，这里有必要在强调一下。在同等条件下，Python程序的运行速度与解释器的“速度”直接相关相关。无论开发者怎样优化自己的代码，程序的执行速度还是受限于解释器的执行效率。很明显，这就是为什么做了如此多的工作去优化Python解释器。这大概是离Python开发者最近的免费的午餐。免费午餐结束了还是没有结束？摩尔定律告诉了我们硬件提速的时间表，同时，整整一代程序员学会了如何在摩尔定律下编写代码。如果程序员写了比较慢的代码，最简单的办法通常是稍稍等待一下更快的处理器问世即可。事实上，摩尔定律仍然是并且会在很长一段时间内是有效的，不过它生效的方式有了根本的变化。时钟频率不会稳定增长到一个高不可攀的速度，取而代之的是通过多核来利用晶体管密度提高带来的好处。想要程序能够充分利用新处理器的性能，就必须按照并发方式对代码进行重写。大部分开发者听到“并发”通常会马上想到多线程程序。目前，多线程仍是利用多核系统最常见的方式。多线程编程比传统的“顺序”编程要难很多，不过仔细的程序员可以在代码中充分利用多线程的并发性。既然几乎所有应用广泛的现代编程语言都支持多线程编程，语言在多线程方面的实现应该是事后添加上去的。意外的事实现在我们来看一下问题的症结所在。想要利用多核系统，Python必须支持多线程。作为解释型语言，Python的解释器对多线程的支持必须是既安全又高效的。我们都知道多线程编程带来的问题。解释器必须避免不同的线程操作内部共享的数据。同时还要保证用户线程能完成尽量多的计算。那么在不同线程同时访问数据时，怎样才能保护数据呢？答案是全局解释器锁。顾名思义，这是一个加在解释器上的全局锁（从互斥量或者类似意义上来看）。这种方式是很安全，但是（对于Python初学者来说）这也就意味着：对于任何Python程序，不论有多少线程，多少处理器，任何时候都只有一个线程在执行。许多人都是偶然发现这个事实。网上的讨论组和留言板充斥着来自Python初学者和专家提出的类似的问题：为什么我全新的多线程Python程序运行得比其只有一个线程的时候还要慢？在问这个问题时，许多人还觉得自己像个傻瓜，因为如果程序确实是可并行的，那么两个线程的程序显然要比单线程要快。事实上，问及这个问题的次数实在太多了，Python的专家们已经为它准备了一个标准答案：不要使用多线程，请使用多进程。但这个答案比问题本身更加让人困惑：难道我不能在Python中使用多线程？在Python这样流行的语言中使用多线程究竟是有多糟糕，连专家都建议不要使用。是我哪里没有搞明白吗？很遗憾，并不是。由于Python解释器的设计，使用多线程以提高性能可以算是一个困难的任务。在最坏的情况下，多线程反而会降低（有时很明显）程序的运行速度。一个计算机科学专业的新生就可以告诉你：当多个线程竞争一个共享资源时将会发生什么。结果通常不理想。很多情况下多线程都能很好地工作，对于解释器的实现和内核开发人员来说，不要对Python多线程性能有太多抱怨可能是他们最大的心愿。现在该怎么办呢？慌了吗？我们现在能做什么呢？难道作为Python开发人员的我们要放弃使用多线程来实现并行吗？为什么GIL在某一时刻只允许一个线程在运行呢？在并发访问时，难道不可以用粒度更细的锁来保护多个独立对象？为什么没有人做过类似的尝试呢？这些问题很实用，它们的答案也十分有趣。GIL为很多对象的访问提供这保护，比如当前线程状态和为垃圾回收而用的堆分配对象。这对Python语言来说没什么奇怪的，它需要使用一个GIL。这是该实现的一种产物。现在也有不使用GIL的Python解释器（和编译器）。但是对于CPython来说，从其产生到现在GIL就一直在存在了。那么为什么我们不抛弃GIL呢？许多人也许不知道，1999年的时候，GregStein针对Python1.5提交了一个名为“freethreading”的补丁，这个补丁经常被提到却不怎么被人理解。这个补丁就尝试了将GIL完全移除，并用细粒度的锁来代替。然而，GIL移除的代价是单线程程序的执行速度下降，下降的幅度大概有40%。使用两个线程可以让速度有所提升，但是速度的提升并没有随着核数的增加而线性增长。由于执行速度的降低，这一补丁没有被接受了，并且几乎被人遗忘。GIL让人头痛，我们还是想点其他办法吧尽管“freethreading”这个补丁没有被接受，但是它还是有启发性意义。它证明了一个关于Python解释器的基本要点：移除GIL是非常困难的。比起该补丁发布的时候，现在的解释器依赖的全局状态变得更多了，这使得移除GIL变得更加困难。值得一提的是，也正是因为这个原因，许多人对移除GIL变得更感兴趣了。困难的问题通常都很有趣。但是这可能有点被误导了。我们假设一下：如果我们有这样一个神奇的补丁，它其移除了GIL，并且没有使单线程的Python代码性能下降，我们会得到一直想要的东西：一个能并发使用所有处理器的线程API。现在我们已经获得了我们希望的，但这确实是件好事吗？基于线程的编程是困难的。当一个人觉得自己了解关于线程的一切，总会有一些新问题出现。一些非常知名的语言设计者和研究者站出来反对线程模型，因为在这方面想要得到合理的一致性真的是太难了。就像任何一个写过多线程应用程序的人可以告诉你的一样，不管是多线程应用的开发还是调试难度都会是单线程的应用的指数倍。程序员的思维模型往往适应顺序执行模型，恰恰与并行执行模型不匹配。GIL的出现无意中帮助了开发者免于陷入困境。在使用多线程时仍然需要同步原语，GIL事实上帮助我们保证不同线程之间的数据一致性。这么说起来Python最难的问题似乎有点问错了问题。Python专家推荐使用多进程代替多线程是有道理的，而不是想要给Python线程实现遮羞。Python的这种实现方式促使开发者使用更安全也更直观的方式实现并发模型，同时保留使用多线程进行开发，让开发者在必要的时候使用。大多数人可能并不清楚什么是最好的并行编程模型。但是大多数人都清楚多线程的方式并不是最好的并行模型。不要认为GIL是一成不变或者毫无道理的。AntoinePitrou在Python3.2中实现了一个新的GIL，比较显著地改进的Python解释器。这是1992年以来，针对GIL最主要的一次改进。这个改变非常巨大，很难在这里解释清楚，但是从高层次来看，旧的GIL通过对Python指令进行计数来确定何时释放GIL。由于Python指令和翻译成的机器指令并非一一对应的关系，这使得单条Python指令可能包含大量工作。新的GIL用一个固定的超时时间来指示当前的线程释放锁。在当前线程持有锁且第二个线程请求这个锁的时候，当前线程就会在5ms后被强制释放这个锁（这就是说，当前线程每5ms就要检查其是否需要释放这个锁）。在任务可以执行的情况下，这使得预测线程间的切换变得更容易。然而，这并不是一个完美的改进。对于不同类型任务执行过程中GIL的作用的研究，DavidBeazley可能是最活跃的一个。除了对Python3.2之前的GIL研究最深入，他还研究了这个最新的GIL实现，并且发现了很多有趣的程序方案：在这些方案中，即使是新的GIL实现，表现也相当糟糕。他目前仍然通过实践研究来推动着有关GIL的讨论，并发布实践结果。不管人们对Python的GIL看法如何，它仍然是Python语言里最困难的技术挑战。想要理解它的实现需要对操作系统设计、多线程编程、C语言、解释器设计和CPython解释器的实现有着非常透彻的理解。单是这些前提就妨碍了很多开发者去更彻底地研究GIL。然而并没有任何迹象表明GIL会在不久之后远离我们。目前，它将继续给那些新接触Python并对解决技术难题感兴趣的人带来困惑和惊喜。以上内容是基于我目前对Python解释器的研究。我打算写一些关于解释器其它方面的内容，但是没有比GIL知名度更高的了。虽然这些技术细节来自我对CPython代码库的彻底研究，但是仍有可能存在不准确的地方。如果你发现了不准确的内容，请及时告知我，我会尽快修正。1赞2收藏评论关于作者：Coureur简介还没来得及写:）个人主页·我的文章·10"], "art_create_time": ["2017/10/13"], "art_title": ["最令人头疼的Python问题"], "art_url": ["http://python.jobbole.com/88673/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2015/02/591d8b55a524f825dd29a22b8df70000.jpg"]},
{"art_content": ["本文由伯乐在线-学以致用123翻译，刘唱校稿。未经许可，禁止转载！英文出处：RadimRehurek。欢迎加入翻译组。本文是2014年12月我在布拉格经济大学做的名为‘Python数据科学’讲座的笔记。欢迎通过@RadimRehurek进行提问和评论。本次讲座的目的是展示一些关于机器学习的高级概念。该笔记中用具体的代码来做演示，大家可以在自己的电脑上运行（需要安装IPython，如下所示）。本次讲座的听众需要了解一些基础的编程（不一定是Python），并拥有一点基本的数据挖掘背景。本次讲座不是机器学习专家的“高级演讲”。这些代码实例创建了一个有效的、可执行的原型系统：一个使用“spam”（垃圾信息）或“ham”（非垃圾信息）对英文手机短信（”短信类型“的英文）进行分类的app。整套代码使用Python 语言。python是一种在管线（pipeline）的所有环节（I/O、数据清洗重整和预处理、模型训练和评估）都好用的通用语言。尽管python不是唯一选择，但它灵活、易于开发，性能优越，这得益于它成熟的科学计算生态系统。Python庞大的、开源生态系统同时避免了任何单一框架或库的限制（以及相关的信息丢失）。IPythonnotebook，是Python的一个工具，它是一个以 HTML形式呈现的交互环境，可以通过它立刻看到结果。我们也将重温其它广泛用于数据科学领域的实用工具。想交互运行下面的例子（选读）？1.安装免费的AnacondaPython发行版，其中已经包含Python本身。2.安装“自然语言处理”库——TextBlob：安装包在这。3.下载本文的源码（网址：http://radimrehurek.com/data_science_python/data_science_python.ipynb 并运行：$ipythonnotebookdata_science_python.ipynb4.观看IPythonnotebook基本用法教程 IPythontutorialvideo。5.运行下面的第一个代码，如果执行过程没有报错，就可以了。 端到端的例子：自动过滤垃圾信息In [1]:Python%matplotlibinlineimportmatplotlib.pyplotaspltimportcsvfromtextblobimportTextBlobimportpandasimportsklearnimportcPickleimportnumpyasnpfromsklearn.feature_extraction.textimportCountVectorizer,TfidfTransformerfromsklearn.naive_bayesimportMultinomialNBfromsklearn.svmimportSVC,LinearSVCfromsklearn.metricsimportclassification_report,f1_score,accuracy_score,confusion_matrixfromsklearn.pipelineimportPipelinefromsklearn.grid_searchimportGridSearchCVfromsklearn.cross_validationimportStratifiedKFold,cross_val_score,train_test_splitfromsklearn.treeimportDecisionTreeClassifierfromsklearn.learning_curveimportlearning_curve123456789101112131415161718%matplotlibinline importmatplotlib.pyplotaspltimportcsvfromtextblobimportTextBlobimportpandasimportsklearnimportcPickleimportnumpyasnpfromsklearn.feature_extraction.textimportCountVectorizer,TfidfTransformerfromsklearn.naive_bayesimportMultinomialNBfromsklearn.svmimportSVC,LinearSVCfromsklearn.metricsimportclassification_report,f1_score,accuracy_score,confusion_matrixfromsklearn.pipelineimportPipelinefromsklearn.grid_searchimportGridSearchCVfromsklearn.cross_validationimportStratifiedKFold,cross_val_score,train_test_splitfromsklearn.treeimportDecisionTreeClassifierfromsklearn.learning_curveimportlearning_curve第一步：加载数据，浏览一下让我们跳过真正的第一步（完善资料，了解我们要做的是什么，这在实践过程中是非常重要的），直接到 https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection下载demo里需要用的zip文件，解压到data子目录下。你能看到一个大概0.5MB大小，名为SMSSpamCollection的文件：Python$<spanclass=\"kw\">ls</span>-ldata<spanclass=\"kw\">total</span>1352<spanclass=\"kw\">-rw-r--r--@</span>1kofolastaff477907Mar152011SMSSpamCollection<spanclass=\"kw\">-rw-r--r--@</span>1kofolastaff5868Apr182011readme<spanclass=\"kw\">-rw-r-----@</span>1kofolastaff203415Dec115:30smsspamcollection.zip12345$<spanclass=\"kw\">ls</span>-ldata<spanclass=\"kw\">total</span>1352<spanclass=\"kw\">-rw-r--r--@</span>1kofola  staff  477907Mar15  2011SMSSpamCollection<spanclass=\"kw\">-rw-r--r--@</span>1kofola  staff    5868Apr18  2011readme<spanclass=\"kw\">-rw-r-----@</span>1kofola  staff  203415Dec  115:30smsspamcollection.zip这份文件包含了5000多份SMS手机信息（查看readme文件以获得更多信息）：In [2]:messages=[line.rstrip()forlineinopen('./data/SMSSpamCollection')]printlen(messages)12messages=[line.rstrip()forlineinopen('./data/SMSSpamCollection')]printlen(messages)5574文本集有时候也称为“语料库”，我们来打印SMS语料库中的前10条信息：In [3]:Pythonformessage_no,messageinenumerate(messages[:10]):printmessage_no,message12formessage_no,messageinenumerate(messages[:10]):    printmessage_no,messagePython0hamGountiljurongpoint,crazy..Availableonlyinbugisngreatworldlaebuffet...Cinetheregotamorewat...1hamOklar...Jokingwifuoni...2spamFreeentryin2awklycomptowinFACupfinaltkts21stMay2005.TextFAto87121toreceiveentryquestion(stdtxtrate)T&amp;C'sapply08452810075over18's3hamUdunsaysoearlyhor...Ucalreadythensay...4hamNahIdon'tthinkhegoestousf,helivesaroundherethough5spamFreeMsgHeytheredarlingit'sbeen3week'snowandnowordback!I'dlikesomefunyouupforitstill?Tbok!XxXstdchgstosend,£1.50torcv6hamEvenmybrotherisnotliketospeakwithme.Theytreatmelikeaidspatent.7hamAsperyourrequest'MelleMelle(OruMinnaminunginteNurunguVettam)'hasbeensetasyourcallertuneforallCallers.Press*9tocopyyourfriendsCallertune8spamWINNER!!Asavaluednetworkcustomeryouhavebeenselectedtoreceivea£900prizereward!Toclaimcall09061701461.ClaimcodeKL341.Valid12hoursonly.9spamHadyourmobile11monthsormore?URentitledtoUpdatetothelatestcolourmobileswithcameraforFree!CallTheMobileUpdateCoFREEon08002986030123456789100ham    Gountiljurongpoint,crazy..Availableonlyinbugisngreatworldlaebuffet...Cinetheregotamorewat...1ham  Oklar...Jokingwifuoni...2spam  Freeentryin2awklycomptowinFACupfinaltkts21stMay2005.TextFAto87121toreceiveentryquestion(stdtxtrate)T&amp;C'sapply08452810075over18's3ham  Udunsaysoearlyhor...Ucalreadythensay...4ham  NahIdon'tthinkhegoestousf,helivesaroundherethough5spam  FreeMsgHeytheredarlingit'sbeen3week'snowandnowordback!I'dlikesomefunyouupforitstill?Tbok!XxXstdchgstosend,£1.50torcv6ham  Evenmybrotherisnotliketospeakwithme.Theytreatmelikeaidspatent.7ham  Asperyourrequest'MelleMelle(OruMinnaminunginteNurunguVettam)'hasbeensetasyourcallertuneforallCallers.Press*9tocopyyourfriendsCallertune8spam  WINNER!!Asavaluednetworkcustomeryouhavebeenselectedtoreceivea£900prizereward!Toclaimcall09061701461.ClaimcodeKL341.Valid12hoursonly.9spam  Hadyourmobile11monthsormore?URentitledtoUpdatetothelatestcolourmobileswithcameraforFree!CallTheMobileUpdateCoFREEon08002986030我们看到一个TSV文件（用制表符tab分隔），它的第一列是标记正常信息（ham）或“垃圾文件”（spam）的标签，第二列是信息本身。这个语料库将作为带标签的训练集。通过使用这些标记了ham/spam例子，我们将训练一个自动分辨ham/spam的机器学习模型。然后，我们可以用训练好的模型将任意未标记的信息标记为ham或spam。我们可以使用Python的Pandas库替我们处理TSV文件（或CSV文件，或Excel文件）：In [4]:Pythonmessages=pandas.read_csv('./data/SMSSpamCollection',sep='t',quoting=csv.QUOTE_NONE,names=[\"label\",\"message\"])printmessages123messages=pandas.read_csv('./data/SMSSpamCollection',sep='t',quoting=csv.QUOTE_NONE,                          names=[\"label\",\"message\"])printmessagesPythonlabelmessage0hamGountiljurongpoint,crazy..Availableonly...1hamOklar...Jokingwifuoni...2spamFreeentryin2awklycomptowinFACupfina...3hamUdunsaysoearlyhor...Ucalreadythensay...4hamNahIdon'tthinkhegoestousf,helivesaro...5spamFreeMsgHeytheredarlingit'sbeen3week'sn...6hamEvenmybrotherisnotliketospeakwithme....7hamAsperyourrequest'MelleMelle(OruMinnamin...8spamWINNER!!Asavaluednetworkcustomeryouhave...9spamHadyourmobile11monthsormore?URentitle...10hamI'mgonnabehomesoonandidon'twanttotal...11spamSIXchancestowinCASH!From100to20,000po...12spamURGENT!Youhavewona1weekFREEmembership...13hamI'vebeensearchingfortherightwordstotha...14hamIHAVEADATEONSUNDAYWITHWILL!!15spamXXXMobileMovieClub:Touseyourcredit,click...16hamOhk...i'mwatchinghere:)17hamEhurememberhow2spellhisname...Yesidi...18hamFineifthatsthewayufeel.Thatstheway...19spamEnglandvMacedonia-dontmissthegoals/team...20hamIsthatseriouslyhowyouspellhisname?21hamI‘mgoingtotryfor2monthshahaonlyjoking22hamSoüpayfirstlar...Thenwhenisdastockco...23hamAftifinishmylunchthenigostrdownlor....24hamFfffffffff.AlrightnowayIcanmeetupwith...25hamJustforcedmyselftoeataslice.I'mreally...26hamLolyouralwayssoconvincing.27hamDidyoucatchthebus?Areyoufryinganegg...28hamI'mback&amp;amp;we'repackingthecarnow,I'll...29hamAhhh.Work.Ivaguelyrememberthat!Whatdoes............5544hamArmandsaysgetyourassovertoepsilon5545hamUstillhaventgoturselfajacketah?5546hamI'mtakingderek&amp;amp;taylortowalmart,ifI...5547hamHiitsindurbanareyoustillonthisnumber5548hamIc.Therearealottachildporncarsthen.5549spamHadyourcontractmobile11Mnths?LatestMoto...5550hamNo,Iwastryingitallweekend;V5551hamYouknow,wotpeoplewear.Tshirts,jumpers,...5552hamCool,whattimeyouthinkyoucangethere?5553hamWendidyougetsospiritualanddeep.That's...5554hamHaveasafetriptoNigeria.Wishyouhappines...5555hamHahaha..useyourbraindear5556hamWellkeepinmindI'veonlygotenoughgasfor...5557hamYeh.Indianswasnice.Thoitdidkanemeoff...5558hamYesihave.Sothat'swhyutexted.Pshew...mi...5559hamNo.Imeantthecalculationisthesame.That...5560hamSorry,I'llcalllater5561hamifyouaren'thereinthenext&amp;lt;#&amp;gt;hou...5562hamAnythinglor.Juzbothofuslor.5563hamGetmeoutofthisdumpheap.Mymomdecidedt...5564hamOklor...Sonyericssonsalesman...Iaskshuh...5565hamArd6likedatlor.5566hamWhydon'tyouwait'tilatleastwednesdayto...5567hamHuhylei...5568spamREMINDERFROMO2:Toget2.50poundsfreecall...5569spamThisisthe2ndtimewehavetried2contactu...5570hamWillübgoingtoesplanadefrhome?5571hamPity,*wasinmoodforthat.So...anyothers...5572hamTheguydidsomebitchingbutIactedlikei'd...5573hamRofl.Itstruetoitsname[5574rowsx2columns]12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364    label                                            message0      ham  Gountiljurongpoint,crazy..Availableonly...1      ham                      Oklar...Jokingwifuoni...2    spam  Freeentryin2awklycomptowinFACupfina...3      ham  Udunsaysoearlyhor...Ucalreadythensay...4      ham  NahIdon'tthinkhegoestousf,helivesaro...5    spam  FreeMsgHeytheredarlingit'sbeen3week'sn...6      ham  Evenmybrotherisnotliketospeakwithme....7      ham  Asperyourrequest'MelleMelle(OruMinnamin...8    spam  WINNER!!Asavaluednetworkcustomeryouhave...9    spam  Hadyourmobile11monthsormore?URentitle...10    ham  I'mgonnabehomesoonandidon'twanttotal...11    spam  SIXchancestowinCASH!From100to20,000po...12    spam  URGENT!Youhavewona1weekFREEmembership...13    ham  I'vebeensearchingfortherightwordstotha...14    ham                IHAVEADATEONSUNDAYWITHWILL!!15    spam  XXXMobileMovieClub:Touseyourcredit,click...16    ham                        Ohk...i'mwatchinghere:)17    ham  Ehurememberhow2spellhisname...Yesidi...18    ham  Fineifthatsthewayufeel.Thatstheway...19    spam  EnglandvMacedonia-dontmissthegoals/team...20    ham          Isthatseriouslyhowyouspellhisname?21    ham    I‘mgoingtotryfor2monthshahaonlyjoking22    ham  Soüpayfirstlar...Thenwhenisdastockco...23    ham  Aftifinishmylunchthenigostrdownlor....24    ham  Ffffffffff.AlrightnowayIcanmeetupwith...25    ham  Justforcedmyselftoeataslice.I'mreally...26    ham                    Lolyouralwayssoconvincing.27    ham  Didyoucatchthebus?Areyoufryinganegg...28    ham  I'mback&amp;amp;we'repackingthecarnow,I'll...29    ham  Ahhh.Work.Ivaguelyrememberthat!Whatdoes......    ...                                                ...5544  ham          Armandsaysgetyourassovertoepsilon5545  ham            Ustillhaventgoturselfajacketah?5546  ham  I'mtakingderek&amp;amp;taylortowalmart,ifI...5547  ham      Hiitsindurbanareyoustillonthisnumber5548  ham        Ic.Therearealottachildporncarsthen.5549  spam  Hadyourcontractmobile11Mnths?LatestMoto...5550  ham                No,Iwastryingitallweekend;V5551  ham  Youknow,wotpeoplewear.Tshirts,jumpers,...5552  ham        Cool,whattimeyouthinkyoucangethere?5553  ham  Wendidyougetsospiritualanddeep.That's...5554  ham  HaveasafetriptoNigeria.Wishyouhappines...5555  ham                        Hahaha..useyourbraindear5556  ham  WellkeepinmindI'veonlygotenoughgasfor...5557  ham  Yeh.Indianswasnice.Thoitdidkanemeoff...5558  ham  Yesihave.Sothat'swhyutexted.Pshew...mi...5559  ham  No.Imeantthecalculationisthesame.That...5560  ham                            Sorry,I'llcalllater5561  ham  ifyouaren'thereinthenext  &amp;lt;#&amp;gt;  hou...5562  ham                  Anythinglor.Juzbothofuslor.5563  ham  Getmeoutofthisdumpheap.Mymomdecidedt...5564  ham  Oklor...Sonyericssonsalesman...Iaskshuh...5565  ham                                Ard6likedatlor.5566  ham  Whydon'tyouwait'tilatleastwednesdayto...5567  ham                                      Huhylei...5568  spam  REMINDERFROMO2:Toget2.50poundsfreecall...5569  spam  Thisisthe2ndtimewehavetried2contactu...5570  ham              Willübgoingtoesplanadefrhome?5571  ham  Pity,*wasinmoodforthat.So...anyothers...5572  ham  TheguydidsomebitchingbutIactedlikei'd...5573  ham                        Rofl.Itstruetoitsname [5574rowsx2columns]我们也可以使用pandas轻松查看统计信息：In [5]:messages.groupby('label').describe()1messages.groupby('label').describe() out[5]:messagelabelhamcount4827unique4518topSorry,I’llcalllaterfreq30spamcount747unique653topPleasecallourcustomerservicerepresentativ…freq4这些信息的长度是多少：In [6]:Pythonmessages['length']=messages['message'].map(lambdatext:len(text))printmessages.head()12messages['length']=messages['message'].map(lambdatext:len(text))printmessages.head()Pythonlabelmessagelength0hamGountiljurongpoint,crazy..Availableonly...1111hamOklar...Jokingwifuoni...292spamFreeentryin2awklycomptowinFACupfina...1553hamUdunsaysoearlyhor...Ucalreadythensay...494hamNahIdon'tthinkhegoestousf,helivesaro...61123456  label                                            message  length0  ham  Gountiljurongpoint,crazy..Availableonly...    1111  ham                      Oklar...Jokingwifuoni...      292  spam  Freeentryin2awklycomptowinFACupfina...    1553  ham  Udunsaysoearlyhor...Ucalreadythensay...      494  ham  NahIdon'tthinkhegoestousf,helivesaro...      61In [7]:Pythonmessages.length.plot(bins=20,kind='hist')1messages.length.plot(bins=20,kind='hist')Out[7]:Python&lt;matplotlib.axes._subplots.AxesSubplotat0x10dd7a990&gt;1&lt;matplotlib.axes._subplots.AxesSubplotat0x10dd7a990&gt;In [8]:Pythonmessages.length.describe()1messages.length.describe()Out[8]:Pythoncount5574.000000mean80.604593std59.919970min2.00000025%36.00000050%62.00000075%122.000000max910.000000Name:length,dtype:float64123456789count    5574.000000mean      80.604593std        59.919970min        2.00000025%        36.00000050%        62.00000075%      122.000000max      910.000000Name:length,dtype:float64哪些是超长信息？In [9]:printlist(messages.message[messages.length>900])1printlist(messages.message[messages.length>900])[\"Formetheloveshouldstartwithattraction.ishouldfeelthatIneedhereverytimearoundme.sheshouldbethefirstthingwhichcomesinmythoughts.Iwouldstartthedayandenditwithher.sheshouldbethereeverytimeIdream.lovewillbethenwhenmyeverybreathhashername.mylifeshouldhappenaroundher.mylifewillbenamedtoher.Iwouldcryforher.willgiveallmyhappinessandtakeallhersorrows.Iwillbereadytofightwithanyoneforher.IwillbeinlovewhenIwillbedoingthecraziestthingsforher.lovewillbewhenIdon'thavetoprooveanyonethatmygirlisthemostbeautifulladyonthewholeplanet.Iwillalwaysbesingingpraisesforher.lovewillbewhenIstartupmakingchickencurryandendupmakiingsambar.lifewillbethemostbeautifulthen.willgeteverymorningandthankgodforthedaybecausesheiswithme.Iwouldliketosayalot..willtelllater..\"]1234567891011[\"Formetheloveshouldstartwithattraction.ishouldfeelthatIneedhereverytimearoundme.sheshouldbethefirstthingwhichcomesinmythoughts.Iwouldstartthedayandenditwithher.sheshouldbethereeverytimeIdream.lovewillbethenwhenmyeverybreathhashername.mylifeshouldhappenaroundher.mylifewillbenamedtoher.Iwouldcryforher.willgiveallmyhappinessandtakeallhersorrows.Iwillbereadytofightwithanyoneforher.IwillbeinlovewhenIwillbedoingthecraziestthingsforher.lovewillbewhenIdon'thavetoprooveanyonethatmygirlisthemostbeautifulladyonthewholeplanet.Iwillalwaysbesingingpraisesforher.lovewillbewhenIstartupmakingchickencurryandendupmakiingsambar.lifewillbethemostbeautifulthen.willgeteverymorningandthankgodforthedaybecausesheiswithme.Iwouldliketosayalot..willtelllater..\"]spam信息与ham信息在长度上有区别吗？In [10]:Pythonmessages.hist(column='length',by='label',bins=50)1messages.hist(column='length',by='label',bins=50)Out[10]:Pythonarray([&lt;matplotlib.axes._subplots.AxesSubplotobjectat0x11270da50&gt;,&lt;matplotlib.axes._subplots.AxesSubplotobjectat0x1126c7750&gt;],dtype=object)12array([&lt;matplotlib.axes._subplots.AxesSubplotobjectat0x11270da50&gt;,      &lt;matplotlib.axes._subplots.AxesSubplotobjectat0x1126c7750&gt;],dtype=object) 太棒了，但是我们怎么能让电脑自己识别文字信息？它可以理解这些胡言乱语吗？ 第二步：数据预处理这一节我们将原始信息（字符序列）转换为向量（数字序列）；这里的映射并非一对一的，我们要用词袋模型（bag-of-words）把每个不重复的词用一个数字来表示。与第一步的方法一样，让我们写一个将信息分割成单词的函数：In [11]:Pythondefsplit_into_tokens(message):message=unicode(message,'utf8')#convertbytesintoproperunicodereturnTextBlob(message).words123defsplit_into_tokens(message):    message=unicode(message,'utf8')  #convertbytesintoproperunicode    returnTextBlob(message).words这还是原始文本的一部分:In [12]:Pythonmessages.message.head()1messages.message.head()Out[12]:Python0Gountiljurongpoint,crazy..Availableonly...1Oklar...Jokingwifuoni...2Freeentryin2awklycomptowinFACupfina...3Udunsaysoearlyhor...Ucalreadythensay...4NahIdon'tthinkhegoestousf,helivesaro...Name:message,dtype:object1234560    Gountiljurongpoint,crazy..Availableonly...1                        Oklar...Jokingwifuoni...2    Freeentryin2awklycomptowinFACupfina...3    Udunsaysoearlyhor...Ucalreadythensay...4    NahIdon'tthinkhegoestousf,helivesaro...Name:message,dtype:object这是原始文本处理后的样子：In [13]:Pythonmessages.message.head().apply(split_into_tokens)1messages.message.head().apply(split_into_tokens)Out[13]:Python0[Go,until,jurong,point,crazy,Available,o...1[Ok,lar,Joking,wif,u,oni]2[Free,entry,in,2,a,wkly,comp,to,win,F...3[U,dun,say,so,early,hor,U,c,already,t...4[Nah,I,do,n't,think,he,goes,to,usf,he...Name:message,dtype:object1234560    [Go,until,jurong,point,crazy,Available,o...1                      [Ok,lar,Joking,wif,u,oni]2    [Free,entry,in,2,a,wkly,comp,to,win,F...3    [U,dun,say,so,early,hor,U,c,already,t...4    [Nah,I,do,n't,think,he,goes,to,usf,he...Name:message,dtype:object自然语言处理（NLP）的问题：大写字母是否携带信息？单词的不同形式（“goes”和“go”）是否携带信息？叹词和限定词是否携带信息？换句话说，我们想对文本进行更好的标准化。我们使用textblob获取part-of-speech(POS)标签：In [14]:PythonTextBlob(\"Helloworld,howisitgoing?\").tags#listof(word,POS)pairs1TextBlob(\"Helloworld,howisitgoing?\").tags  #listof(word,POS)pairsOut[14]:Python[(u'Hello',u'UH'),(u'world',u'NN'),(u'how',u'WRB'),(u'is',u'VBZ'),(u'it',u'PRP'),(u'going',u'VBG')]123456[(u'Hello',u'UH'),(u'world',u'NN'),(u'how',u'WRB'),(u'is',u'VBZ'),(u'it',u'PRP'),(u'going',u'VBG')]并将单词标准化为基本形式(lemmas)：In [15]:Pythondefsplit_into_lemmas(message):message=unicode(message,'utf8').lower()words=TextBlob(message).words#foreachword,takeits\"baseform\"=lemmareturn[word.lemmaforwordinwords]messages.message.head().apply(split_into_lemmas)1234567defsplit_into_lemmas(message):    message=unicode(message,'utf8').lower()    words=TextBlob(message).words    #foreachword,takeits\"baseform\"=lemma    return[word.lemmaforwordinwords] messages.message.head().apply(split_into_lemmas)Out[15]:0[go,until,jurong,point,crazy,available,o...1[ok,lar,joking,wif,u,oni]2[free,entry,in,2,a,wkly,comp,to,win,f...3[u,dun,say,so,early,hor,u,c,already,t...4[nah,i,do,n't,think,he,go,to,usf,he,...Name:message,dtype:object1234560[go,until,jurong,point,crazy,available,o...1[ok,lar,joking,wif,u,oni]2[free,entry,in,2,a,wkly,comp,to,win,f...3[u,dun,say,so,early,hor,u,c,already,t...4[nah,i,do,n't,think,he,go,to,usf,he,...Name:message,dtype:object 这样就好多了。你也许还会想到更多的方法来改进预处理：解码HTML实体（我们上面看到的&amp和&lt）；过滤掉停用词(代词等)；添加更多特征，比如所有字母大写标识等等。 第三步：数据转换为向量现在，我们将每条消息（词干列表）转换成机器学习模型可以理解的向量。用词袋模型完成这项工作需要三个步骤：1. 对每个词在每条信息中出现的次数进行计数（词频）；2.对计数进行加权，这样经常出现的单词将会获得较低的权重（逆向文件频率）；3.将向量由原始文本长度归一化到单位长度（L2范式）。每个向量的维度等于SMS语料库中包含的独立词的数量。In [16]:Pythonbow_transformer=CountVectorizer(analyzer=split_into_lemmas).fit(messages['message'])printlen(bow_transformer.vocabulary_)12bow_transformer=CountVectorizer(analyzer=split_into_lemmas).fit(messages['message'])printlen(bow_transformer.vocabulary_)Python887418874这里我们使用强大的python机器学习训练库scikit-learn(sklearn)，它包含大量的方法和选项。我们取一个信息并使用新的bow_tramsformer获取向量形式的词袋模型计数:In [17]:Pythonmessage4=messages['message'][3]printmessage412message4=messages['message'][3]printmessage4PythonUdunsaysoearlyhor...Ucalreadythensay...1Udunsaysoearlyhor...Ucalreadythensay...In [18]:Pythonbow4=bow_transformer.transform([message4])printbow4printbow4.shape123bow4=bow_transformer.transform([message4])printbow4printbow4.shapePython(0,1158)1(0,1899)1(0,2897)1(0,2927)1(0,4021)1(0,6736)2(0,7111)1(0,7698)1(0,8013)2(1,8874)12345678910  (0,1158)      1  (0,1899)    1  (0,2897)    1  (0,2927)    1  (0,4021)    1  (0,6736)    2  (0,7111)    1  (0,7698)    1  (0,8013)    2  (1,8874)message4中有9个独立词，它们中的两个出现了两次，其余的只出现了一次。可用性检测，哪些词出现了两次？In [19]:Pythonprintbow_transformer.get_feature_names()[6736]printbow_transformer.get_feature_names()[8013]12printbow_transformer.get_feature_names()[6736]printbow_transformer.get_feature_names()[8013]Pythonsayu12sayu整个SMS语料库的词袋计数是一个庞大的稀疏矩阵：In [20]:Pythonmessages_bow=bow_transformer.transform(messages['message'])print'sparsematrixshape:',messages_bow.shapeprint'numberofnon-zeros:',messages_bow.nnzprint'sparsity:%.2f%%'%(100.0*messages_bow.nnz/(messages_bow.shape[0]*messages_bow.shape[1]))1234messages_bow=bow_transformer.transform(messages['message'])print'sparsematrixshape:',messages_bow.shapeprint'numberofnon-zeros:',messages_bow.nnzprint'sparsity:%.2f%%'%(100.0*messages_bow.nnz/(messages_bow.shape[0]*messages_bow.shape[1]))Pythonsparsematrixshape:(5574,8874)numberofnon-zeros:80272sparsity:0.16%123sparsematrixshape:(5574,8874)numberofnon-zeros:80272sparsity:0.16%最终，计数后，使用scikit-learn的TFidfTransformer实现的TF-IDF 完成词语加权和归一化。In [21]:Pythontfidf_transformer=TfidfTransformer().fit(messages_bow)tfidf4=tfidf_transformer.transform(bow4)printtfidf4123tfidf_transformer=TfidfTransformer().fit(messages_bow)tfidf4=tfidf_transformer.transform(bow4)printtfidf4Python(0,8013)0.305114653686(0,7698)0.225299911221(0,7111)0.191390347987(0,6736)0.523371210191(0,4021)0.456354991921(0,2927)0.32967579251(0,2897)0.303693312742(0,1899)0.24664322833(0,1158)0.274934159477123456789  (0,8013)      0.305114653686  (0,7698)    0.225299911221  (0,7111)    0.191390347987  (0,6736)    0.523371210191  (0,4021)    0.456354991921  (0,2927)    0.32967579251  (0,2897)    0.303693312742  (0,1899)    0.24664322833  (0,1158)    0.274934159477单词“u”的IDF（逆向文件频率）是什么？单词“university”的IDF又是什么？In [22]:Pythonprinttfidf_transformer.idf_[bow_transformer.vocabulary_['u']]printtfidf_transformer.idf_[bow_transformer.vocabulary_['university']]12printtfidf_transformer.idf_[bow_transformer.vocabulary_['u']]printtfidf_transformer.idf_[bow_transformer.vocabulary_['university']]Python2.850681505398.23975323521122.850681505398.23975323521将整个bag-of-words语料库转化为TF-IDF语料库。In [23]:Pythonmessages_tfidf=tfidf_transformer.transform(messages_bow)printmessages_tfidf.shape12messages_tfidf=tfidf_transformer.transform(messages_bow)printmessages_tfidf.shapePython(5574,8874)1(5574,8874)有许多方法可以对数据进行预处理和向量化。这两个步骤也可以称为“特征工程”，它们通常是预测过程中最耗时间和最无趣的部分，但是它们非常重要并且需要经验。诀窍在于反复评估：分析模型误差，改进数据清洗和预处理方法，进行头脑风暴讨论新功能，评估等等。第四步：训练模型,检测垃圾信息我们使用向量形式的信息来训练spam/ham分类器。这部分很简单，有很多实现训练算法的库文件。这里我们使用scikit-learn，首先选择NaiveBayes分类器：In [24]:Python%timespam_detector=MultinomialNB().fit(messages_tfidf,messages['label'])1%timespam_detector=MultinomialNB().fit(messages_tfidf,messages['label'])PythonCPUtimes:user4.51ms,sys:987µs,total:5.49msWalltime:4.77ms12CPUtimes:user4.51ms,sys:987µs,total:5.49msWalltime:4.77ms我们来试着分类一个随机信息：In [25]:Pythonprint'predicted:',spam_detector.predict(tfidf4)[0]print'expected:',messages.label[3]12print'predicted:',spam_detector.predict(tfidf4)[0]print'expected:',messages.label[3]Pythonpredicted:hamexpected:ham12predicted:hamexpected:ham太棒了！你也可以用自己的文本试试。有一个很自然的问题是：我们可以正确分辨多少信息？In [26]:Pythonall_predictions=spam_detector.predict(messages_tfidf)printall_predictions12all_predictions=spam_detector.predict(messages_tfidf)printall_predictionsPython['ham''ham''spam'...,'ham''ham''ham']1['ham''ham''spam'...,'ham''ham''ham']In [27]:Pythonprint'accuracy',accuracy_score(messages['label'],all_predictions)print'confusionmatrixn',confusion_matrix(messages['label'],all_predictions)print'(row=expected,col=predicted)'123print'accuracy',accuracy_score(messages['label'],all_predictions)print'confusionmatrixn',confusion_matrix(messages['label'],all_predictions)print'(row=expected,col=predicted)'Pythonaccuracy0.969501255831confusionmatrix[[48270][170577]](row=expected,col=predicted)12345accuracy0.969501255831confusionmatrix[[4827    0][170  577]](row=expected,col=predicted)In [28]:Pythonplt.matshow(confusion_matrix(messages['label'],all_predictions),cmap=plt.cm.binary,interpolation='nearest')plt.title('confusionmatrix')plt.colorbar()plt.ylabel('expectedlabel')plt.xlabel('predictedlabel')12345plt.matshow(confusion_matrix(messages['label'],all_predictions),cmap=plt.cm.binary,interpolation='nearest')plt.title('confusionmatrix')plt.colorbar()plt.ylabel('expectedlabel')plt.xlabel('predictedlabel')Out[28]:Python&lt;matplotlib.text.Textat0x11643f6d0&gt;1&lt;matplotlib.text.Textat0x11643f6d0&gt; 我们可以通过这个混淆矩阵计算精度（precision）和召回率（recall），或者它们的组合（调和平均值）F1：In [29]:Pythonprintclassification_report(messages['label'],all_predictions)1printclassification_report(messages['label'],all_predictions)Pythonprecisionrecallf1-scoresupportham0.971.000.984827spam1.000.770.87747avg/total0.970.970.975574123456            precision    recall  f1-score  support         ham      0.97      1.00      0.98      4827      spam      1.00      0.77      0.87      747 avg/total      0.97      0.97      0.97      5574有相当多的指标都可以用来评估模型性能，至于哪个最合适是由任务决定的。比如，将“spam”错误预测为“ham”的成本远低于将“ham”错误预测为“spam”的成本。 第五步：如何进行实验？在上述“评价”中，我们犯了个大忌。为了简单的演示，我们使用训练数据进行了准确性评估。永远不要评估你的训练数据。这是错误的。这样的评估方法不能告诉我们模型的实际预测能力，如果我们记住训练期间的每个例子，训练的准确率将非常接近100%，但是我们不能用它来分类任何新信息。一个正确的做法是将数据分为训练集和测试集，在模型拟合和调参时只能使用训练数据，不能以任何方式使用测试数据，通过这个方法确保模型没有“作弊”，最终使用测试数据评价模型可以代表模型真正的预测性能。In [30]:Pythonmsg_train,msg_test,label_train,label_test=train_test_split(messages['message'],messages['label'],test_size=0.2)printlen(msg_train),len(msg_test),len(msg_train)+len(msg_test)1234msg_train,msg_test,label_train,label_test=    train_test_split(messages['message'],messages['label'],test_size=0.2) printlen(msg_train),len(msg_test),len(msg_train)+len(msg_test)Python4459111555741445911155574按照要求，测试数据占整个数据集的20%（总共5574条记录中的1115条），其余的是训练数据（5574条中的4459条）。让我们回顾整个流程，将所有步骤放入scikit-learn的Pipeline中:In [31]:Pythondefsplit_into_lemmas(message):message=unicode(message,'utf8').lower()words=TextBlob(message).words#foreachword,takeits\"baseform\"=lemmareturn[word.lemmaforwordinwords]pipeline=Pipeline([('bow',CountVectorizer(analyzer=split_into_lemmas)),#stringstotokenintegercounts('tfidf',TfidfTransformer()),#integercountstoweightedTF-IDFscores('classifier',MultinomialNB()),#trainonTF-IDFvectorsw/NaiveBayesclassifier])1234567891011defsplit_into_lemmas(message):    message=unicode(message,'utf8').lower()    words=TextBlob(message).words    #foreachword,takeits\"baseform\"=lemma    return[word.lemmaforwordinwords] pipeline=Pipeline([    ('bow',CountVectorizer(analyzer=split_into_lemmas)),  #stringstotokenintegercounts    ('tfidf',TfidfTransformer()),  #integercountstoweightedTF-IDFscores    ('classifier',MultinomialNB()),  #trainonTF-IDFvectorsw/NaiveBayesclassifier])实际当中一个常见的做法是将训练集再次分割成更小的集合，例如，5个大小相等的子集。然后我们用4个子集训练数据，用最后1个子集计算精度（称之为“验证集”）。重复5次（每次使用不同的子集进行验证），这样可以得到模型的“稳定性“。如果模型使用不同子集的得分差异非常大，那么很可能哪里出错了（坏数据或者不良的模型方差）。返回，分析错误，重新检查输入数据有效性，重新检查数据清洗。在这个例子里，一切进展顺利：In [32]:Pythonscores=cross_val_score(pipeline,#stepstoconvertrawmessagesintomodelsmsg_train,#trainingdatalabel_train,#traininglabelscv=10,#splitdatarandomlyinto10parts:9fortraining,1forscoringscoring='accuracy',#whichscoringmetric?n_jobs=-1,#-1=useallcores=faster)printscores12345678scores=cross_val_score(pipeline,  #stepstoconvertrawmessagesintomodels                        msg_train,  #trainingdata                        label_train,  #traininglabels                        cv=10,  #splitdatarandomlyinto10parts:9fortraining,1forscoring                        scoring='accuracy',  #whichscoringmetric?                        n_jobs=-1,  #-1=useallcores=faster                        )printscoresPython[0.937360180.964205820.948545860.941834450.964125560.943820220.946067420.964044940.948314610.94606742]12[0.93736018  0.96420582  0.94854586  0.94183445  0.96412556  0.94382022  0.94606742  0.96404494  0.94831461  0.94606742]得分确实比训练全部数据时差一点点（5574个训练例子中，准确性0.97），但是它们相当稳定：In [33]:Pythonprintscores.mean(),scores.std()1printscores.mean(),scores.std()Python0.95043864760.0094720082138910.95043864760.00947200821389我们自然会问，如何改进这个模型？这个得分已经很高了，但是我们通常如何改进模型呢？NaiveBayes是一个高偏差-低方差的分类器（简单且稳定，不易过度拟合）。与其相反的例子是低偏差-高方差（容易过度拟合）的k最临近（kNN）分类器和决策树。Bagging（随机森林）是一种通过训练许多（高方差）模型和求均值来降低方差的方法。 换句话说：高偏差=分类器比较固执。它有自己的想法，数据能够改变的空间有限。另一方面，也没有多少过度拟合的空间（左图）。低偏差=分类器更听话，但也更神经质。大家都知道，让它做什么就做什么可能造成麻烦（右图）。In [34]:Pythondefplot_learning_curve(estimator,title,X,y,ylim=None,cv=None,n_jobs=-1,train_sizes=np.linspace(.1,1.0,5)):\"\"\"Generateasimpleplotofthetestandtraninglearningcurve.Parameters----------estimator:objecttypethatimplementsthe\"fit\"and\"predict\"methodsAnobjectofthattypewhichisclonedforeachvalidation.title:stringTitleforthechart.X:array-like,shape(n_samples,n_features)Trainingvector,wheren_samplesisthenumberofsamplesandn_featuresisthenumberoffeatures.y:array-like,shape(n_samples)or(n_samples,n_features),optionalTargetrelativetoXforclassificationorregression;Noneforunsupervisedlearning.ylim:tuple,shape(ymin,ymax),optionalDefinesminimumandmaximumyvaluesplotted.cv:integer,cross-validationgenerator,optionalIfanintegerispassed,itisthenumberoffolds(defaultsto3).Specificcross-validationobjectscanbepassed,seesklearn.cross_validationmoduleforthelistofpossibleobjectsn_jobs:integer,optionalNumberofjobstoruninparallel(default1).\"\"\"plt.figure()plt.title(title)ifylimisnotNone:plt.ylim(*ylim)plt.xlabel(\"Trainingexamples\")plt.ylabel(\"Score\")train_sizes,train_scores,test_scores=learning_curve(estimator,X,y,cv=cv,n_jobs=n_jobs,train_sizes=train_sizes)train_scores_mean=np.mean(train_scores,axis=1)train_scores_std=np.std(train_scores,axis=1)test_scores_mean=np.mean(test_scores,axis=1)test_scores_std=np.std(test_scores,axis=1)plt.grid()plt.fill_between(train_sizes,train_scores_mean-train_scores_std,train_scores_mean+train_scores_std,alpha=0.1,color=\"r\")plt.fill_between(train_sizes,test_scores_mean-test_scores_std,test_scores_mean+test_scores_std,alpha=0.1,color=\"g\")plt.plot(train_sizes,train_scores_mean,'o-',color=\"r\",label=\"Trainingscore\")plt.plot(train_sizes,test_scores_mean,'o-',color=\"g\",label=\"Cross-validationscore\")plt.legend(loc=\"best\")returnplt12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758defplot_learning_curve(estimator,title,X,y,ylim=None,cv=None,                        n_jobs=-1,train_sizes=np.linspace(.1,1.0,5)):    \"\"\"    Generateasimpleplotofthetestandtraninglearningcurve.     Parameters    ----------    estimator:objecttypethatimplementsthe\"fit\"and\"predict\"methods        Anobjectofthattypewhichisclonedforeachvalidation.     title:string        Titleforthechart.     X:array-like,shape(n_samples,n_features)        Trainingvector,wheren_samplesisthenumberofsamplesand        n_featuresisthenumberoffeatures.     y:array-like,shape(n_samples)or(n_samples,n_features),optional        TargetrelativetoXforclassificationorregression;        Noneforunsupervisedlearning.     ylim:tuple,shape(ymin,ymax),optional        Definesminimumandmaximumyvaluesplotted.     cv:integer,cross-validationgenerator,optional        Ifanintegerispassed,itisthenumberoffolds(defaultsto3).        Specificcross-validationobjectscanbepassed,see        sklearn.cross_validationmoduleforthelistofpossibleobjects     n_jobs:integer,optional        Numberofjobstoruninparallel(default1).    \"\"\"    plt.figure()    plt.title(title)    ifylimisnotNone:        plt.ylim(*ylim)    plt.xlabel(\"Trainingexamples\")    plt.ylabel(\"Score\")    train_sizes,train_scores,test_scores=learning_curve(        estimator,X,y,cv=cv,n_jobs=n_jobs,train_sizes=train_sizes)    train_scores_mean=np.mean(train_scores,axis=1)    train_scores_std=np.std(train_scores,axis=1)    test_scores_mean=np.mean(test_scores,axis=1)    test_scores_std=np.std(test_scores,axis=1)    plt.grid()     plt.fill_between(train_sizes,train_scores_mean-train_scores_std,                    train_scores_mean+train_scores_std,alpha=0.1,                    color=\"r\")    plt.fill_between(train_sizes,test_scores_mean-test_scores_std,                    test_scores_mean+test_scores_std,alpha=0.1,color=\"g\")    plt.plot(train_sizes,train_scores_mean,'o-',color=\"r\",            label=\"Trainingscore\")    plt.plot(train_sizes,test_scores_mean,'o-',color=\"g\",            label=\"Cross-validationscore\")     plt.legend(loc=\"best\")    returnpltIn [35]:Python%timeplot_learning_curve(pipeline,\"accuracyvs.trainingsetsize\",msg_train,label_train,cv=5)1%timeplot_learning_curve(pipeline,\"accuracyvs.trainingsetsize\",msg_train,label_train,cv=5)PythonCPUtimes:user382ms,sys:83.1ms,total:465msWalltime:28.5s12CPUtimes:user382ms,sys:83.1ms,total:465msWalltime:28.5sOut[35]:Python&lt;module'matplotlib.pyplot'from'/Volumes/work/workspace/vew/sklearn_intro/lib/python2.7/site-packages/matplotlib/pyplot.pyc'&gt;1&lt;module'matplotlib.pyplot'from'/Volumes/work/workspace/vew/sklearn_intro/lib/python2.7/site-packages/matplotlib/pyplot.pyc'&gt;  （我们对数据的64%进行了有效训练：保留20%的数据作为测试集，保留剩余的20%做5折交叉验证=>0.8*0.8*5574=3567个训练数据。）随着性能的提升，训练和交叉验证都表现良好，我们发现由于数据量较少，这个模型难以足够复杂/灵活地捕获所有的细微差别。在这种特殊案例中，不管怎样做精度都很高，这个问题看起来不是很明显。关于这一点，我们有两个选择：使用更多的训练数据，增加模型的复杂性；使用更复杂（更低偏差）的模型，从现有数据中获取更多信息。在过去的几年里，随着收集大规模训练数据越来越容易，机器越来越快。方法1变得越来越流行（更简单的算法，更多的数据）。简单的算法（如NaiveBayes）也有更容易解释的额外优势（相对一些更复杂的黑箱模型，如神经网络）。了解了如何正确地评估模型，我们现在可以开始研究参数对性能有哪些影响。第六步：如何调整参数？到目前为止，我们看到的只是冰山一角，还有许多其它参数需要调整。比如使用什么算法进行训练。上面我们已经使用了NavieBayes，但是scikit-learn支持许多分类器：支持向量机、最邻近算法、决策树、Ensamble方法等…我们会问：IDF加权对准确性有什么影响？消耗额外成本进行词形还原（与只用纯文字相比）真的会有效果吗？让我们来看看：In [37]:Pythonparams={'tfidf__use_idf':(True,False),'bow__analyzer':(split_into_lemmas,split_into_tokens),}grid=GridSearchCV(pipeline,#pipelinefromaboveparams,#parameterstotuneviacrossvalidationrefit=True,#fitusingallavailabledataattheend,onthebestfoundparamcombinationn_jobs=-1,#numberofcorestouseforparallelization;-1for\"allcores\"scoring='accuracy',#whatscoreareweoptimizing?cv=StratifiedKFold(label_train,n_folds=5),#whattypeofcrossvalidationtouse)12345678910111213params={    'tfidf__use_idf':(True,False),    'bow__analyzer':(split_into_lemmas,split_into_tokens),} grid=GridSearchCV(    pipeline,  #pipelinefromabove    params,  #parameterstotuneviacrossvalidation    refit=True,  #fitusingallavailabledataattheend,onthebestfoundparamcombination    n_jobs=-1,  #numberofcorestouseforparallelization;-1for\"allcores\"    scoring='accuracy',  #whatscoreareweoptimizing?    cv=StratifiedKFold(label_train,n_folds=5),  #whattypeofcrossvalidationtouse)In [38]:Python%timenb_detector=grid.fit(msg_train,label_train)printnb_detector.grid_scores_123%timenb_detector=grid.fit(msg_train,label_train) printnb_detector.grid_scores_PythonCPUtimes:user4.09s,sys:291ms,total:4.38sWalltime:20.2s[mean:0.94752,std:0.00357,params:{'tfidf__use_idf':True,'bow__analyzer':&lt;functionsplit_into_lemmasat0x1131e8668&gt;},mean:0.92958,std:0.00390,params:{'tfidf__use_idf':False,'bow__analyzer':&lt;functionsplit_into_lemmasat0x1131e8668&gt;},mean:0.94528,std:0.00259,params:{'tfidf__use_idf':True,'bow__analyzer':&lt;functionsplit_into_tokensat0x11270b7d0&gt;},mean:0.92868,std:0.00240,params:{'tfidf__use_idf':False,'bow__analyzer':&lt;functionsplit_into_tokensat0x11270b7d0&gt;}]123CPUtimes:user4.09s,sys:291ms,total:4.38sWalltime:20.2s[mean:0.94752,std:0.00357,params:{'tfidf__use_idf':True,'bow__analyzer':&lt;functionsplit_into_lemmasat0x1131e8668&gt;},mean:0.92958,std:0.00390,params:{'tfidf__use_idf':False,'bow__analyzer':&lt;functionsplit_into_lemmasat0x1131e8668&gt;},mean:0.94528,std:0.00259,params:{'tfidf__use_idf':True,'bow__analyzer':&lt;functionsplit_into_tokensat0x11270b7d0&gt;},mean:0.92868,std:0.00240,params:{'tfidf__use_idf':False,'bow__analyzer':&lt;functionsplit_into_tokensat0x11270b7d0&gt;}]（首先显示最佳参数组合：在这个案例中是使用idf=True和analyzer=split_into_lemmas的参数组合）快速合理性检查In [39]:Pythonprintnb_detector.predict_proba([\"Himom,howareyou?\"])[0]printnb_detector.predict_proba([\"WINNER!Creditforfree!\"])[0]12printnb_detector.predict_proba([\"Himom,howareyou?\"])[0]printnb_detector.predict_proba([\"WINNER!Creditforfree!\"])[0]Python[0.993839550.00616045][0.296631090.70336891]12[0.99383955  0.00616045][0.29663109  0.70336891]predict_proba返回每类（ham，spam）的预测概率。在第一个例子中，消息被预测为ham的概率>99%，被预测为spam的概率<1%。如果进行选择模型会认为信息是”ham“：In [40]:Pythonprintnb_detector.predict([\"Himom,howareyou?\"])[0]printnb_detector.predict([\"WINNER!Creditforfree!\"])[0]12printnb_detector.predict([\"Himom,howareyou?\"])[0]printnb_detector.predict([\"WINNER!Creditforfree!\"])[0]Pythonhamspam12hamspam在训练期间没有用到的测试集的整体得分：In [41]:Pythonpredictions=nb_detector.predict(msg_test)printconfusion_matrix(label_test,predictions)printclassification_report(label_test,predictions)123predictions=nb_detector.predict(msg_test)printconfusion_matrix(label_test,predictions)printclassification_report(label_test,predictions)Python[[9730][4696]]precisionrecallf1-scoresupportham0.951.000.98973spam1.000.680.81142avg/total0.960.960.96111512345678[[973  0][46  96]]            precision    recall  f1-score  support         ham      0.95      1.00      0.98      973      spam      1.00      0.68      0.81      142 avg/total      0.96      0.96      0.96      1115这是我们使用词形还原、TF-IDF和NavieBayes分类器的ham检测pipeline获得的实际预测性能。让我们尝试另一个分类器：支持向量机（SVM）。SVM可以非常迅速的得到结果，它所需要的参数调整也很少（虽然比NavieBayes稍多一点），在处理文本数据方面它是个好的起点。In [42]:Pythonpipeline_svm=Pipeline([('bow',CountVectorizer(analyzer=split_into_lemmas)),('tfidf',TfidfTransformer()),('classifier',SVC()),#&lt;==changehere])#pipelineparameterstoautomaticallyexploreandtuneparam_svm=[{'classifier__C':[1,10,100,1000],'classifier__kernel':['linear']},{'classifier__C':[1,10,100,1000],'classifier__gamma':[0.001,0.0001],'classifier__kernel':['rbf']},]grid_svm=GridSearchCV(pipeline_svm,#pipelinefromaboveparam_grid=param_svm,#parameterstotuneviacrossvalidationrefit=True,#fitusingalldata,onthebestdetectedclassifiern_jobs=-1,#numberofcorestouseforparallelization;-1for\"allcores\"scoring='accuracy',#whatscoreareweoptimizing?cv=StratifiedKFold(label_train,n_folds=5),#whattypeofcrossvalidationtouse)1234567891011121314151617181920pipeline_svm=Pipeline([    ('bow',CountVectorizer(analyzer=split_into_lemmas)),    ('tfidf',TfidfTransformer()),    ('classifier',SVC()),  #&lt;==changehere]) #pipelineparameterstoautomaticallyexploreandtuneparam_svm=[  {'classifier__C':[1,10,100,1000],'classifier__kernel':['linear']},  {'classifier__C':[1,10,100,1000],'classifier__gamma':[0.001,0.0001],'classifier__kernel':['rbf']},] grid_svm=GridSearchCV(    pipeline_svm,  #pipelinefromabove    param_grid=param_svm,  #parameterstotuneviacrossvalidation    refit=True,  #fitusingalldata,onthebestdetectedclassifier    n_jobs=-1,  #numberofcorestouseforparallelization;-1for\"allcores\"    scoring='accuracy',  #whatscoreareweoptimizing?    cv=StratifiedKFold(label_train,n_folds=5),  #whattypeofcrossvalidationtouse)In [43]:Python%timesvm_detector=grid_svm.fit(msg_train,label_train)#findthebestcombinationfromparam_svmprintsvm_detector.grid_scores_123%timesvm_detector=grid_svm.fit(msg_train,label_train)#findthebestcombinationfromparam_svm printsvm_detector.grid_scores_PythonCPUtimes:user5.24s,sys:170ms,total:5.41sWalltime:1min8s[mean:0.98677,std:0.00259,params:{'classifier__kernel':'linear','classifier__C':1},mean:0.98654,std:0.00100,params:{'classifier__kernel':'linear','classifier__C':10},mean:0.98654,std:0.00100,params:{'classifier__kernel':'linear','classifier__C':100},mean:0.98654,std:0.00100,params:{'classifier__kernel':'linear','classifier__C':1000},mean:0.86432,std:0.00006,params:{'classifier__gamma':0.001,'classifier__kernel':'rbf','classifier__C':1},mean:0.86432,std:0.00006,params:{'classifier__gamma':0.0001,'classifier__kernel':'rbf','classifier__C':1},mean:0.86432,std:0.00006,params:{'classifier__gamma':0.001,'classifier__kernel':'rbf','classifier__C':10},mean:0.86432,std:0.00006,params:{'classifier__gamma':0.0001,'classifier__kernel':'rbf','classifier__C':10},mean:0.97040,std:0.00587,params:{'classifier__gamma':0.001,'classifier__kernel':'rbf','classifier__C':100},mean:0.86432,std:0.00006,params:{'classifier__gamma':0.0001,'classifier__kernel':'rbf','classifier__C':100},mean:0.98722,std:0.00280,params:{'classifier__gamma':0.001,'classifier__kernel':'rbf','classifier__C':1000},mean:0.97040,std:0.00587,params:{'classifier__gamma':0.0001,'classifier__kernel':'rbf','classifier__C':1000}]123CPUtimes:user5.24s,sys:170ms,total:5.41sWalltime:1min8s[mean:0.98677,std:0.00259,params:{'classifier__kernel':'linear','classifier__C':1},mean:0.98654,std:0.00100,params:{'classifier__kernel':'linear','classifier__C':10},mean:0.98654,std:0.00100,params:{'classifier__kernel':'linear','classifier__C':100},mean:0.98654,std:0.00100,params:{'classifier__kernel':'linear','classifier__C':1000},mean:0.86432,std:0.00006,params:{'classifier__gamma':0.001,'classifier__kernel':'rbf','classifier__C':1},mean:0.86432,std:0.00006,params:{'classifier__gamma':0.0001,'classifier__kernel':'rbf','classifier__C':1},mean:0.86432,std:0.00006,params:{'classifier__gamma':0.001,'classifier__kernel':'rbf','classifier__C':10},mean:0.86432,std:0.00006,params:{'classifier__gamma':0.0001,'classifier__kernel':'rbf','classifier__C':10},mean:0.97040,std:0.00587,params:{'classifier__gamma':0.001,'classifier__kernel':'rbf','classifier__C':100},mean:0.86432,std:0.00006,params:{'classifier__gamma':0.0001,'classifier__kernel':'rbf','classifier__C':100},mean:0.98722,std:0.00280,params:{'classifier__gamma':0.001,'classifier__kernel':'rbf','classifier__C':1000},mean:0.97040,std:0.00587,params:{'classifier__gamma':0.0001,'classifier__kernel':'rbf','classifier__C':1000}]因此，很明显的，具有C=1的线性核函数是最好的参数组合。再一次合理性检查：In [44]:Pythonprintsvm_detector.predict([\"Himom,howareyou?\"])[0]printsvm_detector.predict([\"WINNER!Creditforfree!\"])[0]12printsvm_detector.predict([\"Himom,howareyou?\"])[0]printsvm_detector.predict([\"WINNER!Creditforfree!\"])[0]Pythonhamspam12hamspamIn [45]:Pythonprintconfusion_matrix(label_test,svm_detector.predict(msg_test))printclassification_report(label_test,svm_detector.predict(msg_test))12printconfusion_matrix(label_test,svm_detector.predict(msg_test))printclassification_report(label_test,svm_detector.predict(msg_test))Python[[9658][13129]]precisionrecallf1-scoresupportham0.990.990.99973spam0.940.910.92142avg/total0.980.980.98111512345678[[965  8][13129]]            precision    recall  f1-score  support         ham      0.99      0.99      0.99      973      spam      0.94      0.91      0.92      142 avg/total      0.98      0.98      0.98      1115这是我们使用SVM时可以从spam邮件检测流程中获得的实际预测性能。第七步：生成预测器经过基本分析和调优，真正的工作（工程）开始了。生成预测器的最后一步是再次对整个数据集合进行训练，以充分利用所有可用数据。当然，我们将使用上面交叉验证找到的最好的参数。这与我们开始做的非常相似，但这次深入了解它的行为和稳定性。在不同的训练/测试子集进行评价。最终的预测器可以序列化到磁盘，以便我们下次想使用它时，可以跳过所有训练直接使用训练好的模型：In [46]:Python#storethespamdetectortodiskaftertrainingwithopen('sms_spam_detector.pkl','wb')asfout:cPickle.dump(svm_detector,fout)#...andloaditback,wheneverneeded,possiblyonadifferentmachinesvm_detector_reloaded=cPickle.load(open('sms_spam_detector.pkl'))123456#storethespamdetectortodiskaftertrainingwithopen('sms_spam_detector.pkl','wb')asfout:    cPickle.dump(svm_detector,fout) #...andloaditback,wheneverneeded,possiblyonadifferentmachinesvm_detector_reloaded=cPickle.load(open('sms_spam_detector.pkl'))加载的结果是一个与原始对象表现相同的对象：In [47]:Pythonprint'before:',svm_detector.predict([message4])[0]print'after:',svm_detector_reloaded.predict([message4])[0]12print'before:',svm_detector.predict([message4])[0]print'after:',svm_detector_reloaded.predict([message4])[0]Pythonbefore:hamafter:ham12before:hamafter:ham生产执行的另一个重要部分是性能。经过快速、迭代模型调整和参数搜索之后，性能良好的模型可以被翻译成不同的语言并优化。可以牺牲几个点的准确性换取一个更小、更快的模型吗？是否值得优化内存使用情况，或者使用mmap跨进程共享内存？请注意，优化并不总是必要的，要从实际情况出发。还有一些需要考虑的问题，比如，生产流水线还需要考虑鲁棒性（服务故障转移、冗余、负载平衡）、监测（包括异常自动报警）、HR可替代性（避免关于工作如何完成的“知识孤岛”、晦涩/锁定的技术、调整结果的黑艺术）。现在，开源世界都可以为所有这些领域提供可行的解决方法，由于OSI批准的开源许可证，今天展示的所有工具都可以免费用于商业用途。其他实用概念数据稀疏性在线学习，数据流用于内存共享的mmap，系统“冷启动”负载时间可扩展性、分布式（集群）处理无监督学习大多数数据没有结构化。了解这些数据，其中没有自带的标签（不然就成了监督学习！）。我们如何训练没有标签的内容？这是什么魔法？分布假设“在类似语境中出现的词倾向于具有相似的含义”。上下文=句子，文档，滑动窗口……查看google关于无监督学习的word2vec在线演示。简单的模型、大量数据（Google新闻，1000亿词，没有标签）。下一步做什么？这个notebook的静态版本（非交互版本）的HTML，地址：http://radimrehurek.com/data_science_python（你可能已经在看了，但以防万一）交互式notebook源文件在GitHub上，http://radimrehurek.com/data_science_python/data_science_python.ipynb（见上面的安装说明）。 我的公司致力于激动人心的、务实的、商业的系统建设和尖端研究。有兴趣实习/合作？请联系我。打赏支持我翻译更多好文章，谢谢！打赏译者打赏支持我翻译更多好文章，谢谢！任选一种支付方式2赞10收藏1评论关于作者：学以致用123应用软件开发，主要用python、sql个人主页·我的文章·21"], "art_create_time": ["2017/10/11"], "art_title": ["Python 中的实用数据挖掘"], "art_url": ["http://python.jobbole.com/88669/"], "art_img": ["http://radimrehurek.com/data_science_python/python.png"]},
{"art_content": ["原文出处：西北那个峰   先以一个大牛的一段关于PythonMetapgramming的著名的话来做开头：Metaclassesaredeepermagicthan99%ofusersshouldeverworryabout.Ifyouwonderwhetheryouneedthem,youdon’t(thepeoplewhoactuallyneedthemknowwithcertaintythattheyneedthem,anddon’tneedanexplanationaboutwhy).–TimPeters翻译一下：Metaclasses是99%的用户都无需费神的黑科技。如果你还在纠结你是不是需要它的话，答案是NO（真正需要的人根本不需要解释）–TimPeters这是什么鬼话？道可道，非常道吗？Meta?好，装B已毕。这确实是一个冷僻的，不常用的话题。一篇短文肯定讲不完。所以叫做初探。英文meta这个词其实是从希腊语里面借来的。wikipedia上的解释是：indicateaconceptwhichisanabstractionbehindanotherconcept,usedtocompleteoraddtothelatter不看还好，其实看了更晕。好在后面的解释有一句“更高一层的抽象”，可以帮助理解。其实我们可以这样理解。meta的意思就是“关于什么的什么”：比如metadata可以理解为“关于数据的数据”，metaprogramming可以理解为“关于编程的编程”。这就和“更高一层的抽象”比较契合了。同时又隐隐和编程中的另一个永恒主题-recursion联系在了一起。另外，meta这个词天朝这边翻译成“元”，海峡对岸翻译成“后设”。其实我都不大理解从何而来。实例聚焦到我们今天的主题，metaprogramming就是编写用来生成代码的代码。假设我们写了一个NB的函数，用来计算一个任意复杂的算数表达式的值：像1+2,3*6+10,什么的都可以交给它去计算。这样的函数的算法不是我们的主题，所以我们请出python自带的大招eval()，一行就可以搞定了：Pythondefcalc(expression):returneval(expression)12defcalc(expression):    returneval(expression)因为输入的可能性是无限的，所以我们肯定要好好测试一下这个函数了。假定我们想了上百个testcase。又假定我们是用unittest这个module来做测试的。这样的测试程序一般会长成这样：PythonimportunittestclassTestStringMethods(unittest.TestCase):deftest_upper(self):self.assertEqual('foo'.upper(),'FOO')deftest_isupper(self):self.assertTrue('FOO'.isupper())self.assertFalse('Foo'.isupper())deftest_split(self):s='helloworld'self.assertEqual(s.split(),['hello','world'])#checkthats.splitfailswhentheseparatorisnotastringwithself.assertRaises(TypeError):s.split(2)if__name__=='__main__':unittest.main()1234567891011121314151617181920importunittest classTestStringMethods(unittest.TestCase):     deftest_upper(self):        self.assertEqual('foo'.upper(),'FOO')     deftest_isupper(self):        self.assertTrue('FOO'.isupper())        self.assertFalse('Foo'.isupper())     deftest_split(self):        s='helloworld'        self.assertEqual(s.split(),['hello','world'])        #checkthats.splitfailswhentheseparatorisnotastring        withself.assertRaises(TypeError):            s.split(2) if__name__=='__main__':    unittest.main()所以我们的目的就是用metaprogramming的方式来自动产生类似上面的测试类。先上程序后解释：Python#!/usr/bin/python3importunittestdefcalc(expression):returneval(expression)defadd_test(name,asserts):deftest_method(asserts):deffn(self):left,right=asserts.split('=')expected=str(calc(left))self.assertEqual(expected,right)returnfnd={'test1':test_method(asserts)}cls=type(name,(unittest.TestCase,),d)globals()[name]=clsif__name__=='__main__':fori,tinenumerate([\"1+2=3\",\"3*5*6=90\"]):add_test(\"Test%d\"%i,t)unittest.main()123456789101112131415161718192021222324#!/usr/bin/python3importunittest defcalc(expression):    returneval(expression) defadd_test(name,asserts):    deftest_method(asserts):        deffn(self):            left,right=asserts.split('=')            expected=str(calc(left))            self.assertEqual(expected,right)        returnfn     d={'test1':test_method(asserts)}    cls=type(name,(unittest.TestCase,),d)    globals()[name]=cls if__name__=='__main__':    fori,tinenumerate([            \"1+2=3\",            \"3*5*6=90\"]):        add_test(\"Test%d\"%i,t)    unittest.main()NB的calc()函数我们解释过了。main这段也比较简单：我们用声明的方式定义了一组测试，然后通过unittest来执行。有点复杂的是add_test()。我们先来看看最内层的fn(self)这个方法。逻辑上，它就是把输入的测试用例分成两份，一份是calc()的输入，一份是我们期待的结果；然后调用calc(),接着用assertEqual()来测试。但是这个self有点奇怪–这里没有类，哪里来的self?其实fn(self)确实是一个类的方法，只不过这个类是我们通过代码动态生成的。也就是下面这一行：Pythoncls=type(name,(unittest.TestCase,),d)1cls=type(name,(unittest.TestCase,),d)这里的type()就是通常我们用来检查某个变量的类型的那个函数。只不过它还有另外一种不大为人知的形式：Pythonclasstype(name,bases,dict)1classtype(name,bases,dict)这第二种形式，就会产生一个新的类型。以我们的程序为例，就是以unit.TestCase为baseclass,产生了一个名为TestN的新类型，改类型的实现由d给出，而d就包含了通过closure返回的fn(self)这个方法。只不过在这个新类里面，它的名字叫做test1()。最后，我们把这个新产生的类加入到当前全局符号表里面，也就相当于上面给出的unittest的例子。所以，总结一下。当我们运行这个脚本的时候，这段比较短的代码会针对每一个测试的表达式产生一个新的测试类，并动态生成测试的方法加载到该类里面。unitest从globals中找到这些类并一一执行测试。上面的例子中，其实一行一行手打calc(1+2)==3也没什么大不了的。但是当你要表达的逻辑比较复杂的时候，metaprogramming的强大就体现出来了。总结那么，看完这篇文章，我们也成为Tim所说的1%的程序猿了！其实，也许他的意思是，99%的编程工作都用不到这样技巧。在一些特殊的场合，比如编写某种框架的时候，metaprogramming会做到事半功倍。祝你在实践中碰到这样的机会。1赞2收藏评论"], "art_create_time": ["2017/10/15"], "art_title": ["Python Metaclass 初探"], "art_url": ["http://python.jobbole.com/88703/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2015/02/86a1298ce44ca9807871520b81767e6f.gif"]},
{"art_content": ["本文作者：伯乐在线-iPytLab。未经作者许可，禁止转载！欢迎加入伯乐在线专栏作者。前言之前实现了简单的SMO算法来优化SVM的对偶问题，其中在选取α的时候使用的是两重循环通过完全随机的方式选取，具体的实现参考《机器学习算法实践-SVM中的SMO算法》。本文在之前简化版SMO算法的基础上实现了使用启发式选取α对的方式的PlattSMO算法来优化SVM。另外由于最近自己也实现了一个遗传算法框架GAFT，便也尝试使用遗传算法对于SVM的原始形式进行了优化。对于本文算法的相应实现，参考:https://github.com/PytLab/MLBox/tree/master/svm遗传算法框架GAFT项目地址: https://github.com/PytLab/gaft正文SMO中启发式选择变量在SMO算法中，我们每次需要选取一对α来进行优化，通过启发式的选取我们可以更高效的选取待优化的变量使得目标函数下降的最快。针对第一个α1和第二个α2 PlattSMO采取不同的启发式手段。第一个变量的选择第一个变量的选择为外循环，与之前便利整个αα列表不同，在这里我们在整个样本集和非边界样本集间进行交替:首先我们对整个训练集进行遍历,检查是否违反KKT条件，如果改点的αiαi和xi,yixi,yi违反了KKT条件则说明改点需要进行优化。Karush-Kuhn-Tucker(KKT)条件是正定二次规划问题最优点的充分必要条件。针对SVM对偶问题，KKT条件非常简单:在遍历了整个训练集并优化了相应的α后第二轮迭代我们仅仅需要遍历其中的非边界α.所谓的非边界α就是指那些不等于边界0或者C的α值。同样这些点仍然需要检查是否违反KKT条件并进行优化.之后就是不断地在两个数据集中来回交替，最终所有的α都满足KKT条件的时候，算法中止。为了能够快速选取有最大步长的α，我们需要对所有数据对应的误差进行缓存，因此特地写了个SVMUtil类来保存svm中重要的变量以及一些辅助方法:PythonclassSVMUtil(object):'''StructtosaveallimportantvaluesinSVM.'''def__init__(self,dataset,labels,C,tolerance=0.001):self.dataset,self.labels,self.C=dataset,labels,Cself.m,self.n=np.array(dataset).shapeself.alphas=np.zeros(self.m)self.b=0self.tolerance=tolerance#Cachederrors,f(x_i)-y_iself.errors=[self.get_error(i)foriinrange(self.m)]#其他方法......1234567891011121314classSVMUtil(object):    '''    StructtosaveallimportantvaluesinSVM.    '''    def__init__(self,dataset,labels,C,tolerance=0.001):        self.dataset,self.labels,self.C=dataset,labels,C        self.m,self.n=np.array(dataset).shape        self.alphas=np.zeros(self.m)        self.b=0        self.tolerance=tolerance        #Cachederrors,f(x_i)-y_i        self.errors=[self.get_error(i)foriinrange(self.m)]    #其他方法......下面为第一个变量选择交替遍历的大致代码，相应完整的Python实现(完整实现见https://github.com/PytLab/MLBox/blob/master/svm/svm_platt_smo.py):Pythonwhile(it<max_iter):pair_changed=0ifentire:foriinrange(svm_util.m):pair_changed+=examine_example(i,svm_util)print('Fullset-iter:{},pairchanged:{}'.format(i,pair_changed))else:alphas=svm_util.alphasnon_bound_indices=[iforiinrange(svm_util.m)ifalphas[i]>0andalphas[i]<C]foriinnon_bound_indices:pair_changed+=examine_example(i,svm_util)......1234567891011121314while(it<max_iter):    pair_changed=0    ifentire:        foriinrange(svm_util.m):            pair_changed+=examine_example(i,svm_util)            print('Fullset-iter:{},pairchanged:{}'.format(i,pair_changed))    else:        alphas=svm_util.alphas        non_bound_indices=[iforiinrange(svm_util.m)                            ifalphas[i]>0andalphas[i]<C]        foriinnon_bound_indices:            pair_changed+=examine_example(i,svm_util)    ......第二个变量的选择SMO中的第二个变量的选择过程为内循环，当我们已经选取第一个α1之后，我们希望我们选取的第二个变量α2优化后能有较大的变化。根据我们之前推导的式子可以知道，新的α2的变化依赖于|E1−E2|,当E1为正时，那么选择最小的Ei作为E2，通常将每个样本的Ei缓存到一个列表中，通过在列表中选择具有|E1−E2|的α2来近似最大化步长。有时候按照上述的启发式方式仍不能够是的函数值有足够的下降，这是按下述步骤进行选择:在非边界数据集上选择能够使函数值足够下降的样本作为第二个变量如果非边界数据集上没有，则在整个数据仅上进行第二个变量的选择如果仍然没有则重新选择第一个α1第二个变量选取的Python实现:Pythondefselect_j(i,svm_util):'''通过最大化步长的方式来获取第二个alpha值的索引.'''errors=svm_util.errorsvalid_indices=[ifori,ainenumerate(svm_util.alphas)if0<a<svm_util.C]iflen(valid_indices)>1:j=-1max_delta=0forkinvalid_indices:ifk==i:continuedelta=abs(errors[i]-errors[j])ifdelta>max_delta:j=kmax_delta=deltaelse:j=select_j_rand(i,svm_util.m)returnj123456789101112131415161718defselect_j(i,svm_util):    '''通过最大化步长的方式来获取第二个alpha值的索引.    '''    errors=svm_util.errors    valid_indices=[ifori,ainenumerate(svm_util.alphas)if0<a<svm_util.C]    iflen(valid_indices)>1:        j=-1        max_delta=0        forkinvalid_indices:            ifk==i:                continue            delta=abs(errors[i]-errors[j])            ifdelta>max_delta:                j=k                max_delta=delta    else:        j=select_j_rand(i,svm_util.m)    returnjKKT条件允许一定的误差在Platt论文中的KKT条件的判断中有一个tolerance允许一定的误差，相应的Python实现：Pythonr=E_i*y_i#是否违反KKT条件if(r<-toleranceandalpha<C)or(r>toleranceandalpha>0):...1234r=E_i*y_i#是否违反KKT条件if(r<-toleranceandalpha<C)or(r>toleranceandalpha>0):    ...关于PlattSMO的完整实现详见:https://github.com/PytLab/MLBox/blob/master/svm/svm_platt_smo.py针对之前的数据集我们使用PlattSMO进行优化可以得到：Pythonw=[0.8289668843516077,-0.26578914269411114]b=-3.929258304055944812w=[0.8289668843516077,-0.26578914269411114]b=-3.9292583040559448将分割线和支持向量可视化：可见通过PlattSMO优化出来的支持向量与简化版的SMO算法有些许不同。使用遗传算法优化SVM由于最近自己写了个遗传算法框架，遗传算法作为一个启发式无导型的搜索算法非常易用，于是我就尝试使用遗传算法来优化SVM。使用遗传算法优化，我们就可以直接优化SVM的最初形式了也就是最直观的形式:顺便再安利下自己的遗传算法框架，在此框架的帮助下，优化SVM算法我们只需要写几十行的Python代码即可。其中最主要的就是编写适应度函数，根据上面的公式我们需要计算数据集中每个点到分割线的距离并返回最小的距离即可，然后放到遗传算法中进行进化迭代。遗传算法框架GAFT项目地址: https://github.com/PytLab/gaft ,使用方法详见README。Ok，我们开始构建种群用于进化迭代。创建个体与种群对于二维数据点，我们需要优化的参数只有三个也就是[w1,w2]和b,个体的定义如下:Pythonindv_template=GAIndividual(ranges=[(-2,2),(-2,2),(-5,5)],encoding='binary',eps=[0.001,0.001,0.005])123indv_template=GAIndividual(ranges=[(-2,2),(-2,2),(-5,5)],                            encoding='binary',                            eps=[0.001,0.001,0.005])种群大小这里取600，创建种群Pythonpopulation=GAPopulation(indv_template=indv_template,size=600).init()1population=GAPopulation(indv_template=indv_template,size=600).init()创建遗传算子和GA引擎这里没有什么特别的，直接使用框架中内置的算子就好了。Pythonselection=RouletteWheelSelection()crossover=UniformCrossover(pc=0.8,pe=0.5)mutation=FlipBitBigMutation(pm=0.1,pbm=0.55,alpha=0.6)engine=GAEngine(population=population,selection=selection,crossover=crossover,mutation=mutation,analysis=[ConsoleOutput,FitnessStore])123456selection=RouletteWheelSelection()crossover=UniformCrossover(pc=0.8,pe=0.5)mutation=FlipBitBigMutation(pm=0.1,pbm=0.55,alpha=0.6)engine=GAEngine(population=population,selection=selection,                  crossover=crossover,mutation=mutation,                  analysis=[ConsoleOutput,FitnessStore])适应度函数这一部分只要把上面svm初始形式描述出来就好了，只需要三行代码:Python@engine.fitness_registerdeffitness(indv):w,b=indv.variants[:-1],indv.variants[-1]min_dis=min([y*(np.dot(w,x)+b)forx,yinzip(dataset,labels)])returnfloat(min_dis)12345@engine.fitness_registerdeffitness(indv):    w,b=indv.variants[:-1],indv.variants[-1]    min_dis=min([y*(np.dot(w,x)+b)forx,yinzip(dataset,labels)])    returnfloat(min_dis)开始迭代这里迭代300代种群Pythonif'__main__'==__name__:engine.run(300)12if'__main__'==__name__:    engine.run(300)绘制遗传算法优化的分割线Pythonvariants=engine.population.best_indv(engine.fitness).variantsw=variants[:-1]b=variants[-1]#分类数据点classified_pts={'+1':[],'-1':[]}forpoint,labelinzip(dataset,labels):iflabel==1.0:classified_pts['+1'].append(point)else:classified_pts['-1'].append(point)fig=plt.figure()ax=fig.add_subplot(111)#绘制数据点forlabel,ptsinclassified_pts.items():pts=np.array(pts)ax.scatter(pts[:,0],pts[:,1],label=label)#绘制分割线x1,_=max(dataset,key=lambdax:x[0])x2,_=min(dataset,key=lambdax:x[0])a1,a2=wy1,y2=(-b-a1*x1)/a2,(-b-a1*x2)/a2ax.plot([x1,x2],[y1,y2])plt.show()1234567891011121314151617181920212223variants=engine.population.best_indv(engine.fitness).variantsw=variants[:-1]b=variants[-1]#分类数据点classified_pts={'+1':[],'-1':[]}forpoint,labelinzip(dataset,labels):    iflabel==1.0:        classified_pts['+1'].append(point)    else:        classified_pts['-1'].append(point)fig=plt.figure()ax=fig.add_subplot(111)#绘制数据点forlabel,ptsinclassified_pts.items():    pts=np.array(pts)    ax.scatter(pts[:,0],pts[:,1],label=label)#绘制分割线x1,_=max(dataset,key=lambdax:x[0])x2,_=min(dataset,key=lambdax:x[0])a1,a2=wy1,y2=(-b-a1*x1)/a2,(-b-a1*x2)/a2ax.plot([x1,x2],[y1,y2])plt.show()得到的分割曲线如下图：完整的代码详见: https://github.com/PytLab/MLBox/blob/master/svm/svm_ga.py总结本文对SVM的优化进行了介绍，主要实现了PlattSMO算法优化SVM模型，并尝试使用遗传算法框架GAFT对初始SVM进行了优化。参考SequentialMinimalOptimization:AFastAlgorithmforTrainingSupportVectorMachines打赏支持我写出更多好文章，谢谢！打赏作者打赏支持我写出更多好文章，谢谢！1赞2收藏评论关于作者：iPytLab喜欢写程序的计算化学狗，Python/C/C++/Fortran,个人博客http://pytlab.org个人主页·我的文章·22·"], "art_create_time": ["2017/10/16"], "art_title": ["机器学习算法实践-Platt SMO和遗传算法优化SVM"], "art_url": ["http://python.jobbole.com/88706/"], "art_img": ["http://pytlab.org/assets/images/blog_img/2017-10-15-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E5%AE%9E%E8%B7%B5-Platt-SMO%E5%92%8C%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95%E4%BC%98%E5%8C%96SVM/platt_smo.png"]},
{"art_content": ["本文由伯乐在线-MentosZ翻译，黄利民校稿。未经许可，禁止转载！英文出处：EmilWallner。欢迎加入翻译组。从代码中追溯深度学习的历史深度学习发展到如今的地位，离不开下面这6段代码。本文介绍了这些代码的创作者及其完成这些突破性成就的故事背景。每个故事都有简单的代码示例，读者们可以在FloydHub和GitHub找到相关代码。Source:Googlepressimage图片来源：Google新闻图片 要运行FloydHub上的代码示例，请确保您的电脑已经安装了Floyd命令行工具，并将我上传的代码示例克隆到本地计算机。如果您是第一次使用FloydHub，可以先阅读我之前发布的文章中关于如何使用FloydHub的部分。在本地计算机示例项目中初始化命令行界面之后，您就可以运行以下命令在FloydHub上启动项目：floydrun--dataemilwallner/datasets/mnist/1:mnist--tensorboard--modejupyter1floydrun--dataemilwallner/datasets/mnist/1:mnist--tensorboard--modejupyter最小二乘法所有的深度学习算法都始于下面这个数学公式（我已将其转成Python代码）Python#y=mx+b#misslope,bisy-interceptdefcompute_error_for_line_given_points(b,m,coordinates):totalError=0foriinrange(0,len(coordinates)):x=coordinates[i][0]y=coordinates[i][1]totalError+=(y-(m*x+b))**2returntotalError/float(len(coordinates))#examplecompute_error_for_line_given_points(1,2,[[3,6],[6,9],[12,18]])123456789101112#y=mx+b#misslope,bisy-intercept defcompute_error_for_line_given_points(b,m,coordinates):    totalError=0    foriinrange(0,len(coordinates)):        x=coordinates[i][0]        y=coordinates[i][1]        totalError+=(y-(m*x+b))**2    returntotalError/float(len(coordinates))#examplecompute_error_for_line_given_points(1,2,[[3,6],[6,9],[12,18]])最小二乘法在1805年由Adrien-MarieLegendre首次提出（1805,Legendre），这位巴黎数学家也以测量仪器闻名。他极其痴迷于预测彗星的方位，坚持不懈地寻找一种可以基于彗星方位历史数据计算其轨迹的算法。他尝试了许多种算法，一遍遍试错，终于找到了一个算法与结果相符。Legendre的算法是首先预测彗星未来的方位，然后计算误差的平方，最终目的是通过修改预测值以减少误差平方和。而这也正是线性回归的基本思想。读者可以在Jupyternotebook中运行上述代码来加深对这个算法的理解。m是系数，b是预测的常数项，coordinates是彗星的位置。目标是找到合适的m和b使其误差尽可能小。这是深度学习的核心思想：给定输入值和期望的输出值，然后寻找两者之间的相关性。梯度下降Legendre这种通过手动尝试来降低错误率的方法非常耗时。荷兰的诺贝尔奖得主PeterDebye在一个世纪后（1909年）正式提出了一种简化这个过程的方法。假设Legendre的算法需要考虑一个参数——我们称之为X。Y轴表示每个X的误差值。Legendre的算法是找到使得误差最小的X。在下图中，我们可以看到当X=1.1时，误差Y取到最小值。PeterDebye注意到最低点左边的斜率是负的，而另一边则是正的。因此，如果知道了任意给定X的斜率值，就可以找到Y的最小值点。这便是梯度下降算法的基本思想。几乎所有的深度学习模型都会用到梯度下降算法。要实现这个算法，我们假设误差函数是Error=x^5-2x^3-2。要得到任意给定X的斜率，我们需要对其求导，即5x^4–6x^2：如果您需要复习导数的相关知识，请观看KhanAcademy的视频。下面我们用Python实现Debye的算法：Pythoncurrent_x=0.5#thealgorithmstartsatx=0.5learning_rate=0.01#stepsizemultipliernum_iterations=60#thenumberoftimestotrainthefunction#thederivativeoftheerrorfunction(x**4=thepowerof4orx^4)defslope_at_given_x_value(x):return5*x**4-6*x**2#MoveXtotherightorleftdependingontheslopeoftheerrorfunctionforiinrange(num_iterations):previous_x=current_xcurrent_x+=-learning_rate*slope_at_given_x_value(previous_x)print(previous_x)print(\"Thelocalminimumoccursat%f\"%current_x)123456789101112131415current_x=0.5#thealgorithmstartsatx=0.5learning_rate=0.01#stepsizemultipliernum_iterations=60#thenumberoftimestotrainthefunction #thederivativeoftheerrorfunction(x**4=thepowerof4orx^4)defslope_at_given_x_value(x):  return5*x**4-6*x**2 #MoveXtotherightorleftdependingontheslopeoftheerrorfunctionforiinrange(num_iterations):  previous_x=current_x  current_x+=-learning_rate*slope_at_given_x_value(previous_x)  print(previous_x) print(\"Thelocalminimumoccursat%f\"%current_x)这里的窍门在于learning_rate。我们通过沿斜率的相反方向行进来逼近最低点。此外，越接近最低点，斜率越小。因此当斜率接近零时，每一步下降的幅度会越来越小。num_iterations是你预计到达最小值之前所需的迭代次数。可以通过调试该参数训练自己关于梯度下降算法的直觉。线性回归最小二乘法配合梯度下降算法，就是一个完整的线性回归过程。在20世纪50年代和60年代，一批实验经济学家在早期的计算机上实现了这些想法。这个过程是通过实体打卡——真正的手工软件程序实现的。准备这些打孔卡就需要几天的时间，而通过计算机进行一次回归分析最多需要24小时。下面是用Python实现线性回归的一个示例（我们不需要在打卡机上完成这个操作）：Python#Priceofwheat/kgandtheaveragepriceofbreadwheat_and_bread=[[0.5,5],[0.6,5.5],[0.8,6],[1.1,6.8],[1.4,7]]defstep_gradient(b_current,m_current,points,learningRate):b_gradient=0m_gradient=0N=float(len(points))foriinrange(0,len(points)):x=points[i][0]y=points[i][1]b_gradient+=-(2/N)*(y-((m_current*x)+b_current))m_gradient+=-(2/N)*x*(y-((m_current*x)+b_current))new_b=b_current-(learningRate*b_gradient)new_m=m_current-(learningRate*m_gradient)return[new_b,new_m]defgradient_descent_runner(points,starting_b,starting_m,learning_rate,num_iterations):b=starting_bm=starting_mforiinrange(num_iterations):b,m=step_gradient(b,m,points,learning_rate)return[b,m]gradient_descent_runner(wheat_and_bread,1,1,0.01,100)123456789101112131415161718192021222324#Priceofwheat/kgandtheaveragepriceofbreadwheat_and_bread=[[0.5,5],[0.6,5.5],[0.8,6],[1.1,6.8],[1.4,7]] defstep_gradient(b_current,m_current,points,learningRate):    b_gradient=0    m_gradient=0    N=float(len(points))    foriinrange(0,len(points)):        x=points[i][0]        y=points[i][1]        b_gradient+=-(2/N)*(y-((m_current*x)+b_current))        m_gradient+=-(2/N)*x*(y-((m_current*x)+b_current))    new_b=b_current-(learningRate*b_gradient)    new_m=m_current-(learningRate*m_gradient)    return[new_b,new_m] defgradient_descent_runner(points,starting_b,starting_m,learning_rate,num_iterations):    b=starting_b    m=starting_m    foriinrange(num_iterations):        b,m=step_gradient(b,m,points,learning_rate)    return[b,m] gradient_descent_runner(wheat_and_bread,1,1,0.01,100)线性回归本身并没有引入什么新的内容。但是，如何将梯度下降算法运用到误差函数上就需要动动脑子了。运行代码并使用这个线性回归模拟器来加深你的理解吧。感知机接下来让我们来认识一下FrankRosenblatt。这是一个白天解剖老鼠大脑，晚上寻找外星生命迹象的家伙。1958年，他发明了一个模仿神经元的机器（1958,Rosenblatt），并因此登上《纽约时报》的头条：“NewNavyDeviceLearnsByDoing”。如果向Rosenblatt的机器展示50组分别在左右两侧有标记的图像，它可以在没有预先编程的情况下分辨出两张图像（标记的位置）。大众被这个可能真正拥有学习能力的机器震惊了。如上图所示，每个训练周期都是从左侧输入数据开始。给所有输入数据添加一个初始的随机权重。然后将它们相加。如果总和为负，将其输出为0，否则输出为1。如果预测结果是正确的，就不改变循环中的权重。如果预测结果是错误的，可以用误差乘以学习率来相应地调整权重。我们用经典的“或”逻辑来运行感知机。输入输出00=001=110=111=1下面是用Python实现的感知机模型：Pythonfromrandomimportchoicefromnumpyimportarray,dot,random1_or_0=lambdax:0ifx<0else1training_data=[(array([0,0,1]),0),(array([0,1,1]),1),(array([1,0,1]),1),(array([1,1,1]),1),]weights=random.rand(3)errors=[]learning_rate=0.2num_iterations=100foriinrange(num_iterations):input,truth=choice(training_data)result=dot(weights,input)error=truth-1_or_0(result)errors.append(error)weights+=learning_rate*error*inputforx,_intraining_data:result=dot(x,w)print(\"{}:{}->{}\".format(input[:2],result,1_or_0(result)))12345678910111213141516171819202122fromrandomimportchoicefromnumpyimportarray,dot,random1_or_0=lambdax:0ifx<0else1training_data=[(array([0,0,1]),0),                    (array([0,1,1]),1),                    (array([1,0,1]),1),                    (array([1,1,1]),1),]weights=random.rand(3)errors=[]learning_rate=0.2num_iterations=100 foriinrange(num_iterations):    input,truth=choice(training_data)    result=dot(weights,input)    error=truth-1_or_0(result)    errors.append(error)    weights+=learning_rate*error*input    forx,_intraining_data:    result=dot(x,w)    print(\"{}:{}->{}\".format(input[:2],result,1_or_0(result)))经过最初的炒作一年之后，MarvinMinsky和SeymourPapert击碎了这个想法（1969,Minsky&Papert）。当时，Minsky和Papert都在麻省理工学院的AI实验室工作。他们写了一本书，证明感知机只能解决线性问题。他们还批判了关于多层感知机的想法。可悲的是，FrankRosenblatt两年后因船难去世。在Minsky和Papert的书籍出版一年之后，一位芬兰硕士研究生提出了用多层感知机解决非线性问题的理论（Linnainmaa,1970）。由于业内主流对感知机普遍不看好，十多年来AI的研究资金也非常短缺。这是AI首次遇冷。Minsky和Papert对感知机的批判主要针对“异或”问题。这个逻辑与“或”逻辑相同，但有一个例外——对两个true语句取和（1＆1）时，结果返回False（0）。如上图所示，在“或”逻辑中，我们可以将true和false分开。但是可以看出，我们无法使用一个线性函数将“异或”逻辑的结果进行区分。人工神经网络到1986年，几项实验证明，神经网络可以解决复杂的非线性问题（Rumelhartetal.,1986）。当时计算机的运算速度比该理论提出的时候快了一万倍。Rumelhart等人是这样介绍他们赫赫有名的论文的：我们描述了一种新的类神经元网络学习过程——反向传播。该过程通过反复调整网络中的连接权重，最小化网络的实际输出向量与期望输出向量之间的差异。调整权重的结果就是，不属于输入或输出的内部“隐藏”单元成为了描述任务域的重要特征，并且这些单元的交互项还捕获了任务中的正则条件。相较于早期更简单的方法，如“感知机收敛过程”Nature323,533–536(09October1986)，反向传播可以创造出有用的新特征。为了理解这篇文章的核心内容，我会在下面重现DeepMind团队AndrewTrask的代码。这不是一段普通的代码。它曾被用于斯坦福大学AndrewKarpathy的深度学习课程，以及SirajRaval的Udacity课程。最重要的是，它解决了“异或”问题，也结束了AI遇冷的时代。学习这段代码之前，我们首先通过这个模拟器交互学习一到两个小时来掌握神经网络的核心逻辑。然后阅读Trask的博客，然后再阅读四次。需要注意到，X_XOR数据中添加的参数[1]是偏置神经元，它们等价于线性函数中的常数项。PythonimportnumpyasnpX_XOR=np.array([[0,0,1],[0,1,1],[1,0,1],[1,1,1]])y_truth=np.array([[0],[1],[1],[0]])np.random.seed(1)syn_0=2*np.random.random((3,4))-1syn_1=2*np.random.random((4,1))-1defsigmoid(x):output=1/(1+np.exp(-x))returnoutputdefsigmoid_output_to_derivative(output):returnoutput*(1-output)forjinrange(60000):layer_1=sigmoid(np.dot(X_XOR,syn_0))layer_2=sigmoid(np.dot(layer_1,syn_1))error=layer_2-y_truthlayer_2_delta=error*sigmoid_output_to_derivative(layer_2)layer_1_error=layer_2_delta.dot(syn_1.T)layer_1_delta=layer_1_error*sigmoid_output_to_derivative(layer_1)syn_1-=layer_1.T.dot(layer_2_delta)syn_0-=X_XOR.T.dot(layer_1_delta)print(\"OutputAfterTraining:n\",layer_2)1234567891011121314151617181920212223242526importnumpyasnp X_XOR=np.array([[0,0,1],[0,1,1],[1,0,1],[1,1,1]])y_truth=np.array([[0],[1],[1],[0]]) np.random.seed(1)syn_0=2*np.random.random((3,4))-1syn_1=2*np.random.random((4,1))-1 defsigmoid(x):    output=1/(1+np.exp(-x))    returnoutputdefsigmoid_output_to_derivative(output):    returnoutput*(1-output) forjinrange(60000):    layer_1=sigmoid(np.dot(X_XOR,syn_0))    layer_2=sigmoid(np.dot(layer_1,syn_1))    error=layer_2-y_truth    layer_2_delta=error*sigmoid_output_to_derivative(layer_2)    layer_1_error=layer_2_delta.dot(syn_1.T)    layer_1_delta=layer_1_error*sigmoid_output_to_derivative(layer_1)    syn_1-=layer_1.T.dot(layer_2_delta)    syn_0-=X_XOR.T.dot(layer_1_delta)    print(\"OutputAfterTraining:n\",layer_2)反向传播，矩阵乘法和梯度下降放在一起会让人很难理解。这个过程的可视化通常是对其背后原理的简化。专注于理解其背后的逻辑，但不要过多地考虑直觉上的理解。另外，读者们也可以看看AndrewKarpathy关于反向传播的课程，在这个可视化网站交互学习，以及阅读MichaelNielsen关于反向传播的章节。深度神经网络深度神经网络就是在输入层和输出层之间具有多个中间层的神经网络。这个概念最早是由RinaDechter(Dechter,1986)引入的，但在2012年，也就是在IBM的人工智能程序Watson赢得美国电视智力竞赛节目Jeopardy和Google推出猫咪识别器之后才受到广泛关注。深度神经网络与之前神经网络的核心结构相同，但是应用于一些不同的问题。在正则化方面也有很大改进。最初，这只是一组用来简化冗杂的地球数据的数学函数（Tikhonov，A.N.，1963）。而现在被用于神经网络中，以加强其泛化能力。这种技术创新很大程度上依赖于计算机的运算能力。而运算能力的提升大大缩短了研究者的创新周期——如今的GPU技术只需半秒钟就可以完成一个八十年代中期的超级计算机一年的运算量。计算成本的降低和各种深度学习库的发展将深度学习带入了大众视野。我们来看一个常见的深度学习堆栈示例，从底层开始：GPU>NvidiaTeslaK80。该硬件常用于图形处理。它们深度学习的速度平均要比CPU快50-200倍。CUDA>GPU的底层编程语言CuDNN>Nvidia的库，用来优化CUDATensorflow>由Google开发，基于CuDNN的深度学习框架TFlearn>Tensorflow的前端框架下面我们来看看MNIST数字分类图像，它被称作深度学习的“HelloWorld”。我们用TFlearn来实现：Pythonfrom__future__importdivision,print_function,absolute_importimporttflearnfromtflearn.layers.coreimportdropout,fully_connectedfromtensorflow.examples.tutorials.mnistimportinput_datafromtflearn.layers.convimportconv_2d,max_pool_2dfromtflearn.layers.normalizationimportlocal_response_normalizationfromtflearn.layers.estimatorimportregression#Dataloadingandpreprocessingmnist=input_data.read_data_sets(\"/data/\",one_hot=True)X,Y,testX,testY=mnist.train.images,mnist.train.labels,mnist.test.images,mnist.test.labelsX=X.reshape([-1,28,28,1])testX=testX.reshape([-1,28,28,1])#Buildingconvolutionalnetworknetwork=tflearn.input_data(shape=[None,28,28,1],name='input')network=conv_2d(network,32,3,activation='relu',regularizer=\"L2\")network=max_pool_2d(network,2)network=local_response_normalization(network)network=conv_2d(network,64,3,activation='relu',regularizer=\"L2\")network=max_pool_2d(network,2)network=local_response_normalization(network)network=fully_connected(network,128,activation='tanh')network=dropout(network,0.8)network=fully_connected(network,256,activation='tanh')network=dropout(network,0.8)network=fully_connected(network,10,activation='softmax')network=regression(network,optimizer='adam',learning_rate=0.01,loss='categorical_crossentropy',name='target')#Trainingmodel=tflearn.DNN(network,tensorboard_verbose=0)model.fit({'input':X},{'target':Y},n_epoch=20,validation_set=({'input':testX},{'target':testY}),snapshot_step=100,show_metric=True,run_id='convnet_mnist')1234567891011121314151617181920212223242526272829303132333435from__future__importdivision,print_function,absolute_importimporttflearnfromtflearn.layers.coreimportdropout,fully_connectedfromtensorflow.examples.tutorials.mnistimportinput_datafromtflearn.layers.convimportconv_2d,max_pool_2dfromtflearn.layers.normalizationimportlocal_response_normalizationfromtflearn.layers.estimatorimportregression #Dataloadingandpreprocessingmnist=input_data.read_data_sets(\"/data/\",one_hot=True)X,Y,testX,testY=mnist.train.images,mnist.train.labels,mnist.test.images,mnist.test.labelsX=X.reshape([-1,28,28,1])testX=testX.reshape([-1,28,28,1]) #Buildingconvolutionalnetworknetwork=tflearn.input_data(shape=[None,28,28,1],name='input')network=conv_2d(network,32,3,activation='relu',regularizer=\"L2\")network=max_pool_2d(network,2)network=local_response_normalization(network)network=conv_2d(network,64,3,activation='relu',regularizer=\"L2\")network=max_pool_2d(network,2)network=local_response_normalization(network)network=fully_connected(network,128,activation='tanh')network=dropout(network,0.8)network=fully_connected(network,256,activation='tanh')network=dropout(network,0.8)network=fully_connected(network,10,activation='softmax')network=regression(network,optimizer='adam',learning_rate=0.01,                        loss='categorical_crossentropy',name='target') #Trainingmodel=tflearn.DNN(network,tensorboard_verbose=0)model.fit({'input':X},{'target':Y},n_epoch=20,            validation_set=({'input':testX},{'target':testY}),            snapshot_step=100,show_metric=True,run_id='convnet_mnist')关于MNIST问题，有很多不错的文章：https://www.tensorflow.org/get_started/mnist/beginnershttps://www.youtube.com/watch?v=NMd7WjZiCzchttps://www.oreilly.com/learning/not-another-mnist-tutorial-with-tensorflow如果读者想更深入地了解TFlearn，可以浏览我之前的文章。小结我们从TFlearn的示例中可以看出，深度学习的基础逻辑仍与Rosenblatt的感知机类似。如今的神经网络大多使用Relu激活函数，而不是二元Heaviside阶跃函数。在卷积神经网络的最后一层，损失函数使用的是categorical_crossentropy，即分类交叉熵。这是对Legendre最小二乘法的改进，可用于多分类逻辑回归问题。这里的优化算法adam来源于Debye的梯度下降。Tikhonov的正则化概念以dropout层和正则化函数L1/L2的形式得到广泛应用。如果您想要对神经网络的概念以及如何实现有一个更直观的理解，请阅读我在FloydHub博客上发表的文章：我的第一个深度学习周末。感谢IgnacioTonoli，BrianYoung，PaalRingstad，TomasMoska和CharlieHarrington帮我审阅本文的草稿。代码来源储存在Jupyternotebook中。关于EmilWallner这是Emil博客中深度学习系列的第二部分。Emil花了十年时间探索人工学习。他曾在牛津大学商学院工作，投资教育创业公司，并开创了了技术教学业务。去年，他加入Ecole42，并将自己在人工学习领域的知识应用于机器学习。2赞23收藏4评论关于作者：MentosZ统计学小硕，R/Python，数据挖掘，机器学习个人主页·我的文章·11·"], "art_create_time": ["2017/10/18"], "art_title": ["这 6 段代码，成就了如今的深度学习"], "art_url": ["http://python.jobbole.com/88713/"], "art_img": ["http://wx4.sinaimg.cn/mw690/63918611gy1fkmh74kp2lj21400u0417.jpg"]},
{"art_content": ["原文出处：by777   前言NUMPY（以下简称NP）是Python数据分析必不可少的第三方库，np的出现一定程度上解决了Python运算性能不佳的问题，同时提供了更加精确的数据类型。如今，np被Python其它科学计算包作为基础包，已成为Python数据分析的基础，可以说，NP是SciPy、Pandas等数据处理或科学计算库最基本的函数功能库。因此，理解np的数据类型对python数据分析十分有帮助。下面，本文将介绍Np的常用操作和基本数据类型。NP提供了以下重点功能。一个强大的N维数组对象ndarray广功能函数整合C/C++/Fortran代码的工具提供了线性代数、傅里叶变换、随机数生成的相关功能为了更加直观的了解Np的强大与作用，我们先看作用再看方法：使用NUMPY操作数据集在操作数据之前，我们先来理解什么是维度：什么是维度维度是一组数据的组织形式，不同数据维度可能表示不同的含义。一维数据由对等关系的有序或无序数据构成，采用线性方式组织，可以用数组表示。通俗来讲，1，2，3，4这么一行数据就可以称之为一维数据，但如果我们再对其折叠：1，2，3，4那么他就成为了二维数据，又可以称之为矩阵。什么是数据集数据集，顾名思义就是数据的集合，是用以训练程序的数据集合，一般是二维或者多维数表。如果我们想自己手工新建一个数据集，可以直接新建一个文本文件，只要有恰当的数据，都可以称之为数据集：Python城市,环比,同比,定基北京,100.1,100.2,100.3上海,111.1,111.2,111.3南京,133.0,133.3,133.41234城市,环比,同比,定基北京,100.1,100.2,100.3上海,111.1,111.2,111.3南京,133.0,133.3,133.4比如这样，我们就可以称上面的文件称之为数据集。我们还注意到，上面数据是使用逗号作为分隔符分隔数据的，它简单描述了数据的内容和含义，并使用半角逗号作为分隔符。像这样，用逗号分隔的数据集就称之为CSV（Comma-SeparatedValue,逗号分隔值）数据集，它是一种常见的文件格式，用来存储批量的数据。它就像一张excel表，用来存储简单结构的数据。怎么样，数据集的概念是否特别简单呢？生成数据集数据集是一个简单的概念，但每次使用手工的方式去写毕竟不方便，所以，我们可以使用np的内置函数来生成数据集：Pythonnp.savetxt(frame,array,fmt='%.18e\",delimiter=None)1np.savetxt(frame,array,fmt='%.18e\",delimiter=None)frame：文件、字符串、或产生器的名字，可以是.gz，.bz2的压缩文件arrray：存入文件的NP的数组fmt(format):写入文件的格式，如%d,%.2f,%.18e(默认，科学计数法保留18位)delemiter:分割字符串，默认是任何空格。我们可以这样写下代码：Pythona=np.arange(20).reshape(4,5)np.savetxt('demo.csv',a,fmt='%d',delimiter=',')12a=np.arange(20).reshape(4,5)np.savetxt('demo.csv',a,fmt='%d',delimiter=',')这样，我们就会在当前的工作目录下发现一个新的demo.csv，用记事本打开，里面是一个4*5的矩阵，元素0~19。读取数据集既然生成，那就可以读取，同样使用np：Pythonnp.loadtxt(frame,dtype=np.float,delimiter=None,inpack=False)1np.loadtxt(frame,dtype=np.float,delimiter=None,inpack=False)frame:指定读入的文件来源dtype:数据类型，默认为np.float。delimiter:分割字符串unpack：默认为False读入文件写入一个数组，如果为True，读入属性将分别写入不同变量同样的我们只需要写下代码：Pythonnp.loadtxt(\"demo.csv\",delimiter=\",\")1np.loadtxt(\"demo.csv\",delimiter=\",\")就可以查看到我们先前写入的数组a。CSV文件的局限可以发现，CSV文件只能有效存储和读取一维和二维数组，因为更高的维度无法更直观的文本下显现出来，这时，更加灵活的存取方式就呼之欲出了，但讲之前先卖个关子，再介绍一个不太常用的方法：tofile：对于NP中的ndarray数组，我们可以使用NP中的tofile方法。Pythona.tofile(frame,sep='',format='%d')1a.tofile(frame,sep='',format='%d')frame:文件，字符串数据分割字符串，如果不写，将使用二进制文件存储format：写入数据的格式同样，我们只需要命令：Pythonimportnumpyasnpa=np.arange(100).reshape(5,10,2)a.tofile(\"a.dat\",sep=',',format='%d')123importnumpyasnpa=np.arange(100).reshape(5,10,2)a.tofile(\"a.dat\",sep=',',format='%d')就可以生成新的CSV数据集。此时，我们如果打开a.dat文件，我们可以看到数组1,2,3……99。但是与CSV不同，这个文件并没有包含数字的维度信息，他只是将数组所有元素逐一的列出。而且如果我们不指定sep，将保存为二进制文件，虽然对人不可读，但将占用更小的空间。既然tofile可以保存文本文件，那么也很容易猜到对应的fromfile可以还原这些信息。Pythonnp.fromfile(frame,dtype=float,count=-1,sep='')1np.fromfile(frame,dtype=float,count=-1,sep='')frame：文件dtype：读取元素使用的数据类型，默认为floatcount：读文件的个数，默认-1，读取全部sep:数据分割字符串，如果是空串，写入文件为二进制。如果我们想要重新恢复数据的维度信息，我们需要重新使用reshape来恢复维度信息：Pythonc=np.fromfile(\"b.dat\",sep=',',dtype=np.int).reshape(5,10,2)1c=np.fromfile(\"b.dat\",sep=',',dtype=np.int).reshape(5,10,2)不得不说，当我看到这个方法时感觉这两个真是蠢爆了，使用savetxt/loadtxt至少还能保存个二维信息，而使用了tofile/fromfile方法居然把数被伸展为一维的，然后自己记住维度信息(╯‵□′)╯︵┻━┻。因此，为了保存更复杂的数据类型，二维以上的数据信息，save/load函数成功解决了这个问题：（为了方便，两个函数就放到一起了）保存/读取高维度数据Pythonnp.save(frame,array)或np.savez(fname,array)(压缩)+frame：文件名，以.npy为扩展名，压缩扩展名为.npz+array：数组变量np.load(fname)1234np.save(frame,array)或np.savez(fname,array)(压缩)+frame：文件名，以.npy为扩展名，压缩扩展名为.npz+array：数组变量np.load(fname)Demo:Pythona=np.arange(100).reshape(5,10,2)np.save(\"a.npy\",a)b=np.load(\"a.npy\")123a=np.arange(100).reshape(5,10,2)np.save(\"a.npy\",a)b=np.load(\"a.npy\")附录附录中提供NP的常用方法及注释，做查询用。np数组定义Python>>>lst=[[1,3,5],[2,4,6]]>>>np_lst=np.array(lst,dtype=np.float)>>>print(np_lst.shape)#返回数组的行列>>>print(np_lst.ndim)#返回数组的维数>>>print(np_lst.dtype)#返回数据类型，float默认为64>>>print(np_lst.itemsize)#np.array每个元素的大小，float64占8个字节>>>print(np_lst.size)#大小，6个元素(2,3)2float6486123456789101112>>>lst=[[1,3,5],[2,4,6]]>>>np_lst=np.array(lst,dtype=np.float)>>>print(np_lst.shape)#返回数组的行列>>>print(np_lst.ndim)#返回数组的维数>>>print(np_lst.dtype)#返回数据类型，float默认为64>>>print(np_lst.itemsize)#np.array每个元素的大小，float64占8个字节>>>print(np_lst.size)#大小，6个元素(2,3)2float6486初始化数组Python>>>print(np.zeros([2,4])#初始化一个2行4列的数组>>>print(np.ones([2,4])[[0.0.0.0.][0.0.0.0.]][[1.1.1.1.][1.1.1.1.]]123456>>>print(np.zeros([2,4])#初始化一个2行4列的数组>>>print(np.ones([2,4])[[0.  0.  0.  0.][0.  0.  0.  0.]][[1.  1.  1.  1.][1.  1.  1.  1.]]随机序列Python>>>print(np.random.rand(2,4))#将生成一个处于0~1之间2行4列的随机数序列（不加参数将只返回一个）[[0.395312860.48450.14631680.82327991][0.890422550.650499310.438901630.89577744]]123>>>print(np.random.rand(2,4))#将生成一个处于0~1之间2行4列的随机数序列（不加参数将只返回一个）[[0.39531286  0.4845      0.1463168  0.82327991][0.89042255  0.65049931  0.43890163  0.89577744]]如果想要多个随机整数：Pythonprint(np.random.randint(22,55,3))#必须有（前两个参数）指定范围，第三个参数用于指定生成的个数[274029]print(np.random.randn(2，4))#生成标准正态随机数[[-1.155615480.36899530.38253231-1.16346441][-1.32625322-0.41707673-0.11822205-0.95807535]]print(np.random.choice([10,20,40,33]))#从指定可迭代的数组中生成随机数20print(np.random.beta(1,10,4))#生成4个beta分布[0.022585480.258488960.006968990.0609543]123456789print(np.random.randint(22,55,3))#必须有（前两个参数）指定范围，第三个参数用于指定生成的个数[274029]print(np.random.randn(2，4))#生成标准正态随机数[[-1.15561548  0.3689953  0.38253231-1.16346441][-1.32625322-0.41707673-0.11822205-0.95807535]]print(np.random.choice([10,20,40,33]))#从指定可迭代的数组中生成随机数20print(np.random.beta(1,10,4))#生成4个beta分布[0.02258548  0.25848896  0.00696899  0.0609543]多维数组运算Pythonprint(np.arange(1,11,2))#得到step为2的range序列[13579]12print(np.arange(1,11,2))#得到step为2的range序列[13579]还可以使用reshape函数，对数组结构重定义：Pythonprint(np.arange(1,11).reshape(2,5))#（5可以缺省为-1）[[12345][678910]]123print(np.arange(1,11).reshape(2,5))#（5可以缺省为-1）[[1  2  3  4  5][6  7  8  910]]下面介绍一些常用的运算操作：Pythonlst=np.arange(1,11).reshape(2,5)print(np.exp(lst))#自然指数操作[[2.71828183e+007.38905610e+002.00855369e+015.45981500e+011.48413159e+02][4.03428793e+021.09663316e+032.98095799e+038.10308393e+032.20264658e+04]]1234lst=np.arange(1,11).reshape(2,5)print(np.exp(lst))#自然指数操作[[  2.71828183e+00  7.38905610e+00  2.00855369e+01  5.45981500e+01    1.48413159e+02][  4.03428793e+02  1.09663316e+03  2.98095799e+03  8.10308393e+03    2.20264658e+04]]此外，还可以sqrt、log、sin、sum、max等操作：我们下定义一个三维数组：Pythonlst=np.array([[[1,2,3,4],[4,5,6,7]],[[7,8,9,10],[10,11,12,13]],[[14,15,16,17],[18,19,20,21]]])print(lst.sum())2521234567lst=np.array([                [[1,2,3,4],[4,5,6,7]],                [[7,8,9,10],[10,11,12,13]],                [[14,15,16,17],[18,19,20,21]]            ])print(lst.sum())252我们可以看到sum方法对lst的所有元素都进行了求和，此外我们还可以通过对sum方法增加参数axis的方式来设置求和的深入维度：Pythonprint(lst.sum(axis=0))[[22252831]#22=1+7+14；25=2+8+15[32353841]]print(lst.sum(axis=1))[[57911]#5=1+4；7=2+5[17192123][32343638]]print(lst.sum(axis=2))[[1022]#10=1+2+3+4；22=4+5+6+7[3446][6278]]1234567891011print(lst.sum(axis=0))[[22252831]#22=1+7+14；25=2+8+15[32353841]]print(lst.sum(axis=1))[[5  7  911]#5=1+4；7=2+5[17192123][32343638]]print(lst.sum(axis=2))[[1022]#10=1+2+3+4；22=4+5+6+7[3446][6278]]这里的axis取值为数组维数-1，axis可以理解为进行运算操作时的深入程度，axis越大，深入程度越大。同理，不仅sum函数，max等函数也可以一样理解。相加运算numpy.array是np最简单的数据结构。np.array相比与Python原生列表其强大之处在于可以实现对数组数据的运算。我们知道，list只能对元素的追加。而numpy是真正意义上的数据运算。例如PythonIn[1]:importnumpyasnpIn[2]:list1=np.array([10,20,30,40])In[3]:list2=np.array([4,3,2,1])In[4]:print(list1)[10203040]In[5]:print(list1+list2)[14233241]1234567    In[1]:importnumpyasnp    In[2]:list1=np.array([10,20,30,40])    In[3]:list2=np.array([4,3,2,1])    In[4]:print(list1)    [10203040]    In[5]:print(list1+list2)    [14233241]但np最强大的地方不在于简单的一维运算，Np对矩阵也能进行基本的运算操作：Pythonlst1=np.array([10,20,30,40])lst2=np.array([4,3,2,1])print(np.dot(lst1.reshape([2,2]),lst2.reshape([2,2])))[[1022][3446][6278]][[8050][200130]]12345678lst1=np.array([10,20,30,40])lst2=np.array([4,3,2,1])print(np.dot(lst1.reshape([2,2]),lst2.reshape([2,2])))[[1022][3446][6278]][[80  50][200130]]此外，由于原生list没有确定的数据类型，所以维护起来成本较高，而使用C编写的numpy，则可以声明各种常见的数据类型：Pythonlst=[[1,3,5],[2,4,6]]np_lst=np.array(lst,dtype=np.float)12lst=[[1,3,5],[2,4,6]]np_lst=np.array(lst,dtype=np.float)np所支持的数据类型都有bool、int8/16/32/64/128/、uint8/16/32/64/128、float16/32/43、complex64/128、string。总结Python作为一门弱类型语言，有其不可避免的缺点。但NP的出现，弥补了这些缺点，使其具备了构造复杂数据类型的能力，为Python数据分析提供了基础。2赞9收藏2评论"], "art_create_time": ["2017/10/21"], "art_title": ["Python数据分析 - Numpy"], "art_url": ["http://python.jobbole.com/88726/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2015/02/edecceebedd0d04aa17bccba430ddcaf.jpg"]},
{"art_content": ["原文出处：一曲广陵散   一,引用[书]流畅的Python[书]EffectivePython二,基本概念问题１：装饰器是什么？解答：　严格来说，装饰器只是语法糖，　装饰器是可调用的对象，可以像常规的可调用对象那样调用，特殊的地方是装饰器的参数是一个函数问题２：装饰器有什么特性？解答：　装饰器有２个特性，一是可以把被装饰的函数替换成其他函数，　二是可以在加载模块时候立即执行Pythondefdecorate(func):print('runningdecorate',func)defdecorate_inner():print('runningdecorate_innerfunction')returnfunc()returndecorate_inner<ahref=\"http://www.jobbole.com/members/decorate\">@decorate</a>deffunc_1():print('runningfunc_1')if__name__=='__main__':print(func_1)＃返回值runningdecorate<functionfunc_1at0x7f29f644d268><functiondecorate.<locals>.decorate_innerat0x7f29f641cb70>123456789101112131415161718defdecorate(func):    print('runningdecorate',func)    defdecorate_inner():        print('runningdecorate_innerfunction')        returnfunc()    returndecorate_inner <ahref=\"http://www.jobbole.com/members/decorate\">@decorate</a>deffunc_1():    print('runningfunc_1') if__name__=='__main__':     print(func_1) ＃返回值runningdecorate<functionfunc_1at0x7f29f644d268><functiondecorate.<locals>.decorate_innerat0x7f29f641cb70>问题３：如何使用被装饰函数中的参数？解答：　通过args和　*kwargs传递被修饰函数中的参数Pythondefdecorate(func):defdecorate_inner(*args,**kwargs):print(type(args),type(kwargs))print('args',args,'kwargs',kwargs)returnfunc(*args,**kwargs)returndecorate_inner<ahref=\"http://www.jobbole.com/members/decorate\">@decorate</a>deffunc_1(*args,**kwargs):print(args,kwargs)if__name__=='__main__':func_1('1','2','3',para_1='1',para_2='2',para_3='3')#返回值<class'tuple'><class'dict'>args('1','2','3')kwargs{'para_2':'2','para_1':'1','para_3':'3'}('1','2','3'){'para_2':'2','para_1':'1','para_3':'3'}12345678910111213141516171819defdecorate(func):    defdecorate_inner(*args,**kwargs):        print(type(args),type(kwargs))        print('args',args,'kwargs',kwargs)        returnfunc(*args,**kwargs)    returndecorate_inner <ahref=\"http://www.jobbole.com/members/decorate\">@decorate</a>deffunc_1(*args,**kwargs):    print(args,kwargs) if__name__=='__main__':     func_1('1','2','3',para_1='1',para_2='2',para_3='3') #返回值<class'tuple'><class'dict'>args('1','2','3')kwargs{'para_2':'2','para_1':'1','para_3':'3'}('1','2','3'){'para_2':'2','para_1':'1','para_3':'3'}三,叠放装饰器问题１：叠放装饰器执行顺序是什么？解答：　如果一个函数被多个装饰器修饰，其实应该是该函数先被最里面的装饰器修饰后（下面例子中函数main()先被inner装饰，变成新的函数），变成另一个函数后，再次被装饰器修饰Pythondefouter(func):print('enterouter',func)defwrapper():print('runningouter')func()returnwrapperdefinner(func):print('enterinner',func)defwrapper():print('runninginner')func()returnwrapper@outer@innerdefmain():print('runningmain')if__name__=='__main__':main()＃返回值enterinner<functionmainat0x7fa1c96e8b70>enterouter<functioninner.<locals>.wrapperat0x7fa1c96e8bf8>runningouterrunninginnerrunningmain123456789101112131415161718192021222324252627282930defouter(func):    print('enterouter',func)    defwrapper():        print('runningouter')        func()    returnwrapper definner(func):    print('enterinner',func)    defwrapper():        print('runninginner')        func()    returnwrapper @outer@innerdefmain():    print('runningmain') if__name__=='__main__':     main() ＃返回值 enterinner<functionmainat0x7fa1c96e8b70>enterouter<functioninner.<locals>.wrapperat0x7fa1c96e8bf8>runningouterrunninginnerrunningmain四,标准库中的装饰器问题１：　标准库中都有哪些装饰器？解答：　标准库中有多种装饰器，　例如：装饰方法的函数有property,classmethod,staticmethod；　functools模块中的lru_cache, singledispatch, wraps等等Pythonfromfunctoolsimportlru_cachefromfunctoolsimportsingledispatchfromfunctoolsimportwraps123fromfunctoolsimportlru_cachefromfunctoolsimportsingledispatchfromfunctoolsimportwraps问题２：为什么要使用@wraps装饰器？它的作用是什么？解答：　使用装饰器会产生我们可能不希望出现的副作用，　例如：改变被修饰函数名称，对于调试器或者对象序列化器等需要使用内省机制的那些工具，可能会无法正常运行；其实调用装饰器后，会将同一个作用域中原来函数同名的那个变量（例如下面的func_1）,重新赋值为装饰器返回的对象；使用＠wraps后，会把与内部函数（被修饰函数，例如下面的func_1）相关的重要元数据全部复制到外围函数（例如下面的decorate_inner）Pythonfromfunctoolsimportwrapsdefdecorate(func):print('runningdecorate',func)@wraps(func)defdecorate_inner():print('runningdecorate_innerfunction',decorate_inner)returnfunc()returndecorate_inner<ahref=\"http://www.jobbole.com/members/decorate\">@decorate</a>deffunc_1():print('runningfunc_1',func_1)if__name__=='__main__':func_1()＃返回值runningdecorate<functionfunc_1at0x7f145d2c2268>runningdecorate_innerfunction<functionfunc_1at0x7f145b9731e0>runningfunc_1<functionfunc_1at0x7f145b9731e0>12345678910111213141516171819202122fromfunctoolsimportwraps defdecorate(func):    print('runningdecorate',func)    @wraps(func)    defdecorate_inner():        print('runningdecorate_innerfunction',decorate_inner)        returnfunc()    returndecorate_inner <ahref=\"http://www.jobbole.com/members/decorate\">@decorate</a>deffunc_1():    print('runningfunc_1',func_1) if__name__=='__main__':     func_1() ＃返回值runningdecorate<functionfunc_1at0x7f145d2c2268>runningdecorate_innerfunction<functionfunc_1at0x7f145b9731e0>runningfunc_1<functionfunc_1at0x7f145b9731e0>五,装饰器设计模式问题１：　什么是装饰器设计模式？解答：　动态的给一个对象添加一些额外的职责，就扩展功能而言，装饰器模式比子类化更加灵活，在设计模式中，装饰器和组件都是抽象类，为了给具体的组件添加行为，具体的装饰器实例要包装具体组件的实例，即，装饰器和所装饰的组件接口一致，对使用该组建的客户透明，将客户的请求转发给该组件，并且可能在转发前后执行一些额外的操作，透明性使得可以递归嵌套多个装饰器，从而可以添加任意多个功能问题２：　Python中的装饰器函数和设计模式中的装饰器模式有什么关系？解答： 修饰器模式和Python修饰器之间并不是一对一的等价关系，　Python装饰器函数更为强大，不仅仅可以实现装饰器模式。1赞4收藏1评论"], "art_create_time": ["2017/09/05"], "art_title": ["Python 装饰器"], "art_url": ["http://python.jobbole.com/88512/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2017/09/2b40543f2ca8191002fc449a3ef4a516.png"]},
{"art_content": ["原文出处：chichao   re模块下的函数compile(pattern)：创建模式对象Pythonimportrepat=re.compile('A')m=pat.search('CBA')#等价于re.search('A','CBA')printm<_sre.SRE_Matchobjectat0x9d690c8>#匹配到了，返回MatchObject（True）m=pat.search('CBD')printmNone#没有匹配到，返回None（False）123456789importrepat=re.compile('A')m=pat.search('CBA')                    #等价于re.search('A','CBA')printm<_sre.SRE_Matchobjectat0x9d690c8>  #匹配到了，返回MatchObject（True） m=pat.search('CBD')printmNone                                  #没有匹配到，返回None（False）search(pattern,string)：在字符串中寻找模式Pythonm=re.search('asd','ASDasd')printm<_sre.SRE_Matchobjectat0xb72cd6e8>#匹配到了，返回MatchObject（True）m=re.search('asd','ASDASD')printmNone#没有匹配到，返回None（False）123456m=re.search('asd','ASDasd')printm<_sre.SRE_Matchobjectat0xb72cd6e8>  #匹配到了，返回MatchObject（True）m=re.search('asd','ASDASD')printmNone                                  #没有匹配到，返回None（False）match(pattern,string)：在字符串开始处匹配模式Pythonm=re.search('asd','ASDasd')printm<_sre.SRE_Matchobjectat0xb72cd6e8>#匹配到了，返回MatchObject（True）m=re.search('asd','ASDASD')printmNone#没有匹配到，返回None（False）123456m=re.search('asd','ASDasd')printm<_sre.SRE_Matchobjectat0xb72cd6e8>  #匹配到了，返回MatchObject（True）m=re.search('asd','ASDASD')printmNone                                  #没有匹配到，返回None（False）等价于Pythonpat=re.compile('a')printpat.match('Aasd')Noneprintpat.match('aASD')<_sre.SRE_Matchobjectat0xb72cd6e8>12345pat=re.compile('a')printpat.match('Aasd')Noneprintpat.match('aASD')<_sre.SRE_Matchobjectat0xb72cd6e8>上面的函数返回都可以在if条件语句中进行判断：Pythonifpat.search('asd'):...print'OK'...OK#找到返回ifre.search('a','ASD'):...print\"OK\"...#没有找到1234567ifpat.search('asd'):...    print'OK'...OK        #找到返回ifre.search('a','ASD'):...    print\"OK\"...      #没有找到split(pattern,string)：根据模式分割字符串,返回列表Pythonre.split(',','a,s,d,asd')['a','s','d','asd']#返回列表pat=re.compile(',')pat.split('a,s,d,asd')['a','s','d','asd']#返回列表re.split('[,]+','a,s,d,,,,,asd')#正则匹配：[,]+，后面说明['a','s','d','asd']re.split('[,]+','a,s,d,,,,,asd',maxsplit=2)#maxsplit最多分割次数['a','s','d,,,,,asd']pat=re.compile('[,]+')#正则匹配：[,]+，后面说明pat.split('a,s,d,,,,,asd',maxsplit=2)#maxsplit最多分割次数['a','s','d,,,,,asd']12345678910111213141516re.split(',','a,s,d,asd')['a','s','d','asd']          #返回列表 pat=re.compile(',')pat.split('a,s,d,asd')['a','s','d','asd']          #返回列表 re.split('[,]+','a,  s  ,d    ,,,,,asd')  #正则匹配：[,]+，后面说明['a','s','d','asd'] re.split('[,]+','a,  s  ,d    ,,,,,asd',maxsplit=2)#maxsplit最多分割次数['a','s','d    ,,,,,asd'] pat=re.compile('[,]+')                    #正则匹配：[,]+，后面说明pat.split('a,  s  ,d    ,,,,,asd',maxsplit=2)        #maxsplit最多分割次数['a','s','d    ,,,,,asd']findall(pattern,string)：列表形式返回匹配项Pythonre.findall('a','ASDaDFGAa')['a','a']#列表形式返回匹配到的字符串pat=re.compile('a')pat.findall('ASDaDFGAa')['a','a']#列表形式返回匹配到的字符串pat=re.compile('[A-Z]+')#正则匹配：'[A-Z]+'后面有说明pat.findall('ASDcDFGAa')['ASD','DFGA']#找到匹配到的字符串pat=re.compile('[A-Z]')pat.findall('ASDcDFGAa')#正则匹配：'[A-Z]+'后面有说明['A','S','D','D','F','G','A']#找到匹配到的字符串pat=re.compile('[A-Za-z]')#正则匹配：'[A-Za-z]+'匹配所有单词，后面有说明pat.findall('ASDcDFGAa')['A','S','D','c','D','F','G','A','a']123456789101112131415161718re.findall('a','ASDaDFGAa')['a','a']                          #列表形式返回匹配到的字符串 pat=re.compile('a')pat.findall('ASDaDFGAa')['a','a']                          #列表形式返回匹配到的字符串 pat=re.compile('[A-Z]+')      #正则匹配：'[A-Z]+'后面有说明pat.findall('ASDcDFGAa')['ASD','DFGA']                      #找到匹配到的字符串 pat=re.compile('[A-Z]')pat.findall('ASDcDFGAa')        #正则匹配：'[A-Z]+'后面有说明['A','S','D','D','F','G','A']  #找到匹配到的字符串 pat=re.compile('[A-Za-z]')    #正则匹配：'[A-Za-z]+'匹配所有单词，后面有说明pat.findall('ASDcDFGAa')['A','S','D','c','D','F','G','A','a']sub(pat,repl,string)：用repl替换pat匹配项(留的是中间的，因为中间在中心)Pythonre.sub('a','A','abcasd')#找到a用A替换，后面见和group的配合使用'AbcAsd'pat=re.compile('a')pat.sub('A','abcasd')'AbcAsd'pat=re.compile(r'www\\.(.*)\\..{3}')#正则表达式#在Python的string前面加上‘r’，是为了告诉编译器这个string是个rawstring，不要转译反斜杠'\\'。#例如，\\n在rawstring中，是两个字符，\\和n，而不会转译为换行符。#由于正则表达式和\\会有冲突，因此，当一个字符串使用了正则表达式后，最好在前面加上'r'。#与大多数编程语言相同，正则表达式里使用\"\\\"作为转义字符，这就可能造成反斜杠困扰。#假如你需要匹配文本中的字符\"\\\"，那么使用编程语言表示的正则表达式里将需要4个反斜杠\"\\\\\\\\\"：#前两个和后两个分别用于在编程语言里转义成反斜杠，转换成两个反斜杠后再在正则表达式里转义成一个反斜杠。#Python里的原生字符串很好地解决了这个问题，这个例子中的正则表达式可以使用r\"\\\\\"表示。#同样，匹配一个数字的\"\\\\d\"可以写成r\"\\d\"。#有了原生字符串，你再也不用担心是不是漏写了反斜杠，写出来的表达式也更直观。#不是说加了r\\就没有转译功能，好乱，就直接记住1句话：#当一个字符串使用了正则表达式后，最好在前面加上'r'，这样你再也不用担心是不是漏写了反斜杠，写出来的表达式也更直观pat.match('www.dxy.com').group(1)'dxy're.sub(r'www\\.(.*)\\..{3}',r'\\1','hello,www.dxy.com')pat.sub(r'\\1','hello,www.dxy.com')'hello,dxy'#r'1'是第一组的意思#通过正则匹配找到符合规则的\"www.dxy.com\"，取得组1字符串去替换整个匹配。pat=re.compile(r'(\\w+)(\\w+)')#正则表达式s='helloworld!hellohz!'pat.findall('helloworld!hellohz!')[('hello','world'),('hello','hz')]pat.sub(r'\\2\\1',s)#通过正则得到组1(hello)，组2(world)，再通过sub去替换。即组1替换组2，组2替换组1，调换位置。'worldhello!hzhello!'1234567891011121314151617181920212223242526272829303132333435363738394041re.sub('a','A','abcasd')  #找到a用A替换，后面见和group的配合使用'AbcAsd' pat=re.compile('a')pat.sub('A','abcasd')'AbcAsd' pat=re.compile(r'www\\.(.*)\\..{3}')#正则表达式  #在Python的string前面加上‘r’，是为了告诉编译器这个string是个rawstring，不要转译反斜杠'\\'。  #例如，\\n在rawstring中，是两个字符，\\和n，而不会转译为换行符。  #由于正则表达式和\\会有冲突，因此，当一个字符串使用了正则表达式后，最好在前面加上'r'。      #与大多数编程语言相同，正则表达式里使用\"\\\"作为转义字符，这就可能造成反斜杠困扰。  #假如你需要匹配文本中的字符\"\\\"，那么使用编程语言表示的正则表达式里将需要4个反斜杠\"\\\\\\\\\"：  #前两个和后两个分别用于在编程语言里转义成反斜杠，转换成两个反斜杠后再在正则表达式里转义成一个反斜杠。  #Python里的原生字符串很好地解决了这个问题，这个例子中的正则表达式可以使用r\"\\\\\"表示。  #同样，匹配一个数字的\"\\\\d\"可以写成r\"\\d\"。  #有了原生字符串，你再也不用担心是不是漏写了反斜杠，写出来的表达式也更直观。    #不是说加了r\\就没有转译功能，好乱，就直接记住1句话：  #当一个字符串使用了正则表达式后，最好在前面加上'r'，这样你再也不用担心是不是漏写了反斜杠，写出来的表达式也更直观pat.match('www.dxy.com').group(1)'dxy' re.sub(r'www\\.(.*)\\..{3}',r'\\1','hello,www.dxy.com') pat.sub(r'\\1','hello,www.dxy.com')'hello,dxy'#r'1'是第一组的意思#通过正则匹配找到符合规则的\"www.dxy.com\"，取得组1字符串去替换整个匹配。  pat=re.compile(r'(\\w+)(\\w+)')    #正则表达式s='helloworld!hellohz!' pat.findall('helloworld!hellohz!')[('hello','world'),('hello','hz')]pat.sub(r'\\2\\1',s)                #通过正则得到组1(hello)，组2(world)，再通过sub去替换。即组1替换组2，组2替换组1，调换位置。  'worldhello!hzhello!'escape(string)：对字符串里面的特殊字符串进行转义Pythonre.escape('www.dxy.cn')'www\\\\.dxy\\\\.cn'#转义12re.escape('www.dxy.cn')'www\\\\.dxy\\\\.cn'                  #转义上面的函数中，只有match、search有group方法，其他的函数没有。函数的方法group：获取子模式(组)的匹配项Pythonpat=re.compile(r'www\\.(.*)\\.(.*)')#用()表示1个组，2个组m=pat.match('www.dxy.com')m.group()#默认为0，表示匹配整个字符串'www.dxy.com'm.group(1)#返回给定组1匹配的子字符串'dxy'm.group(2)'com'12345678910pat=re.compile(r'www\\.(.*)\\.(.*)')      #用()表示1个组，2个组m=pat.match('www.dxy.com')m.group()                                  #默认为0，表示匹配整个字符串  'www.dxy.com' m.group(1)                                #返回给定组1匹配的子字符串'dxy' m.group(2)'com'start：给定组匹配项的开始位置Pythonm.start(2)#组2开始的索引812m.start(2)                                #组2开始的索引8end：给定组匹配项的结束位置Pythonm.end(2)#组2结束的索引1112m.end(2)                                  #组2结束的索引11span：给定组匹配项的开始结束位置Pythonm.span(2)#组2开始、结束的索引(8,11)12m.span(2)                                  #组2开始、结束的索引(8,11)正则表达式元字符“.”：通配符,除换行符外的任意的1个字符Pythonpat=re.compile('.')pat.match('abc')<_sre.SRE_Matchobjectat0xb72b6170>pat.match('abc').group()'a'#匹配到了首个字符pat.search('abc').group()'a'pat.match('\\n').group()#换行符匹配出错Traceback(mostrecentcalllast):File\"<stdin>\",line1,in<module>AttributeError:'NoneType'objecthasnoattribute'group'1234567891011pat=re.compile('.')pat.match('abc')<_sre.SRE_Matchobjectat0xb72b6170>pat.match('abc').group()'a'                                #匹配到了首个字符pat.search('abc').group()'a'pat.match('\\n').group()        #换行符匹配出错Traceback(mostrecentcalllast):  File\"<stdin>\",line1,in<module>AttributeError:'NoneType'objecthasnoattribute'group'“\\”:转义符Pythonpat=re.compile('\\.')pat.search('abc.efg').group()#匹配到.'.'pat.findall('abc.efg')#不用group,返回列表['.']12345pat=re.compile('\\.')pat.search('abc.efg').group()  #匹配到.'.'pat.findall('abc.efg')        #不用group,返回列表['.']“[…]”:字符集合，匹配里面的任意一个元素Python>>>pat=re.compile('[abc]')>>>pat.match('axbycz').group()'a'>>>pat.search('axbycz').group()'a'>>>pat.findall('axbycz')['a','b','c']1234567>>>pat=re.compile('[abc]')>>>pat.match('axbycz').group()'a'>>>pat.search('axbycz').group()'a'>>>pat.findall('axbycz')['a','b','c']“\\d”:数字Python>>>pat=re.compile('\\d')>>>pat.search('ax1by2cz3').group()#匹配到第一个数字:1，返回'1'>>>pat.match('ax1by2cz3').group()#匹配不到（首个不是）返回None，报错，match匹配字符串头Traceback(mostrecentcalllast):File\"<stdin>\",line1,in<module>AttributeError:'NoneType'objecthasnoattribute'group'>>>pat.findall('ax1by2cz3')#匹配所有的数字，列表返回['1','2','3']1234567891011>>>pat=re.compile('\\d')          >>>pat.search('ax1by2cz3').group()  #匹配到第一个数字:1，返回'1' >>>pat.match('ax1by2cz3').group()    #匹配不到（首个不是）返回None，报错，match匹配字符串头Traceback(mostrecentcalllast):  File\"<stdin>\",line1,in<module>AttributeError:'NoneType'objecthasnoattribute'group' >>>pat.findall('ax1by2cz3')          #匹配所有的数字，列表返回['1','2','3']“\\D”:非数字Python>>>pat=re.compile('\\D')>>>pat.match('ax1by2cz3').group()'a'>>>pat.search('ax1by2cz3').group()'a'>>>pat.findall('ax1by2cz3')['a','x','b','y','c','z']1234567>>>pat=re.compile('\\D')>>>pat.match('ax1by2cz3').group()'a'>>>pat.search('ax1by2cz3').group()'a'>>>pat.findall('ax1by2cz3')['a','x','b','y','c','z']“\\s”：空白字符、\\t、\\r、\\n、空格Python>>>pat=re.compile('\\s')>>>pat.findall('\\rax1\\nby2\\tcz3')['\\r','','\\n','','\\t']>>>pat.search('\\rax1\\nby2\\tcz3').group()'\\r'>>>pat.match('\\rax1\\nby2\\tcz3').group()'\\r'1234567>>>pat=re.compile('\\s')>>>pat.findall('\\rax1\\nby2\\tcz3')['\\r','','\\n','','\\t']>>>pat.search('\\rax1\\nby2\\tcz3').group()'\\r'>>>pat.match('\\rax1\\nby2\\tcz3').group()'\\r'“S”:非空白字符Python>>>pat=re.compile('\\S')>>>pat.search('\\rax1\\nby2\\tcz3').group()'a'>>>pat.findall('\\rax1\\nby2\\tcz3')['a','x','1','b','y','2','c','z','3']12345>>>pat=re.compile('\\S')>>>pat.search('\\rax1\\nby2\\tcz3').group()'a'>>>pat.findall('\\rax1\\nby2\\tcz3')['a','x','1','b','y','2','c','z','3']“\\w”：单个的数字和字母，[A-Za-z0-9]Python>>>pat=re.compile('\\w')>>>pat.search('1a2b3c').group()'1'>>>pat.findall('1a2b3c')['1','a','2','b','3','c']>>>pat.match('1a2b3c').group()'1'1234567>>>pat=re.compile('\\w')>>>pat.search('1a2b3c').group()'1'>>>pat.findall('1a2b3c')['1','a','2','b','3','c']>>>pat.match('1a2b3c').group()'1'“\\W”:非单词字符,除数字和字母外Python>>>pat=re.compile('\\W')>>>pat.findall('1a2我b3c')#python是用三字节表示一个汉字['\\xe6','\\x88','\\x91']>>>pat.search('1a2我b3c').group()'\\xe6'12345>>>pat=re.compile('\\W')>>>pat.findall('1a2我b3c')#python是用三字节表示一个汉字['\\xe6','\\x88','\\x91']>>>pat.search('1a2我b3c').group()'\\xe6'数量词“*”：0次或多次（ 乘0会变成0）Python>>>pat=re.compile('[abc]*')>>>pat.match('abcabcdefabc').group()'abcabc'#2次>>>pat.search('abcabcdefabc').group()'abcabc'#2次>>>pat.findall('abcabcdefabc')['abcabc','','','','abc','']#2次和1次,因为有0次，所以匹配了''1234567>>>pat=re.compile('[abc]*')>>>pat.match('abcabcdefabc').group()'abcabc'                              #2次>>>pat.search('abcabcdefabc').group()'abcabc'                              #2次>>>pat.findall('abcabcdefabc')['abcabc','','','','abc','']    #2次和1次,因为有0次，所以匹配了''“+”：1次或多次（ 加0不会变成0）Python>>>pat=re.compile('[abc]+')>>>pat.match('abcdefabcabc').group()'abc'>>>pat.search('abcdefabcabc').group()'abc'>>>pat.findall('abcdefabcabc')['abc','abcabc']1234567>>>pat=re.compile('[abc]+')>>>pat.match('abcdefabcabc').group()'abc'>>>pat.search('abcdefabcabc').group()'abc'>>>pat.findall('abcdefabcabc')['abc','abcabc']“?”：0次或1次，match,search不会出现none，会出现’‘（因为0次也是符合的）0次或1次不是指[xxx]这个集合，而是其中的任何的一个字符Python>>>pat=re.compile('[abc]?')>>>pat.match('defabc').group()#0次''>>>pat.match('abcdefabc').group()'a'>>>pat.search('defabc').group()#0次''>>>pat.findall('defabc')#0次和1次['','','','a','b','c','']#后面总再加个''123456789>>>pat=re.compile('[abc]?')>>>pat.match('defabc').group()    #0次''>>>pat.match('abcdefabc').group()'a'>>>pat.search('defabc').group()    #0次''>>>pat.findall('defabc')          #0次和1次['','','','a','b','c','']    #后面总再加个''“数量词?”：非贪婪模式：只匹配最少的（尽可能少）；默认贪婪模式：匹配最多的（尽可能多）Python>>>pat=re.compile('[abc]+')#贪婪模式>>>pat.match('abcdefabcabc').group()#匹配尽可能多的：abc'abc'>>>pat.match('bbabcdefabcabc').group()'bbabc'>>>pat.search('dbbabcdefabcabc').group()'bbabc'>>>pat.findall('abcdefabcabc')['abc','abcabc']>>>pat=re.compile('[abc]+?')#非贪婪模式：+?>>>pat.match('abcdefabcabc').group()#匹配尽可能少的：a、b、c'a'>>>pat.search('dbbabcdefabcabc').group()'b'>>>pat.findall('abcdefabcabc')['a','b','c','a','b','c','a','b','c']1234567891011121314151617>>>pat=re.compile('[abc]+')        #贪婪模式>>>pat.match('abcdefabcabc').group()  #匹配尽可能多的：abc'abc'>>>pat.match('bbabcdefabcabc').group()'bbabc'>>>pat.search('dbbabcdefabcabc').group()'bbabc'>>>pat.findall('abcdefabcabc')['abc','abcabc'] >>>pat=re.compile('[abc]+?')        #非贪婪模式：+?>>>pat.match('abcdefabcabc').group()  #匹配尽可能少的：a、b、c'a'>>>pat.search('dbbabcdefabcabc').group()'b'>>>pat.findall('abcdefabcabc')['a','b','c','a','b','c','a','b','c']“{m}”：匹配字符串出现m次Python>>>pat=re.compile('[op]{2}')#o或p出现2次>>>pat.search('abcooapp').group()#匹配第一次出现的字符串,o比p先出现'oo'>>>pat.findall('abcooapp')#匹配出现的所有字符串，列表形式返回['oo','pp']12345>>>pat=re.compile('[op]{2}')    #o或p出现2次>>>pat.search('abcooapp').group()  #匹配第一次出现的字符串,o比p先出现'oo'>>>pat.findall('abcooapp')        #匹配出现的所有字符串，列表形式返回['oo','pp']“{m,n}”：匹配字符串出现m到n次Python>>>pat=re.compile('[op]{2,4}')#o或则p出现2到4次>>>pat.match('pppabcooapp').group()#匹配开头'ppp'>>>pat.search('pppabcooapp').group()#匹配第一次出现'ppp'>>>pat.findall('pppabcooapp')#匹配所有['ppp','oo','pp']1234567>>>pat=re.compile('[op]{2,4}')    #o或则p出现2到4次>>>pat.match('pppabcooapp').group()  #匹配开头'ppp'>>>pat.search('pppabcooapp').group()#匹配第一次出现'ppp'>>>pat.findall('pppabcooapp')        #匹配所有['ppp','oo','pp'].group()#匹配第一次出现边界“^”：匹配字符串开头或行头Python>>>pat=re.compile('^[abc]')#开头是a、b、c中的任意一个>>>pat.search('defabc').group()>>>pat.match('defabc').group()#均找不到>>>pat.findall('defabc')[]>>>pat.search('adefabc').group()'a'>>>pat.match('adefabc').group()#开头是a、b、c中的任意一个'a'>>>pat.findall('adefabc')['a']>>>pat=re.compile('^[abc]+')#开头是a、b、c中的任意一个的一次或则多次，贪婪：匹配多个>>>pat.findall('cbadefab')['cba']>>>pat=re.compile(r'^[abc]+?')#开头是a、b、c中的任意一个的一次或则多次，非贪婪：匹配一个>>>pat.findall('cbadefab')['c']12345678910111213141516171819>>>pat=re.compile('^[abc]')    #开头是a、b、c中的任意一个>>>pat.search('defabc').group()    >>>pat.match('defabc').group()    #均找不到>>>pat.findall('defabc')[] >>>pat.search('adefabc').group()'a'>>>pat.match('adefabc').group()  #开头是a、b、c中的任意一个'a'>>>pat.findall('adefabc')['a'] >>>pat=re.compile('^[abc]+')    #开头是a、b、c中的任意一个的一次或则多次，贪婪：匹配多个>>>pat.findall('cbadefab')['cba']>>>pat=re.compile(r'^[abc]+?')  #开头是a、b、c中的任意一个的一次或则多次，非贪婪：匹配一个>>>pat.findall('cbadefab')['c']“$”：匹配字符串结尾或则行尾Python>>>pat=re.compile('[abc]$')>>>pat.match('adefAbc').group()#match匹配的是字符串开头，所以查找$的时，总是返回None>>>pat.search('adefAbc').group()#结尾是a、b、c中的任意一个'c'>>>pat.findall('adefAbc')['c']>>>pat=re.compile('[abc]+$')>>>pat.search('adefAbc').group()#结尾是a、b、c中的任意一个的一次或则多次，贪婪：匹配多个'bc'>>>pat.findall('adefAbc')['bc']1234567891011>>>pat=re.compile('[abc]$')>>>pat.match('adefAbc').group()  #match匹配的是字符串开头，所以查找$的时，总是返回None>>>pat.search('adefAbc').group()  #结尾是a、b、c中的任意一个'c'>>>pat.findall('adefAbc')        ['c']>>>pat=re.compile('[abc]+$')>>>pat.search('adefAbc').group()  #结尾是a、b、c中的任意一个的一次或则多次，贪婪：匹配多个'bc'>>>pat.findall('adefAbc')['bc']“\\A”：匹配字符串开头Python>>>pat=re.compile('\\A[abc]+')>>>pat.findall('cbadefab')['cba']>>>pat.search('cbadefab').group()'cba'12345>>>pat=re.compile('\\A[abc]+')>>>pat.findall('cbadefab')['cba']>>>pat.search('cbadefab').group()'cba'“\\Z”：匹配字符串结尾Python>>>pat=re.compile('[abc]+\\Z')>>>pat.search('cbadefab').group()'ab'>>>pat.findall('cbadefab')['ab']12345>>>pat=re.compile('[abc]+\\Z')>>>pat.search('cbadefab').group()'ab'>>>pat.findall('cbadefab')['ab']分组(…)：分组匹配,从左到右,每遇到一个(编号+1，分组后面可加数量词Python>>>pat=re.compile(r'(a)\\w(c)')#\\w:单个的数字或字母[A-Za-z0-9]>>>pat.match('abcdef').group()'abc'>>>pat=re.compile('(a)b(c)')#分2组，匿名分组>>>pat.match('abcdef').group()#默认返回匹配的字符串'abc'>>>pat.match('abcdef').group(1)#取分组1，适用于search'a'>>>pat.match('abcdef').group(2)#取分组2，适用于search'c'>>>pat.match('abcdef').groups()#取所有分组，元组形式返回('a','c')12345678910111213>>>pat=re.compile(r'(a)\\w(c)')  #\\w:单个的数字或字母[A-Za-z0-9]>>>pat.match('abcdef').group()'abc'>>>pat=re.compile('(a)b(c)')    #分2组，匿名分组                                >>>pat.match('abcdef').group()  #默认返回匹配的字符串'abc'>>>pat.match('abcdef').group(1)#取分组1，适用于search'a'>>>pat.match('abcdef').group(2)#取分组2，适用于search'c'>>>pat.match('abcdef').groups()#取所有分组，元组形式返回('a','c')<number>：引用编号为<number>的分组匹配到的字符串Python>>>pat=re.compile(r'www\\.(.*)\\..{3}')>>>pat.match('www.dxy.com').group(1)'dxy'123>>>pat=re.compile(r'www\\.(.*)\\..{3}')>>>pat.match('www.dxy.com').group(1)'dxy'“(?P<name>…)” ：在模式里面用()来表示分组（命名分组）,适用于提取目标字符串中的某一些部位。Python>>>pat=re.compile(r'(?P<K>a)\\w(c)')#分2组：命名分组+匿名分组>>>pat.search('abcdef').groups()#取所有分组，元组形式返回('a','c')>>>pat.search('abcdef').group(1)#取分组1，适用于match'a'>>>pat.search('abcdef').group(2)#取分组2，适用于match'c'>>>pat.search('abcdef').group()#默认返回匹配的字符串'abc'>>>pat.search('abcdef').groupdict()#命名分组可以返回一个字典【专有】，匿名分组也没有{'K':'a'}1234567891011>>>pat=re.compile(r'(?P<K>a)\\w(c)')    #分2组：命名分组+匿名分组>>>pat.search('abcdef').groups()      #取所有分组，元组形式返回('a','c')>>>pat.search('abcdef').group(1)      #取分组1，适用于match'a'>>>pat.search('abcdef').group(2)      #取分组2，适用于match'c'>>>pat.search('abcdef').group()        #默认返回匹配的字符串'abc'>>>pat.search('abcdef').groupdict()    #命名分组可以返回一个字典【专有】，匿名分组也没有{'K':'a'}“(?P=name)”：引用别名为<name>的分组匹配到的串Python>>>pat=re.compile(r'(?P<K>a)\\w(c)(?P=K)')#(?P=K)引用分组1的值，就是a>>>pat.search('abcdef').group()#匹配不到，因为完整'a\\wca',模式的第4位是aTraceback(mostrecentcalllast):File\"<stdin>\",line1,in<module>AttributeError:'NoneType'objecthasnoattribute'group'>>>pat.search('abcadef').group()#匹配到，模式的第4位和组1一样,值是c'abca'>>>pat.search('abcadef').groups()('a','c')>>>pat.search('abcadef').group(1)'a'>>>pat.search('abcadef').group(2)'c1234567891011121314>>>pat=re.compile(r'(?P<K>a)\\w(c)(?P=K)')    #(?P=K)引用分组1的值，就是a>>>pat.search('abcdef').group()              #匹配不到，因为完整'a\\wca',模式的第4位是aTraceback(mostrecentcalllast):  File\"<stdin>\",line1,in<module>AttributeError:'NoneType'objecthasnoattribute'group' >>>pat.search('abcadef').group()            #匹配到，模式的第4位和组1一样,值是c'abca'>>>pat.search('abcadef').groups()('a','c')>>>pat.search('abcadef').group(1)'a'>>>pat.search('abcadef').group(2)'c“<number>”：引用分组编号匹配：Python>>>pat=re.compile(r'(?P<K>a)\\w(c)(?P=K)\\2')#\\2引用分组2的值，就是c>>>pat.findall('Aabcadef')#匹配不到，因为完整'a\\wcac',模式的第5位是c[]>>>pat.findall('Aabcacdef')#匹配到，模式的第5位和组2一样,值是c[('a','c')]>>>pat.search('Aabcacdef').groups()('a','c')>>>pat.search('Aabcacdef').group()'abcac'>>>pat.search('Aabcacdef').group(1)'a'>>>pat.search('Aabcacdef').group(2)'c'12345678910111213>>>pat=re.compile(r'(?P<K>a)\\w(c)(?P=K)\\2')  #\\2引用分组2的值，就是c>>>pat.findall('Aabcadef')                  #匹配不到，因为完整'a\\wcac',模式的第5位是c[]>>>pat.findall('Aabcacdef')                  #匹配到，模式的第5位和组2一样,值是c[('a','c')]>>>pat.search('Aabcacdef').groups()('a','c')>>>pat.search('Aabcacdef').group()'abcac'>>>pat.search('Aabcacdef').group(1)'a'>>>pat.search('Aabcacdef').group(2)'c'特殊构造(?:…)(…)不分组版本,用于使用|或者后接数量词(?iLmsux)iLmsux的每个字符代表一个匹配模式,只能用在正则表达式的开头,可选多个(?#…)#号后的内容将作为注释(?=…)之后的字符串内容需要匹配表达式才能成功匹配(?!…)之后的字符串不匹配表达式才能成功(?(?(?(id/name)yes|no)如果编号为id/名字为name的组匹配到字符串,则需要匹配yes,否则匹配no,no可以省略“(?:…)”：()里面有?:表示该()不是分组Python>>>pat=re.compile(r'a(?:bc)')>>>pat.findall('abc')['abc']>>>pat.match('abc').groups()#显示不出分组1234>>>pat=re.compile(r'a(?:bc)')>>>pat.findall('abc')['abc']>>>pat.match('abc').groups()      #显示不出分组“(?=…)”：匹配…表达式，返回。对后进行匹配，总是对后面进行匹配Python>>>pat=re.compile(r'\\w(?=\\d)')#匹配表达式\\d，返回数字的前一位，\\w：单词字符[A-Za-z0-9]>>>pat.findall('abc1def1xyz1')['c','f','z']>>>pat.findall('zhoujy20130628hangzhou')#匹配数字的前一位，列表返回['y','2','0','1','3','0','6','2']>>>pat=re.compile(r'\\w+(?=\\d)')>>>pat.findall('abc1,def1,xyz1')#匹配最末数字的前字符串，列表返回['abc','def','xyz']>>>pat.findall('abc21,def31,xyz41')['abc2','def3','xyz4']>>>pat.findall('zhoujy20130628hangzhou')['zhoujy2013062']>>>pat=re.compile(r'[A-Za-z]+(?=\\d)')#[A-Za-z],匹配字母,可以用其他的正则方法>>>pat.findall('zhoujy20130628hangzhou123')#匹配后面带有数字的字符串，列表返回['zhoujy','hangzhou']>>>pat.findall('abc21,def31,xyz41')['abc','def','xyz']1234567891011121314151617>>>pat=re.compile(r'\\w(?=\\d)')    #匹配表达式\\d，返回数字的前一位，\\w：单词字符[A-Za-z0-9]>>>pat.findall('abc1def1xyz1')['c','f','z']>>>pat.findall('zhoujy20130628hangzhou')  #匹配数字的前一位，列表返回['y','2','0','1','3','0','6','2']>>>pat=re.compile(r'\\w+(?=\\d)')>>>pat.findall('abc1,def1,xyz1')          #匹配最末数字的前字符串，列表返回['abc','def','xyz']>>>pat.findall('abc21,def31,xyz41')['abc2','def3','xyz4']>>>pat.findall('zhoujy20130628hangzhou')['zhoujy2013062']>>>pat=re.compile(r'[A-Za-z]+(?=\\d)')      #[A-Za-z],匹配字母,可以用其他的正则方法>>>pat.findall('zhoujy20130628hangzhou123')#匹配后面带有数字的字符串，列表返回['zhoujy','hangzhou']>>>pat.findall('abc21,def31,xyz41')['abc','def','xyz']“(?!…)”不匹配…表达式，返回。对后进行匹配Python>>>pat=re.compile(r'[A-Za-z]+(?!\\d)')#[A-Za-z],匹配字母,可以用其他的正则方法>>>pat.findall('zhoujy20130628hangzhou123,12,binjiang310')#匹配后面不是数字的字符串，列表返回['zhouj','hangzho','binjian']>>>pat.findall('abc21,def31,xyz41')['ab','de','xy']12345>>>pat=re.compile(r'[A-Za-z]+(?!\\d)')      #[A-Za-z],匹配字母,可以用其他的正则方法>>>pat.findall('zhoujy20130628hangzhou123,12,binjiang310')  #匹配后面不是数字的字符串，列表返回['zhouj','hangzho','binjian']>>>pat.findall('abc21,def31,xyz41')['ab','de','xy']“(?<=…)”：匹配…表达式，返回。对前进行匹配,总是对前面进行匹配Python>>>pat=re.compile(r'(?<=\\d)[A-Za-z]+')#匹配前面是数字的字母>>>pat.findall('abc21,def31,xyz41')[]>>>pat.findall('1abc21,2def31,3xyz41')['abc','def','xyz']>>>pat.findall('zhoujy20130628hangzhou123,12,binjiang310')['hangzhou']1234567>>>pat=re.compile(r'(?<=\\d)[A-Za-z]+')      #匹配前面是数字的字母>>>pat.findall('abc21,def31,xyz41')[]>>>pat.findall('1abc21,2def31,3xyz41')['abc','def','xyz']>>>pat.findall('zhoujy20130628hangzhou123,12,binjiang310')['hangzhou']“(?<!…)”：不匹配…表达式，返回。对前进行匹配,总是对前面进行匹配Python>>>pat=re.compile(r'(?<!\\d)[A-Za-z]+')#匹配前面不是数字的字母>>>pat.findall('abc21,def31,xyz41')['abc','def','xyz']>>>pat.findall('zhoujy20130628hangzhou123,12,binjiang310')['zhoujy','angzhou','binjiang']12345>>>pat=re.compile(r'(?<!\\d)[A-Za-z]+')      #匹配前面不是数字的字母>>>pat.findall('abc21,def31,xyz41')['abc','def','xyz']>>>pat.findall('zhoujy20130628hangzhou123,12,binjiang310')['zhoujy','angzhou','binjiang']“(?(id/name)yes|no)”:组是否匹配，匹配返回Python>>>pat=re.compile(r'a(\\d)?bc(?(1)\\d)')#no省略了，完整的是a\\dbc\\d==>a2bc3,总共5位，第2位是可有可无的数字，第5为是数字>>>pat.findall('abc9')#返回组1，但第2位（组1）没有，即返回了''['']>>>pat.findall('a8bc9')#完整的模式，返回组1['8']>>>pat.match('a8bc9').group()'a8bc9'>>>pat.match('a8bc9').group(1)'8'>>>pat.findall('a8bc')#第5位不存在，则没有匹配到[]1234567891011>>>pat=re.compile(r'a(\\d)?bc(?(1)\\d)')  #no省略了，完整的是a\\dbc\\d==>a2bc3,总共5位，第2位是可有可无的数字，第5为是数字>>>pat.findall('abc9')                  #返回组1，但第2位（组1）没有，即返回了''['']>>>pat.findall('a8bc9')                  #完整的模式，返回组1['8']>>>pat.match('a8bc9').group()'a8bc9'>>>pat.match('a8bc9').group(1)'8'>>>pat.findall('a8bc')                  #第5位不存在，则没有匹配到[]“(?iLmsux)”:这里就介绍下i参数：大小写区分匹配Python>>>pat=re.compile(r'abc')>>>pat.findall('abc')['abc']>>>pat.findall('ABC')[]>>>pat=re.compile(r'(?i)abc')#(?i)不区分大小写>>>pat.findall('ABC')['ABC']>>>pat.findall('abc')['abc']>>>pat.findall('aBc')['aBc']>>>pat.findall('aBC')['aBC']>>>pat=re.compile(r'abc',re.I)#re.I作为参数使用，推荐>>>pat.findall('aBC')['aBC']>>>pat.findall('abc')['abc']>>>pat.findall('ABC')['ABC']123456789101112131415161718192021>>>pat=re.compile(r'abc')>>>pat.findall('abc')['abc']>>>pat.findall('ABC')[]>>>pat=re.compile(r'(?i)abc')            #(?i)不区分大小写>>>pat.findall('ABC')['ABC']>>>pat.findall('abc')['abc']>>>pat.findall('aBc')['aBc']>>>pat.findall('aBC')['aBC']>>>pat=re.compile(r'abc',re.I)          #re.I作为参数使用，推荐>>>pat.findall('aBC')['aBC']>>>pat.findall('abc')['abc']>>>pat.findall('ABC')['ABC']http://www.cnblogs.com/huxi/a…2赞14收藏5评论"], "art_create_time": ["2017/10/23"], "art_title": ["Python Re模块"], "art_url": ["http://python.jobbole.com/88729/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2014/12/6da94dec8f6f96417f14c8291e6345801.png"]},
{"art_content": ["原文出处：AngelaJuvetBranæs   译文出处：疯狂的技术宅   如果你遵循前面的教程中的步骤，现在应该有了一个可以工作的全栈Web应用程序框架。点击直达前文>>【译】一个小时搭建一个全栈Web应用框架(上)如果没有，但还是要继续学习本教程，可以到我的GitHub页面下载代码。对于下一个魔术，我们将把一个显示“HelloWorld！”的简单静态页面转换成一个漂亮的单页面WEB应用。这个页面能够与后端通信，并且在收到新信息时立即更新，而无需用户刷新页面。我们即将创建的页面，在每次点击按钮时，会以随机的欧洲语言返回“Hello”。这是我们的页面：为了能够实现这一点，我们需要先弄清楚以下的问题：每次调用/hello端点时，如何返回一个随机的欧洲语言“Hello”。如何从服务器请求信息。如何将返回的信息无缝显示给用户，从而无需刷新页面。如何在页面上添加样式，可以在页面上创建一个大的居中的按钮并添加文本。最后，我们必须搞清楚应该如何添加一个背景图像。从服务器返回随机语言的“Hello”每当我们与服务器上的/hello端点进行通话时，为了能够请求一个随机的欧洲语言“Hello”，必须更改server/server.py文件中的功能。每次调用它时，都不会返回静态的“HelloWorld”，而是从“Hello”列表中选择一个随机语言的“Hello”。为了实现这个功能，需要进行以下更改：Pythonimportrandomdefget_hello():greeting_list=[‘Ciao’,‘Hei’,‘Salut’,‘Hola’,‘Hallo’,‘Hej’]returnrandom.choice(greeting_list)1234importrandomdefget_hello():  greeting_list=[‘Ciao’,‘Hei’,‘Salut’,‘Hola’,‘Hallo’,‘Hej’]  returnrandom.choice(greeting_list)这个函数定义了一个欧洲语言的“hello”列表，然后我们调用这个函数时，使用random.choice()从列表中随机选择一个项目。修改hello()函数，以便在每次调用它时返回get_hello()。Python@app.route(\"/hello\")defhello():returnget_hello()123@app.route(\"/hello\")defhello():  returnget_hello()修改/hello以返回我们感兴趣的信息，我们现在需要弄清楚如何从前端得到这些信息。组件化的重要性–在React中创建一个Hello类把问题分解被认为是良好的编程习惯。只要有可能，你应该尽量使自己的函数只做一件事情，并且做好。这点同样适用于类。你可以考虑将每个函数或类都作为单独的组件。React是为组件化而设计的。这意味着它是用多个较小的部分来构建你的网站的。就像玩乐高一样，可以轻松地将一个组件替换成另外一个，也可以复用组件，这也能帮助其他开发人员了解你的代码。我们应该努力的编写可理解的代码，因为这样可以使我们的程序更容易维护和扩展。考虑到组件化，我们创建一个Hello类来处理我们网页上的问候语。该类将从服务器上的/hello端点点获取一个“Hello”，并将其显示给用户。它也应该有一个“name”参数，这样就可以向某个具体的人进行问候。通过更改ReactApp类中的render函数，使其调用Hello类，我们就可以很快的完成功能，而不是使用旧的代码。传递名称“Rimini”作为参数。出于结构化的目的，我们将把Hello类放在一个PageHeader中。创建Hello类前面我们修改了App类使其能够调用hello类,接下来需要创建Hello类。在js/目录下创建一个名为hello.jsx的文件,在此文件中定义一个名为hello的类。PythonexportdefaultclassHelloextendsReact.Component{constructor(props){super(props);this.state={greeting:'Hello'+this.props.name};//Thisbindingisnecessarytomake`this`workinthecallbackthis.getPythonHello=this.getPythonHello.bind(this);}}12345678exportdefaultclassHelloextendsReact.Component{  constructor(props){    super(props);    this.state={greeting:'Hello'+this.props.name};    //Thisbindingisnecessarytomake`this`workinthecallback        this.getPythonHello=this.getPythonHello.bind(this);  }}组件与道具在这一点上，你可能对构造函数中发生了什么有很多疑问。在React中有一种叫做组件和道具的东西。Props是创建时传递给构造函数的不可变参数。道具是公开的，修改他们将违反React的基本使用原则。状态是内部的，可变的。每次更新状态时，都会在UI中重新展现。如果希望更深入地了解其运作方式，我强烈建议你阅读React文档中有关生命周期和状态的部分。我们将在Hello类中添加一个名为personaliseGreeting()的函数。当我们点击按钮获得一个新的问候语时，它将会处理网页上的问候语的更新操作。请注意，我们使用this.setState()与名为“greeting”的key。你必须使用这个语法才能让React自动刷新网页上的“greeting”状态。PythonpersonaliseGreeting(greeting){this.setState({greeting:greeting+''+this.props.name+'!'});}123personaliseGreeting(greeting){  this.setState({greeting:greeting+''+this.props.name+'!'});}渲染问候语为了让问候语出现在页面上，必须在render函数中调用“{this.state.greeting}”。我们还必须添加一个带有回调函数的按钮，这个函数叫做getPythonHello()，我们很快就会实现它。这个函数在调用使用Python编写的后端时，将会得到一个新的“Hello”。Pythonrender(){return(<h1>{this.state.greeting}</h1><hr/><ButtonbsSize=\"large\"bsStyle=\"danger\"onClick={this.getPythonHello}>SayHello!</Button>)}123456789render(){  return(    <h1>{this.state.greeting}</h1>    <hr/>    <ButtonbsSize=\"large\"bsStyle=\"danger\"onClick={this.getPythonHello}>      SayHello!    </Button>  )}请注意，我已经将标题和按钮HTML内嵌到了我的代码中，所以可以很轻松地控制他们在页面上的最终位置。绑定“this”因为JavaScript中的类方法没有做默认绑定，所以当我们想在函数回调中使用“this”时，就必须在构造函数中创建一个绑定。否则“this”将会是undefined的。这适用于在JavaScript中调用without()的情况。一个典型的例子就是render()中的“onClick={this.getPythonHello}”。从服务器请求信息React没有提供执行HTTP请求的内置方式。为了能够从服务器请求信息，我们将不得不找一个可以做这件事的库。一个最简单的方法就是引入jQuery库。jQuery是一个javascript库，通过在$符号后面提供缩写函数来简化标准的JavaScript功能。首先安装jQuery依赖关系：Python$npmijquery--save-dev1$npmijquery--save-dev将jQuery依赖添加到要使用的React文件中，也就是Hello.jsx中。应该将此依赖添加到Hello类的定义前面。Pythonvar$=require(‘jquery’);1var$=require(‘jquery’);将查询依赖添加到React文件中意味着可以在自己的React代码中使用标准的jQuery函数，只要它们以我们刚刚定义的“$”变量开始。下面让我们用它来从服务器获取一个“Hello”。我们将使用HTTP协议的GET请求获取信息。GET实质上是HTTP请求的“只读”模式。可以用来获取信息，但是不能要求服务器更改它。在hello.jsx文件中的Hello类中添加以下函数:PythongetPythonHello(){$.get(window.location.href+'hello',(data)=>{console.log(data);this.personaliseGreeting(data);});}123456getPythonHello(){  $.get(window.location.href+'hello',(data)=>{      console.log(data);    this.personaliseGreeting(data);  });}此函数通过jQuery的GET请求，连接/hello端点。然后得到从服务器返回的一个欧洲语言的“hello”信息，再它打印到浏览器的开发控制台，最后将它传递给Hello类中的另一个函数，调用personaliseGreeting()。当rebuild前端代码（npmrunwatch），并重新启动python服务器后。应该能看到以下内容： 有一行问候语，一个按钮，点击按钮可以更改问候语。这个页面看起来很不错，因为我们在index.html中包含了Bootstrap样式。使用CSS样式美化页面我们终于有了一个能够与用户交互的产品。如果我们愿意，就可以到此为止了，并对自己说：我对这些成就感到满意。不过就我个人而言，我更喜欢在我的Web应用中添加一些设计元素，让他们变得更加漂亮。所以，我们将用CSS使标题能够覆盖整个屏幕，而不是在页面的顶部。CSS是为HTML设计的一种样式语言，它的作用相当于在Word文档中更改字体大小，样式和位置。使Webpack能够处理CSS为了能够在我们的WEB应用中使用CSS，必须安装一些加载器和插件，并将它们添加到Webpack配置文件中。这是因为Webpack默认只能处理JavaScript。安装下列插件：css-loaderstyle-loaderextract-text-webpack-plugincss-loader和style-loader能够使Webpack处理CSS。通过添加这些加载器，Webpack将能够将我们需要的任何CSS绑定到bundle.js中。不过在这里存在一个问题，JavaScript和CSS将不会在你的页面上单独进行加载，这可能导致UI组件在JavaScript加载之前无法显示。这点很差劲，因为在糟糕的网络上，我们辛辛苦苦设计出来的页面可能会加载的非常缓慢。不过可以通过添加extract-text-webpack-plugin来解决这个问题。这个插件能够将CSS分解成一个单独的包，我们可以把它附加到HTML上。这样就可以使CSS再次独立于JavaScript进行加载。在你的webpack.config.js文件的modules.rules部分添加下面的CSS规则：Python{test:/\\.css$/,use:ExtractTextPlugin.extract({fallback:'style-loader',use:'css-loader',})},1234567{  test:/\\.css$/,  use:ExtractTextPlugin.extract({        fallback:'style-loader',        use:'css-loader',      })},将ExtractTextPlugin插件添加到webpack.config.js（如果您感到困惑，请查看我的webpack文件）。注意，在创建时，需要将捆绑的CSS文件名传给此插件。我们将调用文件’styles.css’。Pythonplugins:[newExtractTextPlugin('styles.css')]1plugins:[newExtractTextPlugin('styles.css')]最后，我们需要将styles.css包添加到index.html中，以确保样式被加载。将以下行添加到你的index.html文件中的head部分（可以参考我的代码）：<linkrel=\"stylesheet\"href=\"dist/styles.css\">1<linkrel=\"stylesheet\"href=\"dist/styles.css\">添加CSS规则现在可以确保我们的设置可以正确处理CSS了，我们将在css文件夹中创建一个名为fullstack.css的文件。我已经添加了几个不同的规则，以确保文本和按钮出现在正确的位置，并且文本是大号的细体。这是我的fullstack.css文件中的一个规则。它使“HelloRimini”文本变得越来越细：Python.header-contentsh1{font-size:120px;font-weight:300;}1234.header-contentsh1{    font-size:120px;    font-weight:300;}在创建fullstack.css文件之后，我们需要将它添加到使用规则的React组件中，这样它们才能生效。由于标题在App.jsx中定义，所以需要添加以下代码：Pythonrequire('../css/fullstack.css');1require('../css/fullstack.css');fullstack.css文件中的标题样式现在将由Webpack拾取，并绑定到styles.css文件中。当我们刷新页面时，应该如下图所示。注意，如果你用的浏览器不是Chrome的话，字体可能和图中不一样： 被CSS装饰后的页面完成–添加背景图Webpack本身并不理解图像的概念。因此，我们还需要添加一个可以在Web应用程序中使用它们的加载程序。我们需要安装名为“file-loader”的loader。安装file-loader：Python$npmifile-loader--save-dev1$npmifile-loader--save-dev将file-loader规则添加到webpack.config.js文件中的modules.rules部分：Python{test:/\\.(png|svg|jpg|gif)$/,use:'file-loader'}1234{  test:/\\.(png|svg|jpg|gif)$/,  use:'file-loader'}将要使用的图像添加到images/folder。将其命名为“header.jpg”。为了能够使图像成为网页头部的背景，我们需要将其作为背景图像添加到fullstack.css文件的页眉部分。Python.page-header{background-image:url('../images/header.jpg');background-repeat:no-repeat;background-position:center;background-size:cover;}123456.page-header{  background-image:url('../images/header.jpg');  background-repeat:no-repeat;  background-position:center;  background-size:cover;}接下来要做的是把图像加载到使用它的React文件中。如果没有在React中显式加载图片，Webpack将不会棒的它，也不会把它显示在页面上。这种行为不是很直观，在我第一次在自己的应用中添加一个背景图像时，曾经犯过这个错误。在App.jsx中进行如下更改：PythonimportHeaderBackgroundImagefrom'../images/header.jpg';1importHeaderBackgroundImagefrom'../images/header.jpg';将此函数添加到你的App类：PythonaddHeaderImg(){letheaderBg=newImage();headerBg.src=HeaderBackgroundImage;}1234addHeaderImg(){  letheaderBg=newImage();  headerBg.src=HeaderBackgroundImage;}这个函数创建了一个新的Image对象，并将源设置为你的标题图片。我们需要做的最后一件事是，确保在渲染页面时加载图像。这意味着我们必须在render()函数中调用addHeaderImg()函数。将下列代码添加到render()函数中：Python{this.addHeaderImg()}1{this.addHeaderImg()}刷新浏览器窗口时，应该看到以下内容：恭喜你！你已成功创建了一个全栈Web应用程序！2赞9收藏评论"], "art_create_time": ["2017/09/05"], "art_title": ["一个小时搭建一个全栈 Web 应用框架（下）——美化与功能"], "art_url": ["http://python.jobbole.com/88498/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2017/09/0c2f5fb2cc343b0a16533a044fa6e8e4.png"]},
{"art_content": ["本文由伯乐在线-刘志军翻译。未经许可，禁止转载！英文出处：GlenStansberry。欢迎加入翻译组。前言：随着Django1.4第二个候选版的发布，虽然还不支持Python3，但Django团队已经在着手计划中，据官方博客所说，Django1.5将会试验性的支持python3。Django作为一个杰出的Python开源框架，或许得不到和其它流行框架如Rails这样多的赞美，但是它和其他框架一样精炼，非常注重DRY(Don’tRepeatYoursef)原则、组件的重用性,通过自动化过程使编码更简洁。如果在Django项目中能够灵活使用某些方法和技巧的话，它将大大加快软件开发的速度同时避免很多头疼的事。作者在下面列举了几点，这些方法由浅入深，可以帮助任何级别的程序员更加熟练的使用Django。0、 在配置中使用相对路径某些原因使得项目可能常常会被来回的迁移。如果没有事先规划好这种可能性的话这绝对是一个棘手的问题。RobHudson有一个极好的技巧能够确保你的Django项目在部署过程中能够轻松的来回迁移。仅仅只要编写几行代码在你的配置文件(settings.py)中。PythonimportosBASE_DIR=os.path.dirname(os.path.abspath(__file__))TEMPLATE_DIRS=(BASE_DIR+'/templates',)123456importosBASE_DIR=os.path.dirname(os.path.abspath(__file__)) TEMPLATE_DIRS=(    BASE_DIR+'/templates',) 1、 使用{%url%}标签尽可能使用向后兼容的{%url%}标签来替换硬编码形式的href，与使用绝对路径的url(当然最好不要这样做)一样达到相同的效果。你的Django项目迁移起来，那些链接也不会有影响。（译者注：比如说我们有一个views.about函数指向about页面r’^about/$’，就可以{%urlviews.aboutasabout_url%}然后用{{about_url}}这个变量来代替绝对URL地址）尽管它还不是最高级的技巧，但是它确实值得你应用于Django项目中。 Photoby Cloudzilla. 2、 尝试把Djangoadmin应用到PHP项目中Django最伟大的特性之一就是已经成为Django的核心功能的用户验证系统。它易安装，主要用于用户认证和其它一些必要的配置。这个酷毙了的用户系统甚至被建议应用到你的PHP项目中去，这里有一边JeffCroft关于为什么Django能够作为任何语言任何应用中的系统管理模块的一个很好的解决方案。 3、 使用独立的媒体服务器在开发环境中把静态文件放在与Django项目所在的同一台服务器中问题并不大，但是却不要使用在生产环境中，为什么？效率问题。Jacobian.org给出了一个合理的解释。通过一台独立的服务器来处理静态文件，性能将得到有效的提升，如果不想买服务器的话，那么使用AmazonS3相对来更便宜。 4、 使用Debugger工具条调试工具对任何一种语言来说都是不可或缺的.他们能够加快开发的速度,指出潜在的缺陷. RobHudson开发了一个对开发人员非常有用django调试工具。 5、 使用Django单元测试利用单元测试确保你代码的改变和预期的一样，而不会破坏任何老的代码，以便向后兼容。Django一个强大的特性就是他能极其简单地写单元测试。Django也可直接使用python的文本测试和单元测试。Django的文档提供了一个详细的教程和样例代码关于怎样做单元测试使得代码正确地运行，以及去除讨厌的bug 6、 使用速查卡这里有两页厚的速查卡，在Django文档中你可能翻来覆去要找半天的东西在这里一目了然。它包含如下几个主题模板：模板标签及可选项模板过滤器及可选项日期格式化语法快速查阅模型：域和及选项常用域的可选项元类型可选项模型管理可选项表单：域和可选项常用域可选项标准错误消息键值 7、使用Django-chunks除了使用Django的富文本编辑器创建块更容易之外，Django-chunks同样是用于模板中，这是重用代码块的必不可少的工具。 8、使用Memcache如果性能在你的Django项目中已经成为一个棘手的问题，那么你将需要使用一些缓存策略。然而Django为缓存提供很多的选择。目前最好的无疑是Memcache,用Django安装memcache非常地简单，如果你使用cmemcache模块的时候。只要模块安装完成后，你仅仅修改一行配置项，你的Django页面变得轻快起来。 9、使用Django，心动不如行动在你阅读完这篇文章后如果你仍然不完全理解Django的强大，在你的下一个项目中使用Django的一个合符情理的理由就是：它能够节省各种不同软件设计的时间。JeffCroft解释为什么用Django创建一个项目比你自己设计出来的更高效。Django允许你扩展自己的Web站点，不需要担心设计或者代码以及数据库的兼容性，它会工作地很棒。打赏支持我翻译更多好文章，谢谢！打赏译者打赏支持我翻译更多好文章，谢谢！1赞1收藏评论关于作者：刘志军个人博客：http://foofish.net，微信公众号：Python之禅（ID：VTtalk）个人主页·我的文章·45·"], "art_create_time": ["2013/03/29"], "art_title": ["10个实用的Django建议"], "art_url": ["http://python.jobbole.com/15555/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2012/02/python-logo.png"]},
{"art_content": ["本文由伯乐在线-伯乐翻译。未经许可，禁止转载！英文出处：sangkrit。欢迎加入翻译组。Python是一种面向对象、直译式计算机编程语言，具有近二十年的发展历史，成熟且稳定。它包含了一组完善而且容易理解的标准库，能够轻松完成很多常见的任务。它的语法简捷和清晰，尽量使用无异义的英语单词，与其它大多数程序设计语言使用大括号不一样，它使用缩进来定义语句块。Python可以和C/C++语言整合在一起，也能支持命令式程序设计、面向对象程序设计、函数式编程、面向侧面程序设计、泛型编程多种编程范式。（摘自维基百科Python词条）Python的一些重要特性简单：Python是一种代表简单主义思想的语言。阅读一个良好的Python程序就感觉像是在读英语一样。它使你能够专注于解决问题而不是去搞明白语言本身。易学：Python极其容易上手，因为Python有极其简单的说明文档。速度快：Python的底层是用C语言写的，很多标准库和第三方库也都是用C写的，运行速度非常快。免费、开源：Python是FLOSS（自由/开放源码软件）之一。使用者可以自由地发布这个软件的拷贝、阅读它的源代码、对它做改动、把它的一部分用于新的自由软件中。FLOSS是基于一个团体分享知识的概念。高层语言：用Python语言编写程序的时候无需考虑诸如如何管理你的程序使用的内存一类的底层细节。可移植性：由于它的开源本质，Python已经被移植在许多平台上（经过改动使它能够工作在不同平台上）。这些平台包括Linux、Windows、FreeBSD、Macintosh、Solaris、OS/2、Amiga、AROS、AS/400、BeOS、OS/390、z/OS、PalmOS、QNX、VMS、Psion、AcomRISCOS、VxWorks、PlayStation、SharpZaurus、WindowsCE、PocketPC、Symbian以及Google基于linux开发的android平台。解释性：一个用编译性语言比如C或C++写的程序可以从源文件（即C或C++语言）转换到一个你的计算机使用的语言（二进制代码，即0和1）。这个过程通过编译器和不同的标记、选项完成。运行程序的时候，连接/转载器软件把你的程序从硬盘复制到内存中并且运行。而Python语言写的程序不需要编译成二进制代码。你可以直接从源代码运行程序。在计算机内部，Python解释器把源代码转换成称为字节码的中间形式，然后再把它翻译成计算机使用的机器语言并运行。这使得使用Python更加简单。也使得Python程序更加易于移植。面向对象：Python既支持面向过程的编程也支持面向对象的编程。在“面向过程”的语言中，程序是由过程或仅仅是可重用代码的函数构建起来的。在“面向对象”的语言中，程序是由数据和功能组合而成的对象构建起来的。可扩展性：如果需要一段关键代码运行得更快或者希望某些算法不公开，可以部分程序用C或C++编写，然后在Python程序中使用它们。可嵌入性：可以把Python嵌入C/C++程序，从而向程序用户提供脚本功能。丰富的库：Python标准库确实很庞大。它可以帮助处理各种工作，包括正则表达式、文档生成、单元测试、线程、数据库、网页浏览器、CGI、FTP、电子邮件、XML、XML-RPC、HTML、WAV文件、密码系统、GUI（图形用户界面）、Tk和其他与系统有关的操作。这被称作Python的“功能齐全”理念。除了标准库以外，还有许多其他高质量的库，如wxPython、Twisted和Python图像库等等。（摘自百度百科Python词条）下面是sangkrit收集整理的25本免费的Python电子书。如果你是Python新手，并且不知该先看哪本，sangkrit是建议从第12本开始。对于Python新手应该从哪本开始，如果各位朋友有不同看法，欢迎在评论中留言。ThinkStatsDiveIntoPythonAByteOfPythonThinkComplexityDiveIntoPython3DJANGOTUTORIALBuildingSkillsInOOPPyramidForHumansFlaskMicroframeworkBuildingSkillsInPythonKivyProgrammingGuideSnakeWranglingForKidsAnIntroductionToPythonProgrammezAvecPython2ProgrammezAvecPython 3PythonModuleOfTheWeekLearnPythonTheHardWayTheStandardPythonLibraryBuildingSkillsInProgrammingPythonScientificLectureNotesMakingGamesWithPython&PygamePython101(anintroductiontopython)HowToThinkLikeAComputerScientistNaturalLanguageProcessingWithPythonProgrammingComputerVisionWithPython1赞14收藏6评论关于作者：伯乐简介还没来得及写:）个人主页·我的文章·4"], "art_create_time": ["2013/06/21"], "art_title": ["25本免费的Python电子书"], "art_url": ["http://python.jobbole.com/29281/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2011/11/book-logo.jpg"]},
{"art_content": ["本文由伯乐在线-人见人爱的土豆翻译。未经许可，禁止转载！英文出处：ebookglue。欢迎加入翻译组。你有没有曾经好奇过Chrome浏览器是如何知道一个网页的语言，并对外国文字的网页提供翻译服务的？或者，Facebook是如何翻译你朋友用写在你主页上的外国文字？检测一种语言实际上非常简单，改进了用户体验，而且不需要用户做任何的事情。我无意中发现的 ActiveStaterecipeforalanguagedetectorinPython这是非常不错的一段程序，但是我决定做点小小的改进。提供一些背景知识给那些不熟悉自然语言处理或者是程序语言学的人。如果你是有经验的程序员，你也许可以直接跳到这段文字最下端的程序部分。出奇的简单。你需要熟悉Python语法。如果你从来没有用过python，我建议你读一下 ZedShaw的《 LearnPythontheHardWay》。确定你下载并安装了python，而且可以正常运行程序。这段文字中的python不算很长，所以你可以用任何文本编辑器从而省去安装任何软件的麻烦。（译者注：在线的python编译器也可以运行这段程序，其中一种编译器在jobbole的另一篇文章中提到过，http://blog.jobbole.com/53346/）。第一部分，什么检测到了一种语言？在你写区分语言的程序之前，你需要回答一个问题：什么区别了两种语言？有趣的是，这个问题的答案会根据不同的比较语言而有所不同。比如：女性が牛乳を飲んだ。(译者注：日语：女性喝牛奶。)你是怎么知道这句话不是英文的？你也许不熟悉日文，但是你肯定知道这些字符不是英文，你甚至不需要知道具体哪个字符不存在于英文字母中。Lafemmeboitdulait.  (译者注：法语：女性喝牛奶。)你怎么知道这句话不是英文的？有一点麻烦。每个字母都在英文中。甚至每一个字母和句型结构都和英文的同一个意思的那句话很相似——“Thewomandrankmilk.”(译者注：英语：女性喝牛奶。)。你的大脑用了另一个特性去判断这个：尽管字母很相似，这两句话发音没有任何相似之处。还有很多更复杂的方式去检测两种不同的语言（例如，语法、句法等等）上面提到的两个特性似乎足够用来区分很多的书写文字。提问：你可以想到一个相反的例子嘛？两种不能用字符或者发音而区分的语言？（译者注：这是我想到的，和编者没有任何关系。Hindi和Nepali的区分度极低，印度的一种语言和尼泊尔的官方语言的区别度非常低，字符区别很低而发音更高达50％的相似度。当然，他们两个是同一语系的语种。）第二部分，如何用计算机检测到这些特性？第一个特性已经存在于任何一台现代化的机器里——characterencodings 字符解码允许任何一台计算机去通过二进制码而呈现每一个字符。我们要用unicode在Python的程序中。第二个特征更有意思。如何能让一台电脑检测到字符串的发音呢？答案比想象的简单点：字符串顺序是按照声音解码的！他们有直接的稳定的对应关系－语言改变的非常缓慢。因此，你可以用下面的两个特性去检测一行文本语言：单个字符的重复性字符串的重复性实际上，这两个特性浓缩到了一个特性中：字符串的顺序。单个字符的重复性只是字符串的重复性。快速知识补充：在计算机语言学中，字符串的长度n被定义为n-gram。 “a”是一个gram,1-gram.“bc”是两个gram,2-gramorbigram。“def”是三个gram,3-gram或者trigram，以此类推。第三部分，用python实现吧！首先，我们需要计算某个字符串在特定文本中出现的次数。为了封装结果，我们将建立一个NGram类。PythonclassNGram(object):def__init__(self,text,n=3):self.length=Noneself.n=nself.table={}self.parse_text(text)defparse_text(self,text):chars=''*self.n#initialsequenceofspaceswithlengthnforletterin(\"\".join(text.split())+\"\"):chars=chars[1:]+letter#appendlettertosequenceoflengthnself.table[chars]=self.table.get(chars,0)+1#incrementcount12345678910111213classNGram(object):def__init__(self,text,n=3):self.length=Noneself.n=nself.table={}self.parse_text(text) defparse_text(self,text):chars=''*self.n#initialsequenceofspaceswithlengthn forletterin(\"\".join(text.split())+\"\"):chars=chars[1:]+letter#appendlettertosequenceoflengthnself.table[chars]=self.table.get(chars,0)+1#incrementcount代码实际上很短，定义了一个NGram类去接受一个unicode的文本输入作为一个参数。它还定义了一个选择性的参数n作为定义字符序列的长度。这段程序读取了输入文本的每个字符然后建立了一个python的词典（dictionary），该词典包含了所有小于n长度的字符序列以及相对应的出现频率。比如，输入:”SnailMail.”将得到3－gram的词典：Python{'S':1,'Sn':1,'Sna':1,'nai':1,'ail':2,'il':1,'lM':1,'Ma':1,'Mai':1,'il.':1}123456789101112{  '  S':1,  'Sn':1,  'Sna':1,  'nai':1,  'ail':2,  'il':1,  'lM':1,  'Ma':1,  'Mai':1,  'il.':1}第四部分：如何比较两个NGrams?即使上面介绍的NGram类可以用来计算字母序列出现的频率，我们始终不知道如何比较NGrams.我们想要在不同的语言中找到最接近匹配去代表那种语言。我们想要在一组给予的不同语言的Ngram对象中，能找到最接近的匹配对象。为了协调匹配NGram去找到最佳的匹配，我们引进了两个新的函数:calculate_length()和_sub_()去允许Python实现两个NGram对象之间的减法。这样的减法应用于多纬NGrams向量。每个独立的n字符序列代表着向量的一个维度。Calculate_length()函数用来计算向量的长度（分散范围）。找到NGram向量间的角度就是找到向量间的相似性。这个技术被称做基于向量的查询(这篇是基于perl的文章，基本上用Perl实现了这篇作者上面阐述的所有观点)。实现代码：PythonclassNGram(object):def__init__(self,text,n=3):self.length=Noneself.n=nself.table={}self.parse_text(text)self.calculate_length()defparse_text(self,text):chars=''*self.n#initialsequenceofspaceswithlengthnforletterin(\"\".join(text.split())+\"\"):chars=chars[1:]+letter#appendlettertosequenceoflengthnself.table[chars]=self.table.get(chars,0)+1#incrementcountdefcalculate_length(self):\"\"\"TreattheN-Gramtableasavectorandreturnitsscalarmagnitudetobeusedforperformingavector-basedsearch.\"\"\"self.length=sum([x*xforxinself.table.values()])**0.5returnself.lengthdef__sub__(self,other):\"\"\"FindthedifferencebetweentwoNGramobjectsbyfindingthecosineoftheanglebetweenthetwovectorrepresentationsofthetableofN-Grams.Returnafloatvaluebetween0and1where0indicatesthatthetwoNGramsareexactlythesame.\"\"\"ifnotisinstance(other,NGram):raiseTypeError(\"Can'tcompareNGramwithnon-NGramobject.\")ifself.n!=other.n:raiseTypeError(\"Can'tcompareNGramobjectsofdifferentsize.\")total=0forkinself.table:total+=self.table[k]*other.table.get(k,0)return1.0-(float(total))/(float(self.length)*float(other.length))deffind_match(self,languages):\"\"\"OutofalistofNGramsthatrepresentindividuallanguages,returnthebestmatch.\"\"\"returnmin(languages,lambdan:self-n)123456789101112131415161718192021222324252627282930313233343536373839404142434445classNGram(object):def__init__(self,text,n=3):self.length=Noneself.n=nself.table={}self.parse_text(text)self.calculate_length() defparse_text(self,text):chars=''*self.n#initialsequenceofspaceswithlengthn forletterin(\"\".join(text.split())+\"\"):chars=chars[1:]+letter#appendlettertosequenceoflengthnself.table[chars]=self.table.get(chars,0)+1#incrementcount defcalculate_length(self):\"\"\"TreattheN-Gramtableasavectorandreturnitsscalarmagnitudetobeusedforperformingavector-basedsearch.\"\"\"self.length=sum([x*xforxinself.table.values()])**0.5returnself.length def__sub__(self,other):\"\"\"FindthedifferencebetweentwoNGramobjectsbyfindingthecosineoftheanglebetweenthetwovectorrepresentationsofthetableofN-Grams.Returnafloatvaluebetween0and1where0indicatesthatthetwoNGramsareexactlythesame.\"\"\"ifnotisinstance(other,NGram):raiseTypeError(\"Can'tcompareNGramwithnon-NGramobject.\") ifself.n!=other.n:raiseTypeError(\"Can'tcompareNGramobjectsofdifferentsize.\") total=0forkinself.table:total+=self.table[k]*other.table.get(k,0) return1.0-(float(total))/(float(self.length)*float(other.length)) deffind_match(self,languages):\"\"\"OutofalistofNGramsthatrepresentindividuallanguages,returnthebestmatch.\"\"\"returnmin(languages,lambdan:self-n)第五部分：如何比较NGram?选择合适的NGram模型相当的简单。你只需要将unicode的文本改成任何一种你想要选择的语言。Pythonenglish=NGram(training_text,n=3)#trigram1english=NGram(training_text,n=3)#trigram如果你想比较两个NGram模型。你可以用两个模型做减法来寻找两个模型的相似性（_sub_()是用来实现这个功能的）。Pythonsimilarity=english-NGram(text,n=3)1similarity=english-NGram(text,n=3)如果你想用Pythonlist或者iterator实现一个简单的基于向量的搜索，你可以用NGram类中的find_match(language)方式。搜索将在参数languages上实现对NGram对象的叠代。Pythonlanguages=[english,spanish,french]NGram(text,n=3).best_match(languages)12languages=[english,spanish,french]NGram(text,n=3).best_match(languages)正如你所见，真正的生产实现中的问题，在于寻找正确的数据去实现NGram模型。如果你想建立一个很好的语言检测器，你需要找到一些很有代表性的文本例子去代表你想测试的语言。Wiki百科上有很多很好的例子可以作为你的数据来源。除了文本检测，你还可以用NGram去做其他有意思的事情。Google的浏览显示就是一个很好的例子。它用了刚才创建的Python代码去实现了相似的统计应用。Google还公开了做这个统计实验用的数据。第六部分：现在该干些什么了呢？很多事情可以去做！我们从一个文本检测器开始，同样的方法可以在很多其他领域应用。比如说，你可以修改你的代码，让这个文本检测器不再只检测字母字符，而是直接进行词语匹配。理论上来说，这些词法顺序（用词的方式根据个人习惯而有所不同）可以用来鉴定一作者的写作。N-Grams的概念可以在不同的领域应用。比如：语法拼写建议（建议改正非正确语法词汇）鉴定DNA序列提高压缩算法的有效性改进搜索引擎改进语音识别系统和特征，通过某个特定词语会出现在另一个词语后面的概率尽管每种应用都会有所不同，但是本质上都是相似的，需要比较单独个体的相似性。当你需要使用序列时，不妨考虑NGram。1赞2收藏3评论关于作者：人见人爱的土豆Weibo:@人见人爱花见花开的土豆Site:mingcheng.me个人主页·我的文章·7"], "art_create_time": ["2013/06/27"], "art_title": ["50行Python代码写一个语言检测器"], "art_url": ["http://python.jobbole.com/54707/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2012/02/python-logo.png"]},
{"art_content": ["原文出处：TALL,SNARKYCANADIAN   译文出处：开源中国   最近我写了一篇关于我为什么不担心Python流失用户的文章。几分钟之后有人问我Python的用法(usage)，而这篇文章没有提及，但却是一个让人深思的问题。我们看到，使用Python的用户很可能在未来保持高位，但是Python是否会被用到尽可能多的项目中是不能保证的；用户(users)数目很多而且稳定，但是项目中Python的用处(use)并不确定。这篇文章的用意是帮助表明Python仍然对大多数软件项目是切实可行的。我不担心把Python推销给反对其他动态语言(如Ruby)的人，因为我认为这些争论与个人喜好有关。这篇文章是讲给那些推销静态类型语言的人。具体上，这篇文章是针对Go的，但也可以是其他任何静态类型语言。”为什么Go?”，你可能会问。因为Go实际上在获取Python的用户。当2003到2005年间Python的增长曲线是个曲棍球棒时，Python还不是被推下山巅的王者，而是个弱者。传统上，Python从Java之类的语言阵营中获得用户，并且留住了他们(我不想谈C++用户，因为通常他们有严格的性能需求，需要一个系统语言，或者是性能成瘾者，并且需要好好恢复)。但是Go的情况不太一样。如今Python是使用最多的语言之一，而不再是弱者了。一旦在静态类型语言社区中出现一门语言，它的生产效率/性能的取舍相当好，那便足以说服一些Python的程序员选择Go而不再是Python了。如今的Go首先我应该说，Go是目前我第二喜欢的语言。如果今天我要启动一个项目，但不能说服人们使用Python，那我会提议使用Go。不要误解我在本文中说Go是门不好的语言。这篇文章的要点是说服其他人，Python是生产率/性能取舍游戏中Go之外切实可行的替代方案，而不是表达Go是门不好的语言。认为这篇文章是反Go的，那就是你的个人想法，而且不应该这样认为。我应该说，我偶尔在工作中使用Go，并有点想关注这门语言的社区。既然我不能仅凭想象就成为Go专家，但这番话并不是仅从文档或者博客中提取出来的。但是由于我是Python开发团队的一份子，无论我如何试图表现得公平，固有的偏见某种程度上还是有的。那么，带着这些警告，我们来看下Go提供给开发者什么。生产率我看待Go的方式是，使用你最喜欢的编程语言，移除那些难于加速生产率的特性，就是Go。静态类型的影响被降到最小，因为通常只有在API边界时你才会面对它。结构类型同样使事情变得简单(把它认为是鸭子类型)。语法并不笨拙(虽然它使用了花括号)。不要认为Go是C/C++去掉不安全的特性，加上生产率更高的东西，不然你会很失望(比如，“为什么我不能使用make()内置函数，也不能像map类型一样对返回值进行计数”，这种看待Go的方式是错误的；这就是为什么C++开发者没有转到Go的原因)。快速编译也使开发周期更像一个动态语言，而不是一个需要编译的语言。而且事实上有些人喜欢没有异常机制带来的冗长，因为这促使你处理每种异常情形而不是(意外地)忽略它们(这是贯穿Go初始系统语言设计的实例)。还有，这门语言本身相当短小易记，并有严格的前向兼容性要求(forward-compatibilityrequirements)(你不可能更快地获得泛型)，大体上使用Go来编码是件很愉快的事情。由于是静态类型，Go可以很容易地获得工具支持(它对之前以此为设计目标的语言也有帮助)。Go确保核心工具跟随Go本身提供，也是明智之举。gofmt强制执行Go风格的规则，并允许通过用户自定义的规则来重构代码(“采用制表符缩进”不再是问题，因为这意味着你可以随心所欲地设置编辑器来代表制表符，然后gofmt将其转换为普通制表符以适用VCS)。gofix会更新代码以跟最新发布的版本保持一致。goget获取依赖并安装。Go最后一个生产率功能是它静态编译所有东西，使部署更简单。如果你使用容器来开发和部署，这也不算什么。只有当你发布单个文件的命令行工具，而不是一组依赖和你自己的代码时，这才算得上事。性能就性能来说，Go做的很好。很难指出任何基准能准确的证明Go总是最快的选择，甚至计算机语言基准游戏中一些基准证明CPython3是最快的。但是通常情况下可以认为对于你的任何工作来说Go已经足够快了。Go真正出色的地方是并发性(concurrency)。要注意并发代码并不是通常误解的并行(parallelized)代码;并发代码仍然可以是单线程的，仅仅在任务切换方面更加简单/出色。Go通过使用goroutine使连续并发的代码执行起来绝对的简单。如果你不想使用共享内存的方式(虽然也同样支持)，该语言提供的通信管道允许以非常简洁的消息传递方式进行并发编程。将所有特征整合进此语言中成为尽可能使用该语言开发并发代码的又一原因。换句话说，Go程序运行很快，该语言尽力使你在合理的方式上获得该效果。如今的Python如果顺利的话我已经让你相信Go是一种优秀的编程语言，除非因为其他原因，一些人不会认为我在整篇文章对Go的描述很糟糕。现在我们讨论一下Python的生产率/性能是怎么样的。生产率首先也是最重要的，Python非常容易学习。这也是为什么在当前高评价的美国大学中将Python作为首选的教学语言。这相当于该语言拥有成熟稳定的新程序员的来源以及更容易培训其他程序员。我想，要说服别人只用几行Python代码就会完成很多工作这并不难(Go/Python3比较显示Python每次都比Go使用更少的代码完成相同的工作)。所以我会坚持认为使用Python会更高产，即使和Go相比，这不会有人反对。通常大家反对Python的地方是在工具支持方面。但是如果你注意到我指出的Go相关的支持工具，fmt,fix,和get,Python社区也有对等的工具。对遵循PEP8的风格格式化(styleformating),可以在提交检查时使用pep8，或者如果想要更多gofmt风格的自动重写可以使用autopep8。对用于重构的gofix或gofmt，你可以说2to3也可以完成同样的功能。对于goget,Python有pip。我们有venv/virtualenv或cx_Freeze这样的代码冻结工具(跟其他一样，位于容器之上?ontopofcontainerizationlikeanythingelse)，而不是静态编译的二进制包。甚至有贯穿项目的代码分析工具如pylint。说Python因为缺少工具支持而不能用于大型项目，这种观点对我来说是很肤浅的。如果说有哪方面Python完全做的好，那就一定是它丰富的第三方扩展库和相应的工具可供使用，就像在PyPI上面看到的那样（我相信肯定有人忍不住要争论说，“并不是所有的第三方库都能够在Python3上面运行啊”，事实确实如此，然而，这些第三方扩展库对Python3的支持已经相当好了，而且还在继续改善中，所以我不会太在意这个争论，另外，你可以同时使用Python2/3两个版本进行编码，不需要关心针对哪个版本）。看一下godoc.org，上面显示Go也并不缺少社区支持，Pytho之所以能够拥有更多可用的第三方库仅仅是因为它的年龄，这个状态也会继续持续。性能因为Python已经存在很久，且变得如此庞大，简单地去说“Python是足够快的”不能说明整个的情况，那是因为有各种各样的实现加速的方式。但是在深入到VM级别的选项之后，意味着Python的stdlib提供了获得加速的选项。举例来说，concurrent.futures是尴尬地执行并行代码的方式，这种方式是极其简单的。而在Python3.3中，新的asyncio编写了异步代码。它没有像Go那样被集成进语言，在Python中的并发程序设计是可行的，且在方式上也未必是那么痛苦的。但是最好的办法是，你可以在选择的VM里改变Python代码的性能。CPython+Cython如果你在使用C拓展模块，CPython就会使你最好的选择(可能你不知道这个术语，CPython是你可以在python.org获得的解释器)。对大多数的情况而言性能至少合理些–因为某些原因，一些人认为Python开发团队不关心性能，这个一个谎言–而且即将会成为新的特性，因为CPython同时担当着语言规范的作用。如果认为你的一些内循环代码确实需要提高些速度，Cython是CPython的选择。Cython会尽可能的将你的Python代码编译成C拓展代码。有若干种支持的方法可以产生更好的C代码，所以这取决于你需要怎样的Cython特性。Cython同时也使写出C拓展模块更加简单(但要继续读下去，除了CPython还有其他的选择)。PyPy+cffi如果你不依赖于已存在的C拓展模块，PyPy会给你提供总体上最好的性能。它的JIT非常好而且它的团队欢迎受到使用CPython并且运行更快的代码的挑战，因为他们痛恨在speed.pypy.org中显示的那么慢。实话说，除非PyPy不支持你真的想用的那个版本的Python–因为PyPy确实会落后2个版本,比如pypy3现在支持Python3.2然而3.4是最新的CPython发布版;它们期盼在这个问题上能得到帮助(donation)–我只能考虑不使用PyPy因为你依赖于已存在的C拓展模块C(numpy是最常见的问题，虽然PyPyislookingfordonations可修复这个问题)。但这不意味着，如果你想封装一些C代码就用不了PyPy。PyPy项目还有另外一个子项目cffi，这个项目的目的是使Python代码也可以利用封装的C代码。使用cffi的关键好处在于，一旦你使用了cffi，C代码就可以用于CPython和PyPy(我认为IronPython和Jython也在添加对cffi的支持)。所以如果你在封装C代码，我强烈建议你看下cffi，而不是手动写C扩展模块或者使用Cython，这样你有更好的Python实现的支持，还能使用PyPy。Numba如果你在做数值相关的工作，你肯定应该考虑Numba这个选择。在科学计算上，经济学家注意到了它的性能。虽然在普通的Python编程上，它不能帮到什么忙，但是如果在Python非常强大的科学计算栈中用到了numpy或者其他模块，Numba使用LLVM来进行JIT肯定会有帮助。未来的Python考虑到所有内容，Python肯定不是停滞不前的(Go也没有，比如它们都在忙于用Go重写编译器和将连接器以外的东西转移进编译器来获得更快的编译速度）。Python的未来看起来还是光明的。生产率Python是一种在进化的语言。不像Go，Python乐于改变该语言，甚至是以永远不再向后兼容的方式。这意味着Python会比Go更快的速度变得更加高效(虽然在Go2开发之前Go的团队对该语言进化持哪种观点还是未知的)。在工具方面，标准化的函数注解是为了声明类型。这是在PyCon2014语言峰会期间提出的，针对函数参数和返回值，里面提到有大量的项目现在想要有一种声明预期类型的方法，使用函数注解考虑到了在某些方面的标准化，最终可能对标准库函数也会是有用的。在pytypedecl的邮件列表上的讨论还没有开始，但是我知道PEP大概是要开始了。不仅仅是对像Cython和Numba这样的项目在什么地方使用打印信息，还包括在诸如代码分析，重构等的时候使用。性能长远看来，有两个项目可以帮助提升Python的性能。一个是新的Python虚拟机Pyston。尽管Pyston刚出现时间不长，但它的目标是要使用LLVM的JIT(是的这不由得让人想起UnladenSwallow，然而LLVM的JIT已经比它在2009年时好很多了，所以这个项目还是颇有希望取得好效果的)。其实PyPy-STM才是真正能够让我兴奋的项目，”STM”表示”软件事务性内存”，它基本上是允许Python丢弃GIL的。PyPy-STM此时的性能比PyPy要慢大约1.2-3倍，这样的表现已经相当不错了。目前他们正在寻找资助，来继续这项工作，要实现这个目标：使得带有两个线程的PyPy-STM值得在PyPy上普遍运行。在黑暗中做出选择希望这篇博客传达的不是一个总结，而是一个关于生产力/性能折衷的方案。Python已经清晰地拥有了强大的生产力辅助并且没有哪个领域表现不佳，这仍是我选择的语言。如果你发现自己有可能选择Python项目以外的东西，请一定要停下来思考没有使用Python所带来的生产力损失，然后看看你有各种选项让Python加快执行，这样你再去做一个全面的关于Python是否可以为你的项目工作的选择。1赞收藏1评论"], "art_create_time": ["2017/08/31"], "art_title": ["如何为使用 Python 语言而辩论"], "art_url": ["http://python.jobbole.com/88461/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2015/02/edecceebedd0d04aa17bccba430ddcaf.jpg"]},
{"art_content": ["原文出处：IBMdeveloperworks-廖雪峰   您可能听说过，带有yield的函数在Python中被称之为generator（生成器），何谓generator？我们先抛开generator，以一个常见的编程题目来展示yield的概念。如何生成斐波那契數列斐波那契（Fibonacci）數列是一个非常简单的递归数列，除第一个和第二个数外，任意一个数都可由前两个数相加得到。用计算机程序输出斐波那契數列的前N个数是一个非常简单的问题，许多初学者都可以轻易写出如下函数：清单1.简单输出斐波那契數列前N个数Pythondeffab(max):n,a,b=0,0,1whilen<max:printba,b=b,a+bn=n+1123456deffab(max):    n,a,b=0,0,1    whilen<max:        printb        a,b=b,a+b        n=n+1执行fab(5)，我们可以得到如下输出：Python>>>fab(5)11235123456>>>fab(5)11235结果没有问题，但有经验的开发者会指出，直接在fab函数中用print打印数字会导致该函数可复用性较差，因为fab函数返回None，其他函数无法获得该函数生成的数列。要提高fab函数的可复用性，最好不要直接打印出数列，而是返回一个List。以下是fab函数改写后的第二个版本：清单2.输出斐波那契數列前N个数第二版Pythondeffab(max):n,a,b=0,0,1L=[]whilen<max:L.append(b)a,b=b,a+bn=n+1returnL12345678deffab(max):    n,a,b=0,0,1    L=[]    whilen<max:        L.append(b)        a,b=b,a+b        n=n+1    returnL可以使用如下方式打印出fab函数返回的List：Python>>>forninfab(5):...printn...1123512345678>>>forninfab(5):...    printn...11235改写后的fab函数通过返回List能满足复用性的要求，但是更有经验的开发者会指出，该函数在运行中占用的内存会随着参数max的增大而增大，如果要控制内存占用，最好不要用List来保存中间结果，而是通过iterable对象来迭代。例如，在Python2.x中，代码：清单3.通过iterable对象来迭代Pythonforiinrange(1000):pass1foriinrange(1000):pass会导致生成一个1000个元素的List，而代码：Pythonforiinxrange(1000):pass1foriinxrange(1000):pass则不会生成一个1000个元素的List，而是在每次迭代中返回下一个数值，内存空间占用很小。因为xrange不返回List，而是返回一个iterable对象。利用iterable我们可以把fab函数改写为一个支持iterable的class，以下是第三个版本的Fab：清单4.第三个版本PythonclassFab(object):def__init__(self,max):self.max=maxself.n,self.a,self.b=0,0,1def__iter__(self):returnselfdefnext(self):ifself.n<self.max:r=self.bself.a,self.b=self.b,self.a+self.bself.n=self.n+1returnrraiseStopIteration()12345678910111213141516classFab(object):     def__init__(self,max):        self.max=max        self.n,self.a,self.b=0,0,1     def__iter__(self):        returnself     defnext(self):        ifself.n<self.max:            r=self.b            self.a,self.b=self.b,self.a+self.b            self.n=self.n+1            returnr        raiseStopIteration()Fab类通过next()不断返回数列的下一个数，内存占用始终为常数：Python>>>forninFab(5):...printn...1123512345678>>>forninFab(5):...    printn...11235 然而，使用class改写的这个版本，代码远远没有第一版的fab函数来得简洁。如果我们想要保持第一版fab函数的简洁性，同时又要获得iterable的效果，yield就派上用场了：清单5.使用yield的第四版Pythondeffab(max):n,a,b=0,0,1whilen<max:yieldb#printba,b=b,a+bn=n+1'''123456789deffab(max):    n,a,b=0,0,1    whilen<max:        yieldb        #printb        a,b=b,a+b        n=n+1 '''第四个版本的fab和第一版相比，仅仅把printb改为了yieldb，就在保持简洁性的同时获得了iterable的效果。调用第四版的fab和第二版的fab完全一致：Python>>>forninfab(5):...printn...1123512345678>>>forninfab(5):...    printn...11235简单地讲，yield的作用就是把一个函数变成一个generator，带有yield的函数不再是一个普通函数，Python解释器会将其视为一个generator，调用fab(5)不会执行fab函数，而是返回一个iterable对象！在for循环执行时，每次循环都会执行fab函数内部的代码，执行到yieldb时，fab函数就返回一个迭代值，下次迭代时，代码从yieldb的下一条语句继续执行，而函数的本地变量看起来和上次中断执行前是完全一样的，于是函数继续执行，直到再次遇到yield。也可以手动调用fab(5)的next()方法（因为fab(5)是一个generator对象，该对象具有next()方法），这样我们就可以更清楚地看到fab的执行流程：清单6.执行流程Python>>>f=fab(5)>>>f.next()1>>>f.next()1>>>f.next()2>>>f.next()3>>>f.next()5>>>f.next()Traceback(mostrecentcalllast):File\"<stdin>\",line1,in<module>StopIteration123456789101112131415>>>f=fab(5)>>>f.next()1>>>f.next()1>>>f.next()2>>>f.next()3>>>f.next()5>>>f.next()Traceback(mostrecentcalllast):  File\"<stdin>\",line1,in<module>StopIteration当函数执行结束时，generator自动抛出StopIteration异常，表示迭代完成。在for循环里，无需处理StopIteration异常，循环会正常结束。我们可以得出以下结论：一个带有yield的函数就是一个generator，它和普通函数不同，生成一个generator看起来像函数调用，但不会执行任何函数代码，直到对其调用next()（在for循环中会自动调用next()）才开始执行。虽然执行流程仍按函数的流程执行，但每执行到一个yield语句就会中断，并返回一个迭代值，下次执行时从yield的下一个语句继续执行。看起来就好像一个函数在正常执行的过程中被yield中断了数次，每次中断都会通过yield返回当前的迭代值。yield的好处是显而易见的，把一个函数改写为一个generator就获得了迭代能力，比起用类的实例保存状态来计算下一个next()的值，不仅代码简洁，而且执行流程异常清晰。如何判断一个函数是否是一个特殊的generator函数？可以利用isgeneratorfunction判断：清单7.使用isgeneratorfunction判断Python>>>frominspectimportisgeneratorfunction>>>isgeneratorfunction(fab)True123>>>frominspectimportisgeneratorfunction>>>isgeneratorfunction(fab)True要注意区分fab和fab(5)，fab是一个generatorfunction，而fab(5)是调用fab返回的一个generator，好比类的定义和类的实例的区别：清单8.类的定义和类的实例Python>>>importtypes>>>isinstance(fab,types.GeneratorType)False>>>isinstance(fab(5),types.GeneratorType)True12345>>>importtypes>>>isinstance(fab,types.GeneratorType)False>>>isinstance(fab(5),types.GeneratorType)Truefab是无法迭代的，而fab(5)是可迭代的：Python>>>fromcollectionsimportIterable>>>isinstance(fab,Iterable)False>>>isinstance(fab(5),Iterable)True12345>>>fromcollectionsimportIterable>>>isinstance(fab,Iterable)False>>>isinstance(fab(5),Iterable)True每次调用fab函数都会生成一个新的generator实例，各实例互不影响：Python>>>f1=fab(3)>>>f2=fab(5)>>>print'f1:',f1.next()f1:1>>>print'f2:',f2.next()f2:1>>>print'f1:',f1.next()f1:1>>>print'f2:',f2.next()f2:1>>>print'f1:',f1.next()f1:2>>>print'f2:',f2.next()f2:2>>>print'f2:',f2.next()f2:3>>>print'f2:',f2.next()f2:5123456789101112131415161718>>>f1=fab(3)>>>f2=fab(5)>>>print'f1:',f1.next()f1:1>>>print'f2:',f2.next()f2:1>>>print'f1:',f1.next()f1:1>>>print'f2:',f2.next()f2:1>>>print'f1:',f1.next()f1:2>>>print'f2:',f2.next()f2:2>>>print'f2:',f2.next()f2:3>>>print'f2:',f2.next()f2:5return的作用在一个generatorfunction中，如果没有return，则默认执行至函数完毕，如果在执行过程中return，则直接抛出StopIteration终止迭代。另一个例子另一个yield的例子来源于文件读取。如果直接对文件对象调用read()方法，会导致不可预测的内存占用。好的方法是利用固定长度的缓冲区来不断读取文件内容。通过yield，我们不再需要编写读文件的迭代类，就可以轻松实现文件读取：清单9.另一个yield的例子Pythondefread_file(fpath):BLOCK_SIZE=1024withopen(fpath,'rb')asf:whileTrue:block=f.read(BLOCK_SIZE)ifblock:yieldblockelse:return123456789defread_file(fpath):    BLOCK_SIZE=1024    withopen(fpath,'rb')asf:        whileTrue:            block=f.read(BLOCK_SIZE)            ifblock:                yieldblock            else:                return以上仅仅简单介绍了yield的基本概念和用法，yield在Python3中还有更强大的用法，我们会在后续文章中讨论。注：本文的代码均在Python2.7中调试通过1赞2收藏评论"], "art_create_time": ["2013/01/30"], "art_title": ["Python yield 使用浅析"], "art_url": ["http://python.jobbole.com/32876/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2012/02/python-logo.png"]},
{"art_content": ["继2月18日RobDiana发表有关2013年传统编程语言的就业趋势后，他在21日发布了另外一篇文章，有关2013年”Web脚本编程语言“的就业趋势，其中包括 Ruby、Python、 PHP、JavaScript、Groovy 和 Erlang。首先还是先来看看Indeed招聘网站的长期就业趋势：（横坐标中是“Jan‘06″是指”2006年1月“，其他类同）和传统编程语言趋势图大致一样，这个趋势图也是列出了过去几年中这些语言的起起落落。JavaScript虽有较大下降，但是还是大幅领先其他语言。PHP、Python和Ruby展现出相似的趋势，在2011年大部分时间都有一个”高地“，在2012年年底有所下降。Groovy也有平稳的趋势，仍在Erlang之上。再来来看看SimplyHired的趋势：（2012年3月1日–2012年12月1日）总体来说，SimplyHired显示了年底净扁平化趋势。JavaScript在去年有稍微下降，但其需求还很高（相对来说）。PHP和Ruby有着最显著是变化，在年底有很大降幅，它们是否能在2013年恢复呢？Python则保持平稳，领先于PHP。Groovy和Erlang几乎和横轴重合了，所以很难看出真正的变化。最后再看看 Indeed 的相对增长量趋势：（横坐标中是“Jan‘06″是指”2006年1月“，其他类同）Groovy展现出出奇的相对增长量；Erlang在过去两年中有不错的增长量，但过去几个月中有些下降；给人印象最深刻的是Ruby的持续增长，即使整体需求在下滑；Python仍然有稳定的正增长；虽然PHP和JavaScript有过长期的高需求，但其需求增长量并不多，它们目前有扁平化趋势。在今年经过高峰后，所有趋势都有所下降。这和传统语言趋势一致。不过，如果你阅读了科技博客，你会发现这种扁平化趋势令人惊奇。似乎人人都在招程序员，但都没怎么成功招到。我必须承认本文中有关Web编程语言的趋势和传统编程语言的趋势都相当令人失望。我是期待有更好的趋势，但就业数据并没有反映这一点。在接下来的几个月，我会研究其他数据点，看看就业数据是否如实反映出现了现实情况。 英文原文：regulargeek，编译：伯乐在线-黄利民译文链接：http://blog.jobbole.com/33825/【非特殊说明，转载必须在正文中标注并保留原文链接、译文链接和译者等信息，谢谢合作！】1赞收藏5评论"], "art_create_time": ["2013/02/25"], "art_title": ["2013年2月Web编程语言就业趋势"], "art_url": ["http://python.jobbole.com/33825/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2013/02/webScriptingTrends-Feb2013.png"]},
{"art_content": ["英文原文，翻译：开源中国这篇博文是对来自Twitter的这个问题的答复：@nodakai:为什么BIND10非要用C++编写??我认为从这个不幸的事件中，托管语言的支持者们有许多需要学习了解的地方当我开始进行BIND10的项目工作时，唯一已经做的决定就是要使用哪种编程语言，恰逢预料之中的关于自行车棚的那次讨论之后。那时最重要的问题是要让项目继续进行，而不是去重启一场可能永无休止的争论，一直讨论到底要使用哪种（或哪些）编程。话说到这了，我对所选语言感到非常满意。实际上，如果开头就要做这个决定的人是我，我选择的也是同样这些语言。BIND9是用C语言编写的。在它设计和编写之时——在20世纪末——这真是一个唯一合乎逻辑的选择。C语言在阅读和编写方面相对简单，很多平台都支持它，并且还能生成运行速度飞快的代码。此外，C缺乏能够支持软件工程的语言特性，而且完全不安全。于是，当ISC开始正式考虑BIND10时——大约是2006年左右——就提出了新项目要使用哪种语言的问题。第一个很显然的问题是，“为什么不用C？”下面给出部分答案：C中的字符串操作实在是个乏味的苦差事C缺乏很好的内存管理机制错误处理有随意性且难弄封装和其它面向对象的特性只能通过模仿才能实现大家都一致认为，我们可以做得更好。问题是“怎样才能准确的做到这一点？”要选择一门新的语言，当然要满足一些要求：该语言必须处于相对主流的位置。Wikipedia上关于编程语言的页面 中列出了600多种语言，而且还没有列完。然而，BIND10有一个目标是，要让大家很简单就能上手。尽管使用类似Eiffel或Prolog的这类语言后，因为它们比较新奇所以有可能会吸引一些开发者，但对于大多数程序员来说却是个难以逾越的障碍。还有第二个理由，ISC要保证，无论选什么语言都必须能够找到熟练的开发者。该语言必须能够解决C语言中的绝大多数问题。理想情况下，这意味着，该语言必须能优雅的处理字符串、内存垃圾回收，异常处理，并且还是面向对象的。该语言在CPU密集型运算方面速度要非常快。现代的DNS服务器很大程度上是属于计算密集型的，无论是在有授权还是在递归解析的情况下都是这样。DNS服务器要使用特定的数据结构和算法，所以我们无法依赖用C或者C++编写的底层库。这个要求基本上排除了使用任何解释性语言的可能性。我们最终选择的方式是混合使用两种编程语言:Python只要有可能,我们都尽量使用Python。Python是一种非常流行的语言,通常在大多数调查中都是最流行的脚本语言（可能要除PHP之外）。它具有我们要寻找的所有特性。。。就是在性能方面有点问题。C++当有必要时，我们会使用C++.C++也是一门非常流行的语言,而且也具有我们要寻找的所有特性。然而，C++绝不是一门很容易就能使用起来的语言，于是我们想了个主意，就是我们在可能的情况下要避免它的复杂性。如果你在很早的时候学过C++但没有在现在的C++编程环境下编过程序，你可能会对用它来编程有着一些错误的观点。我们使用了Boost库，这里面有个共享指针，能提供一种对动态分配对象的引用计数的手段。实际上，采用资源获得即初始化(RAII)后，就能够解决大量的资源锁定和泄露问题。到目前为止，结果中我们代码的75%用到是C++，17%用的是Python(链接)，这个结果表明，BIND10中的大部分代码对性能要求都很严格。在选择语言时，其它的项目会有不同的考虑因素，所以，尽管C++和Python是BIND10不错的选择，但它们可不是适合于每个项目。但从大的思路上讲，为BIND10考虑语言选择方面的动因和决策方式在我们的项目开始时很有意义，而且我想它们现在仍然很有意义。有一件我们可能会以不同方式来作的事是，选择编写能够同时运行于Python2和Python3中的代码，而不是必须要求Python3。随着时间的推移，这个问题会变得越来越小，因为Python的未来 是 Python3，但这个决定给人们带来很多苦恼，为了运行个软件还不得不安装一个新版本的解释器，这让人很不高兴。我希望2到3年后，我们能够笑谈这些苦恼，而Python2已成为一段退色的记忆。1赞收藏评论"], "art_create_time": ["2013/02/28"], "art_title": ["为什么 BIND 10 要用 C++ 和 Python 来写"], "art_url": ["http://python.jobbole.com/34289/"], "art_img": ["http://bind10.isc.org/raw-attachment/wiki/Mascot/BIND10_Bundy.jpg"]},
{"art_content": ["英文原文：CharmingPython:FunctionalprogramminginPython,Part1，翻译：开源中国摘要：虽然人们总把Python当作过程化的，面向对象的语言，但是他实际上包含了函数化编程中，你需要的任何东西。这篇文章主要讨论函数化编程的一般概念，并说明用Python来函数化编程的技术。我们最好从艰难的问题开始出发：“到底什么是函数化编程呢？”其中一个答案可能是这样的，函数化编程就是你在使用Lisp这样的语言时所做的（还有Scheme，Haskell，ML，OCAML，Mercury，Erlang和其他一些语言）。这是一个保险的回答，但是它解释得并不清晰。不幸的是对于什么是函数化编程，很难能有一个协调一致的定义，即使是从函数化变成本身出发，也很难说明。这点倒很像盲人摸象。不过，把它拿来和命令式编程（imperativeprogramming）做比较也不错（命令式编程就像你在用C，Pascal，C++，Java，Perl，Awk，TCL和很多其他类似语言时所做的，至少大部分一样 ）。让我们回想一下功能模块的绑定类。使用该类的特性，我们可以确认在一个给定的范围块内，一个特定的名字仅仅代表了一个唯一的事物。 我个人粗略总结了一下，认为函数式编程至少应该具有下列几点中的多个特点。在谓之为函数式的语言中，要做到这些就比较容易，但要做到其它一些事情不是很难就是完全不可能：函数具有首要地位(对象)。也就是说，能对“数据”做什么事，就要能对函数本身做到那些事（比如将函数作为参数传递给另外一个函数）。将递归作为主要的控制结构。在有些函数式语言中，都不存在其它的“循环”结构。列表处理作为一个重点（例如，Lisp语言的名字）。列表往往是通过对子列表进行递归取代了循环。“纯”函数式语言会完全避免副作用。这么做就完全弃绝了命令式语言中几乎无处不在的这种做法：将第一个值赋给一个变量之后为了跟踪程序的运行状态，接着又将另外一个值赋给同一个变量。函数式编程不是不鼓励就是完全禁止使用语句，而是通过对表达式(换句话说，就是函数加上参数）求值（evaluationofexpressions）完成任务.在最纯粹的情形下，一个程序就是一个表达式（再加上辅助性的定义）函数式编程中最关心的是要对什么进行计算，而不是要怎么来进行计算。在很多函数式编程语言中都会用到“高阶”（higherorder）函数(换句话说，高阶函数就是对对函数进行运算的函数进行运算的函数）。函数式编程的倡导者们认为，所有这些特性都有助于更快地编写出更多更简洁并且更不容易出Bug的代码。而且，计算机科学、逻辑学和数学这三个领域中的高级理论家发现，函数式编程语言和程序的形式化特性在证明起来比命令式编程语言和程序要简单很多。Python内在的函数式功能自Python1.0起，Python就已具有了以上所列中的绝大多数特点。但是就象Python所具有的大多数特性一样，这些特点出现在了一种混合了各种特性的语言中。 和Python的OOP（面向对象编程）特性非常象，你想用多少就用多少，剩下的都可以不管（直到你随后需要用到它们为止）。在Python2.0中，加入了列表解析（listcomprehensions）这个非常好用的”语法糖“。尽管列表解析没有添加什么新功能，但它让很多旧功能看起来好了不少。Python中函数式编程的基本要素包括functionsmap()、reduce()、filter()和lambda算子（operator）。在Python1.x中，apply()函数也可以非常方便地拿来将一个函数的列表返回值直接用于另外一个函数。Python2.0为此提供了一个改进后的语法。可能有点让人惊奇，使用如此之少的函数（以及基本的算子）几乎就足以写出任何Python程序了；更加特别的是，几乎用不着什么执行流程控制语句。所有(if,elif,else,assert,try,except,finally,for,break,continue,while,def)这些都都能通过仅仅使用函数式编程中的函数和算子就能以函数式编程的风格处理好。尽管真正地在程序中完全排除使用所有流程控制命令可能只在想参加”Python混乱编程“大赛（可将Python代码写得跟Lisp代码非常象）时才有意义，但这对理解函数式编程如何通过函数和递归表达流程控制很有价值。剔除流程控制语句剔除练习首先要考虑的第一件事是，实际上，Python会对布尔表达式求值进行“短路”处理。这就为我们提供了一个if/elif/else分支语句的表达式版（假设每个分支只调用一个函数，不是这种情况时也很容易组织成重新安排成这种情况）。这里给出怎么做：对Python中的条件调用进行短路处理Python#Normalstatement-basedflowcontrolif<cond1>:func1()elif<cond2>:func2()else:func3()#Equivalent\"shortcircuit\"expression(<cond1>andfunc1())or(<cond2>andfunc2())or(func3())#Example\"shortcircuit\"expression>>>x=3>>>defpr(s):returns>>>(x==1andpr('one'))or(x==2andpr('two'))or(pr('other'))'other'>>>x=2>>>(x==1andpr('one'))or(x==2andpr('two'))or(pr('other'))'two'12345678910111213141516#Normalstatement-basedflowcontrolif<cond1>:  func1()elif<cond2>:func2()else:        func3() #Equivalent\"shortcircuit\"expression(<cond1>andfunc1())or(<cond2>andfunc2())or(func3()) #Example\"shortcircuit\"expression>>>x=3>>>defpr(s):returns>>>(x==1andpr('one'))or(x==2andpr('two'))or(pr('other'))'other'>>>x=2>>>(x==1andpr('one'))or(x==2andpr('two'))or(pr('other'))'two'我们的表达式版本的条件调用看上去可能不算什么，更象是个小把戏；然而，如果我们注意到lambda算子必须返回一个表达式，这就更值得关注了。既然如我们所示，表达式能够通过短路包含一个条件判断，那么，lambda表达式就是个完全通用的表达条件判断返回值的手段了。我们来一个例子：Python中短路的LambdaPython>>>pr=lambdas:s>>>namenum=lambdax:(x==1andpr(\"one\"))\\....or(x==2andpr(\"two\"))\\....or(pr(\"other\"))>>>namenum(1)'one'>>>namenum(2)'two'>>>namenum(3)'other'12345678910>>>pr=lambdas:s>>>namenum=lambdax:(x==1andpr(\"one\"))\\....                  or(x==2andpr(\"two\"))\\....                  or(pr(\"other\"))>>>namenum(1)'one'>>>namenum(2)'two'>>>namenum(3)'other'将函数作为具有首要地位的对象前面的例子已经表明了Python中函数具有首要地位，但有点委婉。当我们用lambda操作创建一个函数对象时，我们所得到的东西是完全通用的。就其本质而言，我们可以将我们的对象同名字”pr”和”namenum”绑定到一起,以完全相同的方式，我们也也完全可以将数字23或者字符串”spam”同这些名字绑定到一起。但是，就象我们可以无需将其绑定到任何名字之上就能直接使用数字23（也就是说，它可以用作函数的参数）一样，我们也可以直接使用我们使用lambda创建的函数对象，而无需将其绑定到任何名字之上。在Python中，函数就是另外一种我们能够就像某种处理的值。我们对具有首要地位的对象做的比较多的事情就是，将它们作为参数传递给函数式编程固有的函数map()、reduce()和filter()。这三个函数接受的第一个参数都是一个函数对象。map()针对指定给它的一个或多个列表中每一项对应的内容，执行一次作为参数传递给它的那个函数，最后返回一个结果列表。reduce()针对每个后继项以及最后结果的累积结果，执行一次作为参数传递给它的那个函数；例如，reduce(lambdan,m:n*m,range(1,10))是求”10的阶乘”的意思（换言之，将每一项和前面所得的乘积进行相乘）filter()使用那个作为参数传递给它的函数，对一个列表中的所有项进行”求值“，返回一个由所有能够通过那个函数测试的项组成的经过遴选后的列表。我们经常也会把函数对象传递给我们自己定义的函数，不过一般情况下这些自定义的函数就是前文提及的内建函数的某种形式的组合。通过组合使用这三种函数式编程内建的函数，能够实现范围惊人的“执行流程”操作(全都不用语句，仅仅使用表达式实现)。 Python中的函数式循环替换循环语言和条件状态语言块同样简单。for可以直接翻译成map()函数。正如我们的条件执行，我们会需要简化语句块成简单的函数调用（我们正在接近通常能做的）：替换循环Pythonforeinlst:func(e)#statement-basedloopmap(func,lst)#map()-basedloop12foreinlst:  func(e)      #statement-basedloopmap(func,lst)          #map()-basedloop通过这种方法，对有序程序流将有一个相似的函数式方式。那就是，命令式编程几乎是由大量“做这，然后做那，之后做其它的”语句组成。map()让我们只要做这样：Map-based动作序列Python#let'screateanexecutionutilityfunctiondo_it=lambdaf:f()#letf1,f2,f3(etc)befunctionsthatperformactionsmap(do_it,[f1,f2,f3])#map()-basedactionsequence123456#let'screateanexecutionutilityfunctiondo_it=lambdaf:f() #letf1,f2,f3(etc)befunctionsthatperformactions map(do_it,[f1,f2,f3])  #map()-basedactionsequence通常，我们的整个主要的程序都可以使用一个map表达式加上一些函数列表的执行来完成这个程序。最高级别的函数的另一个方便的特性是你可以把它们放在一个列表里。翻译while会稍稍复杂一些，但仍然可以直接地完成：Python中的函数式”while”循环Python#statement-basedwhileloopwhile<cond>:<pre-suite>if<break_condition>:breakelse:<suite>#FP-stylerecursivewhileloopdefwhile_block():<pre-suite>if<break_condition>:return1else:<suite>return0while_FP=lambda:(<cond>andwhile_block())orwhile_FP()while_FP()12345678910111213141516171819#statement-basedwhileloopwhile<cond>:    <pre-suite>    if<break_condition>:        break    else:        <suite> #FP-stylerecursivewhileloopdefwhile_block():    <pre-suite>    if<break_condition>:        return1    else:        <suite>    return0 while_FP=lambda:(<cond>andwhile_block())orwhile_FP()while_FP()在翻译while循环时，我们仍然需要使用while_block()函数，这个函数本身里面可以包含语句而不是仅仅包含表达式。但我们可能还能够对这个函数再进行更进一步的剔除过程（就像前面模版中的对if/else进行短路处理一样）。还有，<cond>很难对普通的测试有什么用，比如whilemyvar==7，既然循环体（在设计上）不能对任何变量的值进行修改（当然，在while_block()中可以修改全局变量）。有一种方法可以用来为while_block()添加更有用的条件判断，让while_block()返回一个有意义的值，然后将这个返回值同循环结束条件进行比较。现在应该来看一个剔除其中语句的具体例子了：Python中’echo’循环Python#imperativeversionof\"echo()\"defecho_IMP():while1:x=raw_input(\"IMP--\")ifx=='quit':breakelseprintxecho_IMP()#utilityfunctionfor\"identitywithside-effect\"defmonadic_print(x):printxreturnx#FPversionof\"echo()\"echo_FP=lambda:monadic_print(raw_input(\"FP--\"))=='quit'orecho_FP()echo_FP()123456789101112131415161718#imperativeversionof\"echo()\"defecho_IMP():    while1:        x=raw_input(\"IMP--\")        ifx=='quit':            break        else            printxecho_IMP() #utilityfunctionfor\"identitywithside-effect\"defmonadic_print(x):    printx    returnx #FPversionof\"echo()\"echo_FP=lambda:monadic_print(raw_input(\"FP--\"))=='quit'orecho_FP()echo_FP()在上面的例子中我们所做的，就是想办法将一个涉及I/O、循环和条件判断的小程序，表达为一个递归方式的纯粹的表达式 （确切地说，表达为一个可以在需要的情况下传递到别的地方的函数对象）。我们的确仍然使用了实用函数monadic_print()，但这个函数是完全通用的，而且可以用于以后我们可能会创建的每个函数式程序的表达式中（它的代价是一次性的）。请注意，任何包含monadic_print(x)的表达式的值都是一样的，好像它只是包含了一个x而已。函数式编程中（特别是在Haskell中）的函数有一种叫做”monad”（一元）的概念，这种一元函数“实际什么都不做，只是在执行过程中产生一个副作用”。 避免副作用在做完这些没有非常明智的理由陈述，并把晦涩的嵌套表达式代替他们之后，一个很自然的问题是“为什么要这样做？！”　我描述的函数式编程在Python中都实现了。但是最重要的特性和一个有具体用处——就是避免副作用（或至少它们阻止如monads的特殊区域）。程序错误的大部分——并且这些问题驱使程序员去debug——出现是因为在程序的运行中变量获取了非期望的值。函数式编程简单地通过从不给变量赋值而绕过了这个问题。现在让我们看一段非常普通的命令式代码。这段代码的目的是打印出乘积大于25的一对一对数字所组成的一个列表。组成每对数字的每一个数字都是取自另外的两个列表。这种事情和很多程序员在他们的编程中经常做的一些事情比较相似。命令式的解决方式有可能就象下面这样：命令式的”打印大乘积”的Python代码Python#Nestedloopproceduralstyleforfindingbigproductsxs=(1,2,3,4)ys=(10,15,3,22)bigmuls=[]#...morestuff...forxinxs:foryinys:#...morestuff...ifx*y>25:bigmuls.append((x,y))#...morestuff...#...morestuff...printbigmuls12345678910111213#Nestedloopproceduralstyleforfindingbigproductsxs=(1,2,3,4)ys=(10,15,3,22)bigmuls=[]#...morestuff...forxinxs:    foryinys:        #...morestuff...        ifx*y>25:            bigmuls.append((x,y))            #...morestuff...#...morestuff...printbigmuls这个项目足够小了，好像没有地方会出什么差错。但有可能在这段代码中我们会嵌入一些同时完成其它任务的代码。用”morestuff”（其它代码）注释掉的部分，就是有可能存在导致出现bug的副作用的地方。在那三部分的任何一点上，变量sxs、ys、bigmuls、x、y都有可能在这段按照理想情况简化后的代码中取得一个出人意料的值。还有，这段代码执行完后，后继代码有可能需要也有可能不需要对所有这些变量中的值有所预期。显而易见，将这段代码封装到函数/实例中，小心处理变量的作用范围，就能够避免这种类型的错误。你也可以总是将使用完毕的变量del掉。但在实践中，这里指出的这种类型的错误很常见。以一种函数式的途径一举消除这些副作用所产生的错误，这样就达到了我们的目的。一种可能的代码如下：以函数式途径达到我们的目的Pythonbigmuls=lambdaxs,ys:filter(lambda(x,y):x*y>25,combine(xs,ys))combine=lambdaxs,ys:map(None,xs*len(ys),dupelms(ys,len(xs)))dupelms=lambdalst,n:reduce(lambdas,t:s+t,map(lambdal,n=n:[l]*n,lst))printbigmuls((1,2,3,4),(10,15,3,22))1234bigmuls=lambdaxs,ys:filter(lambda(x,y):x*y>25,combine(xs,ys))combine=lambdaxs,ys:map(None,xs*len(ys),dupelms(ys,len(xs)))dupelms=lambdalst,n:reduce(lambdas,t:s+t,map(lambdal,n=n:[l]*n,lst))printbigmuls((1,2,3,4),(10,15,3,22))在例子中我们绑定我们的匿名（lambda）函数对象到变量名，但严格意义上讲这并不是必须的。我们可以用简单的嵌套定义来代替之。这不仅是为了代码的可读性，我们才这样做的；而且是因为combine()函数在任何地方都是一个非常好的功能函数（函数从两个输入的列表读入数据生成一个相应的pair列表）。函数dupelms()只是用来辅助函数combine()的。即使这个函数式的例子跟命令式的例子显得要累赘些，不过一旦你考虑到功能函数的重用，则新的bigmuls()中代码就会比命令式的那个要稍少些。这个函数式例子的真正优点在于：在函数中绝对没有改变变量的值。这样就不可能在之后的代码（或者从之前的代码）中产生不可预期的副作用。显然，在函数中没有副作用，并不能保证代码的正确性，但它仍然是一个优点。无论如何请注意，Python（不像很多其它的函数式语言）不会阻止名字bigmuls，combine和dupelms的再次绑定。如果combine()运行在之后的程序中意味着有所不同时，所有的预测都会失效。你可能会需要新建一个单例类来包含这个不变的绑定（也就是说，s.bigmuls之类的）；但是这一例并没有空间来做这些。一个明显值得注意的是，我们特定的目标是定制Python2的一些特性。而不是命令式的或函数式编程的例子，最好的（也是函数式的）方法是：Pythonprint[(x,y)forxin(1,2,3,4)foryin(10,15,3,22)ifx*y>25]1print[(x,y)forxin(1,2,3,4)foryin(10,15,3,22)ifx*y>25] 结束语我已经列出了把每一个Python控制流替换成一个相等的函数式代码的方法（在程序中减少副作用）。高效翻译一个特定的程序需要一些额外的思考，但我们已经看出内置的函数式功能是全面且完善的。在接下来的文章里，我们会看到更多函数式编程的高级技巧；并且希望我们接下来能够摸索到函数式编程风格的更多优点和缺点。 可爱的Python:Python中函数式编程，第二部分可爱的Python:Python中的函数式编程，第三部分1赞2收藏1评论"], "art_create_time": ["2013/03/07"], "art_title": ["可爱的 Python : Python中函数式编程，第一部分"], "art_url": ["http://python.jobbole.com/35028/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2013/03/python-logo-master-v3-TM.png"]},
{"art_content": ["来源：陈皓在《性能调优攻略》里，我说过，要调优性需要找到程序中的Hotspot，也就是被调用最多的地方，这种地方，只要你能优化一点点，你的性能就会有质的提高。在这里我给大家举三个关于代码执行效率的例子（它们都来自于网上）第一个例子PHP中Getter和Setter的效率（来源reddit）这个例子比较简单，你可以跳过。考虑下面的PHP代码：我们可看到，使用Getter/Setter的方式，性能要比直接读写成员变量要差一倍以上。Python&lt;?php//dog_naive.phpclassdog{public$name=&quot;&quot;;publicfunctionsetName($name){$this-&amp;gt;name=$name;}publicfunctiongetName(){return$this-&amp;gt;name;}}$rover=newdog();//通过Getter/Setter方式for($x=0;$x&lt;10;$x++){$t=microtime(true);for($i=0;$i&lt;1000000;$i++){$rover-&gt;setName(&quot;rover&quot;);$n=$rover-&gt;getName();}echomicrotime(true)-$t;echo&quot;n&quot;;}//直接存取变量方式for($x=0;$x&lt;10;$x++){$t=microtime(true);for($i=0;$i&lt;1000000;$i++){$rover-&gt;name=&quot;rover&quot;;$n=$rover-&gt;name;}echomicrotime(true)-$t;echo&quot;n&quot;;}?&gt;1234567891011121314151617181920212223242526272829303132333435&lt;?php    //dog_naive.php     classdog{        public$name=&quot;&quot;;        publicfunctionsetName($name){            $this-&amp;gt;name=$name;        }        publicfunctiongetName(){            return$this-&amp;gt;name;        }    }     $rover=newdog();        //通过Getter/Setter方式    for($x=0;$x&lt;10;$x++){        $t=microtime(true);        for($i=0;$i&lt;1000000;$i++){            $rover-&gt;setName(&quot;rover&quot;);            $n=$rover-&gt;getName();        }        echomicrotime(true)-$t;        echo&quot;n&quot;;    }        //直接存取变量方式        for($x=0;$x&lt;10;$x++){        $t=microtime(true);        for($i=0;$i&lt;1000000;$i++){            $rover-&gt;name=&quot;rover&quot;;            $n=$rover-&gt;name;        }        echomicrotime(true)-$t;        echo&quot;n&quot;;    }?&gt;这个并没有什么稀，因为有函数调用的开销，函数调用需要压栈出栈，需要传值，有时还要需要中断，要干的事太多了。所以，代码多了，效率自然就慢了。所有的语言都这个德行，这就是为什么C++要引入inline的原因。而且Java在打开优化的时候也可以优化之。但是对于动态语言来说，这个事就变得有点困难了。你可能会以为使用下面的代码（MagicFunction）会好一些，但实际其性能更差。Pythonclassdog{ private$_name=&quot;&quot;;function__set($property,$value){    if($property==&#039;name&#039;)$this-&gt;_name=$value;  }  function__get($property){    if($property==&#039;name&#039;)return$this-&gt;_name;  }}123456789classdog{  private$_name=&quot;&quot;;  function__set($property,$value){    if($property==&#039;name&#039;)$this-&gt;_name=$value;  }  function__get($property){    if($property==&#039;name&#039;)return$this-&gt;_name;  }}动态语言的效率从来都是一个问题，如果你需要PHP有更好的性能，你可能需要使用FaceBook的HipHop来把PHP编译成C语言。第二个例子为什么Python程序在函数内执行得更快？（来源StackOverflow）考虑下面的代码，一个在函数体内，一个是全局的代码。函数内的代码执行效率为1.8sPythondefmain():  foriinxrange(10**8):    passmain()1234defmain():  foriinxrange(10**8):    passmain()函数体外的代码执行效率为4.5sPythonforiinxrange(10**8):  pass12foriinxrange(10**8):  pass不用太纠结时间，只是一个示例，我们可以看到效率查得很多。为什么会这样呢？我们使用 dis module 反汇编函数体内的bytecode代码，使用 compile builtin 反汇编全局bytecode，我们可以看到下面的反汇编（注意我高亮的地方）我们可以看到，差别就是 STORE_FAST 和 STORE_NAME，前者比后者快很多。所以，在全局代码中，变量i成了一个全局变量，而函数中的i是放在本地变量表中，所以在全局变量表中查找变量就慢很多。如果你在main函数中声明globali那么效率也就下来了。原因是，本地变量是存在一个数组中（直到），用一个整型常量去访问，而全局变量存在一个dictionary中，查询很慢。（注：在C/C++中，这个不是一个问题）第三个例子为什么排好序的数据在遍历时会更快？（来源StackOverflow）参看如下C/C++的代码：如果你的data数组是排好序的，那么性能是1.93s，如果没有排序，性能为11.54秒。差5倍多。无论是C/C++/Java，或是别的什么语言都基本上一样。这个问题的原因是—— branchprediction （分支预判）伟大的stackoverflow给了一个非常不错的解释。考虑我们一个铁路分叉，当我们的列车来的时候，扳道员知道分个分叉通往哪，但不知道这个列车要去哪儿，司机知道要去哪，但是不知道走哪条分叉。所以，我们需要让列车停下来，然后司机和扳道员沟通一下。这样的性能太差了。所以，我们可以优化一下，那就是猜，我们至少有50%的概率猜对，如果猜对了，火车行驶性能巨高，猜错了，就得让火车退回来。如果我猜对的概率高，那么，我们的性能就会高，否则老是猜错了，性能就很差。ImagebyMecanismo,fromWikimediaCommons:http://commons.wikimedia.org/wiki/File:Entroncamento_do_Transpraia.JPG我们的if-else就像这个铁路分叉一样，下面红箭头所指的就是搬道器。那么，我们的搬道器是怎么预判的呢？就是使用过去的历史数据，如果历史数据有90%以上的走左边，那么就走左边。所以，我们排好序的数据就更容易猜得对。排好序的PythonT=走分支（条件表达式为true）N=不走分支(条件表达式为false)data[]=0,1,2,3,4,...126,127,128,129,130,...250,251,252,...branch=NNNNN...NNTTT...TTT...=NNNNNNNNNNNN...NNNNNNNTTTTTTTTT...TTTTTTTTTT(easytopredict)1234567T=走分支（条件表达式为true）N=不走分支(条件表达式为false) data[]=0,1,2,3,4,...126,127,128,129,130,...250,251,252,...branch=N  N  N  N  N  ...  N    N    T    T    T  ...  T    T    T  ... =NNNNNNNNNNNN...NNNNNNNTTTTTTTTT...TTTTTTTTTT  (easytopredict)从上面我们可以看到，排好序的数据更容易预测分支。未排序的Pythondata[]=226,185,125,158,198,144,217,79,202,118,14,150,177,182,133,...branch=T,T,N,T,T,T,T,N,T,N,N,T,T,T,N...=TTNTTTTNTNNTTTN...(completelyrandom-hardtopredict)1234data[]=226,185,125,158,198,144,217,79,202,118,  14,150,177,182,133,...branch=  T,  T,  N,  T,  T,  T,  T,  N,  T,  N,  N,  T,  T,  T,  N  ... =TTNTTTTNTNNTTTN...  (completelyrandom-hardtopredict)对此，那我们怎么办？我们需要在这种循环中除去if-else语句。比如：我们把条件语句：Pythonif(data[j]&gt;=128)sum+=data[j];12if(data[j]&gt;=128)sum+=data[j];变成：Pythonintt=(data[j]-128)&gt;&gt;31;sum+=~t&amp;data[j];12intt=(data[j]-128)&gt;&gt;31;sum+=~t&amp;data[j];“没有分叉”的性能基本上和“排好序有分支”一个样，无论是C/C++，还是Java。注：在GCC下，如果你使用 -O3 or -ftree-vectorize 编译参数，GCC会帮你优化分叉语句为无分叉语句。VC++2010没有这个功能。最后，推荐大家一个网站——GoogleSpeed，网站上的有一些教程告诉你如何写出更快的Web程序。（全文完）1赞1收藏评论"], "art_create_time": ["2012/07/14"], "art_title": ["陈皓：代码执行的效率"], "art_url": ["http://python.jobbole.com/23773/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2012/07/Code-execution-efficiency4.jpg"]},
{"art_content": ["自从好奇号火星车在8月6日13点31分（北京时间）着陆火星后，这几天有关它的消息媒体一直有报道。本文将综合整理网上讨论“好奇号”引出的一些软硬件信息。“好奇号”火星车简介“好奇号”火星车（Curiosity），是美国国家航空航天局迄今(2012年)最昂贵的火星探测项目。于2011年11月26日15点02分发射，在2012年8月6日中午13点31分（北京时间）降落火星。这辆探测车比2004年登陆的“机遇号”火星探测器和“精神号”火星探测器大得多，携带更多先进科学仪器。预计将运作至少一个火星年（约2个地球年），调查火星以前或现在存在生命的可能性。（摘自互动百科） “好奇号”的“大脑”硬件与2004年发射的上一代漫游车“精神号”和“机遇号”有何差别？一个显而易见的差异是它们彼此的硬件：“精神号”和“机遇号”包含3MBEEPROM，128MB内存和256MB闪存，处理器是RAD6000（110万个晶体管），运算速度35MIPS；“好奇号”的RoverComputeElement包含两套相同的计算机系统，一个发生故障后另一个会自动配置接管，它包含256KBEEPROM，256MB内存，2GB闪存。抗辐射处理器是BAE系统公司的 RAD750 (1040万晶体管，核心频率110到200MHz)，基于IBM的PowerPC750设计，速度400MIPS。（摘自 Solidot ，英文全文：extremetech。NASA官网也有简要介绍。） “好奇号”的软件信息？？？关于这方面，似乎没看到过NASA官方或其他媒体的正式消息。（注：可能有，但笔者没看到。）今年6月底有一则简讯提到：“50万行代码决定“好奇号”登陆火星最后惊心动魄的7分钟。”但其中并未说这50万行代码都是哪种编程语言。在“好奇号”着陆火星后，StackExchange上有个讨论帖：WhatistheMarsCuriosityRover’ssoftwarebuiltin? TheMarsCuriosityroverhaslandedsuccessfully,andoneofthepromovideos“7minutesofterror”bragsabouttherebeing500,000linesofcode.It’sacomplicatedproblem,nodoubt.Butthatisalotofcode,surelytherewasaprettybigprogrammingeffortbehindit.Doesanyoneknowanythingaboutthisproject?Icanonlyimagineit’ssomekindofembeddedC. 在这个讨论帖的回答中，WorldEngineer 回复说：It’srunning2.5millionlinesofConaRAD750processormanufacturedbyBAE.TheJPLhasabitmoreinformationbutIdosuspectmanyofthedetailsarenotpublicized.ItdoesappearthatthetestingscriptswerewritteninPython.“好奇号”在BAE系统公司制造的RAD750处理器上运行着250万行C代码（基于一份2009年的PDF文档）。NASA喷气推进实验室（JPL）上有些信息，但我怀疑很多细节信息都没有公布。“好奇号”的测试脚本似乎是用Python编写的。 根据WorldEngineer提到的那份来自喷气推进实验室（JPL）的文档《MonitoringtheExecutionofSpaceCraftFlightSoftware|飞船飞行软件执行监控》（注意文档页眉上的Copyright2009年字样），其中提到了“MarsScienceLaboratory（火星科学实验室）任务”当时的一些基本信息：•programmerteamof30 |有30位程序员•testingteamof10+people|测试团队大于10人•programminglanguageisC,2.5MLOC| 250万行C代码 在StackExchange那个讨论帖的回答中，NateParsons 提供了另外一些信息：（下面摘译了部分回复，另外NateParsons 特别强调他不是JPL员工）ThecodeisbasedonthatofMER(SpiritandOpportunity),whichwerebasedoffoftheirfirstlander,MPF(Sojourner).It’s3.5millionlinesofC(muchofitautogenerated),runningonaRA50processormanufacturedbyBAEandtheVxWorksOperatingsystem.Overamillionlineswerehandcoded.“好奇号”的代码基于MER（“精神号”和“机遇号”），MER都是基于第一代登陆器MPF（“索杰纳号”）。有350万行C代码（大部分是自动生成的），运行在BAE公司制造的RA50处理器和VxWorks操作系统。超过1百万行代码是手写。Thecodeisimplementedas150separatemodules,eachperformingadifferentfunction.HighlycoupledmodulesareorganizedintoComponentsthatabstractthemodulestheycontain,and“specifyeitheraspecificfunction,activity,orbehavior.”Thesecomponentsarefutherorganizedintolayers,andthereare“nomorethan10top-levelcomponents.”整个代码由150个独立模块实现，每个模块执行不同的功能。高度耦合的模块组织成部件（提取他们所包含的模块，给各个模块指定具体的功能、动作或行为）。这些部件再进一步组织分层，顶层部件不超过10个。Source:KeynotetalkbyBenjaminCichyat2010WorkshoponSpacecraftFlightSoftware(FSW-10),slides,audio,andvideo(startswithmissionoverview,architecturediscussionatslide80).来源：BenjaminCichy 在2010年飞船飞行软件研讨会（FSW-10）上的演讲。（开始是任务概述，架构讨论从第80页开始） 译注：① BenjaminCichy 是喷气推进实验室（JPL）的高级软件系统工程师，于2002年加入JPL；②飞船飞行软件研讨会（FSW）由美国航天工业总公司主办，喷气推进实验室（JPL）和约翰·霍普金斯大学应用物理实验室协办。 注意，上面这两个回答并不是NASA官方发布的消息，只是根据以前的资料（2009年和2010年）推断得出的。 补充信息：① 《（NASA）JPLInstitutionalCodingStandardfortheCProgrammingLanguage|喷气推进实验室之C语言编码标准》33条规则概要见附图。微盘下载：http://t.cn/zWOyWu7②最后可以回顾一下“好奇号”火星车的全程登陆模拟动画：[youkuid=”XMzI2ODg5Mjk2″w=”500″h=”400″] 文章整理：伯乐在线–黄利民文章链接：http://blog.jobbole.com/25037/【如需转载，请标注并保留原文链接、译文链接和译者等信息，谢谢合作！】1赞收藏评论关于作者：黄利民伯乐在线联合发起人，关注IT和互联网。个人主页·我的文章·99·"], "art_create_time": ["2012/08/08"], "art_title": ["好奇号火星车的一些计算机软硬件信息"], "art_url": ["http://python.jobbole.com/25037/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2012/08/curiosity-rover-in-desert.jpg"]},
{"art_content": ["我最近读到一遍文章其主要关注点是在Python社区，讲的是为什么Python应用如此丑陋？尽管某些情况下他的观点是正确的，但是对于他问的这个问题“亲爱的Python，你为何如此丑陋”真是荒谬至极。他所叙述的每个假设和比对显得非常愚蠢，写那边文章的人视乎对在Python之上构建的博客和Web应用的设计水平完全没有深刻印象。为了揭穿真相，根据他说的，我收集了一系列由Python实现的应用，这些应用绝对是经过精心设计的。不要基于我的HTML布局来批判，真正的内容在服务端——MahdiYusufInstagram这个“小”公司最近卖了10亿美金。它如此受大众欢迎很重要的一个因素之一就是他的优美，Instagram大量使用了Python。Pinterest 另一个精美的应用，确定不是用Python写的吗？错了，它就是用Python写的。WashingtonPost新闻站点虽不是最具创新性的设计，但是它对任何词的定义并不丑陋.EveryBlockEveryBlock由Django的联合作者(AdrianHolovaty)创建，丑吗？一点都不觉得。Mozilla至今使用Python最大的站点之一，Mozillaz在设计和创新上都有很好的造诣Courtside这个应用是我和我团队为DjangoDash所写，尽管我不能拿出任何很好的设计，但是我们可以花时间去实践。YoutubeDouban （译者补个国内的）没人不知道豆瓣吧，阿北是这样说的：“从个人喜好来说，python更简洁，和我口味。python用缩位不知省去了我多少时间。我以前{};敲了十几年了，现在想起来真是不堪回首。”还需要我继续介绍吗？我想Path也是在他们的web应用中大量使用的Python，这个产品在设计和创新都得到了赞许。所以，那些发表声明说用特定的语言写网站很丑，这是没有任何意义的。Python社区充满活力和极多的愿望及利益，而不仅仅是那些漂亮的东西。有个说法是设计师让网站看起来漂亮、简洁，这并不是后端工程师能用语言可以编写的。如果你是程序员，又想学习设计的话，你应该看看这本书《 DesignforHackers:ReverseEngineeringBeauty》。 相关阅读：非典型性吐槽：亲爱的Python，你为何如此丑陋原文：mahdiyusuf  编译：伯乐在线–刘志军【如需转载，请标注并保留原文链接、译文链接和译者等信息，谢谢合作！】1赞1收藏1评论关于作者：刘志军个人博客：http://foofish.net，微信公众号：Python之禅（ID：VTtalk）个人主页·我的文章·45·"], "art_create_time": ["2012/08/14"], "art_title": ["Python也可以很美"], "art_url": ["http://python.jobbole.com/20351/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2012/05/instagram.jpg"]},
{"art_content": ["本文由伯乐在线-刘志军翻译。未经许可，禁止转载！英文出处：DavidSinsky。欢迎加入翻译组。对于绝大多数非程序员来说，学习如何编程是一项不可能完成而且令人畏惧的任务，好在有大量免费的在线资源，当然自学成才没那么容易。我开始学习写代码是在今年的早些时候，这里我可以分享一下自己的一些经验，构建一套你自己的原型没有想象的那么困难。事实上，如果你在两个月內不落下一天，拥有一个完备的原型是完全可行的。下面，我将罗列出一个简单的学习路线：在八个周末內软件开发从零到拥有一个工作原型。 把web栈介绍给自己听（10分钟）陌生术语的存在使得任何主题似乎比实际上的更加混乱。Yipit的创始人/CEOVinVacanti做了一个很好的概述，关于你将要熟悉的语言中术语的理解。 掌握入门级的Python和通用编程技巧（1个周末）笨方法学Python：别在乎这个标题，简明的格式使学习基本的概念非常简单，很多章节所花时间还不到十分钟。然而，我发现有些高级课程并不好，所以我建议42课后的课程不必往下学习了。GooglePython课程：阅读笔记，看视频，在不看答案的情况下完成所有相关练习。挣扎于不断出错的练习是最好的学习经验，如果我只是看着答案学习的话，远没有这么多收获，期间我尽力说服自己要理解概念。上面这两个资源有些部分是交叉的，有些是互补的。我建议同时看这两份资源的开头部分再来决定自己更喜欢哪种。一旦完成了其中的一个任务，再略读另外一份，查找那些还不是完全理解的概念，并从中获得一些额外的习题练习的机会。 理解入门级的Django（1个周末）：学习Dgango教程删除所有代码重新学习该教程第一次学习该教程完全按照说明一步一步操作而不理解每个步骤的原理，因为这对我来说完全是陌生的。第二遍我不再关注概念的新奇，而是把精力放在理解每个部件是如何一起工作的。 深入理解Python及通用编程概念（2-4个周末）：Udacity的CS课程：Udacity的课程一半分为7个课时（每个课时2-3小时）你可以按照自己的进度来学习。（我超级喜欢Udacity的教学方式，建议中级编程班或web开发班能跟进者两个月的课程）MIT的CS课程的第一单元：教的很棒，老师也出乎意料的平易近人。同样的我会分别看看，哪个才是你最喜欢的。 实践构建简单的web应用（1个周末）：根据例子用Django做些练习，这些练习仍然提供了一些指导以至于我觉得是一种好的方式去开始学习。 建立原型（1个周末）：用一个周末建立原型？是的，你会很难为情成什么样子呢（至少我是的），但是这就是全部。就是这样，8个周末（或更少）你已经从学会从零到能够正常运作的原型了，事实上事情并非如此艰巨。 下一步：很明显不用说，在建立简单原型上需要的知识和对一个真正合格的软件工程师所需要的知识和经验有着巨大的差别的。如果你想学习关于构建大规模的web应用程序，想获得专业的web开发经验，在快速增长的创业公司如Yipit是一个伟大的下一步。如果你很聪明，能努力工作，富有创造惊艳的消费者web体验的热情，马上给我发邮件吧，jobs@yipit.com，我们一直在寻找牛人加入我们的团队。ps:下面是一些可能有用的资源，如果你有其他建议，请在评论中留下你的记录。Stanford’sintroCScoursesStanford’siPhonedevelopmentclassHarvard’sintroCScourseCodecademy打赏支持我翻译更多好文章，谢谢！打赏译者打赏支持我翻译更多好文章，谢谢！1赞1收藏评论关于作者：刘志军个人博客：http://foofish.net，微信公众号：Python之禅（ID：VTtalk）个人主页·我的文章·45·"], "art_create_time": ["2012/09/08"], "art_title": ["我是如何在8周内自学编程的"], "art_url": ["http://python.jobbole.com/25858/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2012/08/computer_programming.jpg"]},
{"art_content": ["本文由伯乐在线-唐尤华翻译。未经许可，禁止转载！英文出处：DavidN.Welton。欢迎加入翻译组。事实上，我并非不能胜任——几年中我已经开发了很多开源软件。然而，在某种意义上，这又是事实：只有全职开发开源软件的程序员中的佼佼者才能得到报酬，而我不是。LinusTorvalds（Linux之父）是其中之一，还有GuidovanRossum（Python编程语言创始人）。尽管GuidovanRossum只在开源软件上花了部分时间，没有将全部时间用在Python开发上。考虑一下现实。Python已经成为了非常流行的编程语言，被很多公司和个人采用，并从中获益匪浅。但语言发明者甚至没有将全部时间投入其中。这只是其中一例——也许Guido很享受在Phython之外的时间为Google工作这样的生活方式——但是我认为这从总体上代表了开源软件的现状。就我而言：我写了很多小段的开源代码并得到了大家的认可。一些人甚至在Hecl的基础上开发了商用产品。但我还是不能够胜任全职开发开源软件——我不是那种睿智且知名的程序员，他们在开发免费产品的同时有人为其支付薪水。然而，我是一名优秀的私有软件开发者，在寻找需要为我支付报酬的项目上我从未遇到太多的麻烦。为什么会是这样呢？因为私有软件项目能够很快让资金回笼。如果人们喜欢这个软件并为其买单，公司可以用这些收入为开发者支付报酬。而尽管上百万人使用开源软件并从中得到价值，但开源软件的开发者不能从中得到现金回报，他或她不能以此购买食物或支付房租，所以，我能够很好地编程并贡献更多开源代码，但我还是要为私有软件编写代码，因为这能够支付账单。很明显，我会尽可能在这个过程中使用开源软件，并尽可能地回馈开源社区，但是“秘制调味料（商业秘密）”仍然是需要保守的。只是这双手不能创造更多的开源软件了。我知道我不是一个人，换句话说——很多人在开源世界之外为私有软件项目工作，但是很少有人能够从全职开源工作中得到报酬。所以，当我听到人们将版权作为开源软件的反例时，我感到有些怒不可遏。软件版权的保护和执行是一个复杂的争论，我不打算在这里展开讨论。这里我要指出的是“事情并不只是看到的那样”。当然，开源软件的确存在。但是，需要多少资金才能支持开源软件正常运转呢？有多少开源软件因为没有必要的时间而一直只是开发者脑中的设想呢？最近几年，尽管有大幅改进，人们还是经常批评“Linux桌面系统”。恩，如果有更多人能够从“枯燥的劳动”中得到报酬，比如可用性测试，那么Linux的进步又将如何呢？Ubuntu和Redhat已经开始为一些做类似工作的人支付报酬，但在微软和苹果公司又有多少人在从事这样的工作呢？这并不是否认开源软件“行不通”或者其他的胡说八道。开源当然运作的很好，当然只有在以代码而不是金钱为货币时，开源才能真正的发扬光大。开发者能够给与开源项目很多回馈，比如代码、bug报告、建议、文档等等，这让他们参与的项目变得更好。然而，开源在规模较小、快速变化以及以客户为中心的产品上表现得并不尽如人意。我猜，99%的iPhone用户很少关心他们使用的应用程序源代码，而恰恰相反，绝大部分的Emacs用户至少写过几行Elisp代码。在任何情况下，问题不在于打败开源软件，然而另一种反对“知识产权”的观点是开源软件本身已经“证明”了“软件生产过程都是一样的”。是的，可能它们是一样的，但是只对用户较少的情况下成立。毕竟，我们中的大多数都不能胜任开发开源软件。打赏支持我翻译更多好文章，谢谢！打赏译者打赏支持我翻译更多好文章，谢谢！任选一种支付方式1赞收藏评论关于作者：唐尤华做自己喜欢的，编程、喝茶、看世界个人主页·我的文章·18·"], "art_create_time": ["2012/04/16"], "art_title": ["我不能胜任开发开源软件"], "art_url": ["http://python.jobbole.com/16387/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2012/04/contribute-opensource.jpg"]},
{"art_content": ["英文原文：Github，翻译：oschina使用Python内建的 defaultdict 方法可以轻松定义一个树的数据结构。简单的说树也可以是一个字典数据结构Pythondeftree():returndefaultdict(tree)1deftree():returndefaultdict(tree)这就是全部，就一行代码。如果你继续下面的代码，需要先引入Pythonfromcollectionsimportdefaultdict1fromcollectionsimportdefaultdict实例JSON-esque现在我们创建一个 JSON-esque嵌套字典无需显式的创建子字典：Pythonusers=tree()users['harold']['username']='hrldcpr'users['handler']['username']='matthandlersux'123users=tree()users['harold']['username']='hrldcpr'users['handler']['username']='matthandlersux'然后可通过 <code>print(json.dumps(users))</code> 来打印JSON数据，结果如下：Python{\"harold\":{\"username\":\"hrldcpr\"},\"handler\":{\"username\":\"matthandlersux\"}}1{\"harold\":{\"username\":\"hrldcpr\"},\"handler\":{\"username\":\"matthandlersux\"}}无需赋值我们不需要通过赋值就可以创建结构：Pythontaxonomy=tree()taxonomy['Animalia']['Chordata']['Mammalia']['Carnivora']['Felidae']['Felis']['cat']taxonomy['Animalia']['Chordata']['Mammalia']['Carnivora']['Felidae']['Panthera']['lion']taxonomy['Animalia']['Chordata']['Mammalia']['Carnivora']['Canidae']['Canis']['dog']taxonomy['Animalia']['Chordata']['Mammalia']['Carnivora']['Canidae']['Canis']['coyote']taxonomy['Plantae']['Solanales']['Solanaceae']['Solanum']['tomato']taxonomy['Plantae']['Solanales']['Solanaceae']['Solanum']['potato']taxonomy['Plantae']['Solanales']['Convolvulaceae']['Ipomoea']['sweetpotato']12345678taxonomy=tree()taxonomy['Animalia']['Chordata']['Mammalia']['Carnivora']['Felidae']['Felis']['cat']taxonomy['Animalia']['Chordata']['Mammalia']['Carnivora']['Felidae']['Panthera']['lion']taxonomy['Animalia']['Chordata']['Mammalia']['Carnivora']['Canidae']['Canis']['dog']taxonomy['Animalia']['Chordata']['Mammalia']['Carnivora']['Canidae']['Canis']['coyote']taxonomy['Plantae']['Solanales']['Solanaceae']['Solanum']['tomato']taxonomy['Plantae']['Solanales']['Solanaceae']['Solanum']['potato']taxonomy['Plantae']['Solanales']['Convolvulaceae']['Ipomoea']['sweetpotato']要打印有好的信息，需要转成标准的字典对象：Pythondefdicts(t):return{k:dicts(t[k])forkint}1defdicts(t):return{k:dicts(t[k])forkint}现在可通过 pprint(dicts(taxonomy))进行打印了:Python{'Animalia':{'Chordata':{'Mammalia':{'Carnivora':{'Canidae':{'Canis':{'coyote':{},'dog':{}}},'Felidae':{'Felis':{'cat':{}},'Panthera':{'lion':{}}}}}}},'Plantae':{'Solanales':{'Convolvulaceae':{'Ipomoea':{'sweetpotato':{}}},'Solanaceae':{'Solanum':{'potato':{},'tomato':{}}}}}}1234567{'Animalia':{'Chordata':{'Mammalia':{'Carnivora':{'Canidae':{'Canis':{'coyote':{},                                                                            'dog':{}}},                                                      'Felidae':{'Felis':{'cat':{}},                                                                  'Panthera':{'lion':{}}}}}}},'Plantae':{'Solanales':{'Convolvulaceae':{'Ipomoea':{'sweetpotato':{}}},                          'Solanaceae':{'Solanum':{'potato':{},                                                      'tomato':{}}}}}}子结构也被当作是字典对象了，而叶子节点是一个空的字典对象迭代可以使用有趣的方法对树进行迭代。例如我们解析一个动物的列表并添加到之前定义的taxonomy中，我们可以使用如下代码：Pythonadd(taxonomy,'Animalia,Chordata,Mammalia,Cetacea,Balaenopteridae,Balaenoptera,bluewhale'.split(','))12add(taxonomy,    'Animalia,Chordata,Mammalia,Cetacea,Balaenopteridae,Balaenoptera,bluewhale'.split(','))简化实现：Pythondefadd(t,keys):forkeyinkeys:t=t[key]123defadd(t,keys):  forkeyinkeys:    t=t[key]我们仍然无需赋值：Python{'Animalia':{'Chordata':{'Mammalia':{'Carnivora':{'Canidae':{'Canis':{'coyote':{},'dog':{}}},'Felidae':{'Felis':{'cat':{}},'Panthera':{'lion':{}}}},'Cetacea':{'Balaenopteridae':{'Balaenoptera':{'bluewhale':{}}}}}}},'Plantae':{'Solanales':{'Convolvulaceae':{'Ipomoea':{'sweetpotato':{}}},'Solanaceae':{'Solanum':{'potato':{},'tomato':{}}}}}}12345678{'Animalia':{'Chordata':{'Mammalia':{'Carnivora':{'Canidae':{'Canis':{'coyote':{},                                                                            'dog':{}}},                                                      'Felidae':{'Felis':{'cat':{}},                                                                  'Panthera':{'lion':{}}}},                                        'Cetacea':{'Balaenopteridae':{'Balaenoptera':{'bluewhale':{}}}}}}},'Plantae':{'Solanales':{'Convolvulaceae':{'Ipomoea':{'sweetpotato':{}}},                          'Solanaceae':{'Solanum':{'potato':{},                                                      'tomato':{}}}}}}结论上面提及的这些可能用处不大，只是做了一些有意思的代码。如果你喜欢Python的话，把这个当成是乐趣来理解。1赞1收藏1评论"], "art_create_time": ["2012/04/24"], "art_title": ["一行 Python 代码搞定一棵树"], "art_url": ["http://python.jobbole.com/18159/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2012/02/python-logo.png"]},
{"art_content": ["亲爱的Python，有些事情已经困扰我一段时间了，恩，是这样的，其实你…有点丑。瞧，你的内在是如此美丽：Python是一种优美的编程语言，而且Python社区也是开放和受欢迎的。但Python的一些相关资源已经丑到一定程度了，足以影响其可用性和接受程度了。这对整个Python社区都是有害的。一些文档和教程常常难以导航，用Python构建起来的产品压根儿都没有思考过如何做设计（这里不是指程序的设计，而是指UI），用Python写的博客程序只会帮助产出更多丑陋的有关Python的博客。Pythonist（对Python程序员的尊称）根本不关心产品的演示效果。让我们来对一些Python和Ruby的站点就可用性和UI设计的角度做一下比较。同Python相比，Ruby是一个很好的参照物，因为它们有着相似的年纪，而且在编程语言的生态系统中都占据相似的地位。以上分别是Python和Ruby的在线交互式学习网站的对比。learnpython.org其实一点也不丑，只是TryRuby真的是更加完美。很明显，TryRuby是由专业的设计师打造的，而learnpython.org的设计只是由某个程序员拼凑而成，最后再加上一些广告就算大功告成了。对比一下，你更愿意用哪个教程呢？以上分别是Django和RubyonRails的主页，它们都是流行的Web框架。Django的主页规划的相当稀烂，整个页面就是一堆杂乱的链接，而且并没有任何明确的重点。相比之下，RubyonRails的主页就做的很好，能够很好的引导人们接触Rails框架，并将人们导向站点的不同区域。以上是Django的共同创始人之一的AdrianHolovaty同RubyonRails的创始人DavidHeinemeierHansson的主页对比。两个主页都规划的很好，但一眼看去很明显就会发现谁的设计更加专业。云托管服务：GoogleAppEngine对比Heroku。再一次，Ruby这边显得更加性感。Python和Ruby的博客程序对比。Python的博客设计显得很平淡，不吸引人，而Ruby博客的外观设计则很给力。谁不喜欢一个可爱的小章鱼呢？（OCTOPRESS的标志）以上是在Google中搜索“pythonconsulting”和“rubyconsulting”的结果对比，我们取各自结果的前几名来考察。Python得到的结果看起来已经N年没有更新过了。 Ruby看起来又赢了，也许是因为Ruby更聚焦于Web编程？这很可能是因为Webapp比一般性的主题比如说科学有着更好看的网页外观。要消除这些不公平，那么下面的图取自Google搜索“djangoconsulting”以及“rubyonrailsconsulting”的结果。好吧，无所谓了，Python仍然是个丑小鸭。这个比较并不科学——你可以试着选择一些不同集合的站点以此获得完全相反的结果。但我认为我贴出来的截图还是很有代表性的。我也相信，大多数人只要花一点点时间同时在Python和Ruby社区中待过的话，都会同意这个观点——用Python做的网站比Ruby要丑。 怎么会这样呢？要么是因为Pythonist中的设计师不像Rubyist中那么常见，Pythonist通常缺乏一些设计才能。又或者是因为Pythonist根本不在乎设计，他们不想为此花费时间去做。我自己也不确定到底是因为什么。 这很重要吗？是的！这东西非常重要。这并不是说用Python做的网站就都很丑（尽管事实上它们确实很丑）。关键在于这种丑陋使得网站难以导航，难以使用。没有人会觉得丑陋好，当有更好的选择时，没有人会愿意使用一个丑陋的产品。没人愿意雇佣一个只能构建丑陋webapp的程序员。丑陋，潦草，这都是不专业的表现。这一切都会对Python社区的繁荣造成伤害。 我们该如何改进？如果你是一名Python程序员（或者任何一种语言的使用者，反正就是对设计不太感冒），多花些时间学习一些基本的设计原则吧。试试这几本书吧：《DesignforHackers》、《BootstrappingDesign》或者《StepbyStepUIDesign》。【注：也可以看看这篇访谈attractingdesignerstoyourproject (需梯子)】然后试着和设计师交朋友，知道什么时候应该求助于他们。 英文原文：JessJohnson   编译：伯乐在线— 陈舸【如需转载，请标注并保留原文链接、译文链接和译者等信息，谢谢合作！】1赞收藏11评论关于作者：bigship简介还没来得及写:）个人主页·我的文章·9"], "art_create_time": ["2012/05/07"], "art_title": ["非典型性吐槽：亲爱的Python，你为何如此丑陋"], "art_url": ["http://python.jobbole.com/18629/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2012/05/py-vs-ruby.jpg"]},
{"art_content": ["来源：linux大棚【简介】Python是一种动态解释型的编程语言。Python可以在Windows、UNIX、MAC等多种操作系统上使用，也可以在Java、.NET开发平台上使用。【特点】1Python使用C语言开发，但是Python不再有C语言中的指针等复杂的数据类型。2Python具有很强的面向对象特性，而且简化了面向对象的实现。它消除了保护类型、抽象类、接口等面向对象的元素。3Python代码块使用空格或制表符缩进的方式分隔代码。4Python仅有31个保留字，而且没有分号、begin、end等标记。5Python是强类型语言，变量创建后会对应一种数据类型，出现在统一表达式中的不同类型的变量需要做类型转换。【搭建开发环境】1可以到www.python.org下载安装包，然后通过configure、make、makeinstall进行安装。2也可以到www.activestate.com去下载ActivePython组件包。(ActivePython是对Python核心和常用模块的二进制包装，它是ActiveState公司发布的Python开发环境。ActivePython使得Python的安装更加容易，并且可以应用在各种操作系统上。ActivePython包含了一些常用的Python扩展，以及Windows环境的编程接口)。对ActivePython来说，如果你是windows用户，下载msi包安装即可；如果你是Unix用户，下载tar.gz包直接解压即可。3Python的IDE，包括PythonWin、Eclipse+PyDev插件、Komodo、EditPlus【版本】python2与python3是目前主要的两个版本。如下两种情况下，建议使用python2：1你无法完全控制你即将部署的环境时；2你需要使用一些特定的第三方包或扩展时；python3是官方推荐的且是未来全力支持的版本，目前很多功能提升仅在python3版本上进行。【helloworld】1创建hello.py2编写程序：Pythonif__name__==\\'__main__\\':print\"helloword\"12if__name__==\\'__main__\\':    print\"helloword\"3运行程序：Pythonpython./hello.py1python./hello.py【注释】1无论是行注释还是段注释，均以#加一个空格来注释。2如果需要在代码中使用中文注释，必须在python文件的最前面加上如下注释说明：Python#-*-coding:UTF-8-*-1#-*-coding:UTF-8-*-3如下注释用于指定解释器Python#!/usr/bin/python1#!/usr/bin/python【文件类型】1Python的文件类型分为3种，即源代码、字节代码和优化代码。这些都可以直接运行，不需要进行编译或连接。2源代码以.py为扩展名，由python来负责解释；3源文件经过编译后生成扩展名为.pyc的文件，即编译过的字节文件。这种文件不能使用文本编辑器修改。pyc文件是和平台无关的，可以在大部分操作系统上运行。如下语句可以用来产生pyc文件：Pythonimportpy_compilepy_compile.compile(‘hello.py’)12importpy_compilepy_compile.compile(‘hello.py’)4经过优化的源文件会以.pyo为后缀，即优化代码。它也不能直接用文本编辑器修改，如下命令可用来生成pyo文件：Pythonpython-O-mpy_compliehello.py1python-O-mpy_compliehello.py【变量】1python中的变量不需要声明，变量的赋值操作即使变量声明和定义的过程。2python中一次新的赋值，将创建一个新的变量。即使变量的名称相同，变量的标识并不相同。用id()函数可以获取变量标识：Pythonx=1printid(x)x=2printid(x)1234x=1printid(x)x=2printid(x)3如果变量没有赋值，则python认为该变量不存在4在函数之外定义的变量都可以称为全局变量。全局变量可以被文件内部的任何函数和外部文件访问。5全局变量建议在文件的开头定义。6也可以把全局变量放到一个专门的文件中，然后通过import来引用：gl.py文件中内容如下：Python_a=1_b=212_a=1_b=2use_global.py中引用全局变量：Pythonimportgldeffun():printgl._aprintgl._bfun()12345importgldeffun():  printgl._a  printgl._bfun()【常量】python中没有提供定义常量的保留字。可以自己定义一个常量类来实现常量的功能。Pythonclass_const:classConstError(TypeError):passdef__setattr__(self,name,vlaue):ifself.__dict__.has_key(name):raiseself.ConstError,“Can’trebindconst(%s)”%nameself.__dict__[name]=valueimportsyssys.modules[__name__]=_const()12345678class_const:  classConstError(TypeError):pass    def__setattr__(self,name,vlaue):      ifself.__dict__.has_key(name):        raiseself.ConstError,“Can’trebindconst(%s)”%name        self.__dict__[name]=valueimportsyssys.modules[__name__]=_const()【数据类型】1python的数字类型分为整型、长整型、浮点型、布尔型、复数类型。2python没有字符类型3python内部没有普通类型，任何类型都是对象。4如果需要查看变量的类型，可以使用type类，该类可以返回变量的类型或创建一个新的类型。5python有3种表示字符串类型的方式，即单引号、双引号、三引号。单引号和双引号的作用是相同的。python程序员更喜欢用单引号，C/Java程序员则习惯使用双引号表示字符串。三引号中可以输入单引号、双引号或换行等字符。【运算符和表达式】1python不支持自增运算符和自减运算符。例如i++/i-是错误的，但i+=1是可以的。21/2在python2.5之前会等于0.5，在python2.5之后会等于0。3不等于为!=或<>4等于用==表示5逻辑表达式中and表示逻辑与，or表示逻辑或，not表示逻辑非【控制语句】1条件语句：Pythonif(表达式):语句1else:语句21234if(表达式):    语句1else:    语句22条件语句：Pythonif(表达式):语句1elif(表达式):语句2…elif(表达式):语句nelse:语句m123456789if(表达式):  语句1elif(表达式):  语句2…elif(表达式):  语句nelse:  语句m3条件嵌套：Pythonif(表达式1):if(表达式2):语句1elif(表达式3):语句2…else:语句3elif(表达式n):…else:…123456789101112if(表达式1):  if(表达式2):    语句1  elif(表达式3):    语句2  …  else:    语句3elif(表达式n):  …else:  …4python本身没有switch语句。5循环语句：Pythonwhile(表达式):…else:…1234while(表达式):  …else:  …6循环语句：Pythonfor变量in集合:…else:…1234for变量in集合:  …else:  …7python不支持类似c的for(i=0;i<5;i++)这样的循环语句，但可以借助range模拟：Pythonforxinrange(0,5,2):printx12forxinrange(0,5,2):    printx【数组相关】1元组(tuple)：python中一种内置的数据结构。元组由不同的元素组成，每个元素可以存储不同类型的数据，如字符串、数字甚至元素。元组是写保护的，即元组创建之后不能再修改。元组往往代表一行数据，而元组中的元素代表不同的数据项。可以把元组看做不可修改的数组。创建元组示例如下：Pythontuple_name=(“apple”,”banana”,”grape”,”orange”)1tuple_name=(“apple”,”banana”,”grape”,”orange”)2列表(list)：列表和元组相似，也由一组元素组成，列表可以实现添加、删除和查找操作，元素的值可以被修改。列表是传统意义上的数组。列表创建示例如下：Pythonlist=[“apple”,”banana”,”grage”,”orange”]1list=[“apple”,”banana”,”grage”,”orange”]可以使用append方法来在尾部追加元素，使用remove来删除元素。3字典(dictionary)：由键-值对组成的集合，字典中的值通过键来引用。键和值之间用冒号隔开，键-值对之间用逗号隔开，并且被包含在一对花括号中。创建示例如下：Pythondict={“a”:”apple”,“b”:”banana”,“g”:”grage”,“o”:”orange”}1dict={“a”:”apple”,“b”:”banana”,“g”:”grage”,“o”:”orange”}4序列：序列是具有索引和切片能力的集合。元组、列表和字符串都属于序列。【函数相关】1python程序由包(package)、模块(module)和函数组成。包是由一系列模块组成的集合。模块是处理某一类问题的函数和类的集合。2包就是一个完成特定任务的工具箱。3包必须含有一个__init__.py文件，它用于标识当前文件夹是一个包。4python的程序是由一个个模块组成的。模块把一组相关的函数或代码组织到一个文件中，一个文件即是一个模块。模块由代码、函数和类组成。导入模块使用import语句。5包的作用是实现程序的重用。6函数是一段可以重复多次调用的代码，函数定义示例如下：Pythondefarithmetic(x,y,operator):result={“+”:x+y,“-“:x-y,“*”:x*y,“/”:x/y}1234567defarithmetic(x,y,operator):  result={      “+”:x+y,      “-“:x-y,      “*”:x*y,      “/”:x/y  }7函数返回值可以用return来控制。【字符串相关】1格式化输出：Pythonformat=”%s%d”%(str1,num)printformat12format=”%s%d”%(str1,num)printformat2用+进行字符串的合并：Pythonstr1=”hello”str2=”world”result=str1+str2123str1=”hello”str2=”world”result=str1+str23字符串截取可以通过索引/切片，也可以通过split函数。4通过切片截取字符串：Pythonword=”world”printword[0:3]12word=”world”printword[0:3]5python使用==和!=来进行字符串比较。如果比较的两个变量的类型不相同，那么结果必然为不同。【文件处理】1简单处理文件：Pythoncontext=”hello,world”f=file(“hello.txt”,’w’)f.write(context);f.close()1234context=”hello,world”f=file(“hello.txt”,’w’)f.write(context);f.close()2读取文件可以使用readline()函数、readlines()函数和read函数。3写入文件可以使用write()、writelines()函数【对象和类】1python用class保留字来定义一个类，类名的首字符要大写。当程序员需要创建的类型不能用简单类型来表示时，就需要定义类，然后利用定义的类创建对象。定义类示例：PythonclassFruit:defgrow(self):print“Fruitgrow”123classFruit:    defgrow(self):      print“Fruitgrow”2当一个对象被创建后，包含了三方面的特性，即对象的句柄、属性和方法。创建对象的方法：Pythonfruit=Fruit()fruit.grow()12fruit=Fruit()fruit.grow()3python没有保护类型的修饰符4类的方法也分为公有方法和私有方法。私有函数不能被该类之外的函数调用，私有的方法也不能被外部的类或函数调用。5python使用函数”staticmethod()“或”@staticmethod“指令的方法把普通的函数转换为静态方法。静态方法相当于全局函数。6python的构造函数名为__init__，析构函数名为__del__7继承的使用方法：PythonclassApple(Fruit):def…12classApple(Fruit):  def…【连接mysql】1用MySQLdb模块操作MySQL数据库非常方便。示例代码如下：Pythonimportos,sysimportMySQLdbtry:connMySQLdb.connect(host=’localhost’,user=’root’,passwd=’’,db=’address’exceptException,e:printesys.exit()cursor=conn.cursor()sql=’insertintoaddress(name,address)values(%s,%s)’value=((“zhangsan”,”haidian”),(“lisi”,”haidian”))trycursor.executemany(sql,values)exceptException,e:printesql=”select*fromaddress”cursor.execute(sql)data=cursor.fetchall()ifdataforxindata:printx[0],x[1]cursor.close()conn.close()12345678910111213141516171819202122importos,sysimportMySQLdbtry:    connMySQLdb.connect(host=’localhost’,user=’root’,passwd=’’,db=’address’exceptException,e:    printe    sys.exit()cursor=conn.cursor()sql=’insertintoaddress(name,address)values(%s,%s)’value=((“zhangsan”,”haidian”),(“lisi”,”haidian”))try    cursor.executemany(sql,values)exceptException,e:    printesql=”select*fromaddress”cursor.execute(sql)data=cursor.fetchall()ifdata    forxindata:        printx[0],x[1]cursor.close()conn.close()谢谢！2赞15收藏11评论"], "art_create_time": ["2012/07/07"], "art_title": ["Python十分钟入门"], "art_url": ["http://python.jobbole.com/23425/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2012/02/python-logo.png"]},
{"art_content": ["Python之父，首位“仁慈独裁者（BenevolentDictatorForLife）”荣誉获得者 GuidovanRossum，从Google跳槽到Dropbox。VanRossum于2005年加入Google。他加盟Dropbox，应对Dropbox会有巨大帮助，因为Dropbox软件就是Python编写的。Dropbox这个云存储创业公司最近迎来了第1亿位用户，平均每天保存超过10亿个文件。另外，VanRossum发推说昨日（7日）是他在Google的最后一天，2013年1月将在Dropbox开始工作。在1991年创造Python后，VanRossum是 BDFL（BenevolentDictatorForLife） 荣誉首位获得者，该荣誉后来授予了其他在计算机科学领域的传奇人物，比如Linux之父LinusTorvalds，ROR框架的开发人DavidHeinemeierHansson。（伯乐在线注：维基百科 BDFL 词条中没有DHH，但TechCrunch的却说是？？？） Dropbox官网昨日已发一文，欢迎GuidovanRossum的加盟。 伯乐在线补充：“仁慈独裁者（BenevolentDictatorForLife）”荣誉获得者名单：（Wikipedia版）●GuidovanRossum，Python之父●LinusTorvalds，Linux之父●Yukihiro“Matz”Matsumoto，Ruby之父●MarkShuttleworth ，早期Debian开发者之一，Ubuntu幕后公司Canonical的创始人。他自称是“自封的仁慈独裁者Self-AppointedBenevolentDictatorforLife（SABDFL）”，Ubuntu社区经常用SABDFL代指他。●AlexandreJulliard，Wine 的维护者●RasmusLerdorf，PHP之父●LarryWall，Perl之父●PatrickVolkerding，Linux发行版Slackware 的创造者。●DanielRobbins，Linux发行版 Gentoo 和 Funtoo 的创造者。他现在还是 Funtoo Linux的首席架构师。  英文：TC&Dropbox &wikipedia，编译：伯乐在线——黄利民译文链接：http://blog.jobbole.com/31146/【如需转载，请在正文中标注并保留原文链接、译文链接和译者等信息，谢谢合作！】1赞收藏评论关于作者：黄利民伯乐在线联合发起人，关注IT和互联网。个人主页·我的文章·99·"], "art_create_time": ["2012/12/08"], "art_title": ["Python之父从Google离职，加入Dropbox"], "art_url": ["http://python.jobbole.com/31146/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2012/12/ungood-guido-van-rossum-python.jpg"]},
{"art_content": ["本文由伯乐在线-bigship翻译。未经许可，禁止转载！英文出处：stackoverflow。欢迎加入翻译组。译注：这是一篇在Stackoverflow上很热的帖子。提问者自称已经掌握了有关PythonOOP编程中的各种概念，但始终觉得元类(metaclass)难以理解。他知道这肯定和自省有关，但仍然觉得不太明白，希望大家可以给出一些实际的例子和代码片段以帮助理解，以及在什么情况下需要进行元编程。于是e-satis同学给出了神一般的回复，该回复获得了985点的赞同点数，更有人评论说这段回复应该加入到Python的官方文档中去。而e-satis同学本人在StackOverflow中的声望积分也高达64271分。以下就是这篇精彩的回复（提示：非常长）类也是对象在理解元类之前，你需要先掌握Python中的类。Python中类的概念借鉴于Smalltalk，这显得有些奇特。在大多数编程语言中，类就是一组用来描述如何生成一个对象的代码段。在Python中这一点仍然成立：Python>>>classObjectCreator(object):…pass…>>>my_object=ObjectCreator()>>>printmy_object<__main__.ObjectCreatorobjectat0x8974f2c>123456>>>classObjectCreator(object):…      pass…>>>my_object=ObjectCreator()>>>printmy_object<__main__.ObjectCreatorobjectat0x8974f2c>但是，Python中的类还远不止如此。类同样也是一种对象。是的，没错，就是对象。只要你使用关键字class，Python解释器在执行的时候就会创建一个对象。下面的代码段：Python>>>classObjectCreator(object):…pass…123>>>classObjectCreator(object):…      pass…将在内存中创建一个对象，名字就是ObjectCreator。这个对象（类）自身拥有创建对象（类实例）的能力，而这就是为什么它是一个类的原因。但是，它的本质仍然是一个对象，于是乎你可以对它做如下的操作：1)  你可以将它赋值给一个变量2)  你可以拷贝它3)  你可以为它增加属性4)  你可以将它作为函数参数进行传递下面是示例：Python>>>printObjectCreator#你可以打印一个类，因为它其实也是一个对象<class'__main__.ObjectCreator'>>>>defecho(o):…printo…>>>echo(ObjectCreator)#你可以将类做为参数传给函数<class'__main__.ObjectCreator'>>>>printhasattr(ObjectCreator,'new_attribute')Fasle>>>ObjectCreator.new_attribute='foo'#你可以为类增加属性>>>printhasattr(ObjectCreator,'new_attribute')True>>>printObjectCreator.new_attributefoo>>>ObjectCreatorMirror=ObjectCreator#你可以将类赋值给一个变量>>>printObjectCreatorMirror()<__main__.ObjectCreatorobjectat0x8997b4c>1234567891011121314151617>>>printObjectCreator    #你可以打印一个类，因为它其实也是一个对象<class'__main__.ObjectCreator'>>>>defecho(o):…      printo…>>>echo(ObjectCreator)                #你可以将类做为参数传给函数<class'__main__.ObjectCreator'>>>>printhasattr(ObjectCreator,'new_attribute')Fasle>>>ObjectCreator.new_attribute='foo'#  你可以为类增加属性>>>printhasattr(ObjectCreator,'new_attribute')True>>>printObjectCreator.new_attributefoo>>>ObjectCreatorMirror=ObjectCreator#你可以将类赋值给一个变量>>>printObjectCreatorMirror()<__main__.ObjectCreatorobjectat0x8997b4c> 动态地创建类因为类也是对象，你可以在运行时动态的创建它们，就像其他任何对象一样。首先，你可以在函数中创建类，使用class关键字即可。Python>>>defchoose_class(name):…ifname=='foo':…classFoo(object):…pass…returnFoo#返回的是类，不是类的实例…else:…classBar(object):…pass…returnBar…>>>MyClass=choose_class('foo')>>>printMyClass#函数返回的是类，不是类的实例<class'__main__'.Foo>>>>printMyClass()#你可以通过这个类创建类实例，也就是对象<__main__.Fooobjectat0x89c6d4c>123456789101112131415>>>defchoose_class(name):…      ifname=='foo':…          classFoo(object):…              pass…          returnFoo    #返回的是类，不是类的实例…      else:…          classBar(object):…              pass…          returnBar…>>>MyClass=choose_class('foo')>>>printMyClass              #函数返回的是类，不是类的实例<class'__main__'.Foo>>>>printMyClass()            #你可以通过这个类创建类实例，也就是对象<__main__.Fooobjectat0x89c6d4c>但这还不够动态，因为你仍然需要自己编写整个类的代码。由于类也是对象，所以它们必须是通过什么东西来生成的才对。当你使用class关键字时，Python解释器自动创建这个对象。但就和Python中的大多数事情一样，Python仍然提供给你手动处理的方法。还记得内建函数type吗？这个古老但强大的函数能够让你知道一个对象的类型是什么，就像这样：Python>>>printtype(1)<type'int'>>>>printtype(\"1\")<type'str'>>>>printtype(ObjectCreator)<type'type'>>>>printtype(ObjectCreator())<class'__main__.ObjectCreator'>12345678>>>printtype(1)<type'int'>>>>printtype(\"1\")<type'str'>>>>printtype(ObjectCreator)<type'type'>>>>printtype(ObjectCreator())<class'__main__.ObjectCreator'>这里，type有一种完全不同的能力，它也能动态的创建类。type可以接受一个类的描述作为参数，然后返回一个类。（我知道，根据传入参数的不同，同一个函数拥有两种完全不同的用法是一件很傻的事情，但这在Python中是为了保持向后兼容性）type可以像这样工作：Pythontype(类名,父类的元组（针对继承的情况，可以为空），包含属性的字典（名称和值）)1type(类名,父类的元组（针对继承的情况，可以为空），包含属性的字典（名称和值）)比如下面的代码：Python>>>classMyShinyClass(object):…pass12>>>classMyShinyClass(object):…      pass可以手动像这样创建：Python>>>MyShinyClass=type('MyShinyClass',(),{})#返回一个类对象>>>printMyShinyClass<class'__main__.MyShinyClass'>>>>printMyShinyClass()#创建一个该类的实例<__main__.MyShinyClassobjectat0x8997cec>12345>>>MyShinyClass=type('MyShinyClass',(),{})  #返回一个类对象>>>printMyShinyClass<class'__main__.MyShinyClass'>>>>printMyShinyClass()  #  创建一个该类的实例<__main__.MyShinyClassobjectat0x8997cec>你会发现我们使用“MyShinyClass”作为类名，并且也可以把它当做一个变量来作为类的引用。类和变量是不同的，这里没有任何理由把事情弄的复杂。type接受一个字典来为类定义属性，因此Python>>>classFoo(object):…bar=True12>>>classFoo(object):…      bar=True可以翻译为：Python>>>Foo=type('Foo',(),{'bar':True})1>>>Foo=type('Foo',(),{'bar':True})并且可以将Foo当成一个普通的类一样使用：Python>>>printFoo<class'__main__.Foo'>>>>printFoo.barTrue>>>f=Foo()>>>printf<__main__.Fooobjectat0x8a9b84c>>>>printf.barTrue123456789>>>printFoo<class'__main__.Foo'>>>>printFoo.barTrue>>>f=Foo()>>>printf<__main__.Fooobjectat0x8a9b84c>>>>printf.barTrue当然，你可以向这个类继承，所以，如下的代码：Python>>>classFooChild(Foo):…pass12>>>classFooChild(Foo):…      pass就可以写成：Python>>>FooChild=type('FooChild',(Foo,),{})>>>printFooChild<class'__main__.FooChild'>>>>printFooChild.bar#bar属性是由Foo继承而来True12345>>>FooChild=type('FooChild',(Foo,),{})>>>printFooChild<class'__main__.FooChild'>>>>printFooChild.bar  #bar属性是由Foo继承而来True最终你会希望为你的类增加方法。只需要定义一个有着恰当签名的函数并将其作为属性赋值就可以了。Python>>>defecho_bar(self):…printself.bar…>>>FooChild=type('FooChild',(Foo,),{'echo_bar':echo_bar})>>>hasattr(Foo,'echo_bar')False>>>hasattr(FooChild,'echo_bar')True>>>my_foo=FooChild()>>>my_foo.echo_bar()True1234567891011>>>defecho_bar(self):…      printself.bar…>>>FooChild=type('FooChild',(Foo,),{'echo_bar':echo_bar})>>>hasattr(Foo,'echo_bar')False>>>hasattr(FooChild,'echo_bar')True>>>my_foo=FooChild()>>>my_foo.echo_bar()True你可以看到，在Python中，类也是对象，你可以动态的创建类。这就是当你使用关键字class时Python在幕后做的事情，而这就是通过元类来实现的。 到底什么是元类（终于到主题了）元类就是用来创建类的“东西”。你创建类就是为了创建类的实例对象，不是吗？但是我们已经学习到了Python中的类也是对象。好吧，元类就是用来创建这些类（对象）的，元类就是类的类，你可以这样理解为：PythonMyClass=MetaClass()MyObject=MyClass()12MyClass=MetaClass()MyObject=MyClass()你已经看到了type可以让你像这样做：PythonMyClass=type('MyClass',(),{})1MyClass=type('MyClass',(),{})这是因为函数type实际上是一个元类。type就是Python在背后用来创建所有类的元类。现在你想知道那为什么type会全部采用小写形式而不是Type呢？好吧，我猜这是为了和str保持一致性，str是用来创建字符串对象的类，而int是用来创建整数对象的类。type就是创建类对象的类。你可以通过检查__class__属性来看到这一点。Python中所有的东西，注意，我是指所有的东西——都是对象。这包括整数、字符串、函数以及类。它们全部都是对象，而且它们都是从一个类创建而来。Python>>>age=35>>>age.__class__<type'int'>>>>name='bob'>>>name.__class__<type'str'>>>>deffoo():pass>>>foo.__class__<type'function'>>>>classBar(object):pass>>>b=Bar()>>>b.__class__<class'__main__.Bar'>12345678910111213>>>age=35>>>age.__class__<type'int'>>>>name='bob'>>>name.__class__<type'str'>>>>deffoo():pass>>>foo.__class__<type'function'>>>>classBar(object):pass>>>b=Bar()>>>b.__class__<class'__main__.Bar'>现在，对于任何一个__class__的__class__属性又是什么呢？Python>>>a.__class__.__class__<type'type'>>>>age.__class__.__class__<type'type'>>>>foo.__class__.__class__<type'type'>>>>b.__class__.__class__<type'type'>12345678>>>a.__class__.__class__<type'type'>>>>age.__class__.__class__<type'type'>>>>foo.__class__.__class__<type'type'>>>>b.__class__.__class__<type'type'>因此，元类就是创建类这种对象的东西。如果你喜欢的话，可以把元类称为“类工厂”（不要和工厂类搞混了:D）type就是Python的内建元类，当然了，你也可以创建自己的元类。 __metaclass__属性你可以在写一个类的时候为其添加__metaclass__属性。PythonclassFoo(object):__metaclass__=something…[…]123classFoo(object):__metaclass__=something…[…]如果你这么做了，Python就会用元类来创建类Foo。小心点，这里面有些技巧。你首先写下classFoo(object)，但是类对象Foo还没有在内存中创建。Python会在类的定义中寻找__metaclass__属性，如果找到了，Python就会用它来创建类Foo，如果没有找到，就会用内建的type来创建这个类。把下面这段话反复读几次。当你写如下代码时:PythonclassFoo(Bar):pass12classFoo(Bar):    passPython做了如下的操作：Foo中有__metaclass__这个属性吗？如果是，Python会在内存中通过__metaclass__创建一个名字为Foo的类对象（我说的是类对象，请紧跟我的思路）。如果Python没有找到__metaclass__，它会继续在Bar（父类）中寻找__metaclass__属性，并尝试做和前面同样的操作。如果Python在任何父类中都找不到__metaclass__，它就会在模块层次中去寻找__metaclass__，并尝试做同样的操作。如果还是找不到__metaclass__,Python就会用内置的type来创建这个类对象。现在的问题就是，你可以在__metaclass__中放置些什么代码呢？答案就是：可以创建一个类的东西。那么什么可以用来创建一个类呢？type，或者任何使用到type或者子类化type的东东都可以。 自定义元类元类的主要目的就是为了当创建类时能够自动地改变类。通常，你会为API做这样的事情，你希望可以创建符合当前上下文的类。假想一个很傻的例子，你决定在你的模块里所有的类的属性都应该是大写形式。有好几种方法可以办到，但其中一种就是通过在模块级别设定__metaclass__。采用这种方法，这个模块中的所有类都会通过这个元类来创建，我们只需要告诉元类把所有的属性都改成大写形式就万事大吉了。幸运的是，__metaclass__实际上可以被任意调用，它并不需要是一个正式的类（我知道，某些名字里带有‘class’的东西并不需要是一个class，画画图理解下，这很有帮助）。所以，我们这里就先以一个简单的函数作为例子开始。Python#元类会自动将你通常传给‘type’的参数作为自己的参数传入defupper_attr(future_class_name,future_class_parents,future_class_attr):'''返回一个类对象，将属性都转为大写形式'''#选择所有不以'__'开头的属性attrs=((name,value)forname,valueinfuture_class_attr.items()ifnotname.startswith('__'))12345#元类会自动将你通常传给‘type’的参数作为自己的参数传入defupper_attr(future_class_name,future_class_parents,future_class_attr):    '''返回一个类对象，将属性都转为大写形式'''    #  选择所有不以'__'开头的属性    attrs=((name,value)forname,valueinfuture_class_attr.items()ifnotname.startswith('__'))Python#将它们转为大写形式uppercase_attr=dict((name.upper(),value)forname,valueinattrs)#通过'type'来做类对象的创建returntype(future_class_name,future_class_parents,uppercase_attr)__metaclass__=upper_attr#这会作用到这个模块中的所有类classFoo(object):#我们也可以只在这里定义__metaclass__，这样就只会作用于这个类中bar='bip'1234567891011    #将它们转为大写形式    uppercase_attr=dict((name.upper(),value)forname,valueinattrs)     #通过'type'来做类对象的创建    returntype(future_class_name,future_class_parents,uppercase_attr) __metaclass__=upper_attr  #  这会作用到这个模块中的所有类 classFoo(object):    #我们也可以只在这里定义__metaclass__，这样就只会作用于这个类中    bar='bip'Pythonprinthasattr(Foo,'bar')#输出:Falseprinthasattr(Foo,'BAR')#输出:Truef=Foo()printf.BAR#输出:'bip'12345678printhasattr(Foo,'bar')#输出:Falseprinthasattr(Foo,'BAR')#输出:True f=Foo()printf.BAR#输出:'bip'现在让我们再做一次，这一次用一个真正的class来当做元类。Python#请记住，'type'实际上是一个类，就像'str'和'int'一样#所以，你可以从type继承classUpperAttrMetaClass(type):#__new__是在__init__之前被调用的特殊方法#__new__是用来创建对象并返回之的方法#而__init__只是用来将传入的参数初始化给对象#你很少用到__new__，除非你希望能够控制对象的创建#这里，创建的对象是类，我们希望能够自定义它，所以我们这里改写__new__#如果你希望的话，你也可以在__init__中做些事情#还有一些高级的用法会涉及到改写__call__特殊方法，但是我们这里不用def__new__(upperattr_metaclass,future_class_name,future_class_parents,future_class_attr):attrs=((name,value)forname,valueinfuture_class_attr.items()ifnotname.startswith('__'))uppercase_attr=dict((name.upper(),value)forname,valueinattrs)returntype(future_class_name,future_class_parents,uppercase_attr)1234567891011121314#请记住，'type'实际上是一个类，就像'str'和'int'一样#所以，你可以从type继承classUpperAttrMetaClass(type):    #__new__是在__init__之前被调用的特殊方法    #__new__是用来创建对象并返回之的方法    #而__init__只是用来将传入的参数初始化给对象    #你很少用到__new__，除非你希望能够控制对象的创建    #这里，创建的对象是类，我们希望能够自定义它，所以我们这里改写__new__    #如果你希望的话，你也可以在__init__中做些事情    #还有一些高级的用法会涉及到改写__call__特殊方法，但是我们这里不用    def__new__(upperattr_metaclass,future_class_name,future_class_parents,future_class_attr):        attrs=((name,value)forname,valueinfuture_class_attr.items()ifnotname.startswith('__'))        uppercase_attr=dict((name.upper(),value)forname,valueinattrs)        returntype(future_class_name,future_class_parents,uppercase_attr)但是，这种方式其实不是OOP。我们直接调用了type，而且我们没有改写父类的__new__方法。现在让我们这样去处理:PythonclassUpperAttrMetaclass(type):def__new__(upperattr_metaclass,future_class_name,future_class_parents,future_class_attr):attrs=((name,value)forname,valueinfuture_class_attr.items()ifnotname.startswith('__'))uppercase_attr=dict((name.upper(),value)forname,valueinattrs)#复用type.__new__方法#这就是基本的OOP编程，没什么魔法returntype.__new__(upperattr_metaclass,future_class_name,future_class_parents,uppercase_attr)12345678classUpperAttrMetaclass(type):    def__new__(upperattr_metaclass,future_class_name,future_class_parents,future_class_attr):        attrs=((name,value)forname,valueinfuture_class_attr.items()ifnotname.startswith('__'))        uppercase_attr=dict((name.upper(),value)forname,valueinattrs)         #复用type.__new__方法        #这就是基本的OOP编程，没什么魔法        returntype.__new__(upperattr_metaclass,future_class_name,future_class_parents,uppercase_attr)你可能已经注意到了有个额外的参数upperattr_metaclass，这并没有什么特别的。类方法的第一个参数总是表示当前的实例，就像在普通的类方法中的self参数一样。当然了，为了清晰起见，这里的名字我起的比较长。但是就像self一样，所有的参数都有它们的传统名称。因此，在真实的产品代码中一个元类应该是像这样的：PythonclassUpperAttrMetaclass(type):def__new__(cls,name,bases,dct):attrs=((name,value)forname,valueindct.items()ifnotname.startswith('__')uppercase_attr=dict((name.upper(),value)forname,valueinattrs)returntype.__new__(cls,name,bases,uppercase_attr)12345classUpperAttrMetaclass(type):    def__new__(cls,name,bases,dct):        attrs=((name,value)forname,valueindct.items()ifnotname.startswith('__')        uppercase_attr  =dict((name.upper(),value)forname,valueinattrs)        returntype.__new__(cls,name,bases,uppercase_attr)如果使用super方法的话，我们还可以使它变得更清晰一些，这会缓解继承（是的，你可以拥有元类，从元类继承，从type继承）PythonclassUpperAttrMetaclass(type):def__new__(cls,name,bases,dct):attrs=((name,value)forname,valueindct.items()ifnotname.startswith('__'))uppercase_attr=dict((name.upper(),value)forname,valueinattrs)returnsuper(UpperAttrMetaclass,cls).__new__(cls,name,bases,uppercase_attr)12345classUpperAttrMetaclass(type):    def__new__(cls,name,bases,dct):        attrs=((name,value)forname,valueindct.items()ifnotname.startswith('__'))        uppercase_attr=dict((name.upper(),value)forname,valueinattrs)        returnsuper(UpperAttrMetaclass,cls).__new__(cls,name,bases,uppercase_attr)就是这样，除此之外，关于元类真的没有别的可说的了。使用到元类的代码比较复杂，这背后的原因倒并不是因为元类本身，而是因为你通常会使用元类去做一些晦涩的事情，依赖于自省，控制继承等等。确实，用元类来搞些“黑暗魔法”是特别有用的，因而会搞出些复杂的东西来。但就元类本身而言，它们其实是很简单的：1)  拦截类的创建2) 修改类3) 返回修改之后的类 为什么要用metaclass类而不是函数?由于__metaclass__可以接受任何可调用的对象，那为何还要使用类呢，因为很显然使用类会更加复杂啊？这里有好几个原因：1） 意图会更加清晰。当你读到UpperAttrMetaclass(type)时，你知道接下来要发生什么。2）你可以使用OOP编程。元类可以从元类中继承而来，改写父类的方法。元类甚至还可以使用元类。3） 你可以把代码组织的更好。当你使用元类的时候肯定不会是像我上面举的这种简单场景，通常都是针对比较复杂的问题。将多个方法归总到一个类中会很有帮助，也会使得代码更容易阅读。4）你可以使用__new__,__init__以及__call__这样的特殊方法。它们能帮你处理不同的任务。就算通常你可以把所有的东西都在__new__里处理掉，有些人还是觉得用__init__更舒服些。5）哇哦，这东西的名字是metaclass，肯定非善类，我要小心！ 究竟为什么要使用元类？现在回到我们的大主题上来，究竟是为什么你会去使用这样一种容易出错且晦涩的特性？好吧，一般来说，你根本就用不上它：“元类就是深度的魔法，99%的用户应该根本不必为此操心。如果你想搞清楚究竟是否需要用到元类，那么你就不需要它。那些实际用到元类的人都非常清楚地知道他们需要做什么，而且根本不需要解释为什么要用元类。” ——Python界的领袖TimPeters元类的主要用途是创建API。一个典型的例子是DjangoORM。它允许你像这样定义：PythonclassPerson(models.Model):name=models.CharField(max_length=30)age=models.IntegerField()123classPerson(models.Model):    name=models.CharField(max_length=30)    age=models.IntegerField()但是如果你像这样做的话：Pythonguy=Person(name='bob',age='35')printguy.age12guy  =Person(name='bob',age='35')printguy.age这并不会返回一个IntegerField对象，而是会返回一个int，甚至可以直接从数据库中取出数据。这是有可能的，因为models.Model定义了__metaclass__，并且使用了一些魔法能够将你刚刚定义的简单的Person类转变成对数据库的一个复杂hook。Django框架将这些看起来很复杂的东西通过暴露出一个简单的使用元类的API将其化简，通过这个API重新创建代码，在背后完成真正的工作。 结语首先，你知道了类其实是能够创建出类实例的对象。好吧，事实上，类本身也是实例，当然，它们是元类的实例。Python>>>classFoo(object):pass>>>id(Foo)142630324123>>>classFoo(object):pass>>>id(Foo)142630324Python中的一切都是对象，它们要么是类的实例，要么是元类的实例，除了type。type实际上是它自己的元类，在纯Python环境中这可不是你能够做到的，这是通过在实现层面耍一些小手段做到的。其次，元类是很复杂的。对于非常简单的类，你可能不希望通过使用元类来对类做修改。你可以通过其他两种技术来修改类：1）Monkeypatching2) classdecorators当你需要动态修改类时，99%的时间里你最好使用上面这两种技术。当然了，其实在99%的时间里你根本就不需要动态修改类:D2赞30收藏15评论关于作者：bigship简介还没来得及写:）个人主页·我的文章·9"], "art_create_time": ["2012/06/11"], "art_title": ["深刻理解Python中的元类(metaclass)"], "art_url": ["http://python.jobbole.com/21351/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2012/06/python-icon.jpg"]},
{"art_content": ["原文出处：SerdarYegulalp   译文出处：oschina   无论你是正在使用Python进行快速开发，还是在为Python桌面应用制作原生UI，或者是在优化现有的Python代码，以下这些Python项目都是应该使用的。Python凭借其易用的特点，已经被工业界和学术界广泛采用。另一方面，Python丰富的第三方项目——库、附加组件，和辅助的开发成果——使得Python语言的应用范围被不断扩大。其中一些项目，比如PyInstaller和WxPython，为那些制作桌面应用和终端应用的Python开发者提供了便利。其他的项目,比如PyPy,则是用来给服务器端Python应用提供额外的动力。还有一些，像PBR、CFFI和MyPy,适用于差不多所有五花八门的Python应用，无论在什么地方运行。如果你是一个Python开发者，所有这六个项目都值得你来熟悉一下。而且所有这些项目，在近几周都发布了新的主要版本。Python必备之PyPyPyPy主要用于何处？如果你需要更快的Python应用程序，最简单的实现的方法就是通过PyPy，Python运行时与实时（JIT）编译器。与使用普通的Python对等程序相比，使用PyPy的Python应用程序的运行速度平均提升7.5倍。不幸的是，PyPy与许多Python的明星框架并不是很好地兼容。PyPy5.9在解决这个问题上取得了重大进展。PyPy5.9的功能数据科学框架NumPy和Pandas现在运行在PyPy的Python2.7兼容版本上。这些框架的大部分问题来源于PyPy与现有C代码的接口。为了解决这个问题，PyPy5.9对CFFI库（见下文）和PyPy的PythonCAPI兼容性层进行了改进。此外，在5.9发布版本中，PyPy的JSON解析器在处理多种JSON对象，尤其是那些重复使用的相同的词典键值时，明显更快。何处下载PyPy5.9你可以直接从PyPy的网站下载二进制版本。官方二进制文件包括Windows、MacOS和Linux的不同CPU架构。请注意，为了兼容Python2.7和Python3.5，存在不同的二进制文件，因此请确保你正在获取与你将要运行的脚本所匹配的版本。BitBucket上有源代码和错误跟踪记录。Python必备之CFFICFFI主要用于何处？C外部函数接口库（CFFI）为Python应用程序与独立C库的交互提供了一种机制。虽然Python的stock版本，CPython，也拥有自己的库来完成此类功能，称为Ctypes，但对Python用户来说，比起Ctypes，CFFI使得与C库的交互更容易、更简便。CFFI1.11的功能与PyPy一起更新的CFFIv1.11增加了很小但很有用的改动。现在可以在即将发布的Python3.7上使用betas了，在Windows上更好地支持外部错误处理，并支持C语言中更多的现代标准类型，例如float/double_Complex和char16_t和char_32t类型。最后两个也是最重要的，在C库中默认使用Unicode编码。何处下载CFFI1.11？CFFI在PythonPackageIndex上可以单独下载，或通过Python的pip工具安装：pipinstallcffi。源码和问题跟踪可以在BitBucket上找到。Python必备之PyInstallerPyInstaller主要用于何处？关于Python的最常见的问题之一是“如何从Python脚本中生成独立的可执行文件？”PyInstaller一直是对此最好的答案之一。PyInstaller3.3的功能PyInstaller将Python应用程序打包到单目录或单文件的可执行文件中，捆绑任何所需的第三方库，并可与绝大多数常见的库和框架配合使用。PyInstaller3.3中最大的改进是对Python3.6的支持，因为鉴于Python3.6已经发布这确实是必要的PyInstaller3.3还包括一个更广泛兼容的引导加载程序，适用于Windows可执行文件，并扩展了对捆绑常见库（如QT、GTK+、NumPy和Django）的支持。PyInstaller在不久之后可能添加的一个功能是交叉打包，例如，在Windows上创建Mac兼容的应用程序。你需要在要部署的同一平台上运行该PyInstaller，无论是Windows、Mac还是Linux。何处下载PyInstaller3.3？PyInstaller可以通过PythonPackageIndex安装，也可通过Python的pip工具安装：pipinstallpyinstaller。对于那些需要自己编译引导加载程序的人，源码可以在GitHub上找到，但对多数人而言是不需要这么做的。Python必备之PythonBuildReasonableness(PBR)PythonPBR主要用于何处？Setuptools是用于打包Python项目的标准的Python问题子系统。管理特定项目的Setuptools可能会变得非常繁琐，特别是在自动生成需求、管理文档文件或编辑项目贡献者数据时。PythonPBR的功能PBR,PythonBuildReasonableness的缩写，是以一致的方式用于管理Setuptools包的库。它可以自动化许多Setuptools打包的设置，例如版本号、生成作者和ChangeLog文件，以及生成Sphinx风格的文档。PBR最初是作为OpenStack项目的一部分开发的，但现在你所使用PBR中维护的内容与OpenStack已经没有任何联系了。哪里可以下载PythonPBR？PBR在PythonPackageIndex上可以找到，并且可以和pip一起安装，只需要输入pipinstallpbr即可。源码可在GitHub上下载。Python必备之WxPythonWxPython主要用于何处？想要实现跨平台桌面应用程序的Python开发人员可以从多个工具包中进行选择。WxPython，是WxWidgets库的一个封装，使用了其所支持主机平台的原生UI元素，包括Windows、Mac、Linux和其他类Unix操作系统。WxPython4.0的功能早期版本的WxPython被放弃了是由于其传统的设计决策，使其变得越来越慢，而且不太适合使用。为了解决这个问题，WxPython的开发人员对WxPython的4.0分支做了重大改变。目标是允许开发人员更快地上手WxPython，并且使通过它创建的框架和应用程序更加高性能和易维护。然而，为了使用WxPython4.0，任何现有的使用WxPython项目都需要修改。何处可以下载WxPython4.0？WxPython4.0官方版本依然是beta版。它可以在PythonPackageIndex上找到，即通过pipinstallwxpython命令。在正式发布前它可能会更新数次，注意经常检查更新。那些想直接破解的人可以查看GitHub上的代码库。请注意，WxPython的4.0分支以“Phoenix”代号进行标记的，以使其与早期版本不同。Python必备之MypyMypy主要用于何处？Python的动态性既是一种福音，也是一种烦恼，对于快速构建软件非常棒，但是当代码难以推理、测试和调试时，并不是很棒。Mypy在编译时向Python添加静态类型检查，使Python程序更加一致和可维护，并且不会增加运行时开销。Mypy0.530的功能Mypy0.530添加了不同协议的支持，该协议是用于Python子类的目前实验性类型的功能。它还在仅用于包含特定类型的对象的字典中添加“TypedDict”类型，并且可以逐个对文件进行更严格的类型检查的选项。哪里可以下载Mypy0.530？Mypy可以在PythonPackageIndex上找到，并通过pipinstallmypy来安装。那些对Mypy实现感兴趣的人可以通过GitHub检出源码。1赞10收藏15评论"], "art_create_time": ["2017/11/28"], "art_title": ["Python 开发者的 6 个必备库"], "art_url": ["http://python.jobbole.com/88958/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2017/11/722f435a6aa288edb0916821e53d878d.png"]},
{"art_content": ["本文由伯乐在线-刘志军翻译。未经许可，禁止转载！英文出处：stackoverflow。欢迎加入翻译组。导读：此文由伯乐在线–刘志军编译自stackoverflowPython标签中投票率最高的一个问题《ThePythonyieldkeywordexplained》，e-satis详细回答了关于yield以及generator、iterable、iterator、iteration之间的关系。可迭代对象（Iterbles）创建一个列表（list）时，你可以逐个地读取里面的每一项元素，这个过程称之为迭代（iteration）。Python>>>mylist=[1,2,3]>>>foriinmylist:...print(i)123123456>>>mylist=[1,2,3]>>>foriinmylist:...    print(i)123mylist是一个可迭代对象。当使用列表推导式（listcomprehension）创建了一个列表时，它就是一个可迭代对象：Python>>>mylist=[x*xforxinrange(3)]>>>foriinmylist:...print(i)014123456>>>mylist=[x*xforxinrange(3)]>>>foriinmylist:...    print(i)014任何可以使用在for...in...语句中的对象都可以叫做可迭代对象，例如：lists，strings，files等等。这些可迭代对象使用非常方便因为它能如你所愿的尽可能读取其中的元素，但是你不得不把所有的值存储在内存中，当它有大量元素的时候这并不一定总是你想要的。译者补充：dict对象以及任何实现了__iter__()或者__getitem__()方法的类都是可迭代对象，此外，可迭代对象还可以用在zip,map等函数中，当一个可迭代对象作为参数传递给内建函数iter()时，它会返回一个迭代器对象。通常没必要自己来处理迭代器本身或者手动调用iter()，for语句会自动调用iter()，它会创建一个临时的未命名的变量来持有这个迭代器用于循环期间。为了更好的理解yield，译者引入了迭代器的介绍。迭代器（iterator）迭代器代表一个数据流对象，不断重复调用迭代器的next()方法可以逐次地返回数据流中的每一项，当没有更多数据可用时，next()方法会抛出异常StopIteration。此时迭代器对象已经枯竭了，之后调用next()方法都会抛出异常StopIteration。迭代器需要有一个__iter()__方法用来返回迭代器本身。因此它也是一个可迭代的对象。生成器（Generators）生成器也是一个迭代器，但是你只可以迭代他们一次，不能重复迭代，因为它并没有把所有值存储在内存中，而是实时地生成值：Python>>>mygenerator=(x*xforxinrange(3))>>>foriinmygenerator:...print(i)014123456>>>mygenerator=(x*xforxinrange(3))>>>foriinmygenerator:...    print(i)014从结果上看用()代替[]效果是一样的，但是，你不可能第二次执行foriinmygenerator（译注：这里作者所表达的意思是第二次执行达不到期望的效果）因为生成器只能使用一次：首先计算出0，然后计算出1，最后计算出4。YieldYield是关键字，它类似于return，只是函数会返回一个生成器。Python>>>classBank():#创建银行，构建ATM机，只要没有危机，就可以不断地每次从中取100...crisis=False...defcreate_atm(self):...whilenotself.crisis:...yield\"$100\">>>hsbc=Bank()#wheneverything'soktheATMgivesyouasmuchasyouwant>>>corner_street_atm=hsbc.create_atm()>>>print(corner_street_atm.next())$100>>>print(corner_street_atm.next())$100>>>print([corner_street_atm.next()forcashinrange(5)])['$100','$100','$100','$100','$100']>>>hsbc.crisis=True#危机来临，没有更多的钱了>>>print(corner_street_atm.next())<type'exceptions.StopIteration'>>>>wall_street_atm=hsbc.create_atm()#即使创建一个新的ATM，银行还是没钱>>>print(wall_street_atm.next())<type'exceptions.StopIteration'>>>>hsbc.crisis=False#危机过后，银行还是空的，因为该函数之前已经不满足while条件>>>print(corner_street_atm.next())<type'exceptions.StopIteration'>>>>brand_new_atm=hsbc.create_atm()#必须构建一个新的atm，恢复取钱业务>>>forcashinbrand_new_atm:...printcash$100$100$100$100$100$100$100$100$100...1234567891011121314151617181920212223242526272829303132333435>>>classBank():#创建银行，构建ATM机，只要没有危机，就可以不断地每次从中取100...    crisis=False...    defcreate_atm(self):...        whilenotself.crisis:...            yield\"$100\">>>hsbc=Bank()#wheneverything'soktheATMgivesyouasmuchasyouwant>>>corner_street_atm=hsbc.create_atm()>>>print(corner_street_atm.next())$100>>>print(corner_street_atm.next())$100>>>print([corner_street_atm.next()forcashinrange(5)])['$100','$100','$100','$100','$100']>>>hsbc.crisis=True#危机来临，没有更多的钱了>>>print(corner_street_atm.next())<type'exceptions.StopIteration'>>>>wall_street_atm=hsbc.create_atm()#即使创建一个新的ATM，银行还是没钱>>>print(wall_street_atm.next())<type'exceptions.StopIteration'>>>>hsbc.crisis=False#危机过后，银行还是空的，因为该函数之前已经不满足while条件>>>print(corner_street_atm.next())<type'exceptions.StopIteration'>>>>brand_new_atm=hsbc.create_atm()#必须构建一个新的atm，恢复取钱业务>>>forcashinbrand_new_atm:...    printcash$100$100$100$100$100$100$100$100$100...对于类似资源的访问控制等场景，生成器显得很实用。Itertools是你最好的朋友itertools模块包含一些特殊的函数用来操作可迭代对象。曾经想复制一个生成器？两个生成器链接？在内嵌列表中一行代码处理分组？不会创建另外一个列表的Map/Zip函数？你要做的就是importitertools。无例子无真相，我们来看看4匹马赛跑到达终点所有可能的顺序：Python>>>horses=[1,2,3,4]>>>races=itertools.permutations(horses)>>>print(races)<itertools.permutationsobjectat0xb754f1dc>>>>print(list(itertools.permutations(horses)))[(1,2,3,4),(1,2,4,3),(1,3,2,4),(1,3,4,2),(1,4,2,3),(1,4,3,2),(2,1,3,4),(2,1,4,3),(2,3,1,4),(2,3,4,1),(2,4,1,3),(2,4,3,1),(3,1,2,4),(3,1,4,2),(3,2,1,4),(3,2,4,1),(3,4,1,2),(3,4,2,1),(4,1,2,3),(4,1,3,2),(4,2,1,3),(4,2,3,1),(4,3,1,2),(4,3,2,1)]1234567891011121314151617181920212223242526272829>>>horses=[1,2,3,4]>>>races=itertools.permutations(horses)>>>print(races)<itertools.permutationsobjectat0xb754f1dc>>>>print(list(itertools.permutations(horses)))[(1,2,3,4),(1,2,4,3),(1,3,2,4),(1,3,4,2),(1,4,2,3),(1,4,3,2),(2,1,3,4),(2,1,4,3),(2,3,1,4),(2,3,4,1),(2,4,1,3),(2,4,3,1),(3,1,2,4),(3,1,4,2),(3,2,1,4),(3,2,4,1),(3,4,1,2),(3,4,2,1),(4,1,2,3),(4,1,3,2),(4,2,1,3),(4,2,3,1),(4,3,1,2),(4,3,2,1)]理解迭代的内部机制迭代是操作可迭代对象（实现了__iter__()方法）和迭代器（实现了__next__()方法）的过程。可迭代对象是任何你可以从其得到一个迭代器对象的任意对象（译注：调用内建函数iter()），迭代器是能让你在可迭代对象上进行迭代的对象（译注：这里好绕，迭代器实现了__iter__()方法，因此它也是一个可迭代对象）。最后更新时间：2015/06/13打赏支持我翻译更多好文章，谢谢！打赏译者打赏支持我翻译更多好文章，谢谢！3赞2收藏5评论关于作者：刘志军个人博客：http://foofish.net，微信公众号：Python之禅（ID：VTtalk）个人主页·我的文章·45·"], "art_create_time": ["2013/01/29"], "art_title": ["Python 关键字 yield 详解"], "art_url": ["http://python.jobbole.com/28506/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2012/02/python-logo.png"]},
{"art_content": ["原文出处：xybaby   本文除非特殊指明，”python“都是代表CPython，即C语言实现的标准python，且本文所讨论的是版本为2.7的CPython。另外，本文会不定期更新，如果大家有一些好的想法，请在评论里面留言，我会补充到文章中去。姊妹篇：《Python内存优化》姊妹篇：《使用gc、objgraph干掉python内存泄露与循环引用！》python为什么性能差：当我们提到一门编程语言的效率时：通常有两层意思，第一是开发效率，这是对程序员而言，完成编码所需要的时间；另一个是运行效率，这是对计算机而言，完成计算任务所需要的时间。编码效率和运行效率往往是鱼与熊掌的关系，是很难同时兼顾的。不同的语言会有不同的侧重，python语言毫无疑问更在乎编码效率，lifeisshort，weusepython。虽然使用python的编程人员都应该接受其运行效率低的事实，但python在越多越来的领域都有广泛应用，比如科学计算、web服务器等。程序员当然也希望python能够运算得更快，希望python可以更强大。首先，python相比其他语言具体有多慢，这个不同场景和测试用例，结果肯定是不一样的。这个网址给出了不同语言在各种case下的性能对比，这一页是python3和C++的对比，下面是两个case：从上图可以看出，不同的case，python比C++慢了几倍到几十倍。python运算效率低，具体是什么原因呢，下列罗列一些第一：python是动态语言一个变量所指向对象的类型在运行时才确定，编译器做不了任何预测，也就无从优化。举一个简单的例子：　r=a+b。　a和b相加，但a和b的类型在运行时才知道，对于加法操作，不同的类型有不同的处理，所以每次运行的时候都会去判断a和b的类型，然后执行对应的操作。而在静态语言如C++中，编译的时候就确定了运行时的代码。另外一个例子是属性查找，关于具体的查找顺序在《python属性查找》中有详细介绍。简而言之，访问对象的某个属性是一个非常复杂的过程，而且通过同一个变量访问到的python对象还都可能不一样（参见Lazyproperty的例子）。而在C语言中，访问属性用对象的地址加上属性的偏移就可以了。第二：python是解释执行，但是不支持JIT（justintimecompiler）。虽然大名鼎鼎的google曾经尝试UnladenSwallow这个项目，但最终也折了。第三：python中一切都是对象，每个对象都需要维护引用计数，增加了额外的工作。第四：pythonGIL，GIL是Python最为诟病的一点，因为GIL，python中的多线程并不能真正的并发。如果是在IObound的业务场景，这个问题并不大，但是在CPUBOUND的场景，这就很致命了。所以笔者在工作中使用python多线程的情况并不多，一般都是使用多进程（prefork），或者在加上协程。即使在单线程，GIL也会带来很大的性能影响，因为python每执行100个opcode（默认，可以通过sys.setcheckinterval()设置）就会尝试线程的切换，具体的源代码在ceval.c::PyEval_EvalFrameEx。第五：垃圾回收，这个可能是所有具有垃圾回收的编程语言的通病。python采用标记和分代的垃圾回收策略，每次垃圾回收的时候都会中断正在执行的程序，造成所谓的顿卡。infoq上有一篇文章，提到禁用Python的GC机制后，Instagram性能提升了10%。感兴趣的读者可以去细读。Bepythonic我们都知道过早的优化是罪恶之源，一切优化都需要基于profile。但是，作为一个python开发者应该要pythonic，而且pythonic的代码往往比non－pythonic的代码效率高一些，比如：使用迭代器iterator，forexample：dict的iteritems而不是items（同itervalues，iterkeys）使用generator，特别是在循环中可能提前break的情况判断是否是同一个对象使用is而不是==判断一个对象是否在一个集合中，使用set而不是list利用短路求值特性，把“短路”概率过的逻辑表达式写在前面。其他的lazyideas也是可以的对于大量字符串的累加，使用join操作使用forelse（whileelse）语法交换两个变量的值使用：a,b=b,a基于profile的优化即使我们的代码已经非常pythonic了，但可能运行效率还是不能满足预期。我们也知道80/20定律，绝大多数的时间都耗费在少量的代码片段里面了，优化的关键在于找出这些瓶颈代码。方式很多：到处加log打印时间戳、或者将怀疑的函数使用timeit进行单独测试，但最有效的是使用profile工具。pythonprofilers对于python程序，比较出名的profile工具有三个：profile、cprofile和hotshot。其中profile是纯python语言实现的，Cprofile将profile的部分实现native化，hotshot也是C语言实现，hotshot与Cprofile的区别在于：hotshot对目标代码的运行影响较小，代价是更多的后处理时间，而且hotshot已经停止维护了。需要注意的是，profile（Cprofilehotshot）只适合单线程的python程序。对于多线程，可以使用yappi，yappi不仅支持多线程，还可以精确到CPU时间对于协程（greenlet），可以使用greenletprofiler，基于yappi修改，用greenletcontexthook住threadcontext下面给出一段编造的”效率低下“的代码，并使用Cprofile来说明profile的具体方法以及我们可能遇到的性能瓶颈。Python#-*-coding:UTF-8-*-fromcProfileimportProfileimportmathdeffoo():returnfoo1()deffoo1():returnfoo2()deffoo2():returnfoo3()deffoo3():returnfoo4()deffoo4():return\"thiscalltreeseemsugly,butitalwayshappen\"defbar():ret=0foriinxrange(10000):ret+=i*i+math.sqrt(i)returnretdefmain():foriinrange(100000):ifi%10000==0:bar()else:foo()if__name__=='__main__':prof=Profile()prof.runcall(main)prof.print_stats()#prof.dump_stats('test.prof')#dumpprofileresulttotest.profcodeforprofile123456789101112131415161718192021222324252627282930313233343536373839#-*-coding:UTF-8-*- fromcProfileimportProfileimportmathdeffoo():    returnfoo1() deffoo1():    returnfoo2() deffoo2():    returnfoo3() deffoo3():    returnfoo4() deffoo4():    return\"thiscalltreeseemsugly,butitalwayshappen\" defbar():    ret=0    foriinxrange(10000):        ret+=i*i+math.sqrt(i)    returnret defmain():    foriinrange(100000):        ifi%10000==0:            bar()        else:            foo() if__name__=='__main__':    prof=Profile()    prof.runcall(main)    prof.print_stats()    #prof.dump_stats('test.prof')#dumpprofileresulttotest.prof codeforprofile运行结果如下：对于上面的的输出，每一个字段意义如下：ncalls函数总的调用次数tottime函数内部（不包括子函数）的占用时间percall（第一个）tottime/ncallscumtime函数包括子函数所占用的时间percall（第二个）cumtime/ncallsfilename:lineno(function)文件：行号（函数）代码中的输出非常简单，事实上可以利用pstat，让profile结果的输出多样化，具体可以参见官方文档pythonprofiler。profileGUItools虽然Cprofile的输出已经比较直观，但我们还是倾向于保存profile的结果，然后用图形化的工具来从不同的维度来分析，或者比较优化前后的代码。查看profile结果的工具也比较多，比如，visualpytune、qcachegrind、runsnakerun，本文用visualpytune做分析。对于上面的代码，按照注释生成修改后重新运行生成test.prof文件，用visualpytune直接打开就可以了，如下：字段的意义与文本输出基本一致，不过便捷性可以点击字段名排序。左下方列出了当前函数的calller（调用者），右下方是当前函数内部与子函数的时间占用情况。上如是按照cumtime（即该函数内部及其子函数所占的时间和）排序的结果。造成性能瓶颈的原因通常是高频调用的函数、单次消耗非常高的函数、或者二者的结合。在我们前面的例子中，foo就属于高频调用的情况，bar属于单次消耗非常高的情况，这都是我们需要优化的重点。python-profiling-tools中介绍了qcachegrind和runsnakerun的使用方法，这两个colorful的工具比visualpytune强大得多。具体的使用方法请参考原文，下图给出test.prof用qcachegrind打开的结果qcachegrind确实要比visualpytune强大。从上图可以看到，大致分为三部：。第一部分同visualpytune类似，是每个函数占用的时间，其中Incl等同于cumtime，Self等同于tottime。第二部分和第三部分都有很多标签，不同的标签标示从不同的角度来看结果，如图上所以，第三部分的“callgraph”展示了该函数的calltree并包含每个子函数的时间百分比，一目了然。profile针对优化知道了热点，就可以进行针对性的优化，而这个优化往往根具体的业务密切相关，没用万能钥匙，具体问题，具体分析。个人经验而言，最有效的优化是找产品经理讨论需求，可能换一种方式也能满足需求，少者稍微折衷一下产品经理也能接受。次之是修改代码的实现，比如之前使用了一个比较通俗易懂但效率较低的算法，如果这个算法成为了性能瓶颈，那就考虑换一种效率更高但是可能难理解的算法、或者使用dirtyFlag模式。对于这些同样的方法，需要结合具体的案例，本文不做赘述。接下来结合python语言特性，介绍一些让python代码不那么pythonic，但可以提升性能的一些做法第一：减少函数的调用层次每一层函数调用都会带来不小的开销，特别对于调用频率高，但单次消耗较小的calltree，多层的函数调用开销就很大，这个时候可以考虑将其展开。对于之前调到的profile的代码，foo这个calltree非常简单，但频率高。修改代码，增加一个plain_foo()函数,直接返回最终结果，关键输出如下：跟之前的结果对比：可以看到，优化了差不多3倍。第二：优化属性查找上面提到，python的属性查找效率很低，如果在一段代码中频繁访问一个属性（比如for循环），那么可以考虑用局部变量代替对象的属性。第三：关闭GC在本文的第一章节已经提到，关闭GC可以提升python的性能，GC带来的顿卡在实时性要求比较高的应用场景也是难以接受的。但关闭GC并不是一件容易的事情。我们知道python的引用计数只能应付没有循环引用的情况，有了循环引用就需要靠GC来处理。在python语言中,写出循环引用非常容易。比如：Pythoncase1：　　a,b=SomeClass(),SomeClass()　　a.b,b.a=b,a　　case2：　　lst=[]　　lst.append(lst)　　case3：　　self.handler=self.some_func12345678910case1：　　a,b=SomeClass(),SomeClass()　　a.b,b.a=b,a　　　　　case2：　　lst=[]　　lst.append(lst)　　case3：　　self.handler=self.some_func当然，大家可能说，谁会这么傻，写出这样的代码，是的，上面的代码太明显，当中间多几个层级之后，就会出现“间接”的循环应用。在python的标准库collections里面的OrderedDict就是case2：要解决循环引用，第一个办法是使用弱引用（weakref），第二个是手动解循环引用。第四：setcheckinterval如果程序确定是单线程，那么修改checkinterval为一个更大的值，这里有介绍。第五：使用__slots__slots最主要的目的是用来节省内存，但是也能一定程度上提高性能。我们知道定义了__slots__的类，对某一个实例都会预留足够的空间，也就不会再自动创建__dict__。当然，使用__slots__也有许多注意事项，最重要的一点，继承链上的所有类都必须定义__slots__，pythondoc有详细的描述。下面看一个简单的测试例子：PythonclassBaseSlots(object):__slots__=['e','f','g']classSlots(BaseSlots):__slots__=['a','b','c','d']def__init__(self):self.a=self.b=self.c=self.d=self.e=self.f=self.g=0classBaseNoSlots(object):passclassNoSlots(BaseNoSlots):def__init__(self):super(NoSlots,self).__init__()self.a=self.b=self.c=self.d=self.e=self.f=self.g=0deflog_time(s):begin=time.time()foriinxrange(10000000):s.a,s.b,s.c,s.d,s.e,s.f,s.greturntime.time()-beginif__name__=='__main__':print'Slotscost',log_time(Slots())print'NoSlotscost',log_time(NoSlots())12345678910111213141516171819202122232425classBaseSlots(object):    __slots__=['e','f','g'] classSlots(BaseSlots):    __slots__=['a','b','c','d']    def__init__(self):        self.a=self.b=self.c=self.d=self.e=self.f  =self.g=0 classBaseNoSlots(object):        pass classNoSlots(BaseNoSlots):    def__init__(self):        super(NoSlots,self).__init__()        self.a=self.b=self.c=self.d=self.e=self.f  =self.g=0 deflog_time(s):    begin=time.time()    foriinxrange(10000000):        s.a,s.b,s.c,s.d,s.e,s.f,s.g    returntime.time()-begin if__name__=='__main__':    print'Slotscost',log_time(Slots())    print'NoSlotscost',log_time(NoSlots())输出结果：PythonSlotscost3.12999987602NoSlotscost3.4810001850112Slotscost3.12999987602NoSlotscost3.48100018501pythonC扩展也许通过profile，我们已经找到了性能热点，但这个热点就是要运行大量的计算，而且没法cache，没法省略。。。这个时候就该python的C扩展出马了，C扩展就是把部分python代码用C或者C++重新实现，然后编译成动态链接库，提供接口给其它python代码调用。由于C语言的效率远远高于python代码，所以使用C扩展是非常普遍的做法，比如我们前面提到的cProfile就是基于_lsprof.so的一层封装。python的大所属对性能有要求的库都使用或者提供了C扩展，如gevent、protobuff、bson。笔者曾经测试过纯python版本的bson和cbson的效率，在综合的情况下，cbson快了差不多10倍！python的C扩展也是一个非常复杂的问题，本文仅给出一些注意事项：第一：注意引用计数的正确管理这是最难最复杂的一点。我们都知道python基于指针技术来管理对象的生命周期，如果在扩展中引用计数出了问题，那么要么是程序崩溃，要么是内存泄漏。更要命的是，引用计数导致的问题很难debug。。。C扩展中关于引用计数最关键的三个词是：stealreference，borrowedreference，newreference。建议编写扩展代码之前细读python的官方文档。第二：C扩展与多线程这里的多线程是指在扩展中new出来的C语言线程，而不是python的多线程，出了pythondoc里面的介绍，也可以看看《pythoncookbook》的相关章节。第三：C扩展应用场景仅适合与业务代码的关系不那么紧密的逻辑，如果一段代码大量业务相关的对象属性的话，是很难C扩展的将C扩展封装成python代码可调用的接口的过程称之为binding，Cpython本身就提供了一套原生的API，虽然使用最为广泛，但该规范比较复杂。很多第三方库做了不同程度的封装，以便开发者使用，比如boost.python、cython、ctypes、cffi（同时支持pypycpython），具体怎么使用可以google。beyondCPython尽管python的性能差强人意，但是其易学易用的特性还是赢得越来越多的使用者，业界大牛也从来没有放弃对python的优化。这里的优化是对python语言设计上、或者实现上的一些反思或者增强。这些优化项目一些已经夭折，一些还在进一步改善中，在这个章节介绍目前还不错的一些项目。cython前面提到cython可以用到bindingc扩展，但是其作用远远不止这一点。Cython的主要目的是加速python的运行效率，但是又不像上一章节提到的C扩展那么复杂。在Cython中，写C扩展和写python代码的复杂度差不多（多亏了Pyrex）。Cython是python语言的超集，增加了对C语言函数调用和类型声明的支持。从这个角度来看，cython将动态的python代码转换成静态编译的C代码，这也是cython高效的原因。使用cython同C扩展一样，需要编译成动态链接库，在linux环境下既可以用命令行，也可以用distutils。如果想要系统学习cython，建议从cythondocument入手，文档写得很好。下面通过一个简单的示例来展示cython的使用方法和性能（linux环境）。首先，安装cython：PythonpipinstallCython1pipinstallCython下面是测试用的python代码，可以看到这两个case都是运算复杂度比较高的例子：PythonpythonC扩展回到顶部　　也许通过profile，我们已经找到了性能热点，但这个热点就是要运行大量的计算，而且没法cache，没法省略。。。这个时候就该python的C扩展出马了，C扩展就是把部分python代码用C或者C++重新实现，然后编译成动态链接库，提供接口给其它python代码调用。由于C语言的效率远远高于python代码，所以使用C扩展是非常普遍的做法，比如我们前面提到的cProfile就是基于_lsprof.so的一层封装。python的大所属对性能有要求的库都使用或者提供了C扩展，如gevent、protobuff、bson。　　笔者曾经测试过纯python版本的bson和cbson的效率，在综合的情况下，cbson快了差不多10倍！　　python的C扩展也是一个非常复杂的问题，本文仅给出一些注意事项：第一：注意引用计数的正确管理　　这是最难最复杂的一点。我们都知道python基于指针技术来管理对象的生命周期，如果在扩展中引用计数出了问题，那么要么是程序崩溃，要么是内存泄漏。更要命的是，引用计数导致的问题很难debug。。。　　C扩展中关于引用计数最关键的三个词是：stealreference，borrowedreference，newreference。建议编写扩展代码之前细读python的官方文档。第二：C扩展与多线程　　这里的多线程是指在扩展中new出来的C语言线程，而不是python的多线程，出了pythondoc里面的介绍，也可以看看《pythoncookbook》的相关章节。第三：C扩展应用场景　　仅适合与业务代码的关系不那么紧密的逻辑，如果一段代码大量业务相关的对象属性的话，是很难C扩展的　　将C扩展封装成python代码可调用的接口的过程称之为binding，Cpython本身就提供了一套原生的API，虽然使用最为广泛，但该规范比较复杂。很多第三方库做了不同程度的封装，以便开发者使用，比如boost.python、cython、ctypes、cffi（同时支持pypycpython），具体怎么使用可以google。beyondCPython回到顶部　　尽管python的性能差强人意，但是其易学易用的特性还是赢得越来越多的使用者，业界大牛也从来没有放弃对python的优化。这里的优化是对python语言设计上、或者实现上的一些反思或者增强。这些优化项目一些已经夭折，一些还在进一步改善中，在这个章节介绍目前还不错的一些项目。cython　　前面提到cython可以用到bindingc扩展，但是其作用远远不止这一点。　　Cython的主要目的是加速python的运行效率，但是又不像上一章节提到的C扩展那么复杂。在Cython中，写C扩展和写python代码的复杂度差不多（多亏了Pyrex）。Cython是python语言的超集，增加了对C语言函数调用和类型声明的支持。从这个角度来看，cython将动态的python代码转换成静态编译的C代码，这也是cython高效的原因。使用cython同C扩展一样，需要编译成动态链接库，在linux环境下既可以用命令行，也可以用distutils。　　如果想要系统学习cython，建议从cythondocument入手，文档写得很好。下面通过一个简单的示例来展示cython的使用方法和性能（linux环境）。　　首先，安装cython：　　pipinstallCython　　下面是测试用的python代码，可以看到这两个case都是运算复杂度比较高的例子：12345678910111213141516171819202122232425262728pythonC扩展回到顶部　　也许通过profile，我们已经找到了性能热点，但这个热点就是要运行大量的计算，而且没法cache，没法省略。。。这个时候就该python的C扩展出马了，C扩展就是把部分python代码用C或者C++重新实现，然后编译成动态链接库，提供接口给其它python代码调用。由于C语言的效率远远高于python代码，所以使用C扩展是非常普遍的做法，比如我们前面提到的cProfile就是基于_lsprof.so的一层封装。python的大所属对性能有要求的库都使用或者提供了C扩展，如gevent、protobuff、bson。　　笔者曾经测试过纯python版本的bson和cbson的效率，在综合的情况下，cbson快了差不多10倍！　　python的C扩展也是一个非常复杂的问题，本文仅给出一些注意事项：第一：注意引用计数的正确管理　　这是最难最复杂的一点。我们都知道python基于指针技术来管理对象的生命周期，如果在扩展中引用计数出了问题，那么要么是程序崩溃，要么是内存泄漏。更要命的是，引用计数导致的问题很难debug。。。　　C扩展中关于引用计数最关键的三个词是：stealreference，borrowedreference，newreference。建议编写扩展代码之前细读python的官方文档。第二：C扩展与多线程　　这里的多线程是指在扩展中new出来的C语言线程，而不是python的多线程，出了pythondoc里面的介绍，也可以看看《pythoncookbook》的相关章节。第三：C扩展应用场景　　仅适合与业务代码的关系不那么紧密的逻辑，如果一段代码大量业务相关的对象属性的话，是很难C扩展的　　将C扩展封装成python代码可调用的接口的过程称之为binding，Cpython本身就提供了一套原生的API，虽然使用最为广泛，但该规范比较复杂。很多第三方库做了不同程度的封装，以便开发者使用，比如boost.python、cython、ctypes、cffi（同时支持pypycpython），具体怎么使用可以google。beyondCPython回到顶部　　尽管python的性能差强人意，但是其易学易用的特性还是赢得越来越多的使用者，业界大牛也从来没有放弃对python的优化。这里的优化是对python语言设计上、或者实现上的一些反思或者增强。这些优化项目一些已经夭折，一些还在进一步改善中，在这个章节介绍目前还不错的一些项目。cython　　前面提到cython可以用到bindingc扩展，但是其作用远远不止这一点。　　Cython的主要目的是加速python的运行效率，但是又不像上一章节提到的C扩展那么复杂。在Cython中，写C扩展和写python代码的复杂度差不多（多亏了Pyrex）。Cython是python语言的超集，增加了对C语言函数调用和类型声明的支持。从这个角度来看，cython将动态的python代码转换成静态编译的C代码，这也是cython高效的原因。使用cython同C扩展一样，需要编译成动态链接库，在linux环境下既可以用命令行，也可以用distutils。　　如果想要系统学习cython，建议从cythondocument入手，文档写得很好。下面通过一个简单的示例来展示cython的使用方法和性能（linux环境）。　　首先，安装cython：　　pipinstallCython　　下面是测试用的python代码，可以看到这两个case都是运算复杂度比较高的例子：运行结果：Python　callfcost:0.215116024017　　callintegrate_fcost:4.3369801044512　callfcost:0.215116024017　　callintegrate_fcost:4.33698010445不改动任何python代码也可以享受到cython带来的性能提升，具体做法如下：step1：将文件名（cython_example.py）改为cython_example.pyxstep2：增加一个setup.py文件，添加一下代码：Pythonfromdistutils.coreimportsetupfromCython.Buildimportcythonizesetup(name='cython_example',ext_modules=cythonize(\"cython_example.pyx\"),)1234567fromdistutils.coreimportsetupfromCython.Buildimportcythonize setup(  name='cython_example',  ext_modules=cythonize(\"cython_example.pyx\"),)step3：执行pythonsetup.pybuild_ext–inplace可以看到增加了两个文件，对应中间结果和最后的动态链接库step4：执行命令python-c“importcython_example;cython_example.main()”（注意：保证当前环境下已经没有cython_example.py）运行结果：Python　callfcost:0.0874309539795　　callintegrate_fcost:2.9238119125412　callfcost:0.0874309539795　　callintegrate_fcost:2.92381191254性能提升了大概两倍，我们再来试试cython提供的静态类型（statictyping），修改cython_example.pyx的核心代码，替换f()和integrate_f()的实现如下：Pythondeff(doublex):#参数静态类型returnx**2-xdefintegrate_f(doublea,doubleb,intN):cdefinticdefdoubles,dxs=0dx=(b-a)/Nforiinrange(N):s+=f(a+i*dx)returns*dx1234567891011deff(doublex):#参数静态类型    returnx**2-x defintegrate_f(doublea,doubleb,intN):    cdefinti    cdefdoubles,dx    s=0    dx=(b-a)/N    foriinrange(N):        s+=f(a+i*dx)    returns*dx然后重新运行上面的第三四步：结果如下Python　　callfcost:0.042387008667　　callintegrate_fcost:0.95862007141112　　callfcost:0.042387008667　　callintegrate_fcost:0.958620071411上面的代码，只是对参数引入了静态类型判断，下面对返回值也引入静态类型判断。替换f()和integrate_f()的实现如下：Pythoncdefdoublef(doublex):#返回值也有类型判断returnx**2-xcdefdoubleintegrate_f(doublea,doubleb,intN):cdefinticdefdoubles,dxs=0dx=(b-a)/Nforiinrange(N):s+=f(a+i*dx)returns*dx1234567891011cdefdoublef(doublex):#返回值也有类型判断    returnx**2-x cdefdoubleintegrate_f(doublea,doubleb,intN):    cdefinti    cdefdoubles,dx    s=0    dx=(b-a)/N    foriinrange(N):        s+=f(a+i*dx)    returns*dx然后重新运行上面的第三四步：结果如下Pythoncallfcost:1.19209289551e-06　　callintegrate_fcost:0.18703818321212callfcost:1.19209289551e-06　　callintegrate_fcost:0.187038183212Amazing！pypypypy是CPython的一个替代实现，其最主要的优势就是pypy的速度，下面是官网的测试结果：在实际项目中测试，pypy大概比cpython要快3到5倍！pypy的性能提升来自JITCompiler。在前文提到google的UnladenSwallow项目也是想在CPython中引入JIT，在这个项目失败后，很多开发人员都开始加入pypy的开发和优化。另外pypy占用的内存更少，而且支持stackless，基本等同于协程。pypy的缺点在于对C扩展方面支持的不太好，需要使用CFFi来做binding。对于使用广泛的library来说，一般都会支持pypy，但是小众的、或者自行开发的C扩展就需要重新封装了。ChangeLog2017.03.10增加了对__slots__的介绍references回到顶部编程语言benchmarkpython属性查找pythonprofileryappigreenletprofilerpython-profiling-toolspythonCAPIcythonPyrexcythondocumentpypy2赞4收藏1评论"], "art_create_time": ["2017/11/27"], "art_title": ["Python 性能优化"], "art_url": ["http://python.jobbole.com/88926/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2017/11/7acbaf502a752190a99c5c9f68548d53.png"]},
{"art_content": ["原文出处：xybaby   在Python中，属性查找（attributelookup）是比较复杂的，特别是涉及到描述符descriptor的时候。在上一文章末尾，给出了一段代码，就涉及到descriptor与attributelookup的问题。而get系列函数(__get__,__getattr__,__getattribute__)也很容易搞晕，本文就这些问题简单总结一下。首先，我们知道：python中一切都是对象，“everythingisobject”，包括类，类的实例，数字，模块任何object都是类（classortype）的实例（instance）如果一个descriptor只实现了__get__方法，我们称之为non-datadescriptor，如果同时实现了__get____set__我们称之为datadescriptor。实例属性查找按照pythondoc，如果obj是某个类的实例，那么obj.name（以及等价的getattr(obj,’name’)）首先调用__getattribute__。如果类定义了__getattr__方法，那么在__getattribute__抛出AttributeError的时候就会调用到__getattr__，而对于描述符(__get__）的调用，则是发生在__getattribute__内部的。官网文档是这么描述的Theimplementationworksthroughaprecedencechainthatgivesdatadescriptorspriorityoverinstancevariables,instancevariablespriorityovernon-datadescriptors,andassignslowestpriorityto__getattr__()ifprovided.obj=Clz(),那么obj.attr顺序如下：（1）如果“attr”是出现在Clz或其基类的__dict__中，且attr是datadescriptor，那么调用其__get__方法,否则（2）如果“attr”出现在obj的__dict__中，那么直接返回obj.__dict__[‘attr’]，否则（3）如果“attr”出现在Clz或其基类的__dict__中（3.1）如果attr是non-datadescriptor，那么调用其__get__方法，否则（3.2）返回__dict__[‘attr’]（4）如果Clz有__getattr__方法，调用__getattr__方法，否则（5）抛出AttributeError下面是测试代码：#coding=utf-8classDataDescriptor(object):def__init__(self,init_value):self.value=init_valuedef__get__(self,instance,typ):return'DataDescriptor__get__'def__set__(self,instance,value):print('DataDescriptor__set__')self.value=valueclassNonDataDescriptor(object):def__init__(self,init_value):self.value=init_valuedef__get__(self,instance,typ):return('NonDataDescriptor__get__')classBase(object):dd_base=DataDescriptor(0)ndd_base=NonDataDescriptor(0)classDerive(Base):dd_derive=DataDescriptor(0)ndd_derive=NonDataDescriptor(0)same_name_attr='attrinclass'def__init__(self):self.not_des_attr='Iamnotdescriptorattr'self.same_name_attr='attrinobject'def__getattr__(self,key):return'__getattr__withkey%s'%keydefchange_attr(self):self.__dict__['dd_base']='dd_basenowinobjectdict'self.__dict__['ndd_derive']='ndd_derivenowinobjectdict'defmain():b=Base()d=Derive()print'Deriveobjectdict',d.__dict__assertd.dd_base==\"DataDescriptor__get__\"assertd.ndd_derive=='NonDataDescriptor__get__'assertd.not_des_attr=='Iamnotdescriptorattr'assertd.no_exists_key=='__getattr__withkeyno_exists_key'assertd.same_name_attr=='attrinobject'd.change_attr()print'Deriveobjectdict',d.__dict__assertd.dd_base!='dd_basenowinobjectdict'assertd.ndd_derive=='ndd_derivenowinobjectdict'try:b.no_exists_keyexceptException,e:assertisinstance(e,AttributeError)if__name__=='__main__':main()12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061#coding=utf-8classDataDescriptor(object):    def__init__(self,init_value):        self.value=init_value     def__get__(self,instance,typ):        return'DataDescriptor__get__'     def__set__(self,instance,value):        print('DataDescriptor__set__')        self.value=value classNonDataDescriptor(object):    def__init__(self,init_value):        self.value=init_value     def__get__(self,instance,typ):        return('NonDataDescriptor__get__') classBase(object):    dd_base=DataDescriptor(0)    ndd_base=NonDataDescriptor(0)  classDerive(Base):    dd_derive=DataDescriptor(0)    ndd_derive=NonDataDescriptor(0)    same_name_attr='attrinclass'     def__init__(self):        self.not_des_attr='Iamnotdescriptorattr'        self.same_name_attr='attrinobject'     def__getattr__(self,key):        return'__getattr__withkey%s'%key     defchange_attr(self):        self.__dict__['dd_base']='dd_basenowinobjectdict'        self.__dict__['ndd_derive']='ndd_derivenowinobjectdict' defmain():    b=Base()    d=Derive()    print'Deriveobjectdict',d.__dict__    assertd.dd_base==\"DataDescriptor__get__\"    assertd.ndd_derive=='NonDataDescriptor__get__'    assertd.not_des_attr=='Iamnotdescriptorattr'    assertd.no_exists_key=='__getattr__withkeyno_exists_key'    assertd.same_name_attr=='attrinobject'    d.change_attr()    print'Deriveobjectdict',d.__dict__    assertd.dd_base!='dd_basenowinobjectdict'    assertd.ndd_derive=='ndd_derivenowinobjectdict'     try:        b.no_exists_key    exceptException,e:        assertisinstance(e,AttributeError) if__name__=='__main__':    main()注意第50行，change_attr给实例的__dict__里面增加了两个属性。通过上下两条print的输出如下：　　Deriveobjectdict{‘same_name_attr’:‘attrinobject’,‘not_des_attr’:‘Iamnotdescriptorattr’}Deriveobjectdict{‘same_name_attr’:‘attrinobject’,‘ndd_derive’:‘ndd_derivenowinobjectdict‘,‘not_des_attr’:‘Iamnotdescriptorattr’,‘dd_base’:‘dd_basenowinobjectdict‘}调用change_attr方法之后，dd_base既出现在类的__dict__（作为datadescriptor）,也出现在实例的__dict__，因为attributelookup的循序，所以优先返回的还是Clz.__dict__[‘dd_base’]。而ndd_base虽然出现在类的__dict__，但是因为是nondatadescriptor，所以优先返回obj.__dict__[‘dd_base’]。其他：line48,line56表明了__getattr__的作用。line49表明obj.__dict__优先于Clz.__dict__cached_property例子我们再来看看上一文章的这段代码。importfunctools,timeclasscached_property(object):\"\"\"Apropertythatisonlycomputedonceperinstanceandthenreplacesitselfwithanordinaryattribute.Deletingtheattributeresetstheproperty.\"\"\"def__init__(self,func):functools.update_wrapper(self,func)self.func=funcdef__get__(self,obj,cls):ifobjisNone:returnselfvalue=obj.__dict__[self.func.__name__]=self.func(obj)returnvalueclassTestClz(object):@cached_propertydefcomplex_calc(self):print'verycomplex_calc'returnsum(range(100))if__name__=='__main__':t=TestClz()print'>>>firstcall'printt.complex_calcprint'>>>secondcall'printt.complex_calc123456789101112131415161718192021222324252627importfunctools,timeclasscached_property(object):    \"\"\"Apropertythatisonlycomputedonceperinstanceandthenreplaces        itselfwithanordinaryattribute.Deletingtheattributeresetsthe        property.\"\"\"     def__init__(self,func):        functools.update_wrapper(self,func)        self.func=func     def__get__(self,obj,cls):        ifobjisNone:returnself        value=obj.__dict__[self.func.__name__]=self.func(obj)        returnvalue classTestClz(object):    @cached_property    defcomplex_calc(self):        print'verycomplex_calc'        returnsum(range(100)) if__name__=='__main__':    t=TestClz()    print'>>>firstcall'    printt.complex_calc    print'>>>secondcall'    printt.complex_calccached_property是一个non-datadescriptor。在TestClz中，用cached_property装饰方法complex_calc，返回值是一个descriptor实例，所以在调用的时候没有使用小括号。第一次调用t.complex_calc之前，obj(t)的__dict__中没有”complex_calc“，根据查找顺序第三条，执行cached_property.__get__,这个函数代用缓存的complex_calc函数计算出结果，并且把结果放入obj.__dict__。那么第二次访问t.complex_calc的时候，根据查找顺序，第二条有限于第三条，所以就直接返回obj.__dict__[‘complex_calc’]。bottle的源码中还有两个descriptor，非常厉害！类属性查找前面提到过，类的也是对象，类是元类（metaclass）的实例，所以类属性的查找顺序基本同上。区别在于第二步，由于Clz可能有基类，所以是在Clz及其基类的__dict__”查找“attr，注意这里的查找并不是直接返回clz.__dict__[‘attr’]。具体来说，这第二步分为以下两种情况：（2.1）如果clz.__dict__[‘attr’]是一个descriptor（不管是datadescriptor还是non-datadescriptor），都调用其__get__方法（2.2）否则返回clz.__dict__[‘attr’]这就解释了一个很有意思的问题：method与function的问题Python>>>classWidget(object):...deffunc(self):...pass...>>>w=Widget()>>>Widget.__dict__dict_proxy({'__dict__':<attribute'__dict__'of'Widget'objects>,'__module__':'__main__','__weakref__':<attribute'__weakref__'of'Widget'objects>,'__doc__':None,'func':<functionfuncat0x7fdc7d0d1668>})>>>w.__dict__{}>>>Widget.__dict__['func']<functionfuncat0x7fdc7d0d1668>>>>Widget.func<unboundmethodWidget.func>>>>12345678910111213141516>>>classWidget(object):...deffunc(self):...pass...>>>w=Widget()>>>Widget.__dict__dict_proxy({'__dict__':<attribute'__dict__'of'Widget'objects>,'__module__':'__main__','__weakref__':<attribute'__weakref__'of'Widget'objects>,'__doc__':None,'func':<functionfuncat0x7fdc7d0d1668>})>>>w.__dict__{}  >>>Widget.__dict__['func']<functionfuncat0x7fdc7d0d1668>>>>Widget.func<unboundmethodWidget.func>>>>Widget是一个之定义了一个func函数的类，func是类的属性，这个也可以通过Widget.__dict__、w.__dict__看到。Widget.__dict__[‘func’]返回的是一个function，但Widget.func是一个unboundmethod，即Widget.func并不等同于Widget.__dict__[‘func’]，按照前面的类属性的访问顺序，我们可以怀疑，func是一个descriptor，这样才不会走到第2.2这种情况。验证如下：Python>>>dir(Widget.__dict__['func'])['__call__','__class__','__closure__','__code__','__defaults__','__delattr__','__dict__','__doc__','__format__','__get__','__getattribute__','__globals__','__hash__','__init__','__module__','__name__','__new__','__reduce__','__reduce_ex__','__repr__','__setattr__','__sizeof__','__str__','__subclasshook__','func_closure','func_code','func_defaults','func_dict','func_doc','func_globals','func_name']12>>>dir(Widget.__dict__['func'])['__call__','__class__','__closure__','__code__','__defaults__','__delattr__','__dict__','__doc__','__format__','__get__','__getattribute__','__globals__','__hash__','__init__','__module__','__name__','__new__','__reduce__','__reduce_ex__','__repr__','__setattr__','__sizeof__','__str__','__subclasshook__','func_closure','func_code','func_defaults','func_dict','func_doc','func_globals','func_name']属性赋值Python的属性赋值（attributeassignment）也会受到descriptor（datadescriptor）的影响，同时也会受到__setattr__函数的影响。当然Python中还有一个setattr，setattr(x,‘foobar’,123)等价于x.foobar=123，二者都叫attributeassignment。首先看看__setattr__:object.__setattr__(self,name,value)Calledwhenanattributeassignmentisattempted.Thisiscalledinsteadofthenormalmechanism那什么是normalmechanism，简单来说就是x.__dict__[‘foobar’]=123，不管’foobar’之前是否是x的属性（当然赋值之后就一定是了）。但是如果‘’foobar‘’是类属性，且是datadescriptor，那么回优先调用__set__。我们来看一个例子：classMaxValDes(object):def__init__(self,attr,max_val):self.attr=attrself.max_val=max_valdef__get__(self,instance,typ):returninstance.__dict__[self.attr]def__set__(self,instance,value):instance.__dict__[self.attr]=min(self.max_val,value)print'MaxValDes__set__',self.attr,instance.__dict__[self.attr]classWidget(object):a=MaxValDes('a',10)def__init__(self):self.a=0#def__setattr__(self,name,value):#self.__dict__[name]=value#print'Widget__setattr__',name,self.__dict__[name]if__name__=='__main__':w0=Widget()w0.a=123123456789101112131415161718192021222324classMaxValDes(object):    def__init__(self,attr,max_val):        self.attr=attr        self.max_val=max_val     def__get__(self,instance,typ):        returninstance.__dict__[self.attr]     def__set__(self,instance,value):        instance.__dict__[self.attr]=min(self.max_val,value)        print'MaxValDes__set__',self.attr,instance.__dict__[self.attr] classWidget(object):    a=MaxValDes('a',10)    def__init__(self):        self.a=0     #def__setattr__(self,name,value):    #    self.__dict__[name]=value    #    print'Widget__setattr__',name,self.__dict__[name] if__name__=='__main__':    w0=Widget()    w0.a=123输出如下：PythonMaxValDes__set__a0MaxValDes__set__a1012MaxValDes__set__a0MaxValDes__set__a10可以看到，即使Widget的实例也有一个‘a’属性，但是调用w.a的时候会调用类属性‘a’（一个descriptor）的__set__方法。如果不注释掉第18到第20行，输出如下PythonWidget__setattr__a0Widget__setattr__a12312Widget__setattr__a0Widget__setattr__a123可以看到，优先调用Widget的__setattr__方法。因此：对于属性赋值，obj=Clz(),那么obj.attr=var，按照这样的顺序：如果Clz定义了__setattr__方法，那么调用该方法，否则如果“attr”是出现在Clz或其基类的__dict__中，且attr是datadescriptor，那么调用其__set__方法,否则等价调用obj.__dict__[‘attr’]=varreferencesDescriptorHowToGuide,https://docs.python.org/2/howto/descriptor.html#descriptor-protocolObjectattributelookupinPython,http://www.betterprogramming.com/object-attribute-lookup-in-python.htmlpython__set____get__等解释,http://blog.csdn.net/huithe/article/details/74846061赞3收藏1评论"], "art_create_time": ["2017/11/27"], "art_title": ["深入理解 Python 的属性查找"], "art_url": ["http://python.jobbole.com/88937/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2015/02/edecceebedd0d04aa17bccba430ddcaf.jpg"]},
{"art_content": ["原文出处：freeCodeCamp   译文出处：oschina   第一个问题，什么是Python？根据Python之父GuidovanRossum的话，Python是：一种高级程序语言，其核心设计哲学是代码可读性和语法，能够让程序员用很少的代码来表达自己的想法。对于我来说，学习Python的首要原因是，Python是一种可以优雅编程的语言。它能够简单自然地写出代码和实现我的想法。另一个原因是我们可以将Python用在很多地方：数据科学、Web开发和机器学习等都可以使用Python来开发。Quora、Pinterest和Spotify都使用Python来进行他们的后端Web开发。那么让我们来学习一下Python吧。Python基础1.变量你可以把变量想象成一个用来存储值的单词。我们看个例子。Python中定义一个变量并为它赋值是很容易的。假如你想存储数字1到变量“one”，让我们试试看：Pythonone=11one=1超级简单吧？你只需要把值1分配给变量“one”。Pythontwo=2some_number=1000012two=2some_number=10000只要你想，你可以把任意的值赋给任何其他的变量。正如你从上面看到的那样，变量“two”存储整型变量2，变量“some_number”存储10000。除了整型，我们还可以使用布尔值（True/Flase）、字符串、浮点型和其他数据类型。Python#booleanstrue_boolean=Truefalse_boolean=False#stringmy_name=\"LeandroTk\"#floatbook_price=15.801#booleanstrue_boolean=Truefalse_boolean=False#stringmy_name=\"LeandroTk\"#floatbook_price=15.802.控制流程：条件语句“If”使用一个表达式来判断一个语句是True还是False，如果是True，那么执行if内的代码，例子如下：PythonifTrue:print(\"HelloPythonIf\")if2>1:print(\"2isgreaterthan1\")123ifTrue:  print(\"HelloPythonIf\")if2>1:  print(\"2isgreaterthan1\")2比1大，所以print代码被执行。当“if”里面的表达式是false时，“else”语句将会执行。Pythonif1>2:print(\"1isgreaterthan2\")else:print(\"1isnotgreaterthan2\")123if1>2:  print(\"1isgreaterthan2\")else:  print(\"1isnotgreaterthan2\")1比2小，所以“else”里面的代码会执行。你也可以使用“elif”语句：Pythonif1>2:print(\"1isgreaterthan2\")elif2>1:print(\"1isnotgreaterthan2\")else:print(\"1isequalto2\")1234if1>2:  print(\"1isgreaterthan2\")elif2>1:  print(\"1isnotgreaterthan2\")else:  print(\"1isequalto2\")3.循环和迭代在Python中，我们可以用不同的形式进行迭代。我会说下while和for。While循环：当语句是True时，while内部的代码块会执行。所以下面这段代码会打印出1到10。Pythonnum=1whilenum<=10:print(num)num+=1123num=1whilenum<=10:    print(num)    num+=1while循环需要循环条件，如果条件一直是True，它将会一直迭代，当num的值为11时，循环条件为false。另一段代码可以帮你更好的理解while语句的用法：Pythonloop_condition=Truewhileloop_condition:print(\"LoopConditionkeeps:%s\"%(loop_condition))loop_condition=False123loop_condition=Truewhileloop_condition:    print(\"LoopConditionkeeps:%s\"%(loop_condition))    loop_condition=False循环条件是True所以会一直迭代，直到为False。For循环：你可以在代码块上应用变量“num”，而“for”语句将为你迭代它。此代码将打印与while中相同的代码：从1到10。Pythonforiinrange(1,11):print(i)12foriinrange(1,11):  print(i)瞧见没？这太简单了。i的范围从1开始一直到第11个元素（10是第十个元素）List：集合|数组|数据结构假如你想要在一个变量里存储整数1，但是你也要存储2和3,4,5…不是用成百上千个变量，我有别的方法存储这些我想要存储的整数吗？你已经猜到了，确实有别的存储它们的方法。列表是一个集合，它能够存储一列值（就像你想要存储的这些），那么让我们来用一下它：Pythonmy_integers=[1,2,3,4,5]1my_integers=[1,2,3,4,5]这真的很简单。我们创建了一个叫做my_integer的数组并且把数据存到了里面。也许你会问：“我要怎样获取数组里的值？”问的好。列表有一个叫做索引的概念。第一个元素的下表是索引0（0）。第二个的索引是1，以此类推，你应该明白的。为了使它更加简洁，我们可以用它的索引代表数组元素。我画了出来：用Python的语法，也很好去理解：Pythonmy_integers=[5,7,1,3,4]print(my_integers[0])#5print(my_integers[1])#7print(my_integers[4])#412my_integers=[5,7,1,3,4]print(my_integers[0])#5print(my_integers[1])#7print(my_integers[4])#4假如你不想存整数。你只想去存一些字符串，像你亲戚名字的列表。我的看起来是类似这样的：Pythonrelatives_names=[\"Toshiaki\",\"Juliana\",\"Yuji\",\"Bruno\",\"Kaio\"]print(relatives_names[4])#Kaio123relatives_names=[  \"Toshiaki\",  \"Juliana\",  \"Yuji\",  \"Bruno\",  \"Kaio\"] print(relatives_names[4])#Kaio它的原理跟存整数一样，很友好。我们只学习了列表的索引是如何工作的，我还需要告诉你如何向列表的数据结构中添加一个元素（向列表中添加一个项目）。最常用的向列表中添加新数据的方法是拼接。我们来看一下它是如何使用的：Pythonbookshelf=[]bookshelf.append(\"TheEffectiveEngineer\")bookshelf.append(\"The4HourWorkWeek\")print(bookshelf[0])#TheEffectiveEngineerprint(bookshelf[1])#The4HourWorkW1234bookshelf=[]bookshelf.append(\"TheEffectiveEngineer\")bookshelf.append(\"The4HourWorkWeek\")print(bookshelf[0])#TheEffectiveEngineerprint(bookshelf[1])#The4HourWorkW拼接超级简单，你仅需要把一个元素（比如“有效的机器”）作为拼接参数。好了，关于列表的知识这些就够了，让我们来看一下其它的数据结构。字典：Key-Value数据结构现在我们知道List是有索引的整型数字集合。但如果我们不像使用整型数字作为索引呢？我们可以用其他的一些数据结构，比如数字、字符串或者其他类型的索引。让我们学习下字典这种数据结构。字典是一个键值对的集合。字典差不多长这样：Pythondictionary_example={\"key1\":\"value1\",\"key2\":\"value2\",\"key3\":\"value3\"}12345dictionary_example={  \"key1\":\"value1\",  \"key2\":\"value2\",  \"key3\":\"value3\"}Key是指向value的索引。我们如何访问字典中的value呢？你应该猜到了，那就是使用key。我们试一下：Pythondictionary_tk={\"name\":\"Leandro\",\"nickname\":\"Tk\",\"nationality\":\"Brazilian\"}print(\"Mynameis%s\"%(dictionary_tk[\"name\"]))#MynameisLeandroprint(\"Butyoucancallme%s\"%(dictionary_tk[\"nickname\"]))#ButyoucancallmeTkprint(\"AndbythewayI'm%s\"%(dictionary_tk[\"nationality\"]))#AndbythewayI'mBrazilian123456789dictionary_tk={  \"name\":\"Leandro\",  \"nickname\":\"Tk\",  \"nationality\":\"Brazilian\"} print(\"Mynameis%s\"%(dictionary_tk[\"name\"]))#MynameisLeandroprint(\"Butyoucancallme%s\"%(dictionary_tk[\"nickname\"]))#ButyoucancallmeTkprint(\"AndbythewayI'm%s\"%(dictionary_tk[\"nationality\"]))#AndbythewayI'mBrazilian我们有个key(age)value(24），使用字符串作为key整型作为value。我创建了一个关于我的字典，其中包含我的名字、昵称和国籍。这些属性是字典中的key。就像我们学过的使用索引访问list一样，我们同样使用索引（在字典中key就是索引）来访问存储在字典中的value。正如我们使用list那样，让我们学习下如何向字典中添加元素。字典中主要是指向value的key。当我们添加元素的时候同样如此：Pythondictionary_tk={\"name\":\"Leandro\",\"nickname\":\"Tk\",\"nationality\":\"Brazilian\",\"age\":24}print(\"Mynameis%s\"%(dictionary_tk[\"name\"]))#MynameisLeandroprint(\"Butyoucancallme%s\"%(dictionary_tk[\"nickname\"]))#ButyoucancallmeTkprint(\"AndbythewayI'm%iand%s\"%(dictionary_tk[\"age\"],dictionary_tk[\"nationality\"]))#AndbythewayI'mBrazilian12345678910dictionary_tk={  \"name\":\"Leandro\",  \"nickname\":\"Tk\",  \"nationality\":\"Brazilian\",  \"age\":24} print(\"Mynameis%s\"%(dictionary_tk[\"name\"]))#MynameisLeandroprint(\"Butyoucancallme%s\"%(dictionary_tk[\"nickname\"]))#ButyoucancallmeTkprint(\"AndbythewayI'm%iand%s\"%(dictionary_tk[\"age\"],dictionary_tk[\"nationality\"]))#AndbythewayI'mBrazilian我们只需要将一个字典中的一个key指向一个value。没什么难的，对吧？迭代：通过数据结构进行循环跟我们在Python基础中学习的一样，List迭代十分简单。我们Python开发者通常使用For循环。我们试试看：Pythonbookshelf=[\"TheEffectiveEngineer\",\"The4hoursworkweek\",\"ZerotoOne\",\"LeanStartup\",\"Hooked\"]forbookinbookshelf:print(book)12345678910bookshelf=[  \"TheEffectiveEngineer\",  \"The4hoursworkweek\",  \"ZerotoOne\",  \"LeanStartup\",  \"Hooked\"] forbookinbookshelf:    print(book)对于在书架上的每本书，我们打印（可以做任何操作）到控制台上。超级简单和直观吧。这就是Python的美妙之处。对于哈希数据结构，我们同样可以使用for循环，不过我们需要使用key来进行：Pythondictionary={\"some_key\":\"some_value\"}forkeyindictionary:print(\"%s-->%s\"%(key,dictionary[key]))#some_key-->some_value123dictionary={\"some_key\":\"some_value\"}forkeyindictionary:    print(\"%s-->%s\"%(key,dictionary[key]))#some_key-->some_value上面是如何在字典中使用For循环的例子。对于字典中的每个key，我们打印出key和key所对应的value。另一种方式是使用iteritems方法。Pythondictionary={\"some_key\":\"some_value\"}forkey,valueindictionary.items():print(\"%s-->%s\"%(key,value))#some_key-->some_value123dictionary={\"some_key\":\"some_value\"}forkey,valueindictionary.items():    print(\"%s-->%s\"%(key,value))#some_key-->some_value我们命名两个参数为key和value，但是这不是必要的。我们可以随意命名。我们看下：Pythondictionary_tk={\"name\":\"Leandro\",\"nickname\":\"Tk\",\"nationality\":\"Brazilian\",\"age\":24}forattribute,valueindictionary_tk.items():print(\"My%sis%s\"%(attribute,value))#MynameisLeandro#MynicknameisTk#MynationalityisBrazilian#Myageis241234567891011121314dictionary_tk={  \"name\":\"Leandro\",  \"nickname\":\"Tk\",  \"nationality\":\"Brazilian\",  \"age\":24} forattribute,valueindictionary_tk.items():    print(\"My%sis%s\"%(attribute,value))    #MynameisLeandro#MynicknameisTk#MynationalityisBrazilian#Myageis24可以看到我们使用了attribute作为字典中key的参数，这与使用key命名具有同样的效果。真是太棒了！类&对象一些理论:对象是对现实世界实体的表示，如汽车、狗或自行车。这些对象有两个共同的主要特征：数据和行为。汽车有数据，如车轮的数量，车门的数量和座位的空间，并且它们可以表现出其行为：它们可以加速，停止，显示剩余多少燃料，以及许多其他的事情。我们将数据看作是面向对象编程中的属性和行为。又表示为：数据→属性和行为→方法而类是创建单个对象的蓝图。在现实世界中，我们经常发现许多相同类型的对象。比如说汽车。所有的汽车都有相同的构造和模型（都有一个引擎，轮子，门等）。每辆车都是由同一套蓝图构造成的，并具有相同的组件。Python面向对象编程模式：ONPython，作为一种面向对象编程语言，存在这样的概念：类和对象。一个类是一个蓝图，是对象的模型。那么，一个类是一个模型，或者是一种定义属性和行为的方法（正如我们在理论部分讨论的那样）。举例来说，一个车辆类有它自己的属性来定义这个对象是个什么样的车辆。一辆车的属性有轮子数量，能源类型，座位容量和最大时速这些。考虑到这一点，让我们来看看Python的类的语法：PythonclassVehicle:pass12classVehicle:    pass上边的代码，我们使用class语句来定义一个类。是不是很容易？对象是一个类的实例化，我们可以通过类名来进行实例化。Pythoncar=Vehicle()print(car)#<__main__.Vehicleinstanceat0x7fb1de6c2638>12car=Vehicle()print(car)#<__main__.Vehicleinstanceat0x7fb1de6c2638>在这里，car是类Vehicle的对象（或者实例化）。记得车辆类有四个属性：轮子的数量，油箱类型，座位容量和最大时速。当我们新建一个车辆对象时要设置所有的属性。所以在这里，我们定义一个类在它初始化的时候接受参数：PythonclassVehicle:def__init__(self,number_of_wheels,type_of_tank,seating_capacity,maximum_velocity):self.number_of_wheels=number_of_wheelsself.type_of_tank=type_of_tankself.seating_capacity=seating_capacityself.maximum_velocity=maximum_velocity123456classVehicle:    def__init__(self,number_of_wheels,type_of_tank,seating_capacity,maximum_velocity):        self.number_of_wheels=number_of_wheels        self.type_of_tank=type_of_tank        self.seating_capacity=seating_capacity        self.maximum_velocity=maximum_velocity这个init方法。我们称之为构造函数。因此当我们在创建一个车辆对象时，可以定义这些属性。想象一下，我们喜欢TeslaModelS，所以我们想创建一个这种类型的对象。它有四个轮子，使用电能源，五座并且最大时时速是250千米（155英里）。我们开始创建这样一个对象：Pythontesla_model_s=Vehicle(4,'electric',5,250)1tesla_model_s=Vehicle(4,'electric',5,250)四轮+电能源+五座+最大时速250千米。所有的属性已经设置了。但我们该如何访问这些属性值呢？我们给对象发送消息以向其请求该值。我们称之为方法。它是对象的行为。让我们实现它：PythonclassVehicle:def__init__(self,number_of_wheels,type_of_tank,seating_capacity,maximum_velocity):self.number_of_wheels=number_of_wheelsself.type_of_tank=type_of_tankself.seating_capacity=seating_capacityself.maximum_velocity=maximum_velocitydefnumber_of_wheels(self):returnself.number_of_wheelsdefset_number_of_wheels(self,number):self.number_of_wheels=number12345678classVehicle:    def__init__(self,number_of_wheels,type_of_tank,seating_capacity,maximum_velocity):        self.number_of_wheels=number_of_wheels        self.type_of_tank=type_of_tank        self.seating_capacity=seating_capacity        self.maximum_velocity=maximum_velocity    defnumber_of_wheels(self):        returnself.number_of_wheels    defset_number_of_wheels(self,number):        self.number_of_wheels=number这是两个方法number_of_wheels和set_number_of_wheels的实现。我们将其称为getter&setter。因为第一个函数是获取属性值，第二个函数是给属性设置新的值。在Python中，我们可以使用@property(修饰符)来定义getters和setters。让我们看看实际代码：PythonclassVehicle:def__init__(self,number_of_wheels,type_of_tank,seating_capacity,maximum_velocity):self.number_of_wheels=number_of_wheelsself.type_of_tank=type_of_tankself.seating_capacity=seating_capacityself.maximum_velocity=maximum_velocity@propertydefnumber_of_wheels(self):returnself.number_of_wheels@number_of_wheels.setterdefnumber_of_wheels(self,number):self.number_of_wheels=number12345678910classVehicle:    def__init__(self,number_of_wheels,type_of_tank,seating_capacity,maximum_velocity):        self.number_of_wheels=number_of_wheels        self.type_of_tank=type_of_tank        self.seating_capacity=seating_capacity        self.maximum_velocity=maximum_velocity    @property    defnumber_of_wheels(self):        returnself.number_of_wheels    @number_of_wheels.setter    defnumber_of_wheels(self,number):        self.number_of_wheels=number并且我们可以将这些方法作为属性使用：Pythontesla_model_s=Vehicle(4,'electric',5,250)print(tesla_model_s.number_of_wheels)#4tesla_model_s.number_of_wheels=2#settingnumberofwheelsto2print(tesla_model_s.number_of_wheels)#212tesla_model_s=Vehicle(4,'electric',5,250)print(tesla_model_s.number_of_wheels)#4tesla_model_s.number_of_wheels=2#settingnumberofwheelsto2print(tesla_model_s.number_of_wheels)#2这和方法定义有轻微的不同。这里的方法是按照属性执行的。例如当我们设置新的轮胎数目时，我们并不将这两个看做参数，而是将数值2设置给number_of_wheels。这是编写python风格的getter和setter代码的一种方式。但我们也可以将该方法用于其他事项，例如“make_noise”方法。让我们看看：PythonclassVehicle:def__init__(self,number_of_wheels,type_of_tank,seating_capacity,maximum_velocity):self.number_of_wheels=number_of_wheelsself.type_of_tank=type_of_tankself.seating_capacity=seating_capacityself.maximum_velocity=maximum_velocitydefmake_noise(self):print('VRUUUUUUUM')1234567classVehicle:    def__init__(self,number_of_wheels,type_of_tank,seating_capacity,maximum_velocity):        self.number_of_wheels=number_of_wheels        self.type_of_tank=type_of_tank        self.seating_capacity=seating_capacity        self.maximum_velocity=maximum_velocity    defmake_noise(self):        print('VRUUUUUUUM')当我们调用此方法时，它仅仅返回一个字符串“VRRRRUUUUM.”Pythontesla_model_s=Vehicle(4,'electric',5,250)tesla_model_s.make_noise()#VRUUUUUUUM12tesla_model_s=Vehicle(4,'electric',5,250)tesla_model_s.make_noise()#VRUUUUUUUM封装:隐藏信息封装是一种限制直接访问对象数据和方法的机制。但与此同时，它使得在数据上操作更简单（对象的方法）。“封装可被用于隐藏数据成员和成员函数。按照这个定义，封装意味着对象的内部表示一般在对象定义的外部视图中隐藏。” — Wikipedia对象的所有内部表示都对外部隐藏了。只有对象本身可以与其内部数据交互。首先，我们需要理解公开的、非公开的实例变量和方法的工作原理。公共实例变量对于Python类，我们可以在我们的构造函数方法中初始化一个公共实例变量。让我们看看这个：在这个构造方法中:PythonclassPerson:def__init__(self,first_name):self.first_name=first_name123classPerson:    def__init__(self,first_name):        self.first_name=first_name在这里，我们将first_name值作为参数应用于公共实例变量。Pythontk=Person('TK')print(tk.first_name)#=>TK12tk=Person('TK')print(tk.first_name)#=>TK在类中:PythonclassPerson:first_name='TK'12classPerson:    first_name='TK'在这里，我们不需要将first_name作为参数，所有的实例对象都有一个用TK初始化的类属性。Pythontk=Person()print(tk.first_name)#=>TK12tk=Person()print(tk.first_name)#=>TK太酷了，现在我们已经了解到，我们可以使用公共实例变量和类属性。关于公共部分的另一个有趣的事情是我们可以管理变量值。我的意思是什么呢？我们的对象可以管理它的变量值：Get和Set变量值。还是在Person类中，我们想为它的first_name变量设置另一个值：Pythontk=Person('TK')tk.first_name='Kaio'print(tk.first_name)#=>Kaio12tk=Person('TK')tk.first_name='Kaio'print(tk.first_name)#=>Kaio这就可以了，我们只是为first_name实例变量设置另一个值（kaio），并更新了值。就这么简单。因为这是一个公共变量，我们是可以这么做的。Non-public实例变量这里我们并没有使用术语“private”，因为在Python中所有属性都不是真的私有的（没有通常不必要的工作量）。 — PEP8作为publicinstancevariable（公共实例变量），我们可以在构造方法或类内部定义non-publicinstancevariable（非公共实例变量）。语法上的区别是：对于non-publicinstancevariables（非公共实例变量），在变量名前使用下划线(_)。“除了从对象内部外无法被访问的‘Private’实例变量在Python中并不存在。然而，这里有一个多数Python代码都会遵守的惯例：使用下划线作为前缀的命名(例如_spam)应该被认为是API的非公开部分（不管是函数、方法还是数据成员）” — Python软件基础这里是示例代码:PythonclassPerson:def__init__(self,first_name,email):self.first_name=first_nameself._email=email1234classPerson:    def__init__(self,first_name,email):        self.first_name=first_name        self._email=email你看到了email变量了吗？这就是我们如何定义非公共变量的方法：Pythontk=Person('TK','tk@mail.com')print(tk._email)#tk@mail.com12tk=Person('TK','tk@mail.com')print(tk._email)#tk@mail.com我们可以访问并更新它。非公共变量仅仅是一个惯用法，并且应该被当做API的非公共部分。所以我们使用一个在类定义内部的方法来实现该功能。让我们实现两个方法(email和update_email)以加深理解：PythonclassPerson:def__init__(self,first_name,email):self.first_name=first_nameself._email=emaildefupdate_email(self,new_email):self._email=new_emaildefemail(self):returnself._email123456classPerson:    def__init__(self,first_name,email):        self.first_name=first_name        self._email=email    defupdate_email(self,new_email):        self._email=new_email    defemail(self):        returnself._email现在我们可以使用这两个方法来更新及访问非公开变量了。示例如下Pythontk=Person('TK','tk@mail.com')print(tk.email())#=>tk@mail.comtk._email='new_tk@mail.com'print(tk.email())#=>tk@mail.comtk.update_email('new_tk@mail.com')print(tk.email())#=>new_tk@mail.com123tk=Person('TK','tk@mail.com')print(tk.email())#=>tk@mail.comtk._email='new_tk@mail.com'print(tk.email())#=>tk@mail.comtk.update_email('new_tk@mail.com')print(tk.email())#=>new_tk@mail.com我们使用first_nameTK和emailtk@mail.com初始化了一个新对象使用方法访问非公开变量email并输出它尝试在类外部设置一个新的email我们需要将非公开变量视为API的非公开部分使用我们的实例方法来更新非公开变量成功!我们使用辅助方法在类内部更新了它。公共方法对于公共方法，我们也可以在类中使用它们：PythonclassPerson:def__init__(self,first_name,age):self.first_name=first_nameself._age=agedefshow_age(self):returnself._age12345classPerson:    def__init__(self,first_name,age):        self.first_name=first_name        self._age=age    defshow_age(self):        returnself._age让我们来测试一下:Pythontk=Person('TK',25)print(tk.show_age())#=>2512tk=Person('TK',25)print(tk.show_age())#=>25很好–我们在类中使用它没有任何问题。非公共方法但是用非公开的方法，我们无法做到这一点。如果我们想实现相同的Person类，现在使用有下划线（_）的show_age非公共方法。PythonclassPerson:def__init__(self,first_name,age):self.first_name=first_nameself._age=agedef_show_age(self):returnself._age12345classPerson:    def__init__(self,first_name,age):        self.first_name=first_name        self._age=age    def_show_age(self):        returnself._age现在，我们将尝试用我们的对象来调用这个非公共的方法：Pythontk=Person('TK',25)print(tk._show_age())#=>2512tk=Person('TK',25)print(tk._show_age())#=>25我们可以访问和更新它。非公共的方法只是一个惯例，应该被视为API的非公开部分。以下是我们如何使用它的一个例子：PythonclassPerson:def__init__(self,first_name,age):self.first_name=first_nameself._age=agedefshow_age(self):returnself._get_age()def_get_age(self):returnself._agetk=Person('TK',25)print(tk.show_age())#=>25123456789classPerson:    def__init__(self,first_name,age):        self.first_name=first_name        self._age=age    defshow_age(self):        returnself._get_age()    def_get_age(self):        returnself._age tk=Person('TK',25)print(tk.show_age())#=>25这里有一个_get_age非公共方法和一个show_age公共方法。show_age可以被我们的对象（不在我们的类中）使用，而_get_age只用在我们的类定义里面使用（在show_age方法里面）。但是同样的，这样的做法通常是惯例。封装小结通过封装，我们可以确保对象的内部表示是对外部隐藏的。继承：行为和特征某些物体有一些共同之处：它们的行为和特征。例如，我继承了我父亲的一些特征和行为。我继承了他的眼睛和头发的特征，以及他的急躁和内向的行为。在面向对象编程中，类可以继承另一个类的共同特征（数据）和行为（方法）。我们来看另一个例子，并用Python实现它。想象一下汽车。车轮数量，座位容量和最大速度都是一辆车的属性。我们可以说ElectricCar类从普通的Car类继承了这些相同的属性。PythonclassCar:def__init__(self,number_of_wheels,seating_capacity,maximum_velocity):self.number_of_wheels=number_of_wheelsself.seating_capacity=seating_capacityself.maximum_velocity=maximum_velocity12345classCar:    def__init__(self,number_of_wheels,seating_capacity,maximum_velocity):        self.number_of_wheels=number_of_wheels        self.seating_capacity=seating_capacity        self.maximum_velocity=maximum_velocity我们Car类的实现:Pythonmy_car=Car(4,5,250)print(my_car.number_of_wheels)print(my_car.seating_capacity)print(my_car.maximum_velocity)1234my_car=Car(4,5,250)print(my_car.number_of_wheels)print(my_car.seating_capacity)print(my_car.maximum_velocity)一旦初始化，我们就可以使用所有创建的实例变量。太棒了。在Python中，我们将父类作为子的参数来进行继承。ElectricCar类可以继承我们的Car类。PythonclassElectricCar(Car):def__init__(self,number_of_wheels,seating_capacity,maximum_velocity):Car.__init__(self,number_of_wheels,seating_capacity,maximum_velocity)123classElectricCar(Car):    def__init__(self,number_of_wheels,seating_capacity,maximum_velocity):        Car.__init__(self,number_of_wheels,seating_capacity,maximum_velocity)就这么简单。我们不需要实现任何其他方法，因为这个类已经完成了父类的继承（继承自Car类）。我们来证明一下：Pythonmy_electric_car=ElectricCar(4,5,250)print(my_electric_car.number_of_wheels)#=>4print(my_electric_car.seating_capacity)#=>5print(my_electric_car.maximum_velocity)#=>2501234my_electric_car=ElectricCar(4,5,250)print(my_electric_car.number_of_wheels)#=>4print(my_electric_car.seating_capacity)#=>5print(my_electric_car.maximum_velocity)#=>250干的漂亮。2赞21收藏10评论"], "art_create_time": ["2017/11/28"], "art_title": ["从 Zero 到 Hero ，一文掌握 Python"], "art_url": ["http://python.jobbole.com/88940/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2017/11/8ef4df4888b257b5ea7bbd4b033a519c.png"]},
{"art_content": ["本文作者：伯乐在线-iPytLab。未经作者许可，禁止转载！欢迎加入伯乐在线专栏作者。前言上一篇中介绍了计算图以及前向传播的实现，本文中将主要介绍对于模型优化非常重要的反向传播算法以及反向传播算法中梯度计算的实现。因为在计算梯度的时候需要涉及到矩阵梯度的计算，本文针对几种常用操作的梯度计算和实现进行了较为详细的介绍。如有错误欢迎指出。首先先简单总结一下,实现反向传播过程主要就是完成两个任务:实现不同操作输出对输入的梯度计算实现根据链式法则计算损失函数对不同节点的梯度计算再附上SimpleFlow的代码地址: https://github.com/PytLab/simpleflow正文反向传播对于我们构建的模型进行优化通常需要两步：1.求损失函数针对变量的梯度；2.根据梯度信息进行参数优化(例如梯度下降).那么该如何使用我们构建的计算图来计算损失函数对于图中其他节点的梯度呢？通过链式法则。我们还是通过上篇中的表达式Loss(x,y,z)=z(x+y)对应的计算图来说明:我们把上面的操作节点使用字母进行标记，可以将每个操作看成一个函数，接受一个或两个输入有一个或者多个输出,则上面的表达Loss(x,y,z)=z(x+y) 可以写成 Loss(x,y,z)=g(z,f(x,y))那么根据链式法则我们可以得到Loss对x的导数为: 假设图中的节点已经计算出了自己的输出值，我们把节点的输出值放到节点里面如下:然后再把链式法则的式子每一项一次计算，在图中也就是从后向前进行计算:1. ∂Loss/∂g=12.∂g/∂f=z=6 (当然也可以计算出∂g/∂z=x+y=5).进而求出∂Loss/∂f=∂Loss/∂g*∂g/∂f=1×z=63. ∂f/∂x=1 (同时也可以算出∂f∂y=1).进而求出∂Loss/∂x=∂Loss/∂g*∂g/∂f*f∂/f∂x=1×z×1=6这样从后向前逐级计算通过链式法则就可以计算出与损失值对其相关节点的梯度了。因此我们下一步要做的就是给定某个损失函数节点并计算它对于某一节点的梯度计算。下面在看一个不同的计算图:这里的x节点有将输出到两个不同的节点中，此时我们需要计算所有从g到x的路径然后按照上面单挑路径的链式法则计算方法计算每条路径的梯度值，最终再将不同路径的梯度求和即可。因此Loss对x的梯度为:梯度计算通过上面对反向传播的介绍我们已经知道损失值对某个节点的梯度是怎么求的(具体的实现方法在下一部分说明)，下面就是如何求取针对某个节点上的梯度了，只要每个节点上的梯度计算出来沿着路径反方向不断乘下去就会得到你想要的节点的梯度了。本部分就介绍如何求损失值对具体某个节点的梯度值。本部分我们就是干这么一个事，首先我们先画个节点:f节点可以看成一个函数z=f(x,y)，我们需要做的就是求∂f(x,y)/∂x和∂f(x,y)/∂y.平方运算的梯度计算我们先用一个平方运算（之所以不用求和和乘积/矩阵乘积来做例子，因为这里面涉及到矩阵求导维度的处理，会在稍后进行总结,而平方运算并不会涉及到维度的变化比较简单):PythonclassSquare(Operation):'''Squareoperation.'''#...defcompute_gradient(self,grad=None):'''Computethegradientforsquareoperationwrtinputvalue.:paramgrad:Thegradientofotheroperationwrtthesquareoutput.:typegrad:ndarray.'''input_value=self.input_nodes[0].output_valueifgradisNone:grad=np.ones_like(self.output_value)returngrad*np.multiply(2.0,input_value)123456789101112classSquare(Operation):    '''Squareoperation.'''    #...    defcompute_gradient(self,grad=None):        '''Computethegradientforsquareoperationwrtinputvalue.        :paramgrad:Thegradientofotheroperationwrtthesquareoutput.        :typegrad:ndarray.        '''        input_value=self.input_nodes[0].output_value        ifgradisNone:            grad=np.ones_like(self.output_value)        returngrad*np.multiply(2.0,input_value)其中grad为损失值对Square输出的梯度值，也就是上图中的∂Loss/∂z的值,它的shape一定与Square的输出值的shape一致。神经网络反向传播的矩阵梯度计算矩阵梯度的计算是实现反向传播算法重要的一部分,但是在实现神经网络反向传播的矩阵求导与许多公式列表上罗列出来的还是有差别的。矩阵/向量求导首先先看下矩阵的求导，其实矩阵的求导本质上就是目标矩阵中的元素对变量矩阵中的元素求偏导，至于求导后的导数矩阵的形状大都也都是为了形式上的美观方便求导之后的继续使用。所以不必被那些复杂的矩阵求导形式迷惑了双眼。这里上传了一份矩阵求导公式法则的列表PDF版本，可以一步一步通过（行/列）向量对标量求导再到（行/列）向量对（行/列）向量求导再到矩阵对矩阵的求导逐渐扩展。例如标量yy对矩阵求导, 我们就对标量y对于X的所有元素求偏导，最终得到一个导数矩阵，矩阵形状同X相同:神经网络反向传播中的矩阵求导之所以把矩阵求导分成两部分，是因为在实现矩阵求导的时候发现做反向传播的时候的矩阵求导与矩阵求导公式的形式上还是有区别的。所谓的区别就是，我们在神经网络进行矩阵求导的时候其实是Loss(损失)函数对节点中的矩阵进行求导，而损失函数是标量，那每次我们对计算图中的每个节点计算梯度的时候其实是计算的标量(损失值)对矩阵(节点输出值)的求导.也就是说在进行反向传播的时候我们用的只是矩阵求导中的一种，即标量对矩阵的求导,也就是上面举的例子的形式。再进一步其实就是损失函数对矩阵中每个元素进行求偏导的过程，通俗的讲就是计算图中矩阵中的每个元素对损失值的一个影响程度。因此这样计算出来的导数矩阵的形状与变量的形状一定是一致的。直观上理解就是计算图中对向量/矩阵求导的时候计算的是矩阵中的元素对损失值影响程度的大小，其形状与矩阵形状相同。求和操作的梯度计算现在我们以求和操作的梯度计算为例说明反向传播过程中矩阵求导的实现方法。对于求和操作: C=A+b,其中则损失值LL对C梯度矩阵为下面我们计算∂L/∂b,根据我们之前说的这个梯度的维度(形状)应该与b相同，也就是一个标量,那么具体要怎么计算呢？我们分成两部分来处理：下面是求和操作梯度计算的Python实现:PythonclassAdd(object):#...defcompute_gradient(self,grad=None):'''Computethegradientsforthisoperationwrtinputvalues.:paramgrad:Thegradientofotheroperationwrttheadditionoutput.:typegrad:numberorandarray,defaultvalueis1.0.'''x,y=[node.output_valuefornodeinself.input_nodes]ifgradisNone:grad=np.ones_like(self.output_value)grad_wrt_x=gradwhilenp.ndim(grad_wrt_x)>len(np.shape(x)):grad_wrt_x=np.sum(grad_wrt_x,axis=0)foraxis,sizeinenumerate(np.shape(x)):ifsize==1:grad_wrt_x=np.sum(grad_wrt_x,axis=axis,keepdims=True)grad_wrt_y=gradwhilenp.ndim(grad_wrt_y)>len(np.shape(y)):grad_wrt_y=np.sum(grad_wrt_y,axis=0)foraxis,sizeinenumerate(np.shape(y)):ifsize==1:grad_wrt_y=np.sum(grad_wrt_y,axis=axis,keepdims=True)return[grad_wrt_x,grad_wrt_y]1234567891011121314151617181920212223classAdd(object):    #...    defcompute_gradient(self,grad=None):        '''Computethegradientsforthisoperationwrtinputvalues.        :paramgrad:Thegradientofotheroperationwrttheadditionoutput.        :typegrad:numberorandarray,defaultvalueis1.0.        '''        x,y=[node.output_valuefornodeinself.input_nodes]        ifgradisNone:            grad=np.ones_like(self.output_value)        grad_wrt_x=grad        whilenp.ndim(grad_wrt_x)>len(np.shape(x)):            grad_wrt_x=np.sum(grad_wrt_x,axis=0)        foraxis,sizeinenumerate(np.shape(x)):            ifsize==1:                grad_wrt_x=np.sum(grad_wrt_x,axis=axis,keepdims=True)        grad_wrt_y=grad        whilenp.ndim(grad_wrt_y)>len(np.shape(y)):            grad_wrt_y=np.sum(grad_wrt_y,axis=0)        foraxis,sizeinenumerate(np.shape(y)):            ifsize==1:                grad_wrt_y=np.sum(grad_wrt_y,axis=axis,keepdims=True)        return[grad_wrt_x,grad_wrt_y]其中grad参数就是上面公式中的GG它的shape应该与该节点的输出值(output_value的形状一直)。矩阵乘梯度的计算这部分主要介绍如何在反向传播求梯度中运用维度分析来帮助我们快速获取梯度。先上一个矩阵乘操作的例子:C=AB其中， C是M×K的矩阵, A是M×N的矩阵, B是N×K的矩阵。损失值L对C的梯度为 G=∂L/∂C, 其形状与矩阵C相同同为M×K通过维度分析可以通过我们标量求导的知识再稍微对矩阵的形状进行处理(左乘，右乘，转置)来凑出正确的梯度。当然如果需要分析每个元素的导数也是可以的，可以参考这篇神经网络中利用矩阵进行反向传播运算的实质,下面我们主要使用维度分析来快速计算反向传播中矩阵乘节点中矩阵对矩阵的导数。若我们想求∂L/∂B,根据标量计算的链式法则应该有:下面是矩阵乘操作梯度计算的Python实现:PythonclassMatMul(Operation):#...defcompute_gradient(self,grad=None):'''Computeandreturnthegradientformatrixmultiplication.:paramgrad:Thegradientofotheroperationwrtthematmuloutput.:typegrad:numberorandarray,defaultvalueis1.0.'''#Getinputvalues.x,y=[node.output_valuefornodeinself.input_nodes]#Defaultgradientwrtthematmuloutput.ifgradisNone:grad=np.ones_like(self.output_value)#Gradientswrtinputs.dfdx=np.dot(grad,np.transpose(y))dfdy=np.dot(np.transpose(x),grad)return[dfdx,dfdy]12345678910111213141516classMatMul(Operation):    #...    defcompute_gradient(self,grad=None):        '''Computeandreturnthegradientformatrixmultiplication.        :paramgrad:Thegradientofotheroperationwrtthematmuloutput.        :typegrad:numberorandarray,defaultvalueis1.0.        '''        #Getinputvalues.        x,y=[node.output_valuefornodeinself.input_nodes]        #Defaultgradientwrtthematmuloutput.        ifgradisNone:            grad=np.ones_like(self.output_value)        #Gradientswrtinputs.        dfdx=np.dot(grad,np.transpose(y))        dfdy=np.dot(np.transpose(x),grad)        return[dfdx,dfdy]其他操作的梯度计算这里就不一一介绍了其他操作的梯度计算了，类似的我们根据维度分析以及理解反向传播里矩阵梯度其实就是标量求梯度放到了矩阵的规则里的一种变形的本质，其他梯度也可以推导并实现出来了。在simpleflow里目前实现了求和，乘法，矩阵乘法，平方，Sigmoid，ReduceSum以及Log等操作的梯度实现，可以参考:https://github.com/PytLab/simpleflow/blob/master/simpleflow/operations.py总结本文介绍了通过计算图的反向传播快速计算梯度的原理以及每个节点相应梯度的计算和实现，有了每个节点的梯度计算我们就可以通过实现反向传播算法来实现损失函数对所有节点的梯度计算了，下一篇中将会总结通过广度优先搜索实现图中节点梯度的计算以及梯度下降优化器的实现。参考http://www.deepideas.net/deep-learning-from-scratch-iv-gradient-descent-and-backpropagation/https://zhuanlan.zhihu.com/p/25496760http://blog.csdn.net/magic_anthony/article/details/77531552https://www.zhihu.com/question/47024992/answer/103962301打赏支持我写出更多好文章，谢谢！打赏作者打赏支持我写出更多好文章，谢谢！1赞1收藏评论关于作者：iPytLab喜欢写程序的计算化学狗，Python/C/C++/Fortran,个人博客http://pytlab.org个人主页·我的文章·22·"], "art_create_time": ["2018/02/04"], "art_title": ["实现属于自己的TensorFlow(二) - 梯度计算与反向传播"], "art_url": ["http://python.jobbole.com/89012/"], "art_img": ["http://pytlab.org/assets/images/blog_img/2018-01-24-%E5%AE%9E%E7%8E%B0%E5%B1%9E%E4%BA%8E%E8%87%AA%E5%B7%B1%E7%9A%84TensorFlow-%E4%B8%80-%E8%AE%A1%E7%AE%97%E5%9B%BE%E4%B8%8E%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD/feature.png"]},
{"art_content": ["还剩十几天，2012年就要过去了。Linux业界知名的杂志LinuxJournal近日在其官网公布了今年LinuxJournal读者选择奖（Readers’ChoiceAwards）投票结果。投票覆盖 LinuxJournal一年内报道的产品、服务和公司。在今年，Python再次成为“最佳编程语言（BestProgrammingLanguage）”和“最佳脚本语言（BestScriptingLanguage）”，Eclipse是“最佳IDE”。今年投票主题分类有 52种，以下是部分分类结果：BestLinuxDistribution最佳Linux发行版①Ubuntu(anyflavor,Kubuntu,Edubuntu,etc.):30.1% ②Debian:14.7%；③LinuxMint:13.4%④ArchLinux:10%⑤Fedora:8.6%⑥OpenSUSE:4.2%…… BestDistributionforNetbooks/LimitedHardware  ① UbuntuNetbookRemix(anyflavor):25.9%②Debian:14.9%③AndroidOS:14.1%④ ArchLinux:12.7%…… BestMobileLinuxOS →AndroidBestDesktopEnvironment→KDEBestWebBrowser①Firefox:50.3%②Chrome/Chromium:40.8%③Opera:4.7%…… BestE-mailClient最佳邮件客户端①Thunderbird:40.2% ②GmailWebclient:31.3%③Evolution:7.5%…… BestIMClient最佳即时通讯软件客户端①Pidgin(formerlyGaim):56%②Skype:15.1%③Empathy:9.2%…… BestIRCClient最佳IRC客户端①pidgin:31.2% ②XChat:27.8%③Irssi:13.2%…… BestOfficeSuite①LibreOffice:73.3%、②GoogleDocs:12%、③OpenOffice.org:8.4%…… BestGraphics/DesignTool①GIMP:67.8%、②Inkscape:13.3%、③ Blender:8%…… BestDigitalPhotoManagementTool①digiKam:22.5、② Picasa:22.2%、③ Shotwell:17.3% …… BestAudioTool：AudacityBestAudioPlayer：VLC:24.9%  BestOn-lineCollaborationTool①GoogleDocs:61.3%、②Wikis:18.5%、③ Other:6.9%…… BestCloud-BasedFileStorage：Dropbox:53.4%BestKid-FriendlyApplication：TuxPaint:42.8%BestSQLDatabase①MySQL:47.4%、②PostgreSQL:28.8%、③SQLite:12.2%……  BestNoSQLDatabase①MongoDB:33.4%、②CouchDB:22.3%、③Cassandra:21.9%、④Other:12.2%、⑤ Redis:6.6%、⑥Dynomite:1.8%、⑦ Riak:1.8% BestMonitoringApplication：NagiosBestRevisionControlSystem①Git:63%、②Subversion:18.6%、③Mercurial:6.6%、④CVS:6.2%、⑤Other:2.7%BestProgrammingLanguage最佳编程语言①Python:28%、② C++:19%、③ C:18.9%、④ Java:8.9%、⑤ Perl:8.2%Other:6.1%、Ruby:4%、JavaScript:3.1%、C#:2.4%、Erlang:.7%、Haskell:.7%  BestScriptingLanguage最佳脚本语言①Python:36.3%、②Bash:24.4%、③Perl:14.2%、④PHP:13.8%、⑤Ruby:4.8%、⑥Awk:2.2%、⑦Other:2.2%、⑧ Lua:1.6%、⑨ Groovy:0.4% （译注：针对这个投票结果，尤其是“最佳编程语言（BestProgrammingLanguage）”和“最佳脚本语言（BestScriptingLanguage）”，Reddit上有些人在讨论并吐槽→ 链接。） BestIDE最佳集成开发环境（IDE）Eclipse:26.7%、vim:25.6%、Geany:7.5%、Other:6.9%*、QtCreator:6.1%、NetBeans:5.4%、KDevelop:4.1%、Sublime2:3.1%、IntelliJIDEA:1.3%、Anjuta:1.2%、MonoDevelop:.6%、Codewarrior:.5%、ZendStudio:.3%、Eric4:.1%BestOpen-SourceConfigurationManagementTool|最佳开源配置管理工具：PuppetBestPlatformforDevelopingRichInternetApps：HTML5BestPackageManagementApplication：aptBestRSSReader：GmailRSSreader BestJavaAppServertomcat:60.1%、jboss:21.9%、Other:10.9%*、glassfish:7.1% BestContentManagementSystem最佳内容管理系统WordPress:34.9%、Drupal:27.8%、Joomla!:15.3%、Other:8.1%…… 最佳Linux图书《LinuxinNutshell》   英文来源：LinuxJournal，编译：伯乐在线——黄利民文章链接：http://blog.jobbole.com/31329/【如需转载，请在正文中标注并保留原文链接、译文链接和译者等信息，谢谢合作！】1赞收藏评论关于作者：黄利民伯乐在线联合发起人，关注IT和互联网。个人主页·我的文章·99·"], "art_create_time": ["2012/12/17"], "art_title": ["2012年Linux Journal读者选择奖结果公布"], "art_url": ["http://python.jobbole.com/31329/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2012/10/ubuntu-logo.jpg"]},
{"art_content": ["本文由伯乐在线-精算狗翻译。未经许可，禁止转载！英文出处：ageitgey。欢迎加入翻译组。人人都恨验证码——那些恼人的图片，显示着你在登陆某网站前得输入的文本。设计验证码的目的是，通过验证你是真实的人来避免电脑自动填充表格。但是随着深度学习和计算机视觉的兴起，现在验证码常常易被攻破。我拜读了AdrianRosebrock写的《DeepLearningforComputerVisionwithPython》。在书中，Adrian描述了他是怎样用机器学习绕过纽约E-ZPass网站上的验证码：Adrian无法接触到该应用生成验证码的源代码。为了攻破该系统，他不得不下载数百张示例图片，并手动处理它们来训练他自己的系统。但是如果我们想攻破的是一个开源验证码系统，我们确实能接触到源代码该怎么办呢？我访问了WordPress.org的插件频道，并搜索了“验证码”。第一条搜索结果是ReallySimpleCAPTCHA，并且有超过一百万次的活跃安装：最好的一点是，它是开源的！既然我们已经有了生成验证码的源代码，那它应该挺容易被攻破的。为了让这件事更有挑战性，让我们给自己规定个时限吧。我们能在15分钟内完全攻破这个验证码系统吗？来试试吧！重要说明：这绝不是对ReallySimpleCAPTCHA插件或对其作者的批评。该插件作者自己说它已经不再安全了，建议使用其他插件。这仅仅是一次好玩又迅速的技术挑战。但是如果你是那剩余的一百多万用户之一，也许你应该改用其他插件:)挑战为了构思一个攻击计划，来看看ReallySimpleCAPTCHA会生成什么样的图片。在示例网站上，我们看到了以下图片：好了，所以验证码似乎是四个字母。在PHP源代码中对其进行验证：publicfunction__construct(){/*Charactersavailableinimages*/$this->chars='ABCDEFGHJKLMNPQRSTUVWXYZ23456789';/*Lengthofawordinanimage*/$this->char_length=4;/*Arrayoffonts.Randomlypickeduppercharacter*/$this->fonts=array(dirname(__FILE__).'/gentium/GenBkBasR.ttf',dirname(__FILE__).'/gentium/GenBkBasI.ttf',dirname(__FILE__).'/gentium/GenBkBasBI.ttf',dirname(__FILE__).'/gentium/GenBkBasB.ttf',);1234567891011121314publicfunction__construct(){/*Charactersavailableinimages*/$this->chars='ABCDEFGHJKLMNPQRSTUVWXYZ23456789'; /*Lengthofawordinanimage*/$this->char_length=4; /*Arrayoffonts.Randomlypickeduppercharacter*/$this->fonts=array(dirname(__FILE__).'/gentium/GenBkBasR.ttf',dirname(__FILE__).'/gentium/GenBkBasI.ttf',dirname(__FILE__).'/gentium/GenBkBasBI.ttf',dirname(__FILE__).'/gentium/GenBkBasB.ttf',);没错，它用四种不同字体的随机组合来生成四个字母的验证码。并且可以看到，它在代码中从未使用O或者I，以此避免用户混淆。总共有32个可能的字母和数字需要我们识别。没问题！计时：2分钟工具在进行下一步前，提一下我们要用来解决问题的工具：Python3 Python是一种有趣的编程语言，它有大量的机器学习和计算机视觉库。OpenCV OpenCV是一种流行的计算机视觉和图片处理框架。我们要使用OpenCV来处理验证码图片。由于它有PythonAPI，所以我们可以直接从Python中使用它。KerasKeras是用Python编写的深度学习框架。它使得定义、训练和用最少的代码使用深度神经网络容易实现。TensorFlow TensorFlow是Google的机器学习库。我们会用Keras编程，但是Keras并没有真正实现神经网络的逻辑本身，而是在幕后使用Google的TensorFlow库来挑起重担。好了，回到我们的挑战吧！创造我们的数据集为了训练任何机器学习系统，我们需要训练数据。为了攻破一个验证码系统，我们想要像这样的训练数据：鉴于我们有WordPress插件的源代码，我们可以调整它，一起保存10,000张验证码图片及分别对应的答案。经过几分钟对代码的攻击，并添加了一个简单的for循环之后，我有了一个训练数据的文件夹——10,000个PNG文件，文件名为对应的正确答案：这是唯一一个我不会给你示例代码的部分。我们做这个是为了教育，我不希望你们真去黑WordPress网站。但是，我最后会给你生成的这10,000张图片，这样你就能重复我的结果了。计时：5分钟简化问题既然有了训练数据，就可以直接用它来训练神经网络了：有了足够的训练数据，这个方法可能会有用——但是我们可以使问题更简化来解决。问题越简单，要解决它需要的训练数据就越少，需要的计算能力也越低。毕竟我们只有15分钟！幸运的是，验证码图片总是由仅仅四个字母组成。如果我们能想办法把图片分开，使得每个字母都在单独的图片中，这样我们只需要训练神经网络一次识别一个字母：我没有时间去浏览10,000张训练图片并在Photoshop中手动把它们拆分开。这得花掉好几天的时间，而我只剩下10分钟了。我们还不能把图片分成相等大小的四块，因为该验证码插件把字母随机摆放在不同的水平位置上以防止这一做法：幸运的是，我们仍然可以自动处理。在图像处理中，常常需要检测有相同颜色的像素块。这些连续像素块周围的界限被称为轮廓。OpenCV中有一个LndContours()函数，可以被用来检测这些连续区域。所以我们用一个未经处理的验证码图片开始：接下来把该图片转换成纯黑白（这叫做thresholding），这样容易找到连续区域：接着，使用OpenCV的LndContours()函数来检测该图片中包含相同颜色像素块的不同部分：接下来就是简单地把每个区域存成不同的图片文件。鉴于我们知道每张图片都应该包含从左到右的四个字母，我们可以利用这一点在保存的同时给字母标记。只要我们是按顺序保存的，我们就应该能保存好每个图片字母及其对应的字母名。但是等等——我看到一个问题！有时验证码中有像这样重叠的字母：这意味着我们会把两个字母分离成一个区域：如果不处理这个问题，会创造出糟糕的训练数据。我们得解决这个问题，这样就不会意外地教机器把两个重叠的字母识别成一个字母了。一个简单的方法是，如果一个轮廓区域比它的高度更宽，这意味着很可能有两个字母重叠在一起了。在这种情况下，我们可以把重叠的字母从中间拆分成两个，并将其看作两个不同的字母：既然我们找到拆分出单个字母的方法了，就对所有验证码图片进行该操作。目标是收集每个字母的不同变体。我们可以将每个字母保存在各自对应的文件夹中，以保持条理。在我分离出所有字母后，我的W文件夹长这样：计时：10分钟构建并训练神经系统由于我们只需要识别单个字母和数字的图片，我们不需要非常复杂的神经网络结构。识别字母要比识别像猫狗这样复杂的图片容易得多。我们要使用简单的卷积神经网络结构，有两层卷积层以及两层完全连接层：如果你想要了解更多神经网络的工作，以及为什么它们是图片识别的理想工具，请参考Adrian的书或者我之前的文章。定义该神经网络结构，只需要使用Keras的几行代码：#Buildtheneuralnetwork!model=Sequential()#Firstconvolutionallayerwithmaxpoolingmodel.add(Conv2D(20,(5,5),padding=\"same\",input_shape=(20,20,1),activation=\"relu\"))model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))#Secondconvolutionallayerwithmaxpoolingmodel.add(Conv2D(50,(5,5),padding=\"same\",activation=\"relu\"))model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))#Hiddenlayerwith500nodesmodel.add(Flatten())model.add(Dense(500,activation=\"relu\"))#Outputlayerwith32nodes(oneforeachpossibleletter/numberwepredict)model.add(Dense(32,activation=\"softmax\"))#AskKerastobuildtheTensorFlowmodelbehindthescenesmodel.compile(loss=\"categorical_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])1234567891011121314151617181920#Buildtheneuralnetwork!model=Sequential() #Firstconvolutionallayerwithmaxpoolingmodel.add(Conv2D(20,(5,5),padding=\"same\",input_shape=(20,20,1),activation=\"relu\"))model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2))) #Secondconvolutionallayerwithmaxpoolingmodel.add(Conv2D(50,(5,5),padding=\"same\",activation=\"relu\"))model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2))) #Hiddenlayerwith500nodesmodel.add(Flatten())model.add(Dense(500,activation=\"relu\")) #Outputlayerwith32nodes(oneforeachpossibleletter/numberwepredict)model.add(Dense(32,activation=\"softmax\")) #AskKerastobuildtheTensorFlowmodelbehindthescenesmodel.compile(loss=\"categorical_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])现在我们可以训练它了！#Traintheneuralnetworkmodel.fit(X_train,Y_train,validation_data=(X_test,Y_test),batch_size=32,epochs=10,verbose=1)12#Traintheneuralnetworkmodel.fit(X_train,Y_train,validation_data=(X_test,Y_test),batch_size=32,epochs=10,verbose=1)在10通过了训练数据集后，我们达到了几乎100%的正确率。此时，我们应该能随时自动绕过这个验证码了！我们成功了！计时：15分钟（好险！）使用训练后的模型来处理验证码既然有了一个训练后的神经网络，利用它来攻破真实的验证码要很容易了：1.从一个使用WordPress插件的网站上下载一张验证码图片。2.使用文章中生成训练数据集的方法，把该验证码图片拆分成四张字母图片。3.用神经网络对每张字母图片分别作预测。4.用四个预测字母作为验证码的答案。5.狂欢！在破解验证码时，我们的模型看起来是这样：或者从命令来看：来试试吧！如果你想自己试试，你可以从这里找到代码（http://t.cn/R8yFJiN）。它包含10,000张示例图片和文章中每一步的所有代码。参考文件README.md中的运行指导。但是如果你想了解每一行代码都做了什么，我强烈建议你看看《 DeepLearningforComputerVisionwithPython》。该书覆盖了更多的细节，而且有大量的详细示例。这本书是我目前见过的唯一一本既包含了运行原理，又包含了如何在现实生活中用其来解决复杂问题的书。去看看吧！5赞19收藏10评论关于作者：精算狗简介还没来得及写:）个人主页·我的文章·20·"], "art_create_time": ["2018/01/29"], "art_title": ["15 分钟用 ML 破解一个验证码系统"], "art_url": ["http://python.jobbole.com/89004/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2018/01/86b4998ac58e5925e767a18e41b5438f.png"]},
{"art_content": ["原文出处：SofiaHeisler   译文出处：无阻我飞扬   摘要：Pandas是PythonDataAnalysisLibrary的简写，它是为了解决数据分析任务而创建的工具，本文介绍了五种由慢到快逐步优化其效率的方法，以下是译文如果你用Python语言做过任何的数据分析，那么可能会用到Pandas,一个由WesMcKinney写的奇妙的分析库。通过赋予Python数据帧以分析功能，Pandas已经有效地把Python和一些诸如R或者SAS这样比较成熟的分析工具置于相同的地位。不幸的是，在早期，Pandas因“慢”而声名狼藉。的确，Pandas代码不可能达到如完全优化的原始C语言代码的计算速度。然而，好消息是，对于大多数应用程序来说，写的好的Pandas代码已足够快；Pandas强大的功能和友好的用户体验弥补了其速度的缺点。在这篇文章中，我们将回顾应用于PandasDataFrame函数的几种方法的效率，从最慢到最快：1．在用索引的DataFrame行上的Crudelooping2．用iterrows()循环3．用apply()循环4．PandasSeries矢量化5．NumPy数组矢量化对于我们的实例函数，将使用Haversine（半正矢）距离公式。函数取两点的经纬度，调整球面的曲率，计算它们之间的直线距离。这个函数看起来像这样：Pythonimportnumpyasnp#DefineabasicHaversinedistanceformuladefhaversine(lat1,lon1,lat2,lon2):MILES=3959lat1,lon1,lat2,lon2=map(np.deg2rad,[lat1,lon1,lat2,lon2])dlat=lat2-lat1dlon=lon2-lon1a=np.sin(dlat/2)**2+np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2)**2c=2*np.arcsin(np.sqrt(a))total_miles=MILES*creturntotal_miles123456789101112importnumpyasnp #DefineabasicHaversinedistanceformuladefhaversine(lat1,lon1,lat2,lon2):    MILES=3959    lat1,lon1,lat2,lon2=map(np.deg2rad,[lat1,lon1,lat2,lon2])    dlat=lat2-lat1    dlon=lon2-lon1    a=np.sin(dlat/2)**2+np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2)**2    c=2*np.arcsin(np.sqrt(a))    total_miles=MILES*c    returntotal_miles为了在真实数据上测试函数，我们用一个包含纽约所有酒店坐标的数据集，该数据来自Expedia开发者网站。要计算每一个酒店和一个样本集坐标之间的距离（这恰好属于在纽约市名为布鲁克林超级英雄供应店的一个梦幻般的小商店）大家可以下载数据集，Jupyternotebook（是一个交互式笔记本，支持运行40多种编程语言）包含了用于这篇博客的函数，请点击这里下载。这篇文章基于我的PyCon访谈，大家可以在这里观看。Pandas中的Crudelooping，或者你永远不应该这么做首先，让我们快速回顾一下Pandas数据结构的基本原理。Pandas的基本结构有两种形式：DataFrame和Series。一个DataFrame是一个二维数组标记轴，很多功能与R中的data.frame类似，可以将DataFrame理解为Series的容器。换句话说，一个DataFrame是一个有行和列的矩阵，列有列名标签，行有索引标签。在PandasDataFrame中一个单独的列或者行是一个PandasSeries—一个带有轴标签的一维数组。几乎每一个与我合作过的Pandas初学者，都曾经试图通过一次一个的遍历DataFrame行去应用自定义函数。这种方法的优点是，它是Python对象之间交互的一致方式；例如，一种可以通过列表或数组循环的方式。反过来说，不利的一面是，在Pandas中，Crudeloop是最慢的方法。与下面将要讨论的方法不同，Pandas中的Crudeloop没有利用任何内置优化，通过比较，其效率极低（而且代码通常不那么具有可读性）例如，有人可能会写像下面这样的代码：Python#Defineafunctiontomanuallyloopoverallrowsandreturnaseriesofdistancesdefhaversine_looping(df):distance_list=[]foriinrange(0,len(df)):d=haversine(40.671,-73.985,df.iloc[i]['latitude'],df.iloc[i]['longitude'])distance_list.append(d)returndistance_list1234567#Defineafunctiontomanuallyloopoverallrowsandreturnaseriesofdistancesdefhaversine_looping(df):    distance_list=[]    foriinrange(0,len(df)):        d=haversine(40.671,-73.985,df.iloc[i]['latitude'],df.iloc[i]['longitude'])        distance_list.append(d)    returndistance_list为了了解执行上述函数所需要的时间，我们用%timeit命令。%timeit是一个“神奇的”命令，专用于Jupyternotebook（所有的魔法命令都以%标识开始，如果%命令只应用于一行，那么%%命令应用于整个Jupyter单元）。%timeit命令将多次运行一个函数，并打印出获得的运行时间的平均值和标准差。当然，通过%timeit命令获得的运行时间，运行该函数的每个系统都不尽相同。尽管如此，它可以提供一个有用的基准测试工具，用于比较同一系统和数据集上不同函数的运行时间。Python%%timeit#Runthehaversineloopingfunctiondf['distance']=haversine_looping(df)12345%%timeit  #Runthehaversineloopingfunctiondf['distance']=haversine_looping(df)结果是：645ms±31msperloop(mean±std.dev.of7runs,1loopeach)1645ms±31msperloop(mean±std.dev.of7runs,1loopeach)通过分析，crudelooping函数运行了大约645ms,标准差是31ms。这似乎很快，但考虑到它仅需要处理大约1600行的代码，因此它实际上是很慢的。接下来看看如何改善这种不好的状况。用iterrows()循环如果循环是必须的，找一个更好的方式去遍历行，比如用iterrows（）方法。iterrows()是一个生成器，遍历DataFrame的所有行并返回每一行的索引，除了包含行自身的对象。iterrows() 是用PandasDataFrame优化，尽管它是运行大多数标准函数最不高效的方式（稍后再谈），但相对于Crudelooping，这是一个重大的改进。在我们的案例中，iterrows()解决同一个问题，几乎比手动遍历行快四倍。Python%%timeit#Haversineappliedonrowsviaiterationhaversine_series=[]forindex,rowindf.iterrows():haversine_series.append(haversine(40.671,-73.985,row['latitude'],row['longitude']))df['distance']=haversine_series12345678%%timeit  #Haversineappliedonrowsviaiterationhaversine_series=[]forindex,rowindf.iterrows():    haversine_series.append(haversine(40.671,-73.985,row['latitude'],row['longitude']))df['distance']=haversine_series166ms±2.42msperloop(mean±std.dev.of7runs,1loopeach)1166ms±2.42msperloop(mean±std.dev.of7runs,1loopeach)使用apply()方法实现更好的循环一个比iterrows()更好的选择是用 apply() 方法，它应用一个函数，沿着DataFrame某一个特定的轴线（意思就是行或列）。虽然apply()也固有的通过行循环，但它通过采取一些内部优化比iterrows()更高效，例如在Cython中使用迭代器。我们使用一个匿名的lambda函数，每一行都用Haversine函数，它允许指向每一行中的特定单元格作为函数的输入。为了指定Pandas是否应该将函数应用于行（axis=1）或列（axis=0），Lambda函数包含最终的axis参数。Python%%timeit#TimingapplyontheHaversinefunctiondf['distance']=df.apply(lambdarow:haversine(40.671,-73.985,row['latitude'],row['longitude']),axis=1)12345%%timeit  #TimingapplyontheHaversinefunctiondf['distance']=df.apply(lambdarow:haversine(40.671,-73.985,row['latitude'],row['longitude']),axis=1)90.6ms±7.55msperloop(mean±std.dev.of7runs,10loopseach)190.6ms±7.55msperloop(mean±std.dev.of7runs,10loopseach)iterrows()方法用apply()方法替代后，大致可以将函数的运行时间减半。为了更深入地了解函数中的实际运行时间，可以运行一个在线分析器工具（Jupyter中神奇的命令%lprun）Python#Haversineappliedonrowswithlineprofiler%lprun-fhaversinedf.apply(lambdarow:haversine(40.671,-73.985,row['latitude'],row['longitude']),axis=1)12#Haversineappliedonrowswithlineprofiler%lprun-fhaversinedf.apply(lambdarow:haversine(40.671,-73.985,row['latitude'],row['longitude']),axis=1)结果如下：我们可以从这个信息中得到一些有用的见解。例如，进行三角计算的函数占了总运行时间的近一半。因此，如果想优化函数的各个组件，可以从这里入手。现在，特别值得注意的是每一行都被循环了1631次—apply（）遍历每一行的结果。如果可以减少重复的工作量，就可以降低整个运行时间。矢量化提供了一种更有效的替代方案。PandasSeries矢量化要了解如何可以减少函数所执行的迭代数量，就要记得Pandas的基本单位，DataFrame和Series，它们都基于数组。基本单元的固有结构转换成内置的设计用于对整个数组进行操作的Pandas函数，而不是按各个值的顺序（简称标量）。矢量化是对整个数组执行操作的过程。Pandas包含一个总体的矢量化函数集合，从数学运算到聚合和字符串函数（可用函数的扩展列表，查看Pandasdocs）。对PandasSeries和DataFrame的操作进行内置优化。结果，使用矢量Pandas函数几乎总是会用自定义的循环实现类似的功能。到目前为止，我们仅传递标量给Haversine函数。所有的函数都应用在Haversine函数中，也可以在数组上操作。这使得距离矢量化函数的过程非常的简单：不是传递个别标量值的纬度和经度给它，而是把它传递给整个series（列）。这使得Pandas受益于可用于矢量函数的全套优化，特别是包括同时执行整个数组的所有计算。Python%%timeit#VectorizedimplementationofHaversineappliedonPandasseriesdf['distance']=haversine(40.671,-73.985,df['latitude'],df['longitude'])12345%%timeit  #VectorizedimplementationofHaversineappliedonPandasseriesdf['distance']=haversine(40.671,-73.985,df['latitude'],df['longitude'])1.62ms±41.5µsperloop(mean±std.dev.of7runs,1000loopseach)11.62ms±41.5µsperloop(mean±std.dev.of7runs,1000loopseach)通过使用apply()方法，要比用iterrows()方法改进50倍的效率，通过矢量化函数则改进了iterrows()方法100倍—除了改变输入类型，什么都不要做！看一眼后台，看看函数到底在做什么：注意，鉴于apply()执行函数1631次，矢量化版本仅执行一次，因为它同时应用于整个数组，这就是主要的时间节省来源。用NumPy数组矢量化Pandasseries矢量化可以完成日常计算优化的绝大多数需要。然而，如果速度是最高优先级，那么可以以NumPyPython库的形式调用援军。NumPy库，将自己描述为一个“Python科学计算的基本包”，在后台执行优化操作，预编译C语言代码。跟Pandas一样，NumPy操作数组对象（简称ndarrays）；然而，它省去了Pandasseries操作所带来的大量资源开销，如索引、数据类型检查等。因此，NumPy数组的操作可以明显快于pandasseries的操作。当Pandasseries提供的额外功能不是很关键的时候，NumPy数组可以用于替代Pandasseries。例如，Haversine函数矢量化实现不使用索引的经度和纬度系列，因此没有那些索引，也不会导致函数中断。通过比较，我们所做的操作如DataFrame的连接，它需要按索引来引用值，可能需要坚持使用Pandas对象。仅仅是使用Pandasseries的values的方法，把纬度和经度数组从Pandasseries转换到NumPy数组。就像series矢量化一样，通过NumPy数组直接进入函数将可以让Pandas对整个矢量应用函数。Python%%timeit#VectorizedimplementationofHaversineappliedonNumPyarraysdf['distance']=haversine(40.671,-73.985,df['latitude'].values,df['longitude'].values)12345%%timeit  #VectorizedimplementationofHaversineappliedonNumPyarraysdf['distance']=haversine(40.671,-73.985,df['latitude'].values,df['longitude'].values)370µs±18µsperloop(mean±std.dev.of7runs,1000loopseach)1370µs±18µsperloop(mean±std.dev.of7runs,1000loopseach)NumPy数组操作运行取得了又一个四倍的改善。总之，通过looping改进了运行时间超过半秒，通过NumPy矢量化，运行时间改进到了三分之一毫秒级！总结下面的表格总结了相关结果。用NumPy数组矢量化将会带来最快的运行时间，相对于Pandasseries矢量化的效果而言，这是一个很小的改进，但对比最快的looping版本，NumPy数组矢量化带来了56倍的改进。这给我们带来了一些关于优化Pandas代码的基本结论：避免循环；它们很慢，而且在大多数情况下是不必要的。如果必须使用循环，用apply(),而不是迭代函数。矢量化通常优于标量运算。在Pandas中的大部分常见操作都可以矢量化。NumPy数组矢量化操作比Pandasseries更有效。当然，以上并不是Pandas所有可能优化的全面清单。更爱冒险的用户或许可以考虑进一步用Cython改写函数，或者尝试优化函数的各个组件。然而，这些话题超出了这篇文章的范围。关键的是，在开始一次宏大的优化冒险之前，要确保正在优化的函数实际上是你希望在长期运行中使用的函数。引用XKCD不朽的名言：“过早优化是万恶之源”。1赞1收藏1评论"], "art_create_time": ["2017/11/23"], "art_title": ["Pandas初学者代码优化指南"], "art_url": ["http://python.jobbole.com/88915/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2017/11/0555f99614ede2b6f00917d386e27788.png"]},
{"art_content": ["本文作者：伯乐在线-翱翔的翱。未经作者许可，禁止转载！欢迎加入伯乐在线专栏作者。一、概述在三生万物：决策树中我们提到当决策树和装袋法(Bagging)和提升法(Boosting)结合后会成为更强大的算法，那么今天就介绍一种名叫随机森林(RandomForest)的算法，它是将决策树、装袋法以及随机特征选取结合后衍生出的一种增强型的树算法。它有如下特点：运行起来非常有效率，可以很容易的并行化可以无删减的处理成千上万的输入变量，并可以评估变量的重要性不用将数据专门分为训练集和测试集，随机森林构造完就可以得到近似的测试误差能够很有效的处理缺失值可以有效的分离出离群点对过拟合有很强的抗性还可以用于非监督式学习……看到随机森林有这么多的优点，你是不是心动了呢？那么接下来和我一起来认识一下它吧！二、算法1、基本步骤上文提到随机森林不是一种全新的算法，而是几种算法的强强联合。随机森林的构建一般有这么几个步骤：首先确定树的数量(TreeSize)，而每个树的训练数据通过有放回的抽取原始数据。由于是有放回的抽样，原始数据中约有1/3的量没有被抽到，这些数据称为袋外数据(OOB,OutOfBag)树的树训练数据有了，接下来就该训练了，与决策树不同，这里的树在构建的时候，每一次分裂都要进行随机特征选取，也就是在特征的随机子空间进行分裂，比如一个数据集有5个特征，每次分裂有放回的随机取3(FeatureCount)个到这里，所有的树都应该构造完成了，森林也就有了，那么怎么对响应值进行预测呢？这就要依靠集体的智慧了，每个树都有一个预测值，对于分类问题，取频率最高的那个值；对于回归问题，取所有值的平均2、袋外误差(OOBError)算法作者说OOBError可以作为测试误差的无偏估计，也就是计算出OOBError就可以得到测试误差，不用专门把数据专门拿出来一部分作为测试集。下面举例说明如何计算OOBError，比如我们要在一个有7条数据的数据集上构建一个5棵树的随机森林，那么在步骤1的时候会出现下面这样一张表:数据编号树1树2树3树4树51X√√X√2√XXXX3√√√√√4√√X√√5√X√√X6√√X√√7X√√√√表里的X代表没有选中，√代表选中。对于树1，数据1和7就是OOB，对于树2，数据2和5就是OOB，其他以此类推，那么数据1的预测值由树1和树4决定，数据2的预测值由树2-5来决定，以这样的方式计算出每个数据的预测值，进而得到误差值，即OOBError。3、变量重要性(VariableImportance)假设我们已经计算出了OOBError，一个变量的重要性可以这么计算，将变量打散，然后重新计算打散后的OOBError，取打散前后OOBError差值的绝对值，越大代表这个变量越重要。变量重要性在实践过程中非常好用，比如在一个10000维度的数据集选出100个最重要的变量，即数据的降维。4、相似性(Proximities)相似性由相似性矩阵体现，相似性矩阵是一个NxN的对称矩阵，它的计算方式如下，如果数据n和数据p同属于同一颗树的同一个叶子节点，那么相似性加1，即proximities[n,p]和proximities[p,n]均加1，最后除以树的数目进行标准化。5、离群点(Outliers)有了相似性，也就可以计算离群点了。它基于这样的假设，如果一条数据和其他数据都不相似或者相似性很低，那么这条数据很可能是个离群点。这和人很类似阿，如果一个人不合群，那么他肯定是比较孤立的。不过在我实际操作的过程中，即使计算出了潜在的离群点，如何确定它真的是不是不是那么容易。具体的计算过程如下，定义类别为j的数据n的平均相似性为：得到非相似性：然后在各自的类别中标准化，得到最终的Dissimilarity，算法作者给出的经验值是如果一条数据的Dissimilarity>10，那么可能是一个潜在的离群点。6、缺失值对于缺失值，传统的方法就是数值变量取均值，分组变量取最多的那一类。而随机森林处理缺失值另有一套：先使用一个不太准确的初始值替换缺失值，然后计算数据间的相似性，数值变量取同一类别非缺失值的相似性加权平均；分组变量取频率最高的值，频率要经过相似性加权，然后重复这一过程4-6次。三、案例在决策树代码的基础上稍加改动就得到了随机森林，下面检验一下新算法的能力。1、在三生万物：决策树里我尝试使用花萼长度(Sepal.Length)和花萼宽度(Sepal.Width)这两个变量来预测鸢尾花的种类(Species)，这里用随机森林试一试。首先来看下不同数目的树对分类的影响，下图的分类边界(DecisionBoundary)，使用的NodeSize为1，特征数FeatureCount也为1,可以看到，与决策树相比，随机森林对过拟合(Overfit)有着很强的抗性，且随着树的数目增多过拟合越来越少。但是，另一方面也要看到尽管对过拟合很强的抗性，还是可以看到过拟合的影子，即便我们已经用了1000棵树。所以，还是要为随机森林选择一个合适的NodeSize,从上面的第一张图，可以看到NodeSize从0～100增加时，OOBError先降后增，且在NodeSize为15时达到最低。从第二张图可以看到随机森林分类边界的变化过程，先是轻微的过拟合继而最合适的边界最后严重的欠拟合。第三张图是最合适的分类边界，尽管和决策树一样，预测的错误率都为0.2左右，但是和决策树的分类边界相比，随机森林的边界更平滑。2、北京二手房为了和决策树作对比，我也用随机森林来预测下房价(price)，也是使用区域(area)、是否学区(school)、是否有地铁(subway)、总价(num)这四个变量，使用的参数为树的数目TS=100，特征数FeatureCount=4，节点数目NodeSize=5，得到的结果如下，$r2[1]0.7014904$importancearea0.48257784num0.37338239school0.08469551subway0.0454757212345678$r2[1]0.7014904 $importance    area  0.48257784num    0.37338239school0.08469551subway0.04547572袋外决策系数R2为0.7，使用模型预测所有房屋价格的决策系数为0.74，比决策树的0.7高了4个百分点，大家不要小看了这4个百分点，在机器学习中哪怕1个百分点都要付出很大的努力。况且，我在这里并没有使用交叉验证获取最佳的参数，只是凭经验选取。另外模型还给出了预测房价各个变量的重要性，可以看到决定房价最重要的就是房子所在的区。接下来是个分类问题，使用小区(region)、户型(zone)、面积(meters)、朝向(direction)、区域(con)、楼层(floor)、房龄(year)、学区(school)、地铁(subway)、税(tax)、总价(num)、单价(price)来预测区(area)。随机从29790中抽取了10000条数据构造100颗树的随机森林，构建一个100棵树的森林，OOBError和变量重要性如下，$oob_error[1]0.1241$importancecon0.5454price0.3717school0.1132year0.0556subway0.0490region0.0168direction0.0163meters0.0043num0.0040zone0.0026floor0.0007tax0.000712345678910111213141516$oob_error[1]0.1241 $importancecon      0.5454price    0.3717school    0.1132year      0.0556subway    0.0490region    0.0168direction0.0163meters    0.0043num      0.0040zone      0.0026floor    0.0007tax      0.0007OOBError约为0.12，使用得到的随机森林模型预测29790条房源的区域，误差约为0.08，两者还是比较接近的。不出所料，片区(con)的重要性最高，另外房价(price)、是否学区(school)对房子区域的重要性也决非浪得虚名。上文提到，随机森林可以作为降维的工具，我从中选择前6个重要的变量重新构建一个随机森林，OOBError和变量重要性如下，$oob_error[1]0.11$importancecon0.5638price0.3812school0.1140year0.0831subway0.0490region0.025412345678910$oob_error[1]0.11 $importancecon    0.5638price  0.3812school0.1140year  0.0831subway0.0490region0.0254可以看到，使用6个变量的OOBError与使用全部12个变量的OOBError不相上下。下面看看有没有潜在的离群点，下图是每套房子的Dissimilarity，有两套房子的Dissimilarity>10，看看是什么房子，regionzonemetersdirectionconflooryearschoolsubwaytaxnumpricearea福成公寓A3室1厅135南北燕郊城区二手房低楼层2004无学区无地铁非免税40530000燕郊达观别墅4室2厅270南北燕郊城区二手房低楼层2009无学区无地铁非免税160059260燕郊123regionzonemetersdirectionconflooryearschoolsubwaytaxnumpricearea福成公寓A3室1厅135南北燕郊城区二手房低楼层2004无学区无地铁非免税40530000燕郊达观别墅4室2厅270南北燕郊城区二手房低楼层2009无学区无地铁非免税160059260燕郊初步看来，这两套房子的总价(num)和价格(price)有点高，是不是这一点让它们鹤立鸡群呢？四、总结本文简单介绍了随机森林的特点及算法，并简单分析了iris和北京二手房两个数据集。本文只是抛砖引玉，其实随机森林的还有一些其他的特性，大家可以多多去发掘。参考资料：RandomForestsLeoBreimanandAdeleCutlerRandomforest打赏支持我写出更多好文章，谢谢！打赏作者打赏支持我写出更多好文章，谢谢！任选一种支付方式1赞5收藏1评论关于作者：翱翔的翱Javaisagoodboy!个人主页·我的文章·6·"], "art_create_time": ["2017/11/24"], "art_title": ["疏而不漏：随机森林"], "art_url": ["http://python.jobbole.com/88884/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2017/11/1ed7c6d9074f6464e4702ca32251fc8a.png"]},
{"art_content": ["原文出处：remusao   译文出处：oschina   我最近在涉及大量数据处理的项目中频繁使用sqlite3。我最初的尝试根本不涉及任何数据库，所有的数据都将保存在内存中，包括字典查找、迭代和条件等查询。这很好，但可以放入内存的只有那么多，并且将数据从磁盘重新生成或加载到内存是一个繁琐又耗时的过程。我决定试一试sqlite3。因为只需打开与数据库的连接，这样可以增加可处理的数据量，并将应用程序的加载时间减少到零。此外，我可以通过SQL查询替换很多Python逻辑语句。我想分享一些关于这次经历的心得和发现。TL;DR使用大量操作(又名executemany)。你不需要使用光标(大部分时间)。光标可被迭代。使用上下文管理器。使用编译指示(当它有意义)。推迟索引创建。使用占位符来插入python值。1.使用大量操作如果你需要在数据库中一次性插入很多行，那么你真不应该使用execute。sqlite3模块提供了批量插入的方式：executemany。而不是像这样做：Pythonforrowiniter_data():connection.execute('INSERTINTOmy_tableVALUES(?)',row)12forrowiniter_data():    connection.execute('INSERTINTOmy_tableVALUES(?)',row)你可以利用这个事实，即executemany接受元组的生成器作为参数：Pythonconnection.executemany('INSERTINTOmy_tableVALUE(?)',iter_data())1234connection.executemany(    'INSERTINTOmy_tableVALUE(?)',    iter_data())这不仅更简洁，而且更高效。实际上，sqlite3在幕后利用executemany实现execute，但后者插入一行而不是多行。我写了一个小的基准测试，将一百万行插入空表（数据库在内存中）：executemany:1.6秒execute:2.7秒2.你不需要游标一开始我经常搞混的事情就是，光标管理。在线示例和文档中通常如下：Pythonconnection=sqlite3.connect(':memory:')cursor=connection.cursor()#Dosomethingwithcursor123connection=sqlite3.connect(':memory:')cursor=connection.cursor()#Dosomethingwithcursor但大多数情况下，你根本不需要光标，你可以直接使用连接对象（本文末尾会提到）。像execute和executemany类似的操作可以直接在连接上调用。以下是一个证明此事的示例：Pythonimportsqlite3connection=sqlite3(':memory:')#Createatableconnection.execute('CREATETABLEevents(ts,msg)')#Insertvaluesconnection.executemany('INSERTINTOeventsVALUES(?,?)',[(1,'foo'),(2,'bar'),(3,'baz')])#Printinsertedrowsforrowinconnnection.execute('SELECT*FROMevents'):print(row)1234567891011121314151617181920importsqlite3 connection=sqlite3(':memory:') #Createatableconnection.execute('CREATETABLEevents(ts,msg)') #Insertvaluesconnection.executemany(    'INSERTINTOeventsVALUES(?,?)',    [        (1,'foo'),        (2,'bar'),        (3,'baz')    ]) #Printinsertedrowsforrowinconnnection.execute('SELECT*FROMevents'):    print(row)3.光标（Cursor）可被用于迭代你可能经常会看到使用fetchone或fetchall来处理SELECT查询结果的示例。但是我发现处理这些结果的最自然的方式是直接在光标上迭代：Pythonforrowinconnection.execute('SELECT*FROMevents'):print(row)12forrowinconnection.execute('SELECT*FROMevents'):    print(row)这样一来，只要你得到足够的结果，你就可以终止查询，并且不会引起资源浪费。当然，如果事先知道你需要多少结果，可以改用LIMITSQL语句，但Python生成器是非常方便的，可以让你将数据生成与数据消耗分离。4.使用ContextManagers（上下文管理器）即使在处理SQL事务的中间，也会发生讨厌的事情。为了避免手动处理回滚或提交，你可以简单地使用连接对象作为上下文管理器。在以下示例中，我们创建了一个表，并错误地插入了重复的值：Pythonimportsqlite3connection=sqlite3.connect(':memory:')withconnection:connection.execute('CREATETABLEevents(ts,msg,PRIMARYKEY(ts,msg))')try:withconnection:connection.executemany('INSERTINTOeventsVALUES(?,?)',[(1,'foo'),(2,'bar'),(3,'baz'),(1,'foo'),])except(sqlite3.OperationalError,sqlite3.IntegrityError)ase:print('Couldnotcompleteoperation:',e)#Norowwasinsertedbecausetransactionfailedforrowinconnection.execute('SELECT*FROMevents'):print(row)connection.close()1234567891011121314151617181920212223importsqlite3connection=sqlite3.connect(':memory:') withconnection:    connection.execute(        'CREATETABLEevents(ts,msg,PRIMARYKEY(ts,msg))') try:    withconnection:        connection.executemany('INSERTINTOeventsVALUES(?,?)',[            (1,'foo'),            (2,'bar'),            (3,'baz'),            (1,'foo'),        ])except(sqlite3.OperationalError,sqlite3.IntegrityError)ase:    print('Couldnotcompleteoperation:',e)    #Norowwasinsertedbecausetransactionfailedforrowinconnection.execute('SELECT*FROMevents'):    print(row)    connection.close()5.使用Pragmas…当它真的有用时在你的程序中有几个pragma可用于调整sqlite3的行为。特别地，其中一个可以改善性能的是synchronous：Pythonconnection.execute('PRAGMAsynchronous=OFF')1connection.execute('PRAGMAsynchronous=OFF')你应该知道这可能是危险的。如果应用程序在事务中间意外崩溃，数据库可能会处于不一致的状态。所以请小心使用！但是如果你要更快地插入很多行，那么这可能是一个选择。6.推迟索引创建假设你需要在数据库上创建几个索引，而你需要在插入很多行的同时创建索引。把索引的创建推迟到所有行的插入之后可以导致实质性的性能改善。7.使用占位符插入Python值使用Python字符串操作将值包含到查询中是很方便的。但是这样做非常不安全，而sqlite3给你提供了更好的方法来做到这一点：Python#Donotdothis!my_timestamp=1c.execute(\"SELECT*FROMeventsWHEREts='%s'\"%my_timestamp)#Dothisinsteadmy_timestamp=(1,)c.execute('SELECT*FROMeventsWHEREts=?',my_timestamp)1234567#Donotdothis!my_timestamp=1c.execute(\"SELECT*FROMeventsWHEREts='%s'\"%my_timestamp) #Dothisinsteadmy_timestamp=(1,)c.execute('SELECT*FROMeventsWHEREts=?',my_timestamp)此外，使用Python％s（或格式或格式的字符串常量）的字符串插值对于executemany来说并不是总是可行。所以在此尝试没有什么真正意义！请记住，这些小技巧可能会（也可能不会）给你带来好处，具体取决于特定的用例。你应该永远自己去尝试，决定是否值得这么做。1赞4收藏评论"], "art_create_time": ["2017/12/01"], "art_title": ["让 Python 更加充分的使用 Sqlite3"], "art_url": ["http://python.jobbole.com/88954/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2015/02/edecceebedd0d04aa17bccba430ddcaf.jpg"]},
{"art_content": ["原文出处：xybaby   本文有些零碎，总题来说，包括两个问题：（1）可变对象（最常见的是listdict）被意外修改的问题，（2）对参数（parameter）的检查问题。这两个问题，本质都是因为动态语言（动态类型语言）的特性造成了，动态语言的好处就不细说了，本文是要讨论因为动态－－这种灵活性带来的一些问题。什么是动态语言（DynamicProgramminglanguage）呢，是相对于静态语言而言，将很多静态语言编译（compilation）时期所做的事情推迟到运行时，在运行时修改代码的行为，比如添加新的对象和函数，修改既有代码的功能，改变类型。绝大多数动态语言都是动态类型（DynamicTyped），所谓动态类型，是在运行时确定数据类型，变量使用之前不需要类型声明，通常变量的类型是被赋值的那个值的类型。Python就是属于典型的动态语言。动态语言的魅力在于让开发人员更好的关注需要解决的问题本身，而不是冗杂的语言规范，也不用干啥都得写个类。运行时改变代码的行为也是非常有用，比如python的热更新，可以做到不关服务器就替换代码的逻辑，而静态语言如C++就很难做到这一点。笔者使用得最多的就是C++和Python，C++中的一些复杂的点，比如模板（泛型编程）、设计模式（比如templatemethod），在Python中使用起来非常自然。我也看到过有一些文章指出，设计模式往往是特定静态语言的补丁—为了弥补语言的缺陷或者限制。以笔者的知识水平，远远不足以评价动态语言与静态语言的优劣。本文也只是记录在我使用Python这门动态语言的时候，由于语言的灵活性，由于动态类型，踩过的坑，一点思考，以及困惑。第一个问题：Mutable对象被误改这个是在线上环境出现过的一个BUG事后说起来很简单，服务端数据（放在dict里面的）被意外修改了，但查证的时候也花了许多时间，伪代码如下：defroutine(dct):ifhigh_propability:sub_routine_no_change_dct(dct)else:sub_routine_will_change_dct(dct)12345defroutine(dct):    ifhigh_propability:        sub_routine_no_change_dct(dct)    else:        sub_routine_will_change_dct(dct)上述的代码很简单，dct是一个dict，极大概率会调用一个不用修改dct的子函数，极小概率出会调用到可能修改dct的子函数。问题就在于，调用routine函数的参数是服务端全局变量，理论上是不能被修改的。当然，上述的代码简单到一眼就能看出问题，但在实际环境中，调用链有七八层，而且，在routine这个函数的doc里面，声明不会修改dct，该函数本身确实没有修改dct，但调用的子函数或者子函数的子函数没有遵守这个约定。从python语言特性看这个问题本小节解释上面的代码为什么会出问题，简单来说两点：dict是mutable对象；dict实例作为参数传入函数，然后被函数修改了。Python中一切都是对象(evethingisobject)，不管是intstrdict还是类。比如a=5，5是一个整数类型的对象（实例）；那么a是什么，a是5这个对象吗?不是的，a只是一个名字，这个名字暂时指向（绑定、映射）到5这个对象。b=a是什么意思呢，是b指向a指向的对象，即a，b都指向整数5这个对象那么什么是mutable什么是immutable呢，mutable是说这个对象是可以修改的，immutable是说这个对象是不可修改的(废话)。还是看Python官方怎么说的吧Mutableobjectscanchangetheirvaluebutkeeptheirid().Immutable：Anobjectwithafixedvalue.Immutableobjectsincludenumbers,stringsandtuples.Suchanobjectcannotbealtered.Anewobjecthastobecreatedifadifferentvaluehastobestored.Theyplayanimportantroleinplaceswhereaconstanthashvalueisneeded,forexampleasakeyinadictionary.承接上面的例子（a=5），int类型就是immutable，你可能说不对啊，比如对a赋值，a=6，现在a不是变成6了吗？是的，a现在”变成”6了，但本质是a指向了6这个对象—a不再指向5了检验对象的唯一标准是id，id函数返回对象的地址，每个对象在都有唯一的地址。看下面两个例子就知道了Python>>>a=5;id(a)　　35170056　　>>>a=6;id(a)　　35170044　　>>>lst=[1,2,3];id(lst)　　39117168　　>>>lst.append(4);id(lst)　　39117168123456789>>>a=5;id(a)　　35170056　　>>>a=6;id(a)　　35170044　　>>>lst=[1,2,3];id(lst)　　39117168　　>>>lst.append(4);id(lst)　　39117168或者这么说，对于非可变对象，在对象的生命周期内，没有办法改变对象所在内存地址上的值。python中，不可变对象包括：int,long,float,bool,str,tuple,frozenset；而其他的dictlist自定义的对象等属于可变对象。注意：str也是不可变对象，这也是为什么在多个字符串连接操作的时候，推荐使用join而不是+而且python没有机制，让一个可变对象不可被修改（此处类比的是C++中的const）dict是可变对象！那在python中，调用函数时的参数传递是什么意思呢，是传值、传引用？事实上都不正确，我不清楚有没有专业而统一的说法，但简单理解，就是形参（parameter）和实参（argument）都指向同一个对象，仅此而已。来看一下面的代码：defdouble(v):print'argumentbefore',id(v)v*=2print'argumentafter',id(v)returnvdeftest_double(a):print'parameterbdfore',id(a),adouble(a)print'parameterafter',id(a),aif__name__=='__main__':print'test_doublewithint'test_double(1)print'test_doublewithlist'test_double([1])12345678910111213141516defdouble(v):    print'argumentbefore',id(v)    v*=2    print'argumentafter',id(v)    returnv deftest_double(a):    print'parameterbdfore',id(a),a    double(a)    print'parameterafter',id(a),a if__name__=='__main__':    print'test_doublewithint'    test_double(1)    print'test_doublewithlist'    test_double([1])运行结果：Pythontest_doublewithint　　parameterbdfore305169361　　argumentbefore30516936　　argumentafter30516924　　parameterafter305169361　　test_doublewithlist　　parameterbdfore37758256[1]　　argumentbefore37758256　　argumentafter37758256　　parameterafter37758256[1,1]123456789101112test_doublewithint　　parameterbdfore305169361　　argumentbefore30516936　　argumentafter30516924　　parameterafter305169361  　　test_doublewithlist　　parameterbdfore37758256[1]　　argumentbefore37758256　　argumentafter37758256　　parameterafter37758256[1,1]可以看到，刚进入子函数double的时候，a，v指向的同一个对象（相同的id）。对于testint的例子，v因为v*=2，指向了另外一个对象，但对实参a是没有任何影响的。对于testlst的时候，v*=2是通过v修改了v指向的对象（也是a指向的对象），因此函数调用完之后，a指向的对象内容发生了变化。如何防止mutable对象被函数误改：为了防止传入到子函数中的可变对象被修改，最简单的就是使用copy模块拷贝一份数据。具体来说，包括copy.copy,copy.deepcopy,前者是浅拷贝，后者是深拷贝。二者的区别在于：Thedifferencebetweenshallowanddeepcopyingisonlyrelevantforcompoundobjects(objectsthatcontainotherobjects,likelistsorclassinstances):Ashallowcopyconstructsanewcompoundobjectandthen(totheextentpossible)insertsreferencesintoittotheobjectsfoundintheoriginal.Adeepcopyconstructsanewcompoundobjectandthen,recursively,insertscopiesintoitoftheobjectsfoundintheoriginal.简单来说，深拷贝会递归拷贝，遍历任何compoundobject然后拷贝，例如：Python>>>lst=[1,[2]]　　>>>importcopy　　>>>lst1=copy.copy(lst)　　>>>lst2=copy.deepcopy(lst)　　>>>printid(lst[1]),id(lst1[1]),id(lst2[1])　　440282526444028252644402988816　　>>>lst[1].append(3)　　>>>printlst,lst1,lst2　　[1,[2,3]][1,[2,3]][1,[2]]123456789>>>lst=[1,[2]]　　>>>importcopy　　>>>lst1=copy.copy(lst)　　>>>lst2=copy.deepcopy(lst)　　>>>printid(lst[1]),id(lst1[1]),id(lst2[1])　　440282526444028252644402988816　　>>>lst[1].append(3)　　>>>printlst,lst1,lst2　　[1,[2,3]][1,[2,3]][1,[2]]从例子可以看出浅拷贝的局限性，Python中，对象的基本构造也是浅拷贝，例如Pythondct={1：[1]};dct1=dict(dct)1dct={1：[1]};dct1=dict(dct)正是由于浅拷贝与深拷贝本质上的区别，二者性能代价差异非常之大，即使对于被拷贝的对象来说毫无差异：importcopydeftest_copy(inv):returncopy.copy(inv)deftest_deepcopy(inv):returncopy.deepcopy(inv)dct={str(i):iforiinxrange(100)}deftimeit_copy():importtimeitprinttimeit.Timer('test_copy(dct)','from__main__importtest_copy,dct').timeit(100000)printtimeit.Timer('test_deepcopy(dct)','from__main__importtest_deepcopy,dct').timeit(100000)if__name__=='__main__':timeit_copy()123456789101112131415importcopydeftest_copy(inv):    returncopy.copy(inv)deftest_deepcopy(inv):    returncopy.deepcopy(inv)dct={str(i):iforiinxrange(100)} deftimeit_copy():    importtimeit     printtimeit.Timer('test_copy(dct)','from__main__importtest_copy,dct').timeit(100000)    printtimeit.Timer('test_deepcopy(dct)','from__main__importtest_deepcopy,dct').timeit(100000) if__name__=='__main__':    timeit_copy()运行结果：Python1.19009837668113.11954377121.19009837668113.11954377在上面的示例中，dct这个dict的values都是int类型，immutable对象，因为无论浅拷贝深拷贝效果都是一样的，但是耗时差异巨大。如果在dct中存在自定义的对象，差异会更大那么为了安全起见，应该使用深拷贝；为了性能，应该使用浅拷贝。如果compoundobject包含的元素都是immutable，那么浅拷贝既安全又高效，but，对于python这种灵活性极强的语言，很可能某天某人就加入了一个mutable元素。好的API好的API应该是easytouseright;hardtousewrong。API应该提供一种契约，约定如果使用者按照特定的方式调用，那么API就能实现预期的效果。在静态语言如C++中，函数签名就是最好的契约。在C++中，参数传递大约有三种形式，传值、传指针、传引用（这里不考虑右值引用）。指针和引用虽然表现形式上差异，但效果上是差不多的，因此这里主要考虑传值和传引用。比如下面四个函数签名：Pythonintfunc(inta)　　intfunc(constinta)　　intfunc(int&a)　　intfunc(constint&a)1234intfunc(inta)　　intfunc(constinta)　　intfunc(int&a)　　intfunc(constint&a)对于第1、2个函数，对于调用者来说都是一样的，因为都会进行拷贝（深拷贝），无论func函数内部怎么操作，都不会影响到实参。二者的区别在于函数中能否对a进行修改，比如能否写a*＝2。第3个函数，非const引用，任何对a的修改都会影响到实参。调用者看到这个API就知道预期的行为：函数会改变实参的值。第4个函数，const引用，函数承诺绝对不会修改实参，因此调用者可以放心大胆的传引用，无需拷贝。从上面几个API，可以看到，通过函数签名，调用者就能知道函数调用对传入的参数有没有影响。python是动态类型检查，除了运行时，没法做参数做任何检查。有人说，那就通过pythondoc或者变量名来实现契约吧，比如：Pythondeffunc(dct_only_read):　　“”“param:dct_only_readwillbeonlyread,neverupate”“”123deffunc(dct_only_read):     　　“”“param:dct_only_readwillbeonlyread,neverupate”“”但是人是靠不住的，也是不可靠的，也许在这个函数的子函数（子函数的子函数，。。。）就会修改这个dict。怎么办，对可变类型强制copy（deepcopy），但拷贝又非常耗时。。。第二个问题：参数检查上一节说明没有签名对函数调用者是多么不爽，而本章节则说明没有签名对函数提供者有多么不爽。没有类型检查真的蛋疼，我也遇到过有人为了方便，给一个约定是int类型的形参传入了一个int的list，而可怕的是代码不报错，只是表现不正常。来看一个例子：deffunc(arg):ifarg:print'dolotsofthingshere'else:print'doanothers'12345deffunc(arg):    ifarg:        print'dolotsofthingshere'    else:        print'doanothers'上述的代码很糟糕，根本没法“望名知意”，也看不出有关形参arg的任何信息。但事实上这样的代码是存在的，而且还有比这更严重的，比如挂羊头卖狗肉。这里有一个问题，函数期望arg是某种类型，是否应该写代码判断呢，比如：isinstance(arg,str)。因为没有编译器静态来做参数检查，那么要不要检查，如何检查就完全是函数提供者的事情。如果检查，那么影响性能，也容易违背python的灵活性—ducktyping；不检查，又容易被误用。但在这里，考虑的是另一个问题，看代码的第二行：ifarg。python中，几乎是一切对象都可以当作布尔表达式求值，即这里的arg可以是一切python对象，可以是bool、int、dict、list以及任何自定义对象。不同的类型为“真”的条件不一样，比如数值类型(intfloat)非0即为真；序列类型（str、list、dict）非空即为真；而对于自定义对象，在python2.7种则是看是否定义了__nonzero__、__len__，如果这两个函数都没有定义，那么实例的布尔求值一定返回真。在PEP8，由以下关于对序列布尔求值的规范：PythonForsequences,(strings,lists,tuples),usethefactthatemptysequencesarefalse.Yes:ifnotseq:ifseq:No:iflen(seq):ifnotlen(seq):1234567Forsequences,(strings,lists,tuples),usethefactthatemptysequencesarefalse. Yes:ifnotseq:    ifseq: No:iflen(seq):    ifnotlen(seq):在googlepythonstyleguide中也有一节专门关于bool表达式，指出“尽可能使用隐式的false”。对于序列，推荐的判断方法与pep8相同，另外还由两点比较有意思：如果你需要区分false和None,你应该用像ifnotxandxisnotNone:这样的语句.处理整数时,使用隐式false可能会得不偿失(即不小心将None当做0来处理).你可以将一个已知是整型(且不是len()的返回结果)的值与0比较.第二点我个人很赞同；但第一点就觉得很别扭，因为这样的语句一点不直观，难以表达其真实目的。在pep20thezenofpython中，指出：PythonExplicitisbetterthanimplicit.1Explicitisbetterthanimplicit.这句话简单但实用！代码是写给人读的，清晰的表达代码的意图比什么都重要。也许有的人觉得代码写得复杂隐晦就显得牛逼，比如python中嵌套几层的listcomprehension，且不知这样害人又害己。回到布尔表达式求值这个问题，我觉得很多时候直接使用ifarg：这种形式都不是好主意，因为不直观而且容易出错。比如参数是int类型的情况，defhandle_age(age):ifnotage:return#dolotswithage1234defhandle_age(age):    ifnotage:        return    #dolotswithage很难说当age=0时是不是一个合理的输入，上面的代码对None、0一视同仁，看代码的人也搞不清传入0是否正确。另外一个具有争议性的例子就是对序列进行布尔求值，推荐的都是直接使用ifseq:的形式，但这种形式违背了”Explicitisbetterthanimplicit.“，因为这样写根本无法区分None和空序列，而这二者往往是由区别的，很多时候，空序列是一个合理的输入，而None不是。这个问题，stackoverflow上也有相关的讨论“如何检查列表为空”，诚然，如果写成seq==[]是不那么好的代码，因为不那么灵活—如果seq是tuple类型代码就不能工作了。python语言是典型的ducktyping，不管你传入什么类型，只要具备相应的函数，那么代码就可以工作，但是否正确地工作就完完全全取决于使用者。个人觉得存在宽泛的约束比较好，比如Python中的ABC（abstractbaseclass）,既满足了灵活性需求，后能做一些规范检查。总结以上两个问题，是我使用Python语言以来遇到的诸多问题之二，也是我在同一个地方跌倒过两次的问题。Python语言以开发效率见长，但是我觉得需要良好的规范才能保证在大型线上项目中使用。而且，我也倾向于假设：人是不可靠的，不会永远遵守拟定的规范，不会每次修改代码之后更新docstring…因此，为了保证代码的可持续发展，需要做到以下几点第一：拟定并遵守代码规范代码规范最好在项目启动时就应该拟定好，可以参照PEP8和googlepythonstyleguild。很多时候风格没有优劣之说，但是保证项目内的一致性很重要。并保持定期review、对新人review！第二：静态代码分析只要能静态发现的bug不要放到线上，比如对参数、返回值的检查，在python3.x中可以使用注解（FunctionAnnotations），python2.x也可以自行封装decorator来做检查。对代码行为，既可以使用Coverity这种高大上的商业软件，或者王垠大神的Pysonar2，也可以使用ast编写简单的检查代码。第三：单元测试单元测试的重要性想必大家都知道，在python中出了官方自带的doctest、unittest，还有许多更强大的框架，比如nose、mock。第四：100%的覆盖率测试对于python这种动态语言，出了执行代码，几乎没有其他比较好的检查代码错误的手段，所以覆盖率测试是非常重要的。可以使用python原生的sys.settrace、sys.gettrace，也可以使用coverage等跟更高级的工具。虽然我已经写了几年Python了，但是在Python使用规范上还是很欠缺。我也不知道在其他公司、项目中，是如何使用好Python的，如何扬长避短的。欢迎pythoner留言指导！references:DynamicProgramminglanguageinstagram-pycon-2017https://www.python.org/dev/peps/pep-0008/googlepythonstyleguidethezenofpythonbest-way-to-check-if-a-list-is-empty1赞1收藏评论"], "art_create_time": ["2017/11/19"], "art_title": ["动态语言的灵活性是把双刃剑 －－ 以 Python 语言为例"], "art_url": ["http://python.jobbole.com/88901/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2015/02/edecceebedd0d04aa17bccba430ddcaf.jpg"]},
{"art_content": ["原文出处：布道   11月6日，湖南卫视已经开播被称作年度压轴的大戏“猎场”，迅速占领各大榜单，成为一部高热度的电视剧。但是在豆瓣上却形成了两极分化。截止11月8日，该剧在豆瓣上的评分为5.7分。相比较胡歌之前《琅琊榜》的9.1，《伪装者》的8.3等来说，这一评分确实不高。有趣的是，首页的评分比例与“短评”、“剧评”的比例存在非常大的差异！首页总评分评分两级分化严重，“差评”占主在目前11463个评价中两级分化严重，“1星”占比最高为28.6%，其次为“5星”的25.4%。“好评”（5星、4星）占比为35.80%，“一般”（3星）为16.50%，“差评”（2星、1星）占比为47.80%。很明显，“差评”占了接近一半的比例。《猎场》豆瓣评分占比分布在短评和剧评中的另一种景象首页的豆瓣评分中“差评”占比很高，但是在豆瓣的短评和剧评中却是另一番景象。在目前5979条短评中，“好评”占比71%，“一般”为5%，“差评”占比24%。而在392条剧评中，“5星”占了非常高的比例！84.7%的剧评给了“好评”。《猎场》剧评评分分布我们将三个位置的评分放在一起比较就会出现非常明显的差异。根据这个差异，我们可以大致判断：写出短评或者剧评的观众大部分给予了“好评”，但仍有大量观众直接给了差评，并没有说明任何原因。当然，我们并没有考虑那些不写评论，而只是点“有用”和“没用”观众。才刚刚上映，剧情还在慢慢的铺，所以现在给整部剧下定论还太早。《猎场》到底好不好看？我们还是想通过以11月8日为界，看看人们短评人的情绪，是积极，还是消息。利用词云看看大家都说了什么，希望能大家就是否建议观看给出建议。一、爬取《猎场》热门短评，豆瓣的爬虫做的比较好，不登录爬虫很快就会被屏蔽掉，登录后获取cookies如下：同时建议在循环抓取的时候进行sleep，例如：Pythontime.sleep(1+float(random.randint(1,100))/20)1time.sleep(1+float(random.randint(1,100))/20)《猎场》热门短评内容和时间爬取了22440条评论，代码如下：Pythonimportreimportrequestsimportcodecsimporttimeimportrandomfrombs4importBeautifulSoupabsolute='https://movie.douban.com/subject/26322642/comments'absolute_url='https://movie.douban.com/subject/26322642/comments?start=23&limit=20&sort=new_score&status=P&percent_type='url='https://movie.douban.com/subject/26322642/comments?start={}&limit=20&sort=new_score&status=P'header={'User-Agent':'Mozilla/5.0(WindowsNT10.0;Win64;x64;rv:54.0)Gecko/20100101Firefox/54.0','Connection':'keep-alive'}defget_data(html):soup=BeautifulSoup(html,'lxml')comment_list=soup.select('.comment>p')next_page=soup.select('#paginator>a')[2].get('href')date_nodes=soup.select('..comment-time')returncomment_list,next_page,date_nodesif__name__=='__main__':f_cookies=open('cookie.txt','r')cookies={}forlineinf_cookies.read().split(';'):name,value=line.strip().split('=',1)cookies[name]=valuehtml=requests.get(absolute_url,cookies=cookies,headers=header).contentcomment_list=[]#获取评论comment_list,next_page,date_nodes=get_data(html,)soup=BeautifulSoup(html,'lxml')comment_list=[]while(next_page!=[]):#查看“下一页”的A标签链接print(absolute+next_page)html=requests.get(absolute+next_page,cookies=cookies,headers=header).contentsoup=BeautifulSoup(html,'lxml')comment_list,next_page,date_nodes=get_data(html)withopen(\"comments.txt\",'a',encoding='utf-8')asf:fornodeincomment_list:comment=node.get_text().strip().replace(\"\\n\",\"\")fordateindate_nodes:date=node.get_text().strip()f.writelines((comment,date)+u'\\n')time.sleep(1+float(random.randint(1,100))/20)12345678910111213141516171819202122232425262728293031323334353637383940importreimportrequestsimportcodecsimporttimeimportrandomfrombs4importBeautifulSoupabsolute='https://movie.douban.com/subject/26322642/comments'absolute_url='https://movie.douban.com/subject/26322642/comments?start=23&limit=20&sort=new_score&status=P&percent_type='url='https://movie.douban.com/subject/26322642/comments?start={}&limit=20&sort=new_score&status=P'header={'User-Agent':'Mozilla/5.0(WindowsNT10.0;Win64;x64;rv:54.0)Gecko/20100101Firefox/54.0','Connection':'keep-alive'}defget_data(html):    soup=BeautifulSoup(html,'lxml')    comment_list=soup.select('.comment>p')    next_page=soup.select('#paginator>a')[2].get('href')    date_nodes=soup.select('..comment-time')    returncomment_list,next_page,date_nodesif__name__=='__main__':    f_cookies=open('cookie.txt','r')    cookies={}    forlineinf_cookies.read().split(';'):        name,value=line.strip().split('=',1)        cookies[name]=value    html=requests.get(absolute_url,cookies=cookies,headers=header).content    comment_list=[]    #获取评论    comment_list,next_page,date_nodes=get_data(html,)    soup=BeautifulSoup(html,'lxml')    comment_list=[]    while(next_page!=[]):  #查看“下一页”的A标签链接        print(absolute+next_page)        html=requests.get(absolute+next_page,cookies=cookies,headers=header).content        soup=BeautifulSoup(html,'lxml')        comment_list,next_page,date_nodes=get_data(html)        withopen(\"comments.txt\",'a',encoding='utf-8')asf:            fornodeincomment_list:                comment=node.get_text().strip().replace(\"\\n\",\"\")                fordateindate_nodes:                    date=node.get_text().strip()                    f.writelines((comment,date)+u'\\n')        time.sleep(1+float(random.randint(1,100))/20)二、对数据进行清洗：Pythonimportpandasaspdimportmatplotlib.pyplotaspltdate_name=['date','comment']df=pd.read_csv('./comment.csv',header=None,names=date_name,encoding='gbk')df['date']=pd.to_datetime(df['date'])12345importpandasaspdimportmatplotlib.pyplotaspltdate_name=['date','comment']df=pd.read_csv('./comment.csv',header=None,names=date_name,encoding='gbk')df['date']=pd.to_datetime(df['date'])样本数量：Pythonprint(df['date'].value_counts())获取2017-11-06–2017-11-08数据：12print(df['date'].value_counts())获取2017-11-06–2017-11-08数据：Pythondata6=df['2017-11-06':'2017-11-08']data6.to_csv('6.txt',encoding='utf-8',index=False)print(data6.size)5775获取2017-11-09–2017-11-17数据：data9=df['2017-11-09':'2017-11-17']data9.to_csv('9.txt',encoding='utf-8',index=False)print(data9.size)16665123456789data6=df['2017-11-06':'2017-11-08']data6.to_csv('6.txt',encoding='utf-8',index=False)print(data6.size)5775获取2017-11-09–2017-11-17数据：data9=df['2017-11-09':'2017-11-17']data9.to_csv('9.txt',encoding='utf-8',index=False)print(data9.size)16665三、情感分析和词云对热门短评基于原有SnowNLP进行积极和消极情感分类，读取每段评论并依次进行情感值分析（代码：https://zhuanlan.zhihu.com/p/30107203），最后会计算出来一个0-1之间的值。当值大于0.5时代表句子的情感极性偏向积极，当分值小于0.5时，情感极性偏向消极，当然越偏向两边，情绪越偏激。2017-11-06–2017-11-08分析：从上图情感分析（代码：https://zhuanlan.zhihu.com/p/30107203 ）来看，影评者还是还是非常积极的，对《猎场》的期望很高。从词云（代码：https://zhuanlan.zhihu.com/p/30107203 ）上来看：2017-11-09–2017-11-17分析从上图情感分析（代码：https://zhuanlan.zhihu.com/p/30107203 ）来看，积极的情绪已经远远超过消极的情绪，还是受到大家的好评。从词云（代码：https://zhuanlan.zhihu.com/p/30107203 ）上来看，出现好看、剧情、期待、喜欢等词。总结词云的背景是胡歌，大家看出来了嘛？目前豆瓣的分数已经是6.2分，目前剧情过半，相信接下来会更精彩，个人认为分数会在7.5分以上。抛开豆瓣的推荐分数，通过的热门短评的情感和词云分析，是一部不错的现实剧，剧情犀利、深刻、启迪，很多人期待。如果您有时间，不妨看一下，或许能收获一些意想不到的东西。1赞11收藏5评论"], "art_create_time": ["2017/11/23"], "art_title": ["差评近一半，用 Python 分析胡歌的《猎场》到底值不值得看？"], "art_url": ["http://python.jobbole.com/88912/"], "art_img": ["http://wx4.sinaimg.cn/mw690/63918611gy1fls2ib5qbij20hs0a0gmf.jpg"]},
{"art_content": ["原文出处：GeraldNash   译文出处：黑色巧克力   尽管一些人认为区块链是一个等待问题的解决方案，但毫无疑问，这种新技术是计算机的奇迹。但是，区块链到底是什么呢?区块链它是比特币或其他加密货币进行交易的数字账本，账本按时间顺序记录并对外公开。在更一般的术语中，它是一个公共数据库，新数据存储在一个名为块的容器中，并被添加到一个不可变链（后来的区块链）中添加了过去的数据。在比特币和其他加密货币的情况下，这些数据是一组交易记录。当然，数据可以是任何类型的。区块链技术已经催生了新的、完全数字化的货币，如比特币和莱特币，这些货币并不是由中央政府发行或管理的。因此为那些认为今天的银行系统是骗局或终将失败的人带来了新的自由。区块链所包含的以太坊技术对分布式计算进行了变革创新，它引入了一些有趣的概念，比如智能合约。在本文中，我将用不到50行的Python2代码来做一个简单的区块链。我称它为SnakeCoin。首先将定义块将是什么样子。在区块链中，每个块都存储一个时间戳和一个索引。在SnakeCoin中，需要把两者都存储起来。为了确保整个区块链的完整性，每个块都有一个自动识别散列。与比特币一样，每个块的散列将是块索引、时间戳、数据和前块哈希的加密哈希。数据可以是你想要的任何东西。PythonimporthashlibashasherclassBlock:def__init__(self,index,timestamp,data,previous_hash):self.index=indexself.timestamp=timestampself.data=dataself.previous_hash=previous_hashself.hash=self.hash_block()defhash_block(self):sha=hasher.sha256()sha.update(str(self.index)+str(self.timestamp)+str(self.data)+str(self.previous_hash))returnsha.hexdigest()1234567891011121314151617importhashlibashasher classBlock:  def__init__(self,index,timestamp,data,previous_hash):    self.index=index    self.timestamp=timestamp    self.data=data    self.previous_hash=previous_hash    self.hash=self.hash_block()   defhash_block(self):    sha=hasher.sha256()    sha.update(str(self.index)+              str(self.timestamp)+              str(self.data)+              str(self.previous_hash))    returnsha.hexdigest()这一步后有块结构，但现在是创建区块链，所以需要向实际的链中添加块。如前所述，每个块都需要上一个块的信息。但是按照这个说法就有一个问题，区块链的第一个区块是如何到达那里的呢？不得不说，第一个块，或者说是起源块，它是一个特殊的块。在很多情况下，它是手动添加的，或者有独特的逻辑允许添加。下面将创建一个函数简单地返回一个起源块以便产生第一个区块。这个块是索引0，它具有任意的数据值和“前一个哈希”参数中的任意值。Pythonimportdatetimeasdatedefcreate_genesis_block():#Manuallyconstructablockwith#indexzeroandarbitraryprevioushashreturnBlock(0,date.datetime.now(),\"GenesisBlock\",\"0\")123456importdatetimeasdate defcreate_genesis_block():  #Manuallyconstructablockwith  #indexzeroandarbitraryprevioushash  returnBlock(0,date.datetime.now(),\"GenesisBlock\",\"0\")现在已经创建好了起源块，接下来需要一个函数，以便在区块链中生成后续的块。这个函数将把链中的前一个块作为参数，创建要生成的块的数据，并使用适当的数据返回新块。当新的块哈希信息来自前面的块时，区块链的完整性会随着每个新块而增加。如果不这样做，外部组织就更容易“改变过去”，用全新的方式取代已有的链条。这一系列的散列可以作为加密的证据，有助于确保一旦将块添加到区块链，它就不能被替换或删除。Pythondefnext_block(last_block):this_index=last_block.index+1this_timestamp=date.datetime.now()this_data=\"Hey!I'mblock\"+str(this_index)this_hash=last_block.hashreturnBlock(this_index,this_timestamp,this_data,this_hash)123456defnext_block(last_block):  this_index=last_block.index+1  this_timestamp=date.datetime.now()  this_data=\"Hey!I'mblock\"+str(this_index)  this_hash=last_block.hash  returnBlock(this_index,this_timestamp,this_data,this_hash)大部分的工作已经完成，现在可以创建区块链了。在这次的示例中，区块链本身是一个简单的Python列表。列表的第一个元素是起源块。当然，还需要添加后续的块，因为SnakeCoin是最小的区块链，这里只添加20个新的块。可以用for循环来生成新块。Python#Createtheblockchainandaddthegenesisblockblockchain=[create_genesis_block()]previous_block=blockchain[0]#Howmanyblocksshouldweaddtothechain#afterthegenesisblocknum_of_blocks_to_add=20#Addblockstothechainforiinrange(0,num_of_blocks_to_add):block_to_add=next_block(previous_block)blockchain.append(block_to_add)previous_block=block_to_add#Telleveryoneaboutit!print\"Block#{}hasbeenaddedtotheblockchain!\".format(block_to_add.index)print\"Hash:{}\\n\".format(block_to_add.hash)12345678910111213141516#Createtheblockchainandaddthegenesisblockblockchain=[create_genesis_block()]previous_block=blockchain[0] #Howmanyblocksshouldweaddtothechain#afterthegenesisblocknum_of_blocks_to_add=20 #Addblockstothechainforiinrange(0,num_of_blocks_to_add):  block_to_add=next_block(previous_block)  blockchain.append(block_to_add)  previous_block=block_to_add  #Telleveryoneaboutit!  print\"Block#{}hasbeenaddedtotheblockchain!\".format(block_to_add.index)  print\"Hash:{}\\n\".format(block_to_add.hash)下面来测试一下目前产生的区块链。看到了吧，这就是区块链。如果希望在控制台中查看更多信息，可以编辑完整的源文件并打印每个块的时间戳或数据。这就是SnakeCoin要提供的所有东西。为了使SnakeCoin规模达到今天生产区块链的规模，必须添加更多的功能，比如服务器层，以跟踪多台机器上的链变化，以及在给定的时间段内限制添加的块数量的工作算法。如果想了解更多的技术信息，可以在这里查看原始的比特币白皮书。2赞12收藏3评论"], "art_create_time": ["2017/11/22"], "art_title": ["用不到 50 行的 Python 代码构建最小的区块链"], "art_url": ["http://python.jobbole.com/88907/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2017/11/8cdceb83f2089cfe344252cbd73bb70f.png"]},
{"art_content": ["原文出处：naughty   搜索是大数据领域里常见的需求。Splunk和ELK分别是该领域在非开源和开源领域里的领导者。本文利用很少的Python代码实现了一个基本的数据搜索功能，试图让大家理解大数据搜索的基本原理。布隆过滤器（BloomFilter）第一步我们先要实现一个布隆过滤器。布隆过滤器是大数据领域的一个常见算法，它的目的是过滤掉那些不是目标的元素。也就是说如果一个要搜索的词并不存在与我的数据中，那么它可以以很快的速度返回目标不存在。让我们看看以下布隆过滤器的代码：classBloomfilter(object):\"\"\"ABloomfilterisaprobabilisticdata-structurethattradesspaceforaccuracywhendeterminingifavalueisinaset.Itcantellyouifavaluewaspossiblyadded,orifitwasdefinitelynotadded,butitcan'ttellyouforcertainthatitwasadded.\"\"\"def__init__(self,size):\"\"\"SetuptheBFwiththeappropriatesize\"\"\"self.values=[False]*sizeself.size=sizedefhash_value(self,value):\"\"\"HashthevalueprovidedandscaleittofittheBFsize\"\"\"returnhash(value)%self.sizedefadd_value(self,value):\"\"\"AddavaluetotheBF\"\"\"h=self.hash_value(value)self.values[h]=Truedefmight_contain(self,value):\"\"\"CheckifthevaluemightbeintheBF\"\"\"h=self.hash_value(value)returnself.values[h]defprint_contents(self):\"\"\"DumpthecontentsoftheBFfordebuggingpurposes\"\"\"printself.values1234567891011121314151617181920212223242526272829classBloomfilter(object):    \"\"\"    ABloomfilterisaprobabilisticdata-structurethattradesspaceforaccuracy    whendeterminingifavalueisinaset.  Itcantellyouifavaluewaspossibly    added,orifitwasdefinitelynotadded,butitcan'ttellyouforcertainthat    itwasadded.    \"\"\"    def__init__(self,size):        \"\"\"SetuptheBFwiththeappropriatesize\"\"\"        self.values=[False]*size        self.size=size     defhash_value(self,value):        \"\"\"HashthevalueprovidedandscaleittofittheBFsize\"\"\"        returnhash(value)%self.size     defadd_value(self,value):        \"\"\"AddavaluetotheBF\"\"\"        h=self.hash_value(value)        self.values[h]=True     defmight_contain(self,value):        \"\"\"CheckifthevaluemightbeintheBF\"\"\"        h=self.hash_value(value)        returnself.values[h]     defprint_contents(self):        \"\"\"DumpthecontentsoftheBFfordebuggingpurposes\"\"\"        printself.values基本的数据结构是个数组（实际上是个位图，用1/0来记录数据是否存在），初始化是没有任何内容，所以全部置False。实际的使用当中，该数组的长度是非常大的，以保证效率。利用哈希算法来决定数据应该存在哪一位，也就是数组的索引当一个数据被加入到布隆过滤器的时候，计算它的哈希值然后把相应的位置为True当检查一个数据是否已经存在或者说被索引过的时候，只要检查对应的哈希值所在的位的True／Fasle看到这里，大家应该可以看出，如果布隆过滤器返回False，那么数据一定是没有索引过的，然而如果返回True，那也不能说数据一定就已经被索引过。在搜索过程中使用布隆过滤器可以使得很多没有命中的搜索提前返回来提高效率。我们看看这段code是如何运行的：bf=Bloomfilter(10)bf.add_value('dog')bf.add_value('fish')bf.add_value('cat')bf.print_contents()bf.add_value('bird')bf.print_contents()#Note:contentsareunchangedafteraddingbird-itcollidesfortermin['dog','fish','cat','bird','duck','emu']:print'{}:{}{}'.format(term,bf.hash_value(term),bf.might_contain(term))12345678910bf=Bloomfilter(10)bf.add_value('dog')bf.add_value('fish')bf.add_value('cat')bf.print_contents()bf.add_value('bird')bf.print_contents()#Note:contentsareunchangedafteraddingbird-itcollidesfortermin['dog','fish','cat','bird','duck','emu']:    print'{}:{}{}'.format(term,bf.hash_value(term),bf.might_contain(term))结果：[False,False,False,False,True,True,False,False,False,True][False,False,False,False,True,True,False,False,False,True]dog:5Truefish:4Truecat:9Truebird:9Trueduck:5Trueemu:8False12345678[False,False,False,False,True,True,False,False,False,True][False,False,False,False,True,True,False,False,False,True]dog:5Truefish:4Truecat:9Truebird:9Trueduck:5Trueemu:8False首先创建了一个容量为10的的布隆过滤器然后分别加入‘dog’，‘fish’，‘cat’三个对象，这时的布隆过滤器的内容如下：然后加入‘bird’对象，布隆过滤器的内容并没有改变，因为‘bird’和‘fish’恰好拥有相同的哈希。最后我们检查一堆对象（’dog’,‘fish’,‘cat’,‘bird’,‘duck’,’emu’）是不是已经被索引了。结果发现‘duck’返回True，2而‘emu’返回False。因为‘duck’的哈希恰好和‘dog’是一样的。 分词下面一步我们要实现分词。分词的目的是要把我们的文本数据分割成可搜索的最小单元，也就是词。这里我们主要针对英语，因为中文的分词涉及到自然语言处理，比较复杂，而英文基本只要用标点符号就好了。下面我们看看分词的代码：defmajor_segments(s):\"\"\"Performmajorsegmentingonastring.Splitthestringbyallofthemajorbreaks,andreturnthesetofeverythingfound.Thebreaksinthisimplementationaresinglecharacters,butinSplunkpropertheycanbemultiplecharacters.Asetisusedbecauseorderingdoesn'tmatter,andduplicatesarebad.\"\"\"major_breaks=''last=-1results=set()#enumerate()willgiveus(0,s[0]),(1,s[1]),...foridx,chinenumerate(s):ifchinmajor_breaks:segment=s[last+1:idx]results.add(segment)last=idx#Thelastcharactermaynotbeabreaksoalwayscapture#thelastsegment(whichmayendupbeing\"\",butyolo)segment=s[last+1:]results.add(segment)returnresults12345678910111213141516171819202122232425defmajor_segments(s):    \"\"\"    Performmajorsegmentingonastring.  Splitthestringbyallofthemajor    breaks,andreturnthesetofeverythingfound.  Thebreaksinthisimplementation    aresinglecharacters,butinSplunkpropertheycanbemultiplecharacters.    Asetisusedbecauseorderingdoesn'tmatter,andduplicatesarebad.    \"\"\"    major_breaks=''    last=-1    results=set()     #enumerate()willgiveus(0,s[0]),(1,s[1]),...    foridx,chinenumerate(s):        ifchinmajor_breaks:            segment=s[last+1:idx]            results.add(segment)             last=idx     #Thelastcharactermaynotbeabreaksoalwayscapture    #thelastsegment(whichmayendupbeing\"\",butyolo)        segment=s[last+1:]    results.add(segment)     returnresults主要分割主要分割使用空格来分词，实际的分词逻辑中，还会有其它的分隔符。例如Splunk的缺省分割符包括以下这些，用户也可以定义自己的分割符。]<>(){}|!;,‘”*\\n\\r\\s\\t&?+%21%26%2526%3B%7C%20%2B%3D—%2520%5D%5B%3A%0A%2C%28%29defminor_segments(s):\"\"\"Performminorsegmentingonastring.Thisislikemajorsegmenting,exceptitalsocapturesfromthestartoftheinputtoeachbreak.\"\"\"minor_breaks='_.'last=-1results=set()foridx,chinenumerate(s):ifchinminor_breaks:segment=s[last+1:idx]results.add(segment)segment=s[:idx]results.add(segment)last=idxsegment=s[last+1:]results.add(segment)results.add(s)returnresults12345678910111213141516171819202122232425defminor_segments(s):    \"\"\"    Performminorsegmentingonastring.  Thisislikemajor    segmenting,exceptitalsocapturesfromthestartofthe    inputtoeachbreak.    \"\"\"    minor_breaks='_.'    last=-1    results=set()     foridx,chinenumerate(s):        ifchinminor_breaks:            segment=s[last+1:idx]            results.add(segment)             segment=s[:idx]            results.add(segment)             last=idx     segment=s[last+1:]    results.add(segment)    results.add(s)     returnresults次要分割次要分割和主要分割的逻辑类似，只是还会把从开始部分到当前分割的结果加入。例如“1.2.3.4”的次要分割会有1，2，3，4，1.2，1.2.3defsegments(event):\"\"\"Simplewrapperaroundmajor_segments/minor_segments\"\"\"results=set()formajorinmajor_segments(event):forminorinminor_segments(major):results.add(minor)returnresults1234567defsegments(event):    \"\"\"Simplewrapperaroundmajor_segments/minor_segments\"\"\"    results=set()    formajorinmajor_segments(event):        forminorinminor_segments(major):            results.add(minor)    returnresults分词的逻辑就是对文本先进行主要分割，对每一个主要分割在进行次要分割。然后把所有分出来的词返回。我们看看这段code是如何运行的：forterminsegments('src_ip=1.2.3.4'):printterm12forterminsegments('src_ip=1.2.3.4'):        printtermsrc1.21.2.3.4src_ip311.2.3ip2=41234567891011src1.21.2.3.4src_ip311.2.3ip2=4 搜索好了，有个分词和布隆过滤器这两个利器的支撑后，我们就可以来实现搜索的功能了。上代码：classSplunk(object):def__init__(self):self.bf=Bloomfilter(64)self.terms={}#Dictionaryoftermtosetofeventsself.events=[]defadd_event(self,event):\"\"\"Addsaneventtothisobject\"\"\"#GenerateauniqueIDfortheevent,andsaveitevent_id=len(self.events)self.events.append(event)#Addeachtermtothebloomfilter,andtracktheeventbyeachtermforterminsegments(event):self.bf.add_value(term)iftermnotinself.terms:self.terms[term]=set()self.terms[term].add(event_id)defsearch(self,term):\"\"\"Searchforasingleterm,andyieldalltheeventsthatcontainit\"\"\"#InSplunkthisrunsinO(1),andislikelytobeinfilesystemcache(memory)ifnotself.bf.might_contain(term):return#InSplunkthisprobablyrunsinO(logN)whereNisthenumberoftermsinthetsidxiftermnotinself.terms:returnforevent_idinsorted(self.terms[term]):yieldself.events[event_id]12345678910111213141516171819202122232425262728293031323334classSplunk(object):    def__init__(self):        self.bf=Bloomfilter(64)        self.terms={}  #Dictionaryoftermtosetofevents        self.events=[]        defadd_event(self,event):        \"\"\"Addsaneventtothisobject\"\"\"         #GenerateauniqueIDfortheevent,andsaveit        event_id=len(self.events)        self.events.append(event)         #Addeachtermtothebloomfilter,andtracktheeventbyeachterm        forterminsegments(event):            self.bf.add_value(term)             iftermnotinself.terms:                self.terms[term]=set()            self.terms[term].add(event_id)     defsearch(self,term):        \"\"\"Searchforasingleterm,andyieldalltheeventsthatcontainit\"\"\"                #InSplunkthisrunsinO(1),andislikelytobeinfilesystemcache(memory)        ifnotself.bf.might_contain(term):            return         #InSplunkthisprobablyrunsinO(logN)whereNisthenumberoftermsinthetsidx        iftermnotinself.terms:            return         forevent_idinsorted(self.terms[term]):            yieldself.events[event_id]Splunk代表一个拥有搜索功能的索引集合每一个集合中包含一个布隆过滤器，一个倒排词表（字典），和一个存储所有事件的数组当一个事件被加入到索引的时候，会做以下的逻辑为每一个事件生成一个unqieid，这里就是序号对事件进行分词，把每一个词加入到倒排词表，也就是每一个词对应的事件的id的映射结构，注意，一个词可能对应多个事件，所以倒排表的的值是一个Set。倒排表是绝大部分搜索引擎的核心功能。当一个词被搜索的时候，会做以下的逻辑检查布隆过滤器，如果为假，直接返回检查词表，如果被搜索单词不在词表中，直接返回在倒排表中找到所有对应的事件id，然后返回事件的内容我们运行下看看把：s=Splunk()s.add_event('src_ip=1.2.3.4')s.add_event('src_ip=5.6.7.8')s.add_event('dst_ip=1.2.3.4')foreventins.search('1.2.3.4'):printeventprint'-'foreventins.search('src_ip'):printeventprint'-'foreventins.search('ip'):printevent12345678910111213s=Splunk()s.add_event('src_ip=1.2.3.4')s.add_event('src_ip=5.6.7.8')s.add_event('dst_ip=1.2.3.4') foreventins.search('1.2.3.4'):    printeventprint'-'foreventins.search('src_ip'):    printeventprint'-'foreventins.search('ip'):    printeventsrc_ip=1.2.3.4dst_ip=1.2.3.4-src_ip=1.2.3.4src_ip=5.6.7.8-src_ip=1.2.3.4src_ip=5.6.7.8dst_ip=1.2.3.4123456789src_ip=1.2.3.4dst_ip=1.2.3.4-src_ip=1.2.3.4src_ip=5.6.7.8-src_ip=1.2.3.4src_ip=5.6.7.8dst_ip=1.2.3.4是不是很赞！ 更复杂的搜索更进一步，在搜索过程中，我们想用And和Or来实现更复杂的搜索逻辑。上代码：classSplunkM(object):def__init__(self):self.bf=Bloomfilter(64)self.terms={}#Dictionaryoftermtosetofeventsself.events=[]defadd_event(self,event):\"\"\"Addsaneventtothisobject\"\"\"#GenerateauniqueIDfortheevent,andsaveitevent_id=len(self.events)self.events.append(event)#Addeachtermtothebloomfilter,andtracktheeventbyeachtermforterminsegments(event):self.bf.add_value(term)iftermnotinself.terms:self.terms[term]=set()self.terms[term].add(event_id)defsearch_all(self,terms):\"\"\"SearchforanANDofallterms\"\"\"#Startwiththeuniverseofallevents...results=set(range(len(self.events)))forterminterms:#Ifatermisn'tpresentatallthenwecanstoplookingifnotself.bf.might_contain(term):returniftermnotinself.terms:return#Dropeventsthatdon'tmatchfromourresultsresults=results.intersection(self.terms[term])forevent_idinsorted(results):yieldself.events[event_id]defsearch_any(self,terms):\"\"\"SearchforanORofallterms\"\"\"results=set()forterminterms:#Ifatermisn'tpresent,weskipit,butdon'tstopifnotself.bf.might_contain(term):continueiftermnotinself.terms:continue#Addtheseeventstoourresultsresults=results.union(self.terms[term])forevent_idinsorted(results):yieldself.events[event_id]123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657classSplunkM(object):    def__init__(self):        self.bf=Bloomfilter(64)        self.terms={}  #Dictionaryoftermtosetofevents        self.events=[]        defadd_event(self,event):        \"\"\"Addsaneventtothisobject\"\"\"         #GenerateauniqueIDfortheevent,andsaveit        event_id=len(self.events)        self.events.append(event)         #Addeachtermtothebloomfilter,andtracktheeventbyeachterm        forterminsegments(event):            self.bf.add_value(term)            iftermnotinself.terms:                self.terms[term]=set()                        self.terms[term].add(event_id)     defsearch_all(self,terms):        \"\"\"SearchforanANDofallterms\"\"\"         #Startwiththeuniverseofallevents...        results=set(range(len(self.events)))         forterminterms:            #Ifatermisn'tpresentatallthenwecanstoplooking            ifnotself.bf.might_contain(term):                return            iftermnotinself.terms:                return             #Dropeventsthatdon'tmatchfromourresults            results=results.intersection(self.terms[term])         forevent_idinsorted(results):            yieldself.events[event_id]      defsearch_any(self,terms):        \"\"\"SearchforanORofallterms\"\"\"        results=set()         forterminterms:            #Ifatermisn'tpresent,weskipit,butdon'tstop            ifnotself.bf.might_contain(term):                continue            iftermnotinself.terms:                continue             #Addtheseeventstoourresults            results=results.union(self.terms[term])         forevent_idinsorted(results):            yieldself.events[event_id]利用Python集合的intersection和union操作，可以很方便的支持And（求交集）和Or（求合集）的操作。运行结果如下：s=SplunkM()s.add_event('src_ip=1.2.3.4')s.add_event('src_ip=5.6.7.8')s.add_event('dst_ip=1.2.3.4')foreventins.search_all(['src_ip','5.6']):printeventprint'-'foreventins.search_any(['src_ip','dst_ip']):printevent12345678910s=SplunkM()s.add_event('src_ip=1.2.3.4')s.add_event('src_ip=5.6.7.8')s.add_event('dst_ip=1.2.3.4') foreventins.search_all(['src_ip','5.6']):    printeventprint'-'foreventins.search_any(['src_ip','dst_ip']):    printeventsrc_ip=5.6.7.8-src_ip=1.2.3.4src_ip=5.6.7.8dst_ip=1.2.3.412345src_ip=5.6.7.8-src_ip=1.2.3.4src_ip=5.6.7.8dst_ip=1.2.3.4 总结以上的代码只是为了说明大数据搜索的基本原理，包括布隆过滤器，分词和倒排表。如果大家真的想要利用这代码来实现真正的搜索功能，还差的太远。所有的内容来自于SplunkConf2017。大家如果有兴趣可以去看网上的视频。视频Slides1赞15收藏6评论"], "art_create_time": ["2017/11/25"], "art_title": ["用 Python 实现一个大数据搜索引擎"], "art_url": ["http://python.jobbole.com/88921/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2015/02/591d8b55a524f825dd29a22b8df70000.jpg"]},
{"art_content": ["原文出处：xybaby   实际项目中，pythoner更加关注的是Python的性能问题，之前也写过一篇文章《Python性能优化》介绍Python性能优化的一些方法。而本文，关注的是Python的内存优化，一般说来，如果不发生内存泄露，运行在服务端的Python代码不用太关心内存，但是如果运行在客户端（比如移动平台上），那还是有优化的必要。具体而言，本文主要针对的Cpython，而且不涉及C扩展。我们知道，Python使用引用技术和垃圾回收来管理内存，底层也有各种类型的内存池，那我们怎么得知一段代码使用的内存情况呢？工欲善其事必先利其器，直接看windows下的任务管理器或者linux下的top肯定是不准的。Pytracemalloc对于基本类型，可以通过sys.getsizeof()来查看对象占用的内存大小。以下是在64位Linux下的一些结果：Python>>>importsys>>>sys.getsizeof(1)24>>>sys.getsizeof([])72>>>sys.getsizeof(())56>>>sys.getsizeof({})280>>>sys.getsizeof(True)24123456789101112>>>importsys >>>sys.getsizeof(1)24>>>sys.getsizeof([])72>>>sys.getsizeof(())56>>>sys.getsizeof({})280>>>sys.getsizeof(True)24可以看到，即使是一个int类型(1)也需要占用24个字节，远远高于C语言中int的范围。因为Python中一切都是对象，int也不例外（事实上是PyIntObject），除了真正存储的数值，还需要保存引用计数信息、类型信息，更具体的可以参见《Python源码剖析》。而对于更复杂的组合类型，复杂的代码，使用getsizeof来查看就不准确了，因为在Python中变量仅仅指向一个对象，这个时候就需要更高级的工具，比如guppy，pysizer，pytracemalloc，objgraph。在这里重点介绍pytracemalloc。在Python3.4中，已经支持了pytracemalloc，如果使用python2.7版本，则需要对源码打补丁，然后重新编译。pytracemalloc在pep454中提出，主要有以下几个特点：TracebackwhereanobjectwasallocatedStatisticsonallocatedmemoryblocksperfilenameandperlinenumber:totalsize,numberandaveragesizeofallocatedmemoryblocksComputethedifferencesbetweentwosnapshotstodetectmemoryleaks简单来说，pytracemallochook住了python申请和释放内存的接口，从而能够追踪对象的分配和回收情况。对内存分配的统计数据可以精确到每个文件、每一行代码，也可以按照调用栈做聚合分析。而且还支持快照（snapshot）功能，比较两个快照之间的差异可以发现潜在的内存泄露。下面通过一个例子来简单介绍pytracemalloc的用法和接口，关于更详细用法和API，可以参考这份详尽的文档或者pytracemalloc的作者在pycon上的演讲ppt。importtracemallocNUM_OF_ATTR=10NUM_OF_INSTANCE=100classSlots(object):__slots__=['attr%s'%iforiinrange(NUM_OF_ATTR)]def__init__(self):value_lst=(1.0,True,[],{},())foriinrange(NUM_OF_ATTR):setattr(self,'attr%s'%i,value_lst[i%len(value_lst)])classNoSlots(object):def__init__(self):value_lst=(1.0,True,[],{},())foriinrange(NUM_OF_ATTR):setattr(self,'attr%s'%i,value_lst[i%len(value_lst)])defgenerate_some_objs():lst=[]foriinrange(NUM_OF_INSTANCE):o=Slots()ifi%2elseNoSlots()lst.append(o)returnlstif__name__=='__main__':tracemalloc.start(3)t=generate_some_objs()snapshot=tracemalloc.take_snapshot()top_stats=snapshot.statistics('lineno')#linenofilenametracebackprint(tracemalloc.get_traced_memory())forstatintop_stats[:10]:print(stat)12345678910111213141516171819202122232425262728293031323334353637383940importtracemalloc NUM_OF_ATTR=  10NUM_OF_INSTANCE=100classSlots(object):    __slots__=['attr%s'%iforiinrange(NUM_OF_ATTR)]    def__init__(self):        value_lst=(1.0,True,[],{},())        foriinrange(NUM_OF_ATTR):            setattr(self,'attr%s'%i,value_lst[i%len(value_lst)])  classNoSlots(object):    def__init__(self):        value_lst=(1.0,True,[],{},())        foriinrange(NUM_OF_ATTR):            setattr(self,'attr%s'%i,value_lst[i%len(value_lst)])   defgenerate_some_objs():    lst=[]    foriinrange(NUM_OF_INSTANCE):        o=Slots()ifi%2elseNoSlots()        lst.append(o)    returnlst  if__name__=='__main__':    tracemalloc.start(3)     t=generate_some_objs()     snapshot=tracemalloc.take_snapshot()    top_stats=snapshot.statistics('lineno')#linenofilenametraceback     print(tracemalloc.get_traced_memory())    forstatintop_stats[:10]:        print(stat)在上面的代码中，用到了pytracemalloc几个核心的API：Pythonstart(nframe:int=1)1start(nframe:int=1)pytracemalloc的一大好处就是可以随时启停，start函数即开始追踪内存分配，相应的stop会停止追踪。start函数有一个参数，nframes:内存分配时记录的栈的深度，这个值越大，pytracemalloc本身消耗的内存越多，在计算cumulative数据的时候有用。Python　get_traced_memory()1　get_traced_memory()返回值是拥有两个元素的tuple，第一个元素是当前分配的内存，第二个元素是自内存追踪启动以来的内存峰值。Pythontake_snapshot()1take_snapshot()返回当前内存分配快照，返回值是Snapshot对象，该对象可以按照单个文件、单行、单个调用栈统计内存分配情况运行环境：windows64位python3.4Python(62280,62920)test_pytracemalloc_use_py3.4.py:10:size=16.8KiB,count=144,average=120Btest_pytracemalloc_use_py3.4.py:17:size=16.7KiB,count=142,average=120Btest_pytracemalloc_use_py3.4.py:19:size=9952B,count=100,average=100Btest_pytracemalloc_use_py3.4.py:26:size=9792B,count=102,average=96Btest_pytracemalloc_use_py3.4.py:27:size=848B,count=1,average=848Btest_pytracemalloc_use_py3.4.py:34:size=456B,count=1,average=456Btest_pytracemalloc_use_py3.4.py:36:size=448B,count=1,average=448BD:\\Python3.4\\lib\\tracemalloc.py:474:size=64B,count=1,average=64B12345678910(62280,62920) test_pytracemalloc_use_py3.4.py:10:size=16.8KiB,count=144,average=120Btest_pytracemalloc_use_py3.4.py:17:size=16.7KiB,count=142,average=120Btest_pytracemalloc_use_py3.4.py:19:size=9952B,count=100,average=100Btest_pytracemalloc_use_py3.4.py:26:size=9792B,count=102,average=96Btest_pytracemalloc_use_py3.4.py:27:size=848B,count=1,average=848Btest_pytracemalloc_use_py3.4.py:34:size=456B,count=1,average=456Btest_pytracemalloc_use_py3.4.py:36:size=448B,count=1,average=448BD:\\Python3.4\\lib\\tracemalloc.py:474:size=64B,count=1,average=64B如果将第36行的“lineno“改成“filename”，那么结果如下Python(62136,62764)test_pytracemalloc_use_py3.4.py:0:size=54.5KiB,count=491,average=114BD:\\Python3.4\\lib\\tracemalloc.py:0:size=64B,count=1,average=64B1234(62136,62764) test_pytracemalloc_use_py3.4.py:0:size=54.5KiB,count=491,average=114BD:\\Python3.4\\lib\\tracemalloc.py:0:size=64B,count=1,average=64B有了Profile结果之后，可以看出来在哪个文件中有大量的内存分配。与性能优化相同，造成瓶颈的有两种情况：单个对象占用了大量的内存；同时大量存在的小对象。对于前者，优化的手段并不多，惰性初始化属性可能有一些帮助；而对于后者，当同样类型的对象大量存在时，可以使用slots进行优化。Slots默认情况下，自定义的对象都使用dict来存储属性（通过obj.__dict__查看），而python中的dict大小一般比实际存储的元素个数要大（以此降低hash冲突概率），因此会浪费一定的空间。在新式类中使用__slots__，就是告诉Python虚拟机，这种类型的对象只会用到这些属性，因此虚拟机预留足够的空间就行了，如果声明了__slots__，那么对象就不会再有__dict__属性。使用slots到底能带来多少内存优化呢，首先看看这篇文章，对于一个只有三个属性的Image类，使用__slots__之后内存从25.5G下降到16.2G，节省了9G的空间！到底能省多少，取决于类自身有多少属性、属性的类型，以及同时存在多少个类的实例。下面通过一段简单代码测试一下：#-*-coding:utf-8-*-importsysimporttracemallocNUM_OF_ATTR=3#3#10#30#90NUM_OF_INSTANCE=10#10#100classSlots(object):__slots__=['attr%s'%iforiinrange(NUM_OF_ATTR)]def__init__(self):value_lst=(1.0,True,[],{},())foriinrange(NUM_OF_ATTR):setattr(self,'attr%s'%i,value_lst[i%len(value_lst)])classNoSlots(object):def__init__(self):value_lst=(1.0,True,[],{},())foriinrange(NUM_OF_ATTR):setattr(self,'attr%s'%i,value_lst[i%len(value_lst)])if__name__=='__main__':clz=Slotsiflen(sys.argv)>1elseNoSlotstracemalloc.start()objs=[clz()foriinrange(NUM_OF_INSTANCE)]print(tracemalloc.get_traced_memory()[0])1234567891011121314151617181920212223242526#-*-coding:utf-8-*-importsysimporttracemalloc NUM_OF_ATTR=  3#3#10#30#90NUM_OF_INSTANCE=10#10#100classSlots(object):    __slots__=['attr%s'%iforiinrange(NUM_OF_ATTR)]    def__init__(self):        value_lst=(1.0,True,[],{},())        foriinrange(NUM_OF_ATTR):            setattr(self,'attr%s'%i,value_lst[i%len(value_lst)])  classNoSlots(object):    def__init__(self):        value_lst=(1.0,True,[],{},())        foriinrange(NUM_OF_ATTR):            setattr(self,'attr%s'%i,value_lst[i%len(value_lst)]) if__name__=='__main__':    clz=Slotsiflen(sys.argv)>1elseNoSlots    tracemalloc.start()    objs=[clz()foriinrange(NUM_OF_INSTANCE)]    print(tracemalloc.get_traced_memory()[0])上面的代码，主要是在每个实例的属性数目、并发存在的实例数目两个维度进行测试，并没有测试不同的属性类型。结果如下表：百分比为内存优化百分比，计算公式为(b–a)/b，其中b为没有使用__slots__时分配的内存，a为使用了__slots__时分配的内存。注意事项关于__slots__，Python文档有非常详尽的介绍，这里只强调几点注意事项第一：基类和子类都必须__slots__，即使基类或者子类没有属性Python>>>classBase(object):...pass...>>>classDerived(Base):...__slots__=('a',)...>>>d.__slots__('a',)>>>getattr(d,'__dict__','NoDict'){}1234567891011>>>classBase(object): ...    pass...>>>classDerived(Base):...    __slots__=('a',)...>>>d.__slots__('a',)>>>getattr(d,'__dict__','NoDict'){}从上面的示例可以看到，子类的对象还是有__dict__属性，原因就在于基类没有声明__slots__。因此，可以通过看子类的实例有没有__dict__属性来判断slots的使用是否正确第二：子类会继承基类的__slots__更准确的说，如果访问属性的时候没有在子类的__slots__找到，会继续在基类的__slots__查找，因为Python使用descriptor在类这个层级实现__slots__的，具体可以参见《python属性查找深入理解》一文Python>>>classBase(object):...__slots__=('a',)...>>>classDerived(Base):...__slots__=('b',)...>>>d=Derived()>>>d.__slots__('b',)>>>getattr(d,'__dict__','NoDict')'NoDict'>>>d.a=1>>>d.c=0Traceback(mostrecentcalllast):File\"<stdin>\",line1,in<module>AttributeError:'Derived'objecthasnoattribute'c'1234567891011121314151617>>>classBase(object): ...    __slots__=('a',)...>>>classDerived(Base):...    __slots__=('b',)...>>>d=Derived()>>>d.__slots__('b',)>>>getattr(d,'__dict__','NoDict')'NoDict'>>>d.a=1>>>d.c=0Traceback(mostrecentcalllast):  File\"<stdin>\",line1,in<module>AttributeError:'Derived'objecthasnoattribute'c'objgraph在大型工程中，怎么排查有哪些大量存在的对象呢，毕竟同一个类型存在的对象越多，优化越有效果。除了直接看代码，最好使的就是使用objgraph.py的show_most_common_types(N)函数，该函数返回Pythongc管理的所有对象中，数目前N多的对象，在排除掉pythonbuiltin对象之后，剩下的就是可优化的对象。比如在最上面的代码中：在最后加上这么两句：Pythonimportobjgraphobjgraph.show_most_common_types(25)123importobjgraph objgraph.show_most_common_types(25)输出如下：再论Pythondict前面介绍slots的时候，就提到Python自定义的对象中通过dict来管理属性。这种机制极大的提高了Python的灵活性—可以随时给对象增加属性，但是其实现机制也带来了内存上的浪费。不管是python源码，还是Python程序，都大量使用了dict，因此这部分内存浪费不容小视。python中的dict使用的是散列表（类似C++中的std::unordered_map），当计算出的hash值冲突的时候，采用开放地址法解决冲突（另一种常见的冲突解决算法是链表法）。为了降低冲突概率，当装填因子（实际存储的元素与散列表长度的比值）超过2/3的时候就会对散列表进行扩容，因此散列表中一定会存在一些未使用的槽。下面简单看看PyDictObject的数据结构（python2.7.3dictobject.h）#definePyDict_MINSIZE8typedefstruct{/*Cachedhashcodeofme_key.NotethathashcodesareClongs.*WehavetousePy_ssize_tinsteadbecausedict_popitem()abuses*me_hashtoholdasearchfinger.*/Py_ssize_tme_hash;PyObject*me_key;PyObject*me_value;}PyDictEntry;typedefstruct_dictobjectPyDictObject;struct_dictobject{PyObject_HEADPy_ssize_tma_fill;/*#Active+#Dummy*/Py_ssize_tma_used;/*#Active*//*Thetablecontainsma_mask+1slots,andthat'sapowerof2.*Westorethemaskinsteadofthesizebecausethemaskismore*frequentlyneeded.*/Py_ssize_tma_mask;/*ma_tablepointstoma_smalltableforsmalltables,elseto*additionalmalloc'edmemory.ma_tableisneverNULL!Thisrule*savesrepeatedruntimenull-testsintheworkhorsegetitemand*setitemcalls.*/PyDictEntry*ma_table;PyDictEntry*(*ma_lookup)(PyDictObject*mp,PyObject*key,longhash);PyDictEntryma_smalltable[PyDict_MINSIZE];};12345678910111213141516171819202122232425262728293031323334#definePyDict_MINSIZE8 typedefstruct{    /*Cachedhashcodeofme_key.  NotethathashcodesareClongs.    *WehavetousePy_ssize_tinsteadbecausedict_popitem()abuses    *me_hashtoholdasearchfinger.    */    Py_ssize_tme_hash;    PyObject*me_key;    PyObject*me_value;}PyDictEntry;  typedefstruct_dictobjectPyDictObject;struct_dictobject{    PyObject_HEAD    Py_ssize_tma_fill;  /*#Active+#Dummy*/    Py_ssize_tma_used;  /*#Active*/     /*Thetablecontainsma_mask+1slots,andthat'sapowerof2.    *Westorethemaskinsteadofthesizebecausethemaskismore    *frequentlyneeded.    */    Py_ssize_tma_mask;     /*ma_tablepointstoma_smalltableforsmalltables,elseto    *additionalmalloc'edmemory.  ma_tableisneverNULL!  Thisrule    *savesrepeatedruntimenull-testsintheworkhorsegetitemand    *setitemcalls.    */    PyDictEntry*ma_table;    PyDictEntry*(*ma_lookup)(PyDictObject*mp,PyObject*key,longhash);    PyDictEntryma_smalltable[PyDict_MINSIZE];};从定义可以看出，除了固定的部分（几个Py_ssize_t），PyDictObject中主要是PyDictEntry对象，PyDictEntrty包含一个Py_ssize_t（int）和两个指针。上面源码中的注释（第26行）指出，当dict的元素比较少时，ma_table指向ma_smalltable，当元素增多时，ma_table会指向新申请的空间。ma_smalltable的作用在于Python（不管是源码还是代码）都大量使用dict，一般来说，存储的元素也不会太多，因此Python就先开辟好PyDict_MINSIZE(默认为8)个空间。为什么说PyDictObject存在浪费呢，PyDictEntry在32位下也有12个字节，那么即使在ma_smalltable（ma_table）中大量的位置没有被使用时，也要占用这么多字节。用这篇文章中的例子：假设有这么一个dict：Python　d={'timmy':'red','barry':'green','guido':'blue'}1　d={'timmy':'red','barry':'green','guido':'blue'}在Python源码中的视图就是这样的：Python　#　下面的entries就是ma_smalltableentries=[['--','--','--'],[-8522787127447073495,'barry','green'],['--','--','--'],['--','--','--'],['--','--','--'],[-9092791511155847987,'timmy','red'],['--','--','--'],[-6480567542315338377,'guido','blue']]12345678910　#　下面的entries就是ma_smalltable entries=[['--','--','--'],          [-8522787127447073495,'barry','green'],          ['--','--','--'],          ['--','--','--'],          ['--','--','--'],          [-9092791511155847987,'timmy','red'],          ['--','--','--'],          [-6480567542315338377,'guido','blue']]然而，完全可以这么存储：Pythonindices=[None,1,None,None,None,0,None,2]entries=[[-9092791511155847987,'timmy','red'],[-8522787127447073495,'barry','green'],[-6480567542315338377,'guido','blue']]12345indices=[None,1,None,None,None,0,None,2] entries=  [[-9092791511155847987,'timmy','red'],                [-8522787127447073495,'barry','green'],                [-6480567542315338377,'guido','blue']]indices的作用类似ma_smalltable，但只存储一个数组的索引值，数组只存储实际存在的元素（PyDictEntry），当dict中的元素越稀疏，相比上一种存储方式使用的内存越少。而且，这种实现，dict就是有序的（按插入时间排序）这就是python3.6中新的dict实现，Compactdict!Stackoverflow上也有相关讨论。总结本文中介绍了Python内存优化的Profile工具，最有效的优化方法：使用slots，也介绍了在python3.6中新的dict实现。当然，还有一些良好的编码习惯。比如尽量使用immutable而不是mutable对象：使用tuple而不是list，使用frozenset而不是set；另外，就是尽量使用迭代器，比如python2.7中，使用xrange而不是range，dict的iterxx版本。referencespytracemallocpep454：AddanewtracemallocmoduletotracePythonmemoryallocations　save-ram-with-python-slots/Python源码分析-PyDictObjectMorecompactdictionarieswithfasteriteration1赞4收藏1评论"], "art_create_time": ["2017/11/19"], "art_title": ["Python 内存优化"], "art_url": ["http://python.jobbole.com/88896/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2017/11/f3b43c55d59635277f981c85325df4a5.png"]},
{"art_content": ["本文由伯乐在线-小米云豆粥翻译。未经许可，禁止转载！英文出处：BrunoStecanella。欢迎加入翻译组。目前机器学习红遍全球。男女老少都在学机器学习模型，分类器，神经网络和吴恩达。你也想成为一份子，但你该如何开始？在这篇文章中我们会讲Python的重要特征和它适用于机器学习的原因，介绍一些重要的机器学习包，以及其他你可以获取更详细资源的地方。为什么用Python做机器学习Python很适合用于机器学习。首先，它很简单。如果你完全不熟悉Python但是有一些其他的编程经验（C或者其他编程语言），要上手是很快的。其次，Python的社区很强大。这使得Python的文档不仅条理性好，而且容易读。你还可以在StackOverFlow上找到关于很多问题详细解答（学习基石）。再次，一个强大的社区带来的副产品就是大量有用程序库（Python内部自带的和第三方软件），基本上可以解决你所有的问题（包括机器学习）。但我听说Python很慢Python是很慢。它不是执行最快的语言，拥有那么多好用的抽象是要付出代价的。但这是个可以解决的问题：程序库可以把计算量繁重的部分外包给其他更高效（但更难使用）的语言，例如C和C++。比如NumPy这个提供数值运算的程序库，就是用C写的，运行速度超快。在实际运用中，几乎所有程序库都会使用NumPy去完成计算繁重的部分。如果你看到Numpy，你应该想到它很快。所以你是可以让程序的运行速度跟它的低层语言实现的运行速度相比拟的。你没有必要担心程序的运行速度。 值得知道的Python程序库Scikit-learn你刚开始学机器学习吗？如果你需要一个涵盖了特征工程，模型训练和模型测试所有功能的程序库，scikit-learn是你的最佳选择！这个优秀的免费软件提供了机器学习和数据挖掘所需要的所有工具。它是目前Python机器学习的标准库。要使用任何成熟的机器学习算法都推荐使用这个库。这个程序库支持分类和回归，实现了基本所有的经典算法（支持向量机，随机森林，朴素贝叶斯等等）。程序库的设计让迁移算法十分容易，使用不同的算法做实验非常轻松。这些经典算法可用性很强，能用于大量不同的情况。但这并不是Scikit-learn的全部功能，它同样可以用来做降维，聚类等等任何你所能想到的。由于它构建在Numpy和Scipy之上（所有的数值计算都是由C语言来完成的），它的运行速度也超快。这些例子可以告诉你这个库的功能，如果你想学习如何使用它，可以阅读教程。NLTKNLTK不算是一个机器学习的程序库，但它是做自然语言处理（NLP）必须的一个库。除了用于文字处理的功能，例如聚类，分词，词干提取，标记，解析等，它还包含了大量的数据集和其他关于词法的资源（可用于模型训练）。把所有这些打包在一起的好处就不用再多说了。如果你对NLP感兴趣，可以看看这些教程!TheanoTheano被广泛应用于工业界和学术界，它是所有深度学习架构的鼻祖。Theano是用Python，结合Numpy实现的。你可以用它来构建用多维数组实现神经网络。Theano会处理所有数学计算，你不需要知道底层的数学公式实现。早在支持使用GPU进行计算不像今天这样普及的时候，Theano就已经提供了对GPU计算的支持。这个程序库目前已经非常成熟，能够支持很多不同类型的操作。这使得Theano可以在和其他库比较的时候胜出。目前关于Theano最大的问题是API不是很好用，对于新手来说上手困难。不过市面上已经有了解决这个问题的封装包，比如Keras,Blocks 和 Lasagne，都可以简化Theano的使用。TensorFlow谷歌大脑团队为了内部使用创造了TensorFlow，2015年将其开源化。设计初衷是取代他们已有的封闭机器学习框架DistBelief，据说该构架太过于依赖Google的整体构架，也不够灵活，在分享代码的时候非常不方便。于是就有了TensorFlow。谷歌从以前的错误中吸取了教训。许多人认为TensorFlow是Theano的改进版，它提供了更灵活和好用的API。可以用于科研和工业界，同时支持使用大量的GPU进行模型训练。TensorFlow支持的操作没有Theano多，但是它的计算可视化比Theano好。TensorFlow目前非常流行。如果今天这篇文章里面提到的名字你只听说了一个，那很有可能是这个。每天都有新的提到TensorFlow的博文或学术文章发表。这个流行度提供了大量的用户和教程，新人很容易上手。KerasKeras是一个提供更高层神经网络API的库，它可以基于Theano或者TensorFlow。它拥有这两个库强大的功能却又同时大大地简化了使用难度。它将用户的体验放在首要地位，提供简单的API和很有用的错误信息。同时Keras的设计基于模块，这就使得你能自由组合不同的模型（神经层，成本函数等等），而且模型的可扩展性很好，因为你只需要简单的将新模块跟已有的连起来即可。有人觉得Keras太好用了，简直就是在作弊。如果你开始用深度学习，可以看看例子 和 文档，对于你可以用它做什么有个数。如果你要学习使用它，可以从这个教程开始。两个类似的库有Lasagne 和 Blocks,但它们只支持Theano。如果你试过了Keras但是你不喜欢它你可以试试这些其他的库，也许它们更适合你。PyTorch还有一个有名的深度学习架构是Torch，它是用Lua实现的。Facebook用Python实现了Torch，叫做PyTorch，并将它开源了。用这个库你可以使用Torch使用的低层的库，但是你可以使用Python而不是Lua。PyTorch对查错的支持很好，这是因为Theano和TensorFlow使用符号计算而PyTorch则不是。使用符号计算就表明在一行代码被解释的时候，一个操作（x+y）并不会被执行，在那之前，它必须先被编译（解释成CUDA或者C语言）。这就让用Theano和TensorFlow的时候很难查错，因为很难把报错跟当前的代码联系起来。这样做有它的好处，不过查错简单不在其中。如果你想开始学PyTorch，官方文档适合初学者也会包含有难度的内容。机器学习的第一步？你讲了这么多机器学习的包，我应该用哪一个？我怎样比较它们？我从哪里开始？你可以试用我们面向初学者的平台ApeAdvice™，就不用烦细节的问题了。如果你完全没有接触过机器学习，从scikit-learn开始。你可以了解标记，训练和测试是怎样工作的，以及一个模型是如何被建立的。如果你想试试深度学习，从 Keras开始，毕竟这是大家公认的最简单的框架。你可以先试试，找找感觉。当你有点经验之后，你可以开始考虑你最需要的是什么：速度，不同的API，或者别的什么，之后你就能更好地决定了。目前有海量的文章比较Theano，Torch和TensorFlow。没有人能说哪个最好。你要记住的是所有包都支持很多东西，而且也在不断改进，想相互比较它们也越来越难。六个月前的标准有可能已经过时了，一年前的评价说框架X没有Y功能也不一定还有效。最后，如果你想用NLP，可以试试MonkeyLearn!我们的这个平台所提供的用户界面让建造模型，训练模型和改进NLP模型都非常容易试下。你可以用事先训练好的模型处理常见问题（意见挖掘，话题探测或者提取关键字），也可以为你特有的问题设计一个新的算法。你不需要担心底层实现或者发布你的模型，我们可扩展的云系统会帮你完成这些。你可以免费注册，马上开始试用我们超棒的API。想知道更多？关于机器学习的网络资源很多！下面列举一些：如果你想看看代码，这里有 一个机器学习详细范例的JupyterNotebook如果你想知道更多机器学习的概念，可以看我们的机器学习入门指南如果你要认真学机器学习，你可以从AndrewNg’sStanfordCS229onCoursera开始；如果你要找关于深度学习的网络课程，可以看看 fast.ai.结语这篇关于用Python库做机器学习的简介就到此为止。我想强调的是不要被细节吓住了，放手尝试。让你的好奇心指导你前进，不要害怕进行不同的实验。如果你知道我没有提到的Python库，可以发在评论里面！打赏支持我翻译更多好文章，谢谢！打赏译者打赏支持我翻译更多好文章，谢谢！任选一种支付方式2赞12收藏6评论关于作者：小米云豆粥数据科学进修中，Python小码农。不用QQ，有事站内信联系。个人主页·我的文章·31·"], "art_create_time": ["2017/12/26"], "art_title": ["使用 Python 开始机器学习"], "art_url": ["http://python.jobbole.com/88705/"], "art_img": ["http://wx1.sinaimg.cn/mw690/63918611gy1fmukayefezj20sw0e447u.jpg"]},
{"art_content": ["本文作者：伯乐在线-abel_cao。未经作者许可，禁止转载！欢迎加入伯乐在线专栏作者。数据的重要性毋庸置疑，但是如何让数据产生价值呢？对一个全栈老码农而言，经常在开发或者研发管理的时候遇到各种预测、决策、推断、分类、检测、排序等诸多问题。面对“你的代码还有bug么？”这样的挑战，一种理智的回答是，我们已经执行了若干测试用例，代码中存在bug的可能性是百分之零点几。也就是说，我们对当前程序中没有bug的信心是百分之九十九点几。这实际上就是一直贝叶斯思维，或者说使用了贝叶斯方法。不论我们看到，还是没有看到，它都在那里，熠熠生辉。如果预测当前软件有没有bug呢？还是要从贝叶斯定理看起。贝叶斯定理的浅解对老码农来说，贝叶斯定理的概率表达相对清晰，理解起来会相对容易。回忆一下我们学过的概率论，联合概率是满足交换律的，即：P(AandB)=P(BandA)1P(AandB)=P(BandA)对联合概率以条件概率展开：P(AandB)=P(A)P(B|A)P(BandA)=P(B)P(A|B)12P(AandB)=P(A)P(B|A)P(BandA)=P(B)P(A|B)从而得到：P(A)P(B|A)=P(B)P(A|B)1P(A)P(B|A)=P(B)P(A|B)简单的变换一下，得到：大功告成，这就是神奇的贝叶斯定理。其中：P（B）为先验概率，即在得到新数据前某一假设的概率；P（B|A）为后验概率，即在观察到新数据后计算该假设的概率；P（A|B）为似然度，即在该假设下得到这一数据的概率；P（A）为标准化常量，即在任何假设下得到这一数据的概率。还可以加点料，在计算P（A）的时候，可以用加法定理表示：P（A）=P(AandB)+P(AandB_)=P（A|B）P（B）+P(A|B_)P(B_)1P（A）=P(AandB)+P(AandB_)=P（A|B）P（B）+P(A|B_)P(B_)从而有：其中B_是与B相反的事件。就测试与bug之间的估算而言，《贝叶斯推断的思想》一文给出了贝叶斯推断的结果，其中就使用了这样的方法。贝叶斯方法贝叶斯方法是一个非常通用的推理框架，用客观的新信息更新我们最初关于某个事物的信念后，就会得到一个新的改进了的信念。通过引入先验的不确定性，允许了初始推断的错误，获得了更新的证据后，也没有放弃初始的推断，而是调整为更符合目前的证据。但是，P（A|B）和P（B|A）之类的经常让人混淆，@待字闺中的陈老师给出了理解的一个关键点，区分出规律和现象，就是将A看成“规律”，B看成“现象”，那么贝叶斯公式看成： 陈老师在《这的理解贝叶斯公式吗》和《又一个生活中的贝叶斯应用》给出了几个通俗易懂的例子，这里不再赘述。回归到码农生活，我们在改善系统功能的时候，通常的一个手段是AB测试。AB测试是用来检测两种不同处理方式的差异化程度的一种统计设计模式，例如两个网站谁会带来更高的转化率，这里的转化可以是用户的购买、注册、或其他的行为。AB测试的关键点在于组别之间只能容许一个不同点。实验后的分析一般都是用假设检验完成的，例如均值差异检验或者比例差异检验，往往涉及Z分数或令人困惑的p值，而用贝叶斯方法则会自然的多。对A，B两个网站的转化概率进行建模。转化率在0～1之间，可采用Beta分布。如果先验是Beta（a1，b1），且观测到N次访问里有X次转化，那么此时的后验分布是Beta（a1+X,b1+N-X).假设先验是Beta（1，1），等价于【0，1】上的均匀分布，则示例代码如下：fromspicy.statsimportbetaa1_prior=1b1_prior=1visitors_A=12345//网站A的访问人数visitors_B=1616//网站B的访问人数conversions_from_A=1200//网站A的转化人数conversions_from_B=150//网站B的转化人数posterior_A=beta(a1_prior+conversions_from_A,b1_prior+visitors_A-conversions_from_A)posterior_B=Beta(a1_prior+converiosns_from_B,b1_prior+visitors_B-conversions_from_B)//对后验概率进行采样，用rvs方法生成样本samples=20000samples_posterior_A=posterior_A.rvs(samples)samples_posterior_B=posterior_B.rvs(samples)//对后验概率进行比较print(samples_posterior_A>samples_posterior_B).mean()12345678910111213141516fromspicy.statsimportbetaa1_prior=1b1_prior=1visitors_A=12345//网站A的访问人数visitors_B=1616  //网站B的访问人数conversions_from_A=1200//网站A的转化人数conversions_from_B=150  //网站B的转化人数 posterior_A=beta(a1_prior+conversions_from_A,b1_prior+visitors_A-conversions_from_A)posterior_B=Beta(a1_prior+converiosns_from_B,b1_prior+visitors_B-conversions_from_B)//对后验概率进行采样，用rvs方法生成样本samples=20000samples_posterior_A=posterior_A.rvs(samples)samples_posterior_B=posterior_B.rvs(samples)//对后验概率进行比较print(samples_posterior_A>samples_posterior_B).mean()使用贝叶斯方法，是从思考数据是如何产生的开始。1）什么随机变量能过描述这些统计数据2）确实概率分布的所需参数3）参数对应早期行为，或后期行为，定义各种变化点4）定义参数的概率分布5）参数概率分布的变量选择，直到一个可以假设的均匀分布对先验及后验概率的选择，针对应用场景而定。就先验分布而言，除了常见的分布外，还有：*Gamma分布，指数随机变量的推广*威沙特分布，是所有半正定矩阵的分布，是一个协方差矩阵的适当的先验。*Beta分布，随机变量定义在0到1之间，使其成为概率和比例的热门选择。*幂律分布，满足公司规模和公司数量之间的关系在AB测试中使用了Beta分布，应用了一个Beta先验分布连同二项式生成的观测数据形成一个Beta后验分布这一原理。当面对多种对象之间的因果关系的时候，贝叶斯方法演变成为了贝叶斯网络。贝叶斯网络贝叶斯网络是为了解决不定性和不完整性问题而提出的，在多个领域中获得了广泛应用。贝叶斯网络是基于概率推理的图形化网络，而贝叶斯公式则是这个概率网络的基础。贝叶斯网络中的每个点代表一个随机变量，都是具有实际含义、需要人为设计的，点和点之间的边代表不确定的因果关系，例如节点E直接影响到节点H，即E→H，则用从E指向H的箭头建立结点E到结点H的有向弧(E,H)，权值(即连接强度)用条件概率P(H|E)来表示。实际上，如果事物之间的关系能够用一条链串起来，形成了贝叶斯网络的一个特例——马尔可夫链，换个角度看，贝叶斯网络是马尔可夫链的非线性扩展。贝叶斯网络中当某点的一个证据出现后，整个网络中事件的概率都会变化。简单地，由于多个变量间存在着可能的依赖性，贝叶斯网络说明了其中的联合条件概率分布，允许在变量的子集间定义条件独立性。使用贝叶斯网络的过程与使用贝叶斯方法的过程是类似的：通过多个离散变量建立网络，是一个有向无环图参数的设置或学习，即对DAG进行遍历，计算各节点的概率表网络推理，对因果关系得到置信概率推理结果例如，社交网络中不真实账户的检测问题。首先确定网络中的随机变量：*账户的真实性A*头像的真实性H*发帖即日志的密度L*好友的密度F使用观测值示例化H，L，F，把随机值赋给A，得到P（A|H,L,F)=P(H|A)P(L|A)P(F|A,H)然后就可以在社交网络中尝试使用该推理结果了。在《算法杂货铺——分类算法之贝叶斯网络》一文中对这一例子给出了相对详细的说明。可以说，贝叶斯方法席卷了整个概率论，并将应用延伸到各个问题领域，所有需要作出概率预测的地方都可以见到贝叶斯方法的影子，特别地，贝叶斯方法对机器学习能够有什么帮助呢？贝叶斯与机器学习机器学习在业界炙手可热，但我们在机器学习里同样会遇到预测、决策、分类、检测等问题，贝叶斯方法同样大有用武之地。机器学习中有大量的模型，如线性模型、非线性模型，可以采用贝叶斯方法来做模型的预测。也就是说，某一场景可能采用的模型是无限多的，可以用概率分布去描述它。对于假设的先验，对新来的样本做预测如计算它的似然，然后用前面推出来的后验分布做积分，这个给定模型下样本的似然，就是所有可能模型的分布。机器学习中模型的选择和比较也是一个常见的问题。例如，在分类问题时，我们使用线性模型还是深度学习的非线性模型呢？贝叶斯方法是这样考虑的：用A表示一个模型类别，可能是线性模型，B表示另一个模型类别，可能是非线性模型。在同样的数据集X下，计算在A，B情况下观察到训练集的似然Ma，Mb，然后比较Ma和Mb，这是贝叶斯方法做模型选择的一个基本规则。实际上，贝叶斯定理是信息处理的一种准则，输入是一个先验分布和一个似然函数，输出是一个后验分布。对机器学习中的模型本身，也可以通过贝叶斯方法尝试改进，例如贝叶斯SVM,高斯过程的贝叶斯等等。另外，贝叶斯方法对深度学习而言，至少在调参的这一环节还是很有用的。在神经网络中，每一层参数如卷积核的大小和数量等，都不会在深度学习中被模型自动优化的，需要手工指定，这或许就是贝叶斯优化。感慨一下，码农不识贝叶斯，虽知数据也枉然呀！其他参考资料：《贝叶斯方法－概率编程与贝叶斯推断》《贝叶斯思维：统计建模的python学习法》《数学之美番外篇：平凡而又神奇的贝叶斯方法》《BayesianMethodforMachineLearning》www.cs.toronto.edu/~radford/ftp/bayes-tut.pdf打赏支持我写出更多好文章，谢谢！打赏作者打赏支持我写出更多好文章，谢谢！1赞14收藏2评论关于作者：abel_cao半吊子全栈工匠一枚，20多年老码农，无所成，有初心，自娱自乐的微信公众号wireless_com，记录日常点滴......个人主页·我的文章·25·"], "art_create_time": ["2017/12/27"], "art_title": ["码农不识贝叶斯，虽知数据也枉然"], "art_url": ["http://python.jobbole.com/88318/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2015/02/301f8986e2ad36a068277f2795edeeb9.jpg"]},
{"art_content": ["原文出处：Linuxtoy   本文将向各位推荐9本免费的Python语言编程书籍，希望对你学习Python编程有所帮助。1.AByteofPythonbySwaroopCH十分简明的Python教程。“无论您刚接触电脑还是一个有经验的程序员，本书都将有助您学习使用Python语言。”包含Python2.x和Python3.0两个版本。2.DiveIntoPythonbyMarkPilgrim本书“是为有经验的程序员编写的一本Python书”，具有英文版、中文版等多个版本。在去年9月，该书还针对Python3进行了更新，新书名为《DiveIntoPython3》。3.BuildingSkillsinPythonbyStevenF.Lott这本500多页的书包含42章，通过一系列的练习来帮助你学习Python编程。本书有HTML和PDF格式。4.PythonStandardLibrarybyFredrikLundh该书对Python标准库进行了介绍，并提供范例脚本参考。你可以在这里找到。5.HowtoThinkLikeaComputerScientistbyJeffreyElkner,AllenB.Downey,andChrisMeyers此书通过Python语言来教你如何进行编程。在线阅读。6.InventYourOwnComputerGameswithPythonbyAlbertSweigart通过其它的书学习编程，你可能会感觉枯燥，但这一本不会。该书通过创建游戏的方式来教你学习Python编程，非常有趣。可在线阅读或下载PDF。7.TextProcessinginPythonbyDavidMertz如果你想用Python进行文本处理的话，那么这本书将为你提供全面而有用的参考。TXT格式下载。8.TheDefinitiveGuidetoDjango:WebDevelopmentDoneRightbyAdrianHolovaty,JacobKaplan-Moss关于Django这个Web框架的权威指南。在线版。9.TheDefinitiveGuidetoPylonsJamesGardner另一个Web框架Pylons的权威指南。在线阅读。1赞1收藏4评论"], "art_create_time": ["2013/04/02"], "art_title": ["9本免费的Python编程书"], "art_url": ["http://python.jobbole.com/765/"], "art_img": ["/wp-content/uploads/vb/765-thumb_dip.jpg"]},
{"art_content": ["来源：张颖@developerworks代码优化能够让程序运行更快，它是在不改变程序运行结果的情况下使得程序的运行效率更高，根据80/20原则，实现程序的重构、优化、扩展以及文档相关的事情通常需要消耗80%的工作量。优化通常包含两方面的内容：减小代码的体积，提高代码的运行效率。改进算法，选择合适的数据结构一个良好的算法能够对性能起到关键作用，因此性能改进的首要点是对算法的改进。在算法的时间复杂度排序上依次是：O(1)->O(lgn)->O(nlgn)->O(n^2)->O(n^3)->O(n^k)->O(k^n)->O(n!)因此如果能够在时间复杂度上对算法进行一定的改进，对性能的提高不言而喻。但对具体算法的改进不属于本文讨论的范围，读者可以自行参考这方面资料。下面的内容将集中讨论数据结构的选择。●字典(dictionary)与列表(list)Python字典中使用了hashtable，因此查找操作的复杂度为O(1)，而list实际是个数组，在list中，查找需要遍历整个list，其复杂度为O(n)，因此对成员的查找访问等操作字典要比list更快。清单1.代码dict.pyPythonfromtimeimporttimet=time()list=['a','b','is','python','jason','hello','hill','with','phone','test','dfdf','apple','pddf','ind','basic','none','baecr','var','bana','dd','wrd']#list=dict.fromkeys(list,True)printlistfilter=[]foriinrange(1000000):forfindin['is','hat','new','list','old','.']:iffindnotinlist:filter.append(find)print\"totalruntime:\"printtime()-t12345678910111213fromtimeimporttimet=time()list=['a','b','is','python','jason','hello','hill','with','phone','test','dfdf','apple','pddf','ind','basic','none','baecr','var','bana','dd','wrd']#list=dict.fromkeys(list,True)printlistfilter=[]foriinrange(1000000):forfindin['is','hat','new','list','old','.']:iffindnotinlist:filter.append(find)print\"totalruntime:\"printtime()-t上述代码运行大概需要16.09seconds。如果去掉行#list=dict.fromkeys(list,True)的注释，将list转换为字典之后再运行，时间大约为8.375seconds，效率大概提高了一半。因此在需要多数据成员进行频繁的查找或者访问的时候，使用dict而不是list是一个较好的选择。●集合(set)与列表(list)set的union，intersection，difference操作要比list的迭代要快。因此如果涉及到求list交集，并集或者差的问题可以转换为set来操作。清单2.求list的交集：Pythonfromtimeimporttimet=time()lista=[1,2,3,4,5,6,7,8,9,13,34,53,42,44]listb=[2,4,6,9,23]intersection=[]foriinrange(1000000):forainlista:forbinlistb:ifa==b:intersection.append(a)print\"totalruntime:\"printtime()-t12345678910111213fromtimeimporttimet=time()lista=[1,2,3,4,5,6,7,8,9,13,34,53,42,44]listb=[2,4,6,9,23]intersection=[]foriinrange(1000000):forainlista:forbinlistb:ifa==b:intersection.append(a) print\"totalruntime:\"printtime()-t上述程序的运行时间大概为：totalruntime:38.4070000648清单3.使用set求交集Pythonfromtimeimporttimet=time()lista=[1,2,3,4,5,6,7,8,9,13,34,53,42,44]listb=[2,4,6,9,23]intersection=[]foriinrange(1000000):list(set(lista)&set(listb))print\"totalruntime:\"printtime()-t123456789fromtimeimporttimet=time()lista=[1,2,3,4,5,6,7,8,9,13,34,53,42,44]listb=[2,4,6,9,23]intersection=[]foriinrange(1000000):list(set(lista)&set(listb))print\"totalruntime:\"printtime()-t改为set后程序的运行时间缩减为8.75，提高了4倍多，运行时间大大缩短。读者可以自行使用表1其他的操作进行测试。表1.set常见用法语法                      操作             说明set(list1)|set(list2)      union           包含list1和list2所有数据的新集合set(list1)&set(list2)      intersection     包含list1和list2中共同元素的新集合set(list1)–set(list2)      difference       在list1中出现但不在list2中出现的元素的集合对循环的优化对循环的优化所遵循的原则是尽量减少循环过程中的计算量，有多重循环的尽量将内层的计算提到上一层。下面通过实例来对比循环优化后所带来的性能的提高。程序清单4中，如果不进行循环优化，其大概的运行时间约为132.375。清单4.为进行循环优化前Pythonfromtimeimporttimet=time()lista=[1,2,3,4,5,6,7,8,9,10]listb=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.01]foriinrange(1000000):forainrange(len(lista)):forbinrange(len(listb)):x=lista[a]+listb[b]print\"totalruntime:\"printtime()-t12345678910fromtimeimporttimet=time()lista=[1,2,3,4,5,6,7,8,9,10]listb=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.01]foriinrange(1000000):forainrange(len(lista)):forbinrange(len(listb)):x=lista[a]+listb[b]print\"totalruntime:\"printtime()-t现在进行如下优化，将长度计算提到循环外，range用xrange代替，同时将第三层的计算lista[a]提到循环的第二层。清单5.循环优化后Pythonfromtimeimporttimet=time()lista=[1,2,3,4,5,6,7,8,9,10]listb=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.01]len1=len(lista)len2=len(listb)foriinxrange(1000000):forainxrange(len1):temp=lista[a]forbinxrange(len2):x=temp+listb[b]print\"totalruntime:\"printtime()-t12345678910111213fromtimeimporttimet=time()lista=[1,2,3,4,5,6,7,8,9,10]listb=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.01]len1=len(lista)len2=len(listb)foriinxrange(1000000):forainxrange(len1):temp=lista[a]forbinxrange(len2):x=temp+listb[b]print\"totalruntime:\"printtime()-t上述优化后的程序其运行时间缩短为102.171999931。在清单4中lista[a]被计算的次数为1000000*10*10，而在优化后的代码中被计算的次数为1000000*10，计算次数大幅度缩短，因此性能有所提升。充分利用Lazyif-evaluation的特性python中条件表达式是lazyevaluation的，也就是说如果存在条件表达式ifxandy，在x为false的情况下y表达式的值将不再计算。因此可以利用该特性在一定程度上提高程序效率。清单6.利用Lazyif-evaluation的特性Pythonfromtimeimporttimet=time()abbreviations=['cf.','e.g.','ex.','etc.','fig.','i.e.','Mr.','vs.']foriinrange(1000000):forwin('Mr.','Hat','is','chasing','the','black','cat','.'):ifwinabbreviations:#ifw[-1]=='.'andwinabbreviations:passprint\"totalruntime:\"printtime()-t12345678910fromtimeimporttimet=time()abbreviations=['cf.','e.g.','ex.','etc.','fig.','i.e.','Mr.','vs.']foriinrange(1000000):forwin('Mr.','Hat','is','chasing','the','black','cat','.'):ifwinabbreviations:#ifw[-1]=='.'andwinabbreviations:passprint\"totalruntime:\"printtime()-t在未进行优化之前程序的运行时间大概为8.84，如果使用注释行代替第一个if，运行的时间大概为6.17。字符串的优化python中的字符串对象是不可改变的，因此对任何字符串的操作如拼接，修改等都将产生一个新的字符串对象，而不是基于原字符串，因此这种持续的copy会在一定程度上影响python的性能。对字符串的优化也是改善性能的一个重要的方面，特别是在处理文本较多的情况下。字符串的优化主要集中在以下几个方面：1、在字符串连接的使用尽量使用join()而不是+：在代码清单7中使用+进行字符串连接大概需要0.125s，而使用join缩短为0.016s。因此在字符的操作上join比+要快，因此要尽量使用join而不是+。清单7.使用join而不是+连接字符串Python<spanstyle=\"font-family:Monaco,Consolas,'AndaleMono','DejaVuSansMono',monospace;font-style:normal;\">fromtimeimporttime</span>t=time()s=\"\"list=['a','b','b','d','e','f','g','h','i','j','k','l','m','n']foriinrange(10000):forsubstrinlist:s+=substrprint\"totalruntime:\"printtime()-t12345678910<spanstyle=\"font-family:Monaco,Consolas,'AndaleMono','DejaVuSansMono',monospace;font-style:normal;\">fromtimeimporttime</span> t=time()s=\"\"list=['a','b','b','d','e','f','g','h','i','j','k','l','m','n']foriinrange(10000):forsubstrinlist:s+=substrprint\"totalruntime:\"printtime()-t同时要避免：Pythons=&quot;&quot;forxinlist:s+=func(x)123s=&quot;&quot;forxinlist:    s+=func(x)而是要使用：Pythonslist=[func(elt)foreltinsomelist]s=&quot;&quot;.join(slist)12slist=[func(elt)foreltinsomelist]s=&quot;&quot;.join(slist)2、当对字符串可以使用正则表达式或者内置函数来处理的时候，选择内置函数。如str.isalpha()，str.isdigit()，str.startswith((‘x’,‘yz’))，str.endswith((‘x’,‘yz’))3、对字符进行格式化比直接串联读取要快，因此要使用Pythonout=\"%s%s%s%s\"%(head,prologue,query,tail)1out=\"%s%s%s%s\"%(head,prologue,query,tail)而避免Pythonout=\"\"+head+prologue+query+tail+\"\"1out=\"\"+head+prologue+query+tail+\"\"使用列表解析（listcomprehension）和生成器表达式（generatorexpression）列表解析要比在循环中重新构建一个新的list更为高效，因此我们可以利用这一特性来提高运行的效率。Pythonfromtimeimporttimet=time()list=['a','b','is','python','jason','hello','hill','with','phone','test','dfdf','apple','pddf','ind','basic','none','baecr','var','bana','dd','wrd']total=[]foriinrange(1000000):forwinlist:total.append(w)print\"totalruntime:\"printtime()-t12345678910fromtimeimporttimet=time()list=['a','b','is','python','jason','hello','hill','with','phone','test','dfdf','apple','pddf','ind','basic','none','baecr','var','bana','dd','wrd']total=[]foriinrange(1000000):forwinlist:total.append(w)print\"totalruntime:\"printtime()-t使用列表解析：Pythonforiinrange(1000000):a=[wforwinlist]12foriinrange(1000000):a=[wforwinlist]上述代码直接运行大概需要17s，而改为使用列表解析后，运行时间缩短为9.29s。将近提高了一半。生成器表达式则是在2.4中引入的新内容，语法和列表解析类似，但是在大数据量处理时，生成器表达式的优势较为明显，它并不创建一个列表，只是返回一个生成器，因此效率较高。在上述例子上中代码a=[wforwinlist]修改为a=(wforwinlist)，运行时间进一步减少，缩短约为2.98s。其他优化技巧1、如果需要交换两个变量的值使用a,b=b,a而不是借助中间变量t=a;a=b;b=t；Python&gt;&gt;&gt;fromtimeitimportTimer&gt;&gt;&gt;Timer(&quot;t=a;a=b;b=t&quot;,&quot;a=1;b=2&quot;).timeit()0.25154118749729365&gt;&gt;&gt;Timer(&quot;a,b=b,a&quot;,&quot;a=1;b=2&quot;).timeit()0.17156677734181258&gt;&gt;&gt;123456&gt;&gt;&gt;fromtimeitimportTimer&gt;&gt;&gt;Timer(&quot;t=a;a=b;b=t&quot;,&quot;a=1;b=2&quot;).timeit()0.25154118749729365&gt;&gt;&gt;Timer(&quot;a,b=b,a&quot;,&quot;a=1;b=2&quot;).timeit()0.17156677734181258&gt;&gt;&gt;2、在循环的时候使用xrange而不是range；使用xrange可以节省大量的系统内存，因为xrange()在序列中每次调用只产生一个整数元素。而range()將直接返回完整的元素列表，用于循环时会有不必要的开销。在python3中xrange不再存在，里面range提供一个可以遍历任意长度的范围的iterator。3、使用局部变量，避免”global”关键字。python访问局部变量会比全局变量要快得多，因此可以利用这一特性提升性能。4、ifdoneisnotNone比语句ifdone!=None更快，读者可以自行验证；5、在耗时较多的循环中，可以把函数的调用改为内联的方式；6、使用级联比较“x<y<z”而不是“x<yandy<z”；7、while1要比whileTrue更快（当然后者的可读性更好）；8、buildin函数通常较快，add(a,b)要优于a+b。定位程序性能瓶颈对代码优化的前提是需要了解性能瓶颈在什么地方，程序运行的主要时间是消耗在哪里，对于比较复杂的代码可以借助一些工具来定位，python内置了丰富的性能分析工具，如profile,cProfile与hotshot等。其中Profiler是python自带的一组程序，能够描述程序运行时候的性能，并提供各种统计帮助用户定位程序的性能瓶颈。Python标准模块提供三种profilers:cProfile,profile以及hotshot。profile的使用非常简单，只需要在使用之前进行import即可。具体实例如下：清单8.使用profile进行性能分析PythonimportprofiledefprofileTest():Total=1;foriinrange(10):Total=Total*(i+1)printTotalreturnTotalif__name__==\"__main__\":profile.run(\"profileTest()\")123456789importprofiledefprofileTest():    Total=1;    foriinrange(10):        Total=Total*(i+1)        printTotal    returnTotalif__name__==\"__main__\":    profile.run(\"profileTest()\")程序的运行结果如下：图1.性能分析结果其中输出每列的具体解释如下：●ncalls：表示函数调用的次数；●tottime：表示指定函数的总的运行时间，除掉函数中调用子函数的运行时间；●percall：（第一个percall）等于tottime/ncalls；●cumtime：表示该函数及其所有子函数的调用运行的时间，即函数开始调用到返回的时间；●percall：（第二个percall）即函数运行一次的平均时间，等于cumtime/ncalls；●filename:lineno(function)：每个函数调用的具体信息；如果需要将输出以日志的形式保存，只需要在调用的时候加入另外一个参数。如profile.run(“profileTest()”,”testprof”)。对于profile的剖析数据，如果以二进制文件的时候保存结果的时候，可以通过pstats模块进行文本报表分析，它支持多种形式的报表输出，是文本界面下一个较为实用的工具。使用非常简单：Pythonimportpstatsp=pstats.Stats('testprof')p.sort_stats(\"name\").print_stats()123importpstatsp=pstats.Stats('testprof')p.sort_stats(\"name\").print_stats()其中sort_stats()方法能够对剖分数据进行排序，可以接受多个排序字段，如sort_stats(‘name’,‘file’)将首先按照函数名称进行排序，然后再按照文件名进行排序。常见的排序字段有calls(被调用的次数)，time（函数内部运行时间），cumulative（运行的总时间）等。此外pstats也提供了命令行交互工具，执行python–mpstats后可以通过help了解更多使用方式。对于大型应用程序，如果能够将性能分析的结果以图形的方式呈现，将会非常实用和直观，常见的可视化工具有Gprof2Dot，visualpytune，KCacheGrind等，读者可以自行查阅相关官网，本文不做详细讨论。Python性能优化工具Python性能优化除了改进算法，选用合适的数据结构之外，还有几种关键的技术，比如将关键python代码部分重写成C扩展模块，或者选用在性能上更为优化的解释器等，这些在本文中统称为优化工具。python有很多自带的优化工具，如Psyco，Pypy，Cython，Pyrex等，这些优化工具各有千秋，本节选择几种进行介绍。Psycopsyco是一个just-in-time的编译器，它能够在不改变源代码的情况下提高一定的性能，Psyco将操作编译成有点优化的机器码，其操作分成三个不同的级别，有”运行时”、”编译时”和”虚拟时”变量。并根据需要提高和降低变量的级别。运行时变量只是常规Python解释器处理的原始字节码和对象结构。一旦Psyco将操作编译成机器码，那么编译时变量就会在机器寄存器和可直接访问的内存位置中表示。同时python能高速缓存已编译的机器码以备今后重用，这样能节省一点时间。但Psyco也有其缺点，其本身运行所占内存较大。目前psyco已经不在python2.7中支持，而且不再提供维护和更新了，对其感兴趣的可以参考 http://psyco.sourceforge.net/PypyPyPy表示“用Python实现的Python”，但实际上它是使用一个称为RPython的Python子集实现的，能够将Python代码转成C，.NET，Java等语言和平台的代码。PyPy集成了一种即时(JIT)编译器。和许多编译器，解释器不同，它不关心Python代码的词法分析和语法树。因为它是用Python语言写的，所以它直接利用Python语言的CodeObject.。CodeObject是Python字节码的表示，也就是说，PyPy直接分析Python代码所对应的字节码,，这些字节码即不是以字符形式也不是以某种二进制格式保存在文件中，而在Python运行环境中。目前版本是1.8.支持不同的平台安装，windows上安装Pypy需要先下载https://bitbucket.org/pypy/pypy/downloads/pypy-1.8-win32.zip，然后解压到相关的目录，并将解压后的路径添加到环境变量path中即可。在命令行运行pypy，如果出现如下错误：”没有找到MSVCR100.dll,因此这个应用程序未能启动，重新安装应用程序可能会修复此问题”，则还需要在微软的官网上下载VS2010runtimelibraries解决该问题。具体地址为http://www.microsoft.com/download/en/details.aspx?displaylang=en&id=5555安装成功后在命令行里运行pypy，输出结果如下：PythonC:\\DocumentsandSettings\\Administrator>pypyPython2.7.2(0e28b379d8b3,Feb092012,18:31:47)[PyPy1.8.0withMSCv.150032bit]onwin32Type\"help\",\"copyright\",\"credits\"or\"license\"formoreinformation.Andnowforsomethingcompletelydifferent:``PyPyisvast,andcontainsmultitudes''>>>>1234567C:\\DocumentsandSettings\\Administrator>pypyPython2.7.2(0e28b379d8b3,Feb092012,18:31:47)[PyPy1.8.0withMSCv.150032bit]onwin32Type\"help\",\"copyright\",\"credits\"or\"license\"formoreinformation.Andnowforsomethingcompletelydifferent:``PyPyisvast,andcontainsmultitudes''>>>>以清单5的循环为例子，使用python和pypy分别运行，得到的运行结果分别如下：PythonC:\\DocumentsandSettings\\Administrator\\桌面\\doc\\python>pypyloop.pytotalruntime:8.42199993134C:\\DocumentsandSettings\\Administrator\\桌面\\doc\\python>pythonloop.pytotalruntime:106.391000032123456C:\\DocumentsandSettings\\Administrator\\桌面\\doc\\python>pypyloop.pytotalruntime:8.42199993134C:\\DocumentsandSettings\\Administrator\\桌面\\doc\\python>pythonloop.pytotalruntime:106.391000032 可见使用pypy来编译和运行程序，其效率大大的提高。CythonCython是用python实现的一种语言，可以用来写python扩展，用它写出来的库都可以通过import来载入，性能上比python的快。cython里可以载入python扩展(比如importmath)，也可以载入c的库的头文件(比如:cdefexternfrom“math.h”)，另外也可以用它来写python代码。将关键部分重写成C扩展模块LinuxCpython的安装：第一步：下载Python[root@v5254085f259cpython]#wget-Nhttp://cython.org/release/Cython-0.15.1.zip--2012-04-1622:08:35--http://cython.org/release/Cython-0.15.1.zipResolvingcython.org...128.208.160.197Connectingtocython.org|128.208.160.197|:80...connected.HTTPrequestsent,awaitingresponse...200OKLength:2200299(2.1M)[application/zip]Savingto:`Cython-0.15.1.zip&#039;100%[======================================&gt;]2,200,2991.96M/sin1.1s2012-04-1622:08:37(1.96MB/s)-`Cython-0.15.1.zip&#039;saved[2200299/2200299]123456789[root@v5254085f259cpython]#wget-Nhttp://cython.org/release/Cython-0.15.1.zip--2012-04-1622:08:35--http://cython.org/release/Cython-0.15.1.zipResolvingcython.org...128.208.160.197Connectingtocython.org|128.208.160.197|:80...connected.HTTPrequestsent,awaitingresponse...200OKLength:2200299(2.1M)[application/zip]Savingto:`Cython-0.15.1.zip&#039;100%[======================================&gt;]2,200,2991.96M/sin1.1s2012-04-1622:08:37(1.96MB/s)-`Cython-0.15.1.zip&#039;saved[2200299/2200299] 第二步：解压Python[root@v5254085f259cpython]#unzip-oCython-0.15.1.zip1[root@v5254085f259cpython]#unzip-oCython-0.15.1.zip第三步：安装Pythonpythonsetup.pyinstall1pythonsetup.pyinstall安装完成后直接输入cython，如果出现如下内容则表明安装成功。Python[root@v5254085f259Cython-0.15.1]#cythonCython(http://cython.org)isacompilerforcodewrittenintheCythonlanguage.CythonisbasedonPyrexbyGregEwing.Usage:cython[options]sourcefile.{pyx,py}...Options:-V,--versionDisplayversionnumberofcythoncompiler-l,--create-listingWriteerrormessagestoalistingfile-I,--include-dir<directory>Searchforincludefilesinnameddirectory(multipleincludedirectoriesareallowed).-o,--output-file<filename>SpecifynameofgeneratedCfile-t,--timestampsOnlycompilenewersourcefiles-f,--forceCompileallsourcefiles(overridesimplied-t)-q,--quietDon'tprintmodulenamesinrecursivemode-v,--verboseBeverbose,printfilenamesonmultiplecompilation-p,--embed-positionsIfspecified,thepositionsinCythonfilesofeachfunctiondefinitionisembeddedinitsdocstring.--cleanup<level>Releaseinternedobjectsonpythonexit,formemorydebugging.Levelindicatesaggressiveness,default0releasesnothing.-w,--working<directory>SetstheworkingdirectoryforCython(thedirectorymodulesaresearchedfrom)--gdbOutputdebuginformationforcygdb-D,--no-docstringsStripdocstringsfromthecompiledmodule.-a,--annotateProduceacolorizedHTMLversionofthesource.--line-directivesProduce#linedirectivespointingtothe.pyxsource--cplusOutputaC++ratherthanCfile.--embed[=<method_name>]Generateamain()functionthatembedsthePythoninterpreter.-2CompilebasedonPython-2syntaxandcodesemantics.-3CompilebasedonPython-3syntaxandcodesemantics.--fast-failAbortthecompilationonthefirsterror--warning-error,-WerrorMakeallwarningsintoerrors--warning-extra,-WextraEnableextrawarnings-X,--directive<name>=<value>[,<name=value,...]Overridesacompilerdirective1234567891011121314151617181920212223242526272829303132333435363738394041[root@v5254085f259Cython-0.15.1]#cythonCython(http://cython.org)isacompilerforcodewrittenintheCythonlanguage.  CythonisbasedonPyrexbyGregEwing. Usage:cython[options]sourcefile.{pyx,py}... Options:  -V,--version                  Displayversionnumberofcythoncompiler  -l,--create-listing          Writeerrormessagestoalistingfile  -I,--include-dir<directory>  Searchforincludefilesinnameddirectory                                (multipleincludedirectoriesareallowed).  -o,--output-file<filename>  SpecifynameofgeneratedCfile  -t,--timestamps              Onlycompilenewersourcefiles  -f,--force                    Compileallsourcefiles(overridesimplied-t)  -q,--quiet                    Don'tprintmodulenamesinrecursivemode  -v,--verbose                  Beverbose,printfilenamesonmultiplecompilation  -p,--embed-positions          Ifspecified,thepositionsinCythonfilesofeach  functiondefinitionisembeddedinitsdocstring.  --cleanup<level>  Releaseinternedobjectsonpythonexit,formemorydebugging.    Levelindicatesaggressiveness,default0releasesnothing.  -w,--working<directory>  SetstheworkingdirectoryforCython(thedirectorymodulesaresearchedfrom)  --gdbOutputdebuginformationforcygdb  -D,--no-docstrings              Stripdocstringsfromthecompiledmodule.  -a,--annotate              ProduceacolorizedHTMLversionofthesource.  --line-directives              Produce#linedirectivespointingtothe.pyxsource  --cplus              OutputaC++ratherthanCfile.  --embed[=<method_name>]              Generateamain()functionthatembedsthePythoninterpreter.  -2          CompilebasedonPython-2syntaxandcodesemantics.  -3          CompilebasedonPython-3syntaxandcodesemantics.  --fast-fail    Abortthecompilationonthefirsterror  --warning-error,-Werror      Makeallwarningsintoerrors  --warning-extra,-Wextra      Enableextrawarnings  -X,--directive<name>=<value>  [,<name=value,...]Overridesacompilerdirective其他平台上的安装可以参考文档：http://docs.cython.org/src/quickstart/install.htmlCython代码与python不同，必须先编译，编译一般需要经过两个阶段，将pyx文件编译为.c文件，再将.c文件编译为.so文件。编译有多种方法：●通过命令行编译：假设有如下测试代码，使用命令行编译为.c文件。Pythondefsum(inta,intb):printa+b[root@v5254085f259test]#cythonsum.pyx[root@v5254085f259test]#lstotal764drwxr-xr-x2rootroot4096Apr1702:45.4drwxr-xr-x4rootroot4096Apr1622:20..4-rw-r--r--1rootroot35Apr1702:45160-rw-r--r--1rootroot55169Apr1702:45sum.c4-rw-r--r--1rootroot35Apr1702:45sum.pyx1234567891011defsum(inta,intb):        printa+b [root@v5254085f259test]#cythonsum.pyx[root@v5254085f259test]#lstotal764drwxr-xr-x2rootroot  4096Apr1702:45.4drwxr-xr-x4rootroot  4096Apr1622:20..4-rw-r--r--1rootroot    35Apr1702:45160-rw-r--r--1rootroot55169Apr1702:45sum.c4-rw-r--r--1rootroot    35Apr1702:45sum.pyx在linux上利用gcc编译为.so文件：Python[root@v5254085f259test]#gcc-shared-pthread-fPIC-fwrapv-O2-Wall-fno-strict-aliasing-I/usr/include/python2.4-osum.sosum.c[root@v5254085f259test]#lstotal964drwxr-xr-x2rootroot4096Apr1702:47.4drwxr-xr-x4rootroot4096Apr1622:20..4-rw-r--r--1rootroot35Apr1702:45160-rw-r--r--1rootroot55169Apr1702:45sum.c4-rw-r--r--1rootroot35Apr1702:45sum.pyx20-rwxr-xr-x1rootroot20307Apr1702:47sum.so12345678910[root@v5254085f259test]#gcc-shared-pthread-fPIC-fwrapv-O2-Wall-fno-strict-aliasing-I/usr/include/python2.4-osum.sosum.c[root@v5254085f259test]#lstotal964drwxr-xr-x2rootroot  4096Apr1702:47.4drwxr-xr-x4rootroot  4096Apr1622:20..4-rw-r--r--1rootroot    35Apr1702:45160-rw-r--r--1rootroot55169Apr1702:45sum.c4-rw-r--r--1rootroot    35Apr1702:45sum.pyx20-rwxr-xr-x1rootroot20307Apr1702:47sum.soPython●使用distutils编译1●使用distutils编译建立一个setup.py的脚本：Pythonfromdistutils.coreimportsetupfromdistutils.extensionimportExtensionfromCython.Distutilsimportbuild_extext_modules=[Extension(\"sum\",[\"sum.pyx\"])]setup(name='sumapp',cmdclass={'build_ext':build_ext},ext_modules=ext_modules)[root@v5254085f259test]#pythonsetup.pybuild_ext--inplacerunningbuild_extcythoningsum.pyxtosum.cbuilding'sum'extensiongcc-pthread-fno-strict-aliasing-fPIC-g-O2-DNDEBUG-g-fwrapv-O3-Wall-Wstrict-prototypes-fPIC-I/opt/ActivePython-2.7/include/python2.7-csum.c-obuild/temp.linux-x86_64-2.7/sum.ogcc-pthread-sharedbuild/temp.linux-x86_64-2.7/sum.o-o/root/cpython/test/sum.so123456789101112131415161718192021fromdistutils.coreimportsetupfromdistutils.extensionimportExtensionfromCython.Distutilsimportbuild_ext ext_modules=[Extension(\"sum\",[\"sum.pyx\"])] setup(    name='sumapp',    cmdclass={'build_ext':build_ext},    ext_modules=ext_modules) [root@v5254085f259test]#  pythonsetup.pybuild_ext--inplacerunningbuild_extcythoningsum.pyxtosum.cbuilding'sum'extensiongcc-pthread-fno-strict-aliasing-fPIC-g-O2-DNDEBUG-g-fwrapv-O3-Wall-Wstrict-prototypes-fPIC-I/opt/ActivePython-2.7/include/python2.7  -csum.c-obuild/temp.linux-x86_64-2.7/sum.ogcc-pthread-sharedbuild/temp.linux-x86_64-2.7/sum.o-o/root/cpython/test/sum.so编译完成之后可以导入到python中使用：Python[root@v5254085f259test]#pythonActivePython2.7.2.5(ActiveStateSoftwareInc.)basedonPython2.7.2(default,Jun242011,11:24:26)[GCC4.0.220051125(RedHat4.0.2-8)]onlinux2Type\"help\",\"copyright\",\"credits\"or\"license\"formoreinformation.>>>importpyximport;pyximport.install()>>>importsum>>>sum.sum(1,3)12345678[root@v5254085f259test]#pythonActivePython2.7.2.5(ActiveStateSoftwareInc.)basedonPython2.7.2(default,Jun242011,11:24:26)[GCC4.0.220051125(RedHat4.0.2-8)]onlinux2Type\"help\",\"copyright\",\"credits\"or\"license\"formoreinformation.>>>importpyximport;pyximport.install()>>>importsum>>>sum.sum(1,3)下面来进行一个简单的性能比较：清单9.Cython测试代码Pythonfromtimeimporttimedeftest(intn):cdefinta=0cdefintiforiinxrange(n):a+=ireturnat=time()test(10000000)print\"totalruntime:\"printtime()-t123456789101112fromtimeimporttimedeftest(intn):        cdefinta=0        cdefinti        foriinxrange(n):                a+=i        returna t=time()test(10000000)print\"totalruntime:\"printtime()-t测试结果：Python[GCC4.0.220051125(RedHat4.0.2-8)]onlinux2Type&quot;help&quot;,&quot;copyright&quot;,&quot;credits&quot;or&quot;license&quot;formoreinformation.&gt;&gt;&gt;importpyximport;pyximport.install()&gt;&gt;&gt;importctesttotalruntime:0.00714015960693123456[GCC4.0.220051125(RedHat4.0.2-8)]onlinux2Type&quot;help&quot;,&quot;copyright&quot;,&quot;credits&quot;or&quot;license&quot;formoreinformation.&gt;&gt;&gt;importpyximport;pyximport.install()&gt;&gt;&gt;importctesttotalruntime:0.00714015960693清单10.Python测试代码Pythonfromtimeimporttimedeftest(n):a=0;foriinxrange(n):a+=ireturnat=time()test(10000000)print\"totalruntime:\"printtime()-t[root@v5254085f259test]#pythontest.pytotalruntime:0.971596002579123456789101112131415fromtimeimporttimedeftest(n):        a=0;        foriinxrange(n):                a+=i        returna t=time()test(10000000)print\"totalruntime:\"printtime()-t [root@v5254085f259test]#pythontest.pytotalruntime:0.971596002579 从上述对比可以看到使用Cython的速度提高了将近100多倍。 总结本文初步探讨了python常见的性能优化技巧以及如何借助工具来定位和分析程序的性能瓶颈，并提供了相关可以进行性能优化的工具或语言，希望能够更相关人员一些参考。1赞2收藏评论"], "art_create_time": ["2013/04/06"], "art_title": ["Python 代码性能优化技巧"], "art_url": ["http://python.jobbole.com/24197/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2012/07/Python-code-performance-optimization-techniques.png"]},
{"art_content": ["原文出处：xybaby   程序员都自视清高，觉得自己是创造者，经常鄙视不太懂技术的产品或者QA。可悲的是，程序员之间也相互鄙视，程序员的鄙视链流传甚广，作为一个Python程序员，自然最关心的是下面这幅图啦我们项目组一值使用Python2.7，虽然我们也知道Python3的诸多好处，也曾经蠢蠢欲动过，但由于各种历史原因，以及业务的压力，我们只可能继续使用Python2.7。更悲哀的是，我们组不是那么international，所以代码中还是涉及到大量的中文，因此偶尔也会遇到乱码以及UnicodeError，于是生活在了鄙视链的末端。因此，本文的目标是解释清楚python2.7中unicode、str的编解码关系，力求在鄙视链中前进一步。注意：本文实验主要基于win7，Python2.7；以及Linux，Python2.7。除非特殊说明，所有的命令都是在终端中交互式输入；如果没有强调平台，那么就是window上的结果。下面是一些默认的环境信息（其重要性后文会介绍）windowsPython>>>importsys,locale>>>sys.getdefaultencoding()'ascii'>>>locale.getdefaultlocale()('zh_CN','cp936')>>>sys.stdin.encoding'cp936'>>>sys.stdout.encoding'cp936'>>>sys.getfilesystemencoding()'mbcs'1234567891011>>>importsys,locale>>>sys.getdefaultencoding()'ascii'>>>locale.getdefaultlocale()('zh_CN','cp936')>>>sys.stdin.encoding'cp936'>>>sys.stdout.encoding'cp936'>>>sys.getfilesystemencoding()'mbcs'注意，上面CP936是GBK的别名，在https://docs.python.org/2/library/codecs.html#standard-encodings可以查看。LinuxPython>>>importsys,locale>>>sys.getdefaultencoding()'ascii'>>>locale.getdefaultlocale()('zh_CN','UTF-8')>>>sys.stdin.encoding'UTF-8'>>>sys.stdout.encoding'UTF-8'>>>sys.getfilesystemencoding()'UTF-8'1234567891011>>>importsys,locale>>>sys.getdefaultencoding()'ascii'>>>locale.getdefaultlocale()('zh_CN','UTF-8')>>>sys.stdin.encoding'UTF-8'>>>sys.stdout.encoding'UTF-8'>>>sys.getfilesystemencoding()'UTF-8'从字符编码说起首先来说一说gbkgb2312unicodeutf-8这些术语，这些术语与语言无关。计算机的世界只有0和1，因此任何字符（也就是实际的文字符号）也是由01串组成。计算机为了运算方便，都是8个bit组成一个字节（Byte），字符表达的最小单位就是字节，即一个字符占用一个或者多个字节。字符编码（characterencoding）就是字集码，编码就是将字符集中的字符映射为一个唯一二进制的过程。计算机发源于美国，使用的是英文字母（字符），所有26个字母的大小写加上数字0到10，加上符号和控制字符，总数也不多，用一个字节（8个bit）就能表示所有的字符，这就是ANSI的“Ascii”编码（AmericanStandardCodeforInformationInterchange，美国信息互换标准代码）。比如，小写字母‘a’的ascii码是01100001，换算成十进制就是97，十六进制就是0x61。计算机中，一般都是用十六进制来描述字符编码。但是当计算机传到中国的时候，ASCII编码就行不通了，汉字这么多，一个字节肯定表示不下啊，于是有了GB2312（中国国家标准简体中文字符集）。GB2312使用两个字节来对一个字符进行编码，其中前面的一个字节（称之为高字节）从0xA1用到0xF7，后面一个字节（低字节）从0xA1到0xFE，GB2312能表示几千个汉字，而且与asill吗也是兼容的。但后来发现，GB2312还是不够用，于是进行扩展，产生了GBK（即汉字内码扩展规范），GBK同Gb2312一样，两个字节表示一个字符，但区别在于，放宽了对低字节的要求，因此能表示的范围扩大到了20000多。后来，为了容纳少数名族，以及其他汉字国家的文字，出现了GB13080。GB13080是兼容GBK与GB2312的，能容纳更多的字符，与GBK与GB2312不同的是，GB18030采用单字节、双字节和四字节三种方式对字符编码因此，就我们关心的汉字而言，三种编码方式的表示范围是：GB18030》GBK》GB2312即GBK是GB2312的超集，GB1803又是GBK的超集。后面也会看到，一个汉字可以用GBK表示，但不一定能被GB2312所表示当然，世界上还有更多的语言与文字，每种文字都有自己的一套编码规则，这样一旦跨国就会出现乱码，亟待一个全球统一的解决办法。这个时候ISO（国际标准化组织）出马了，发明了”UniversalMultiple-OctetCodedCharacterSet”，简称UCS,俗称“unicode”。目标很简单：废了所有的地区性编码方案，重新搞一个包括了地球上所有文化、所有字母和符号的编码！unicode每种语言中的每个字符设定了统一并且唯一的二进制编码，以满足跨语言、跨平台进行文本转换、处理的要求。unicode编码一定以u开头。但是，unicode只是一个编码规范，是所有字符对应二进制的集合，而不是具体的编码规则。或者说，unicode是表现形式，而不是存储形式，就是说没用定义每个字符是如何以二进制的形式存储的。这个就跟GBK这些不一样，GBK是表里如下，表现形式即存储形式。比如汉字“严”的unicode编码是u4e25，对应的二进制是100111000100101，但是当其经过网络传输或者文件存储时，是没法知道怎么解析这些二进制的，容易和其他字节混在一起。那么怎么存储unicode呢，于是出现了UTF（UCSTransferFormat），这个是具体的编码规则，即UTF的表现形式与存储格式是一样的。因此，可以说，GBK和UTF-8是同一个层面的东西，跟unicode是另一个层面的东西，unicode飘在空中，如果要落地，需要转换成utf-8或者GBK。只不过，转换成Utf-8，大家都能懂，更懂用，而转换成GBK，只有中国人才看得懂UTF也有不同的实现，如UTF-8，UTF-16，这里以UTF-8为例进行讲解（下面一小节引用了阮一峰的文章）。unicode与utf-8UTF-8最大的一个特点，就是它是一种变长的编码方式。它可以使用1~4个字节表示一个符号，根据不同的符号而变化字节长度。UTF-8的编码规则很简单，只有二条：1）对于单字节的符号，字节的第一位设为0，后面7位为这个符号的unicode码。因此对于英语字母，UTF-8编码和ASCII码是相同的。2）对于n字节的符号（n>1），第一个字节的前n位都设为1，第n+1位设为0，后面字节的前两位一律设为10。剩下的没有提及的二进制位，全部为这个符号的unicode码。下表总结了编码规则，字母x表示可用编码的位。Unicode符号范围|UTF-8编码方式(十六进制)|（二进制）----------------------+---------------------------------------------00000000-0000007F|0xxxxxxx00000080-000007FF|110xxxxx10xxxxxx00000800-0000FFFF|1110xxxx10xxxxxx10xxxxxx00010000-0010FFFF|11110xxx10xxxxxx10xxxxxx10xxxxxx1234567Unicode符号范围      |        UTF-8编码方式(十六进制)          |        （二进制）----------------------+---------------------------------------------00000000-0000007F|0xxxxxxx00000080-000007FF|110xxxxx10xxxxxx00000800-0000FFFF|1110xxxx10xxxxxx10xxxxxx00010000-0010FFFF|11110xxx10xxxxxx10xxxxxx10xxxxxx以汉字“严”为例，演示如何实现UTF-8编码。已知“严”的unicode是4E25（100111000100101），根据上表，可以发现4E25处在第三行的范围内（00000800-0000FFFF），因此“严”的UTF-8编码需要三个字节，即格式是“1110xxxx10xxxxxx10xxxxxx”。然后，从“严”的最后一个二进制位开始，依次从后向前填入格式中的x，多出的位补0。这样就得到了，“严”的UTF-8编码是“111001001011100010100101”，转换成十六进制就是E4B8A5。当编解码遇上Python2.x下面使用Python语言来验证上面的理论。在这一章节中，当提到unicode，一般是指unicodetype，即Python中的类型；也会提到unicode编码、unicode函数，请大家注意区别。另外，对于编码，也有两种意思。第一个是名字，指的是字符的二进制表示，如unicode编码、gbk编码。第二个是动词，指的是从字符到二进制的映射过程。不过后文中，编码作为动词，狭义理解为从unicode类型转换成str类型的过程，解码则是相反的过程。另外强调的是，unicode类型一定是unicode编码，而str类型可能是gbk、ascii或者utf-8编码。unicode与str区别在python2.7中，有两种“字符串”类型，分别是str与unicode，他们有同一个基类basestring。str是plainstring，其实应该称之为字节串，因为是每一个字节换一个单位长度。而unicode就是unicodestring，这才是真正的字符串，一个字符（可能多个字节）算一个单位长度。python2.7中，unicode类型需要在文本之间加u表示。Python>>>us=u'严'>>>printtype(us),len(us)<type'unicode'>1>>>s='严'>>>printtype(s),len(s)<type'str'>2>>>1234567>>>us=u'严'>>>printtype(us),len(us)<type'unicode'>1>>>s='严'>>>printtype(s),len(s)<type'str'>2>>>从上可以看到，第一，us、s的类型是不一样的；其二，同一个汉字，不同的类型其长度也是不一样的，对于unicode类型的实例，其长度一定是字符的个数，而对于str类型的实例，其长度是字符对应的字节数目。这里强调一下，s（s=‘严’）的长度在不同的环境下是不一样的！后文会解释__str____repr__的区别这是python中两个magicmethod，很容易让新手迷糊，因为很多时候，二者的实现是一样的，但是这两个函数是用在不同的地方_str__，主要是用于展示，str(obj)或者printobj的时候调用，返回值一定是一个str对象__repr__，是被repr(obj)，或者在终端直接打obj的时候调用Python>>>us=u'严'>>>usu'\\u4e25'>>>printus严12345>>>us=u'严'>>>usu'\\u4e25'>>>printus严可以看到，不使用print返回的是一个更能反映对象本质的结果，即us是一个unicode对象（最前面的u表示，以及unicode编码是用的u），且“严”的unicode编码确实是4E25。而print调用可us.__str__，等价于printstr(us)，使得结果对用户更友好。那么unicode.__str__是怎么转换成str的呢，答案会在后面揭晓unicodestrutf-8关系前面已经提到，unicode只是编码规范（只是字符与二进制的映射集合），而utf-8是具体的编码规则（不仅包含字符与二进制的映射集合，而且映射后的二进制是可以用于存储和传输的），即utf-8负责把unicode转换成可存储和传输的二进制字符串即str类型，我们称这个转换过程为编码。而从str类型到unicode类型的过程，我们称之为解码。Python中使用decode()和encode()来进行解码和编码，以unicode类型作为中间类型。如下图所示Python　　decode　　　　encodestr--------->unicode--------->str12　　decode　　　　encodestr--------->unicode--------->str即str类型调用decode方法转换成unicode类型，unicode类型调用encode方法转换成str类型。forexamplePython>>>us=u'严'>>>ss=us.encode('utf-8')>>>ss'\\xe4\\xb8\\xa5'>>>type(ss)<type'str'>>>>ss.decode('utf-8')==usTrue12345678>>>us=u'严'>>>ss=us.encode('utf-8')>>>ss'\\xe4\\xb8\\xa5'>>>type(ss)<type'str'>>>>ss.decode('utf-8')==usTrue从上可以看出encode与decode两个函数的作用，也可以看出’严’的utf8编码是E4B8A5。就是说我们使用unicode.encode将unicode类型转换成了str类型，在上面也提到unicode.__str__也是将unicode类型转换成str类型。二者有什么却比呢unicode.encode与 unicode.__str__的区别首先看看文档Pythonstr.encode([encoding[,errors]])　　Returnanencodedversionofthestring.Defaultencodingisthecurrentdefaultstringencoding.object.__str__(self)　　Calledbythestr()built-infunctionandbytheprintstatementtocomputethe“informal”stringrepresentationofanobject.123456str.encode([encoding[,errors]])　　Returnanencodedversionofthestring.Defaultencodingisthecurrentdefaultstringencoding. 　　object.__str__(self)　　Calledbythestr()built-infunctionandbytheprintstatementtocomputethe“informal”stringrepresentationofanobject.注意：str.encode这里的str是basestring，是str类型与unicode类型的基类可以看到encode方法是有可选的参数：encoding和errors，在上面的例子中encoding即为utf-8；而__str__是没有参数的，我们可以猜想，对于unicode类型，__str__函数一定也是使用了某种encoding来对unicode进行编码。首先不禁要问，如果encode方法没有带入参数，是什么样子的：Python>>>us.encode()Traceback(mostrecentcalllast):File\"<stdin>\",line1,in<module>UnicodeEncodeError:'ascii'codeccan'tencodecharacteru'\\u4e25'inposition0:ordinalnotinrange(128)1234>>>us.encode()Traceback(mostrecentcalllast):File\"<stdin>\",line1,in<module>UnicodeEncodeError:'ascii'codeccan'tencodecharacteru'\\u4e25'inposition0:ordinalnotinrange(128)不难看出，默认使用的就是ascii码来对unicode就行编码，为什么是ascii码，其实就是系统默认编码（sys.getdefaultencoding的返回值）。ascii码显然无法表示汉字，于是抛出了异常。而使用utf-8编码的时候，由于utf能够表示这个汉字，所以没报错。如果直接打印ss（us.encode(‘utf-8’)的返回值）会怎么样Python>>>printss涓12>>>printss涓结果略有些奇怪，us.__str__(即直接打印us）的结果不一样，那么试试encoding=gbk呢？Python>>>printus.encode('gbk')严12>>>printus.encode('gbk')严Ugotit!事实上也是如此，python会采用终端默认的编码（用locale.getdefaultlocale()查看，windows是为gbk）将unicode编码成str类型。在Linux（终端编码为utf-8），结果如下：Python>>>us=u'严'>>>printus.encode('utf-8')严>>>printus.encode('gbk')▒▒>>>printus严>>>12345678>>>us=u'严'>>>printus.encode('utf-8')严>>>printus.encode('gbk')▒▒>>>printus严>>>注意上面的乱码！unicodegbk之间的转换在上上小节，介绍了unicode可以通过utf-8编码（encoding=utf-8），转换成utf-8表示的str，在上一节也可以看出unicode也可以通过gbk编码（encoding=gbk），转换成gbk表示的str。这里有点晕，留作第一个问题，后面解释unicode与utf8之间的相互转换可以计算得知，但unicode与gbk之间的相互转换没有计算公式，就只能靠查表了，就是说有一张映射表，有某一个汉字对应的unicode表示与gbk表示的映射关系Python>>>us=u'严'>>>usu'\\u4e25'>>>us.encode('gbk')'\\xd1\\xcf'>>>us.encode('gb2312')'\\xd1\\xcf'>>>us.encode('gb18030')'\\xd1\\xcf'>>>s='严'>>>s'\\xd1\\xcf'>>>12345678910111213>>>us=u'严'>>>usu'\\u4e25'>>>us.encode('gbk')'\\xd1\\xcf'>>>us.encode('gb2312')'\\xd1\\xcf'>>>us.encode('gb18030')'\\xd1\\xcf'>>>s='严'>>>s'\\xd1\\xcf'>>>从上不难看出，严的unicdoe编码是4e25，GBK编码是d1cf，因此us通过gbk编码就是d1cf。同样也能看到，GB18030，GBK，GB2312是兼容的为什么printus.encode(‘utf-8’)打印出“涓”ss=us.encode(‘utf-8’)，ss是一个str类型，直接打印结果有点奇怪，一个“涓”字，那一个str类型的“涓”是哪些二进制组成的呢Python>>>s='涓'>>>s'\\xe4\\xb8'123>>>s='涓'>>>s'\\xe4\\xb8'可以看到，str类型的“涓”，其二进制是E4B8，跟’严’的utf8编码（E4B8A5）相差了一个A5，那么就是因为A5显示不出来，验证如下：Python>>>print'--%s--'%ss--涓?-12>>>print'--%s--'%ss--涓?-因此，只是碰巧显示了“涓”而已，事实上ss跟“”涓“”毫无关系回答第一个问题：str类型到底是什么在上上小节，提到了utf-8编码的str，与gbk编码的str，感觉有点绕。我们知道，一个汉字‘严’，可存储的编码格式可以是gbk（’xd1xcf’），也可以是utf-8（’xe4xb8xa5’），那么当我们在终端敲入这个汉字的时候，是哪一种格式呢？取决于终端默认编码。windows上（默认终端编码为gbk）：Python>>>s='严'>>>s'\\xd1\\xcf'123>>>s='严'>>>s'\\xd1\\xcf'Linux上（默认终端编码为utf-8）：Python>>>a='严'>>>a'\\xe4\\xb8\\xa5'123>>>a='严'>>>a'\\xe4\\xb8\\xa5'同样一个汉字，同样都是Python中的str类型，在不同的编码格式下，其二进制是不一样的。因此，其长度也是不一样的，对于str类型，其长度是对应的字节长度。也能看出gbk编码的字节长度一般小于utf-8，这也是gbk继续存在的一个原因。这里，要强调一下，unicode的二进制形式是与终端的编码格式无关的！这个也不难理解。unicode函数str类型到unicode类型的转换，出了上面提到的str.decode，还有一个unicode函数。两个函数的签名为：Pythonunicode(object[,encoding[,errors]])ReturntheUnicodestringversionofobjectusingoneofthefollowingmodes:str.decode([encoding[,errors]])Decodesthestringusingthecodecregisteredforencoding.encodingdefaultstothedefaultstringencoding.12345unicode(object[,encoding[,errors]])ReturntheUnicodestringversionofobjectusingoneofthefollowingmodes: str.decode([encoding[,errors]])Decodesthestringusingthecodecregisteredforencoding.encodingdefaultstothedefaultstringencoding.二者参数相同，事实上二者是等价的，encoding的默认值也是一样的，都是sys.getdefaultencoding()的结果。forexample：Python>>>s='严'>>>newuse=unicode(s)Traceback(mostrecentcalllast):File\"<stdin>\",line1,in<module>UnicodeDecodeError:'ascii'codeccan'tdecodebyte0xd1inposition0:ordinalnotinrange(128)>>>newuse=unicode(s,'utf-8')Traceback(mostrecentcalllast):File\"<stdin>\",line1,in<module>UnicodeDecodeError:'utf8'codeccan'tdecodebyte0xd1inposition0:invalidcontinuationbyte>>>newuse=unicode(s,'gbk')>>>newuseu'\\u4e25'12345678910111213>>>s='严'>>>newuse=unicode(s)Traceback(mostrecentcalllast):File\"<stdin>\",line1,in<module>UnicodeDecodeError:'ascii'codeccan'tdecodebyte0xd1inposition0:ordinalnotinrange(128) >>>newuse=unicode(s,'utf-8')Traceback(mostrecentcalllast):File\"<stdin>\",line1,in<module>UnicodeDecodeError:'utf8'codeccan'tdecodebyte0xd1inposition0:invalidcontinuationbyte>>>newuse=unicode(s,'gbk')>>>newuseu'\\u4e25'第一个UnicodeDecodeError，就是因为系统默认的编码是asill吗；第二个UnicodeDecodeError，是因为，s（str类型的实例）的编码取决于终端默认编码（即windows下的gbk），为了能打印出来，也就必须用gbk编码来表示这个str，因此只能查询gbk与unicode的映射表将s转换成unicode类型。为啥调用sys.setdefaultencoding在诸多Python代码中，都会看到这么一段：importsysreload(sys)sys.setdefaultencoding('utf-8')123importsysreload(sys)sys.setdefaultencoding('utf-8')不难猜想，setdefaultencoding跟getdefaultencoding是配对的，为啥要将系统的默认编码设置成utf-8，其实就是解决str到unicode的转换问题。上一小节已经提到过，使用unicode函数将str类型转换成unicode类型时，要考虑两个因素：第一，str本身是什么编码的；第二，如果没有传入encoding参数，默认使用sys.getdefaultencoding。encoding参数必须与str本身的编码对应，否则就是UnicodeDecodeError。写python代码的程序都知道，我们要在py文件第一行写上：Python#-*-coding:utf-8-*-1#-*-coding:utf-8-*-这句话的作用在于，告诉编辑器，该文件里面的所有str都采用utf-8编码，且存储文件的时候也是使用utf-8格式。然后文件中就会使用下面的这种代码。Pythons='中文'us=unicode(s)12s='中文'us=unicode(s)使用unicode强制转换的时候，都不习惯带参数，为了保证encoding参数必须与str本身的编码一致，所以使用setdefaultencoding将系统默认编码设置为utf-8乱码与UnicodeError下面介绍几种常见的乱码与异常UnicodeError，大多数乱码或者异常的原因在前面已经讲过了，同时，对于一些乱码，也试图给出可行的解决办法。UnicodeError包括UnicodeDecodeError与UnicodeEncodeError，前者是decode也就是str转unicode的时候出了异常，后者则是encode也就是unicode转str的时候出了异常。对于一个str，直接打印例子就是上面反复提到的例子Python>>>ss=us.encode('utf-8')>>>printss涓123>>>ss=us.encode('utf-8')>>>printss涓如果一个str类型来自网络或者文件读取，最好先按照对端encode的方式先decode成unicode，然后再输出（输出的时候会自动转换成期望终端支持的编码格式的str）编码范围无法包括的汉字直接上例子Python>>>newus=u'囍'>>>newusu'\\u56cd'>>>newus.encode('gbk')'\\x87\\xd6'>>>newus.encode('gb2312')Traceback(mostrecentcalllast):File\"<stdin>\",line1,in<module>UnicodeEncodeError:'gb2312'codeccan'tencodecharacteru'\\u56cd'inposition0:illegalmultibytesequence>>>12345678910>>>newus=u'囍'>>>newusu'\\u56cd'>>>newus.encode('gbk')'\\x87\\xd6'>>>newus.encode('gb2312')Traceback(mostrecentcalllast):File\"<stdin>\",line1,in<module>UnicodeEncodeError:'gb2312'codeccan'tencodecharacteru'\\u56cd'inposition0:illegalmultibytesequence>>>可以看到，‘囍’字可以被gbk编码，但是不能被gb2312编码。str转unicode的时候在上面讲unicode函数的时候已经举过例子，会爆出UnicodeDecodeError异常。这个错误比较的原因，更多来自str到unicode的默认转换，比如一个str与一个unicode相加的时候：Python>>>a='严'>>>b=u'严'>>>c=a+bTraceback(mostrecentcalllast):　　File\"<stdin>\",line1,in<module>UnicodeDecodeError:'ascii'codeccan'tdecodebyte0xd1inposition0:ordinalnotinrange(128)123456>>>a='严'>>>b=u'严'>>>c=a+bTraceback(mostrecentcalllast):　　File\"<stdin>\",line1,in<module>UnicodeDecodeError:'ascii'codeccan'tdecodebyte0xd1inposition0:ordinalnotinrange(128)unicode与str相加，str会转换为unicode,使用默认的unicode(strobj,encoding=sys.getdefaultencoding())看起来向unicode编码的字符串某些情况下，我们打印出一个str类型，看到结果是’\\u4e25’，或者’u4e25’，对于这个字符串，是不是很眼熟，不错，‘严‘的unicode编码就是u’u4e25’。仔细一看，只是在引号前面多了一个u（表示是一个unicode类型）。那么当我们看到一个’u4e25’的时候，怎么知道对应的汉字是什么？对于已知的这种格式的str，自然可以手动加一个u，然后在终端输出，但是如果是一个变量，需要自动转换成unicode呢，这个时候就可以使用python-specific-encodings中的unicode_escapePython>>>s='\\u4e25'>>>s'\\\\u4e25'>>>us=s.decode('unicode_escape')>>>usu'\\u4e25'123456>>>s='\\u4e25'>>>s'\\\\u4e25'>>>us=s.decode('unicode_escape')>>>usu'\\u4e25'十六进制格式的字符串有时候，也会看到类似这样的str，’\\xd1\\xcf’，看起来也很熟悉，跟汉字“严”的gbk编码’xd1xcf’很像，区别在于前者多了一个‘’，这样就无法解释成一个十六进制了。解决办法是python-specific-encodings中的string_escapePython>>>s='\\\\xd1\\\\xcf'>>>s'\\\\xd1\\\\xcf'>>>prints\\xd1\\xcf>>>news=s.decode('string_escape')>>>news'\\xd1\\xcf'>>>printnews严12345678910>>>s='\\\\xd1\\\\xcf'>>>s'\\\\xd1\\\\xcf'>>>prints\\xd1\\xcf>>>news=s.decode('string_escape')>>>news'\\xd1\\xcf'>>>printnews严给读者的一个问题在这里留下一个问题：Pythonu'严'=='严'1u'严'=='严'返回值是True还是False呢？当然这里故意省去了上下文环境，不过明确的说，在不同的编码环境下，答案是不一样的，原因都在上文中！总结与建议不管怎么样解释，python2.x中的字符编码还是一件让人头疼的事情，即使搞懂了，之后遇到了也可能忘记。对于这个问题，诸多建议如下：第一：使用python3，就不用再纠结str于unicode了；但是这个很难开发者说了算；第二：不要使用中文，注释什么的都用英文；理想很丰满，现实很难，只是导致大量的拼音；第三：对于中文字符串，不要用str表示，而是用unicode表示；现实中也不好实施，大家都不愿意多写一个u第四：只在传输，或者持久化的时候对unicode进行encode，相反的过程时decode第五：对于网络接口，约定好编解码格式，强烈建议使用utf-8第六：看到UnicodeXXXError不要慌，如果XXX是Encode，那么一定是unicode转str的时候出了问题；如果是Decode，一定是str转unicode的时候出了问题。referencespythoncodecspython-specific-encodings字符编码笔记：ASCII，Unicode和UTF-8玩转Python让人讨厌的编码问题1赞8收藏3评论"], "art_create_time": ["2017/12/14"], "art_title": ["不想再被鄙视？那就看进来！ 一文搞懂Python2字符编码"], "art_url": ["http://python.jobbole.com/88967/"], "art_img": ["http://jbcdn2.b0.upaiyun.com/2017/11/8ef4df4888b257b5ea7bbd4b033a519c.png"]}